Improving Classification with Frozen DINOv3 Features

Using a frozen DINOv3 backbone (no fine-tuning) still allows many avenues to boost downstream classifier performance. Below, we explore advanced methods in three categories: Augmentation Pipelines, Embedding Separability & Alignment, and Long-Tail Handling. Each method is explained with actionable steps, PyTorch-style implementation hints, and citations to recent (2022–2025) research.

Augmentation Pipelines for Robust Features

Effective data augmentation can significantly improve generalization even when the feature extractor is fixed. The key is to expose the classifier to diverse yet label-consistent variations of the data. Modern strategies go beyond basic flips and crops:

Automated & Multi-Modal Augmentations: Leverage composite augmentation policies (e.g. AutoAugment, RandAugment) to apply geometric (rotation, scaling, flips) and color distortions in combinations. These “policy search” methods have been shown to systematically improve robustness
arxiv.org
. Additionally, using multi-view augmentations – generating multiple augmented views per image – can improve invariance. For example, a recent MVMA framework with multiple augmentation pipelines achieved +4.1% ImageNet top-1 accuracy over DINO by exposing the model to a broader range of appearances
link.springer.com
link.springer.com
. Implementation: define multiple torchvision.transforms pipelines and for each training image, sample one or more random augmentations to feed the classifier (you can treat each augmented view as a separate training sample with the same label, or enforce consistency between predictions).

Patch-Wise and Spatial Augmentations: Augmentations that alter image patches can help the classifier learn local invariances. CutMix and MixUp (mixing image patches or blending images+labels) are classic examples. Recent advances make these patch augmentations more structured. For instance, OcCaMix (2023) uses superpixel segmentation to cut and paste object-centric patches, preserving object contours and avoiding random background chops
sciencedirect.com
. It finds discriminative regions via attention and augments them at varied scales, yielding better classification accuracy than standard CutMix
sciencedirect.com
. Implementation: apply CutMix by replacing a random region of image A with a patch from image B (and adjust labels accordingly); to make this contour-aware, you could precompute superpixels (e.g. with skimage.segmentation) and swap whole superpixel regions instead of arbitrary boxes.

Spectral & Color Augmentations: Simple pixel-level “stylistic” transforms can be surprisingly effective on frozen features. A 2024 study on Frozen Feature Augmentation (FroFA) found that pointwise feature perturbations like random brightness, contrast, or per-channel scaling consistently improved few-shot classification, whereas spatial/geometric transforms of feature maps hurt performance
arxiv.org
. In other words, altering the spectral/color characteristics of inputs (or of the extracted features) can create beneficial variety without breaking the features’ semantics. Implementation: you can perform color jitter on the input (with torchvision.transforms.ColorJitter), or even augment the feature vectors themselves: e.g., add Gaussian noise or apply random channel-wise scale to the frozen feature tensor. For example:

feats = backbone(images)            # frozen DINOv3 features, shape [B, D]
if torch.rand(1).item() < 0.5:      # 50% chance apply feature augmentation
    # Random per-channel brightness offset
    noise = torch.randn_like(feats) * 0.05
    feats = feats + noise
    # Random contrast scaling (common factor across all channels)
    alpha = 0.8 + 0.4 * torch.rand(feats.size(0), 1).to(feats.device)  # in [0.8,1.2]
    feats = feats * alpha
logits = classifier(feats)


This simple feature-space augmentation (adding noise or scaling feature channels) can act like augmenting style without changing content
arxiv.org
.

Multi-View Training & Inference: With no latency constraints, you can exploit multi-crop or multi-view techniques at both training and test time. For example, generate several random crops or rotations of each image per iteration. Train the classifier on all views (possibly with a consistency loss to align their predictions). At inference, classify an image by averaging the classifier’s outputs on multiple augmented views of that image. A diffusion-based multi-view augmentation method (DIMVIDA, 2024) even used a generative model to synthesize new viewpoints of objects, yielding up to a 20% accuracy boost by averaging predictions across 4 views
eurecom.fr
eurecom.fr
. Implementation: for each input image, you could do:

views = [aug_transform(img) for _ in range(num_views)]
feat_views = [backbone(v) for v in views]
logits_views = [classifier(fv) for fv in feat_views]
mean_logits = sum(logits_views) / len(logits_views)


Train with each view as separate samples or use mean_logits for a consistency/ensemble loss. During evaluation, use the mean of outputs from multiple random crops or synthesized views as the final prediction.

Patch Transplantation for Rare Classes: For imbalanced data, a creative augmentation is to paste rare-class content into other images. The recent PANDA method (2025) uses a frozen CLIP model to find salient regions in minority-class images and transplants those patches into majority-class images
arxiv.org
. This effectively oversamples the rare classes by increasing their presence in varied contexts. You can emulate this by detecting important object regions (using, say, an attention map or an external model) in minority class samples and copying them onto random images of other classes, ensuring the label is the minority class. This patch-level oversampling balanced class frequencies and improved accuracy in PANDA
arxiv.org
.

In summary, diverse and structured augmentation – from advanced policy mixes to feature-level and multi-view techniques – can significantly enhance a frozen-feature classifier’s generalization. Importantly, ensure augmented samples remain label-consistent. Empirical studies show that careful augmentations (especially color/style changes or multi-view inputs) bolster performance on top of frozen ViT features
arxiv.org
link.springer.com
.

Embedding Separability & Alignment Techniques

With the backbone frozen, the goal is to train the classifier (and any added layers) to better separate classes in the given feature space. Techniques below enforce tighter class clusters and larger inter-class margins in the embedding or logits space:

Manifold Mixup (Feature Mixup): Mixup is a regularization that linearly combines two samples and their labels, producing an in-between example. Manifold mixup applies this in feature space (hidden manifold) rather than input pixels. For a frozen backbone, manifold mixup can be done by blending two extracted feature vectors. This encourages the classifier to learn linear decision boundaries and handle interpolations of classes. Implementation: after obtaining features f1, f2 = backbone(img1), backbone(img2), create a random mix:

lam = np.random.beta(alpha, alpha)    # e.g. alpha=2.0 for Beta distribution
f_mix = lam * f1 + (1 - lam) * f2
# Mix labels (for one-hot or soft targets)
y_mix = lam * y1_onehot + (1 - lam) * y2_onehot
logits = classifier(f_mix)
loss = soft_cross_entropy_loss(logits, y_mix)  # cross-entropy with soft labels


Training with such mixed feature-target pairs smooths the classifier’s output distribution
mdpi.com
. It expands the convex hull of each class in feature space, making the classifier more robust on unseen combinations. Recent long-tail research even introduced mixup-based oversampling (e.g. mixSMOTE) combining minority and majority features to enrich representation diversity
mdpi.com
mdpi.com
.

Supervised Contrastive Learning on Frozen Embeddings: Contrastive losses can be applied to encourage samples of the same class to cluster together in feature space while pushing apart different classes. With a frozen backbone, you can add a small projection head (e.g. an MLP) on top of the features and train it using a supervised contrastive loss alongside the classifier. For example, use the loss from Khosla et al. (SupCon) where each feature is pulled closer to others of the same class and separated from others
openaccess.thecvf.com
. To adapt for class imbalance, ensure each class is well-represented in contrastive pairs (you might sample batches to contain at least one example per class, or weight losses per class). Research on long-tailed contrastive learning shows that vanilla SupCon can be biased toward head classes, so balanced contrastive techniques introduce adjustments: e.g. class-balanced sampling of pairs and class averaging of negatives
arxiv.org
. The BCL framework (CVPR 2022) adds class-complement (ensuring all classes appear in each batch) and class-averaging in the contrastive loss to form a more uniform, “regular simplex” embedding geometry
arxiv.org
. In practice, you can implement SupCon in PyTorch by computing a cosine similarity matrix between normalized features and applying a contrastive loss formula (with temperature and mask for positives sharing the same label). Several open-source implementations of SupCon loss exist (e.g. in PyTorch Metric Learning library) that you can adapt to use precomputed DINOv3 features.

Metric Learning Heads (ArcFace and Variants): Instead of a standard linear classifier, use a normalized margin-based classifier to enforce larger inter-class distances. ArcFace (Additive Angular Margin) is a popular example: it L2-normalizes the feature vectors and classifier weight vectors, then adds a fixed angular margin to the target class’s logit before softmax
openaccess.thecvf.com
openaccess.thecvf.com
. This forces the classifier to not only separate classes, but to do so by a clear angular gap, yielding tightly clustered classes on the hypersphere. Implementation: Replace the final nn.Linear with a custom layer that does:

x_norm = F.normalize(features) and W_norm = F.normalize(weight).

Compute cosine logits: cosine = x_norm @ W_norm.T (shape [batch, num_classes]).

For each true label y_i, subtract an angular margin: cosine[y_i] = cos(θ_yi + m) as per ArcFace formula
openaccess.thecvf.com
.

Multiply all logits by a scale factor (e.g. s=30) before computing softmax loss.

For example, using PyTorch:

class ArcFaceHead(nn.Module):
    def __init__(self, in_features, num_classes, margin=0.5, scale=30):
        super().__init__()
        self.W = nn.Parameter(torch.randn(num_classes, in_features))
        nn.init.xavier_uniform_(self.W)
        self.m = margin
        self.s = scale
    def forward(self, features, labels=None):
        # Normalize features and weights
        x = F.normalize(features, p=2, dim=1)
        W = F.normalize(self.W, p=2, dim=1)
        cosine = F.linear(x, W)          # cosine similarity scores
        if labels is not None:
            # Add angular margin to target class logit
            target_cos = cosine[torch.arange(x.size(0)), labels]
            target_cos = target_cos.clamp(-1+1e-7, 1-1e-7)            # avoid numerical errors
            theta = torch.acos(target_cos)
            cosine_margin = torch.cos(theta + self.m)
            cosine[torch.arange(x.size(0)), labels] = cosine_margin
        return cosine * self.s


During training, use nn.CrossEntropyLoss on the output of this head (ensure to input true labels to apply margin). This encourages intra-class compactness and inter-class divergence
openaccess.thecvf.com
. Margin-based softmax has been widely successful in face recognition and was recently adopted to mitigate long-tail issues by equalizing classifier weight norms
mdpi.com
openaccess.thecvf.com
.

Learnable Feature Transformation Modules: Even without fine-tuning DINOv3, you can place a trainable module on top of its features to reshape or re-weight the representation for the task. Examples include a small MLP projector, a class-prototype alignment layer, or an attention-based pooling. In one approach, researchers added a Multi-Head Attention Pooling (MAP) layer on ViT frozen features and saw improvements over a simple linear head
arxiv.org
. You could implement a lightweight transformer encoder that takes the patch embeddings from DINOv3 and refines them (with gradients only in this module) before the final classifier. Alternatively, use a center loss or prototype layer: maintain a learnable centroid for each class in feature space and add a loss to pull features toward their class centroid (this can improve compactness, as seen with prototype-based contrastive enhancements
mdpi.com
). For instance, you can have a tensor prototypes = nn.Parameter(torch.randn(num_classes, feat_dim)) and train it such that each feature is close to its class prototype in L2 distance (center loss) while prototypes stay apart. Recent work combines prototypes with contrastive loss to enforce geometric class separation
mdpi.com
. The overall idea is to add capacity to the head so it can learn class-specific feature transformations or corrections that the frozen backbone can’t provide. Just be cautious to avoid overfitting (use dropout, regularization on the head as needed).

By applying these techniques, the classifier will produce embeddings or logits that are well-clustered per class and well-separated between classes. For example, margin losses explicitly enlarge the decision gap
openaccess.thecvf.com
, and contrastive objectives structure the feature space into a balanced simplex where each class is evenly represented
arxiv.org
. These methods are especially powerful when the backbone embeddings are high-quality (as DINOv3’s are
arxiv.org
) but not originally trained for your specific labels – they effectively align the frozen features to the new task without altering the backbone.

Long-Tail and Imbalanced Class Techniques

In imbalanced datasets, improving precision/recall for underrepresented classes is crucial. We assume a high overall sample count (so sampling noise is not an issue), but a heavy class frequency skew. Here are methods to handle long-tail scenarios on frozen features:

Loss Re-weighting (Cost-sensitive Learning): One straightforward approach is to give more importance to errors on minority classes. This can be done by weighting the loss for each sample by a factor inversely proportional to its class frequency. For example, if class A has 1000 samples and class B has 100, you might weight class B’s samples 10× more in the loss. PyTorch’s CrossEntropyLoss supports a weight tensor for classes, or you can implement it manually by multiplying the loss. A more advanced scheme is to use the effective number of samples (ENS) to compute weights
mdpi.com
 – this down-weights extremely frequent classes less aggressively than simple 1/n. Another popular variant is Focal Loss, which focuses on hard, misclassified examples (often the minority class ones) by down-weighting easy losses. Focal loss (γ > 0) will automatically attenuate the loss contribution of well-classified head-class examples, indirectly shifting focus to tail classes. In code, you can use torchvision.ops.sigmoid_focal_loss or implement focal loss for multi-class by modifying cross-entropy: FL = -(1 - p_t)^γ * log(p_t). Tuning the γ and class weights can yield a good balance of precision/recall. These re-weighting strategies are simple yet effective: for instance, methods like LDAM+DRW (Cao et al. 2019) and class-balanced focal loss showed strong improvements on long-tailed CIFAR
openaccess.thecvf.com
openaccess.thecvf.com
. Note: Over-weighting tail classes can sometimes hurt overall accuracy if overdone (head classes might regress), so schedule or calibrate the weights (e.g. gradually increase minority weight or use a sqrt of inverse frequency).

Logit Adjustment (Balanced Softmax): Instead of weighting the loss, adjust the logits before softmax using the prior class frequencies. The idea, rooted in a Bayesian view, is to add δ_y = log(p_y) to the logit of class y (where p_y is the training prior for class y)
openaccess.thecvf.com
. This simple offset encourages the classifier to predict minority classes more often, counteracting the prior. Menon et al. (ICLR 2021) showed that adding - log(\pi_y) to the logits (either during training, or just at inference) is provably effective for long-tail learning
openaccess.thecvf.com
openaccess.thecvf.com
. In practice, a Balanced Softmax loss implements this during training:

# Compute logits
logits = classifier(feats)  # raw scores [B, C]
# Compute adjustment once (e.g., precomputed log of class freq distribution)
log_prior = torch.log(class_freq / class_freq.sum()).to(device)  # shape [C]
# Subtract priors from logits
logits_adj = logits + (-log_prior)   # equivalent to dividing predicted probabilities by prior
loss = F.cross_entropy(logits_adj, labels)


This effectively rescales the decision threshold per class
openaccess.thecvf.com
. Balanced Softmax (Ren et al. 2020) and LADE (Menon et al. 2021) use such logit compensation and report better tail accuracy without requiring manual re-weight tuning
openaccess.thecvf.com
. Notably, you can also apply logit adjustment post-training (as a calibration step) if training from scratch with it is unstable – but typically with a frozen backbone and a flexible head, training with adjusted logits works well to directly learn a bias-corrected classifier
openaccess.thecvf.com
.

Feature Space Oversampling (Minority Feature Augmentation): Since the backbone features for minority classes may be sparse in the feature space, one idea is to generate synthetic feature vectors for the minority classes to augment training. Classic approaches like SMOTE can be applied on the frozen features: SMOTE linearly interpolates between a minority sample’s feature and one of its nearest neighbor’s features to create a new point
mdpi.com
. You can take all feature vectors of a rare class, pick random pairs, and interpolate to produce extra training features (with the same class label). These synthetic points can alleviate the classifier’s tendency to carve overly tight decision boundaries around the few minority points. Alternatively, use a conditional generator: for example, train a small GAN or VAE on the frozen features of minority classes to sample new feature vectors. With modern tools, even diffusion models could be employed to generate plausible feature variations. The key is that because DINOv3 features are high-level, even a simple interpolation can yield a meaningful variant. A recent study proposed Mixup-based oversampling where they blend head and tail features to enrich both ends of the distribution
mdpi.com
mdpi.com
. Their mixed mutual transfer (MMT) method sampled one feature from a head class and one from a tail class, interpolated them, and used that as an extra sample for training, thereby benefiting both classes’ representations
mdpi.com
. In code, this is just like the mixup snippet above, but ensuring one of the two comes from a tail class (or simply apply mixup across the whole batch in an imbalanced dataset, since tail examples will more often pair with head examples). When using feature-level oversampling, you may also want to add slight noise or jitter to avoid all synthetic points falling on the exact same lines between originals.

Balanced Contrastive or Prototype Learning: When dealing with many classes and imbalance, combining the above strategies with a contrastive objective can significantly boost minority class discrimination. Rebalancing Supervised Contrastive Learning (Reb-SupCon, 2023) introduced a gradient reweighting within the contrastive loss: it increases the contribution of tail-class pairs during training (using a dynamic factor that considers class frequency and gradient magnitude)
mdpi.com
. This helps prevent majority classes from dominating the representation learning. Additionally, they incorporate a prototype-aware module: for each class, a prototype (center) is learned, and an auxiliary loss encourages features to align with their class prototype and stay away from others
mdpi.com
. Together, this yields a more balanced feature space and improved decision boundaries for minority classes
mdpi.com
. Actionable tip: You can simulate a simpler balanced contrastive approach by ensuring each batch contains an equal number of samples per class (under-sampling the heads or oversampling the tails in batch composition) and then applying a supervised contrastive loss. Also, maintain and update class centroid vectors (prototypes) during training and use a loss term (e.g. mean squared error or cosine distance) to pull each sample’s feature toward its class centroid. These measures enforce that even with frozen features, the classifier’s intermediate representations/groupings become more uniform across classes. In evaluation on imbalanced benchmarks like ImageNet-LT, such balanced contrastive methods significantly improved tail class top-1 accuracy compared to plain cross-entropy
openaccess.thecvf.com
.

Logit and Score Calibration for Tail Classes: After training, you might still face a situation where the model’s confidence is skewed. In addition to logit adjustment, there are post-processing techniques like temperature scaling per class or threshold adjustment. One recent idea in PANDA (mentioned earlier) was to learn a small calibration network to adjust classifier thresholds using prior task distribution
arxiv.org
. For static datasets, a simpler approach is logit shifting: empirically find a small additive or multiplicative factor to apply to all logits of each class to meet a desired precision/recall trade-off. For example, if you want higher recall on a tail class, you can add a constant to that class’s logit at inference to boost its probability. This is a manual but effective tweak in some deployment scenarios.

Decoupling Representation and Classifier Training: A known trick from long-tail literature (Kang et al. 2020) is to train the feature extractor and classifier in two phases: first learn good features (possibly biasing toward head classes), then freeze features and re-train or fine-tune the classifier with a class-balanced sampling. In our case, the feature extractor is already fixed (pretrained DINOv3), but we can emulate the second phase: after initially training a classifier on the imbalanced data (to get a reasonable starting point), perform a second training pass for the classifier using a balanced sampler (e.g. each class appears equally in each batch, achieved via WeightedRandomSampler in PyTorch). Only the classifier’s weights update in this phase. This two-step training (sometimes called classifier re-training or “cRT”) often improves tail performance without hurting head classes, since it fine-tunes the decision boundary with equal emphasis on all classes. To implement: you can create a DataLoader with sampler=WeightedRandomSampler(weights, len(dataset)) where weights[i] = 1/(count[label_i]), and train the classifier for a few epochs more. Monitor the tail-class validation accuracy to avoid overfitting the few samples.

Finally, whichever techniques you use, remember to evaluate on balanced metrics (like class-wise recall, or balanced accuracy) to ensure improvements for underrepresented classes. Many of the above methods can be combined: e.g., one could train with ArcFace loss plus mixup, or use logit-adjusted contrastive loss – as evidenced by research that adds logit compensation into the contrastive framework
openaccess.thecvf.com
openaccess.thecvf.com
. With a high sample count, you have the freedom to experiment with heavy augmentation and re-balancing without overfitting. By applying these strategies, even with a fixed DINOv3 backbone, you can substantially close the gap between head and tail class performance and achieve strong overall accuracy.

INTERPRETATION AND NEXT STEPS
============================
Interpretation (from latest sweep results)
------------------------------------------
1) DINOv3 LVD backbones consistently outperform CLIP and DINOv3 SAT across macro_f1_fg.
2) Label smoothing (0.1) is a reliable lift across encoders.
3) Effective-number class weights are the largest single improvement; they lift both macro precision
   and recall for rare classes more than any single augmentation knob.
4) The "full" profile (effective + center/std + gelu_ln + hardmine + calibration) yields the best
   peak models, even if its average is slightly below effective-only on some subsets.
5) Calibration improves probability quality but not argmax; it should remain enabled where
   thresholds/margins are used downstream.

Practical implementation plan (prioritized, incremental)
--------------------------------------------------------
Phase 1: Long-tail decision boundary fixes (low risk, high impact)
1) Logit adjustment / Balanced Softmax
   - Implement optional logit prior adjustment for MLP and logreg heads.
   - Store priors in meta and allow toggling at inference for A/B.
   - UI: "Long-tail logit adjustment" toggle + tooltip; default OFF.
   - Benchmark: run A/B on DINOv3 L/16 + B/16 and CLIP L/14 with ls=0.1 + effective weights.

2) ArcFace-style margin head (classifier_type="arcface")
   - Add normalized margin head (m, s) on top of embeddings.
   - Keep CE loss; margin applied to target logit only.
   - UI: margin + scale controls with recommended defaults.
   - Benchmark: compare ArcFace vs CE for top 2 MLP sizes per backbone.

Phase 2: Representation separation (moderate complexity)
3) Supervised contrastive auxiliary loss (SupCon)
   - Add projection head + SupCon loss weighted alongside CE.
   - Balanced batch sampler by class; optional class-weighted SupCon term.
   - UI: toggle, temperature, loss weight.
   - Benchmark: A/B with and without SupCon for DINOv3 L/16 and CLIP L/14.

4) Feature-space augmentation + manifold mixup
   - Add Gaussian noise + channel scaling to embeddings.
   - Add feature-level mixup with soft labels (alpha configurable).
   - UI: "feature aug" + "manifold mixup" toggles with defaults.
   - Benchmark: pairwise with effective weights to isolate net gain.

Phase 3: Class compactness + rebalancing (higher effort)
5) Prototype / center loss (optional)
   - Trainable class centers with a center-loss weight.
   - Useful for tightening clusters on tail classes.

6) cRT (classifier re-training)
   - Add optional second phase: freeze encoder, retrain classifier with balanced sampler.
   - Parameters: epochs, sampler type.

Benchmark plan (must be comparable to current report)
-----------------------------------------------------
- Use the same split seed (42) + fixed 20% test groups.
- Use DINOv3 L/16 (LVD), DINOv3 B/16 (LVD), CLIP L/14.
- Use MLP sizes: [768,384] and [1024,512].
- Fix ls=0.1, class_weight=effective for baseline; then toggle one feature at a time.
- Report: macro_f1_fg, weighted_f1_fg, macro precision/recall, plus calibration error (ECE).
- Append to existing metrics files + add a delta table vs "full" profile.

Questions to resolve before implementation
-----------------------------------------
1) Should logit adjustment be applied only in training, or also optionally at inference?
2) Do we want ArcFace to replace CE in the head, or be an optional head type alongside CE?
3) Are we comfortable adding a projection head for SupCon (adds a small training cost)?
4) Do we need ECE calibration metrics in the benchmark table, or keep it qualitative?

TRADEOFFS FOR OPEN QUESTIONS
============================
1) Logit adjustment (training vs inference)
Training-time:
- Pros: Learns decision boundaries that already compensate for class priors. Helps tail recall without
  needing post-hoc tweaks; can be more stable when thresholds are used downstream.
- Cons: Hard to compare apples-to-apples with other reweighting (can overcompensate if combined with
  effective weights or heavy oversampling). If the training priors differ from deployment priors,
  the learned boundary may be miscalibrated.
Inference-time (post-hoc logit shift):
- Pros: No retraining; easy to A/B by toggling; can match deployment priors and be tuned per task.
- Cons: Does not fix representation or separability; can degrade head-class precision if priors are
  too aggressive. Needs careful accounting to avoid double-correcting if training already rebalanced.
Practical split: keep it optional at inference for quick tuning; if it becomes a default, avoid
simultaneously using heavy class-weighting unless we benchmark the combined effect.

2) ArcFace (optional head vs replace CE)
Replace CE:
- Pros: Stronger inter-class separation and tighter clusters; can improve tail-class precision.
- Cons: Sensitive to margin/scale; can reduce calibration quality and may hurt recall on noisy classes.
Optional head:
- Pros: Keeps CE as safe default; ArcFace can be enabled for advanced users or specific datasets.
- Cons: More surface area in UI and tests; need to ensure inference consumes the same logits scale.
Practical split: keep CE default; add ArcFace as an optional head with conservative defaults.

3) SupCon projection head (add or skip)
Add projection head:
- Pros: Better feature separation; improves tail-class discrimination when batches are balanced.
- Cons: Extra training time and memory; requires careful batch composition (small batch sizes hurt);
  another loss weight to tune. Adds complexity to the training pipeline.
Skip:
- Pros: Simpler, faster, fewer hyperparameters; less risk of destabilizing training.
- Cons: Leaves performance gains on the table for hard datasets with many fine-grained classes.
Practical split: add as optional feature; keep disabled by default and benchmark on large datasets.

4) ECE in benchmark table (quantitative vs qualitative)
Quantitative ECE:
- Pros: Direct measure of probability quality; critical when thresholds and margins are used at
  inference. Enables systematic comparison of calibration methods.
- Cons: Adds evaluation complexity and potential noise when sample sizes are small; requires
  standardized binning and logging.
Qualitative only:
- Pros: Lightweight; avoids noisy metrics for small datasets.
- Cons: Hard to compare calibration across models or runs; subjective.
Practical split: include ECE in benchmark tables when we run full sweeps; keep qualitative plots or
notes for quick smoke tests.
