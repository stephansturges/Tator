from __future__ import annotations

import base64, colorsys, copy, hashlib, io, zipfile, math, uuid, os, tempfile, shutil, time, logging, subprocess, sys, json, re, signal, random, gzip, csv, socket, gc, queue, multiprocessing
from array import array
from contextvars import ContextVar
from pathlib import Path
import numpy as np
import yaml
from typing import Optional, List, Dict, Tuple, Any, Literal, Sequence, Mapping, Callable, Set
from collections import deque, Counter
import torch, clip, joblib, tiktoken
from io import BytesIO
from PIL import Image, ImageDraw, ImageFont
from fastapi import FastAPI, UploadFile, File, Form, Query, Body, HTTPException, Request
from fastapi.responses import StreamingResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, root_validator, Field
from omegaconf import OmegaConf
import psutil
try:
    from packaging import version as packaging_version
except Exception:  # noqa: BLE001
    packaging_version = None
try:
    from transformers import LogitsProcessor, LogitsProcessorList
except Exception:  # noqa: BLE001
    LogitsProcessor = None
    LogitsProcessorList = None
_BASE_LOGITS_PROCESSOR = LogitsProcessor if LogitsProcessor is not None else object
from starlette.background import BackgroundTask
from starlette.status import (
    HTTP_400_BAD_REQUEST,
    HTTP_403_FORBIDDEN,
    HTTP_412_PRECONDITION_FAILED,
    HTTP_413_REQUEST_ENTITY_TOO_LARGE,
    HTTP_415_UNSUPPORTED_MEDIA_TYPE,
    HTTP_404_NOT_FOUND,
    HTTP_422_UNPROCESSABLE_ENTITY,
    HTTP_428_PRECONDITION_REQUIRED,
    HTTP_409_CONFLICT,
    HTTP_500_INTERNAL_SERVER_ERROR,
    HTTP_503_SERVICE_UNAVAILABLE,
)
from collections import OrderedDict
try:
    from scipy.spatial import ConvexHull
except Exception:  # noqa: BLE001
    ConvexHull = None
from segment_anything import sam_model_registry, SamPredictor


def _message_text_for_tool_parse(message: Any) -> str:
    content = getattr(message, "content", None)
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        parts: List[str] = []
        for item in content:
            try:
                item_type, item_value = item.get_type_and_value()
            except Exception:
                continue
            if item_type == "text" and item_value:
                parts.append(str(item_value))
        return "\n".join(parts)
    return ""


def _extract_balanced_json(text: str, start_char: str, end_char: str) -> Optional[str]:
    start = text.find(start_char)
    if start < 0:
        return None
    depth = 0
    for idx in range(start, len(text)):
        ch = text[idx]
        if ch == start_char:
            depth += 1
        elif ch == end_char:
            depth -= 1
            if depth == 0:
                return text[start : idx + 1]
    return None


def _parse_tool_call_payload(payload: str) -> Optional[Dict[str, Any]]:
    candidate = (payload or "").strip()
    if not candidate:
        return None
    for parser in ("json", "json5"):
        try:
            if parser == "json":
                return json.loads(candidate)
            import json5  # type: ignore
            return json5.loads(candidate)
        except Exception:
            continue
    for start_char, end_char in (("{", "}"), ("[", "]")):
        snippet = _extract_balanced_json(candidate, start_char, end_char)
        if not snippet:
            continue
        for parser in ("json", "json5"):
            try:
                if parser == "json":
                    return json.loads(snippet)
                import json5  # type: ignore
                return json5.loads(snippet)
            except Exception:
                continue
    return None


def _extract_tool_call_from_text(text: str) -> Optional[Tuple[str, Any]]:
    if not text:
        return None
    lower_text = text.lower()
    if "<tool_call>" in lower_text:
        start = lower_text.find("<tool_call>")
        end = lower_text.find("</tool_call>", start + 11)
        if end > start:
            payload = text[start + len("<tool_call>") : end].strip()
            parsed = _parse_tool_call_payload(payload)
            if isinstance(parsed, dict):
                name = str(parsed.get("name") or "").strip()
                args = parsed.get("arguments", {})
                if name:
                    return name, args
    if "✿function✿" in lower_text and "✿args✿" in lower_text:
        fn_idx = text.find("✿FUNCTION✿")
        args_idx = text.find("✿ARGS✿", fn_idx + 1)
        if fn_idx >= 0 and args_idx > fn_idx:
            name_chunk = text[fn_idx + len("✿FUNCTION✿") : args_idx]
            if ":" in name_chunk:
                name_chunk = name_chunk.split(":", 1)[1]
            name = name_chunk.strip().splitlines()[0].strip()
            args_chunk = text[args_idx + len("✿ARGS✿") :]
            stop_tokens = ["✿RESULT✿", "✿RETURN✿"]
            stop_pos = [args_chunk.find(tok) for tok in stop_tokens if tok in args_chunk]
            if stop_pos:
                args_chunk = args_chunk[: min(stop_pos)]
            parsed = _parse_tool_call_payload(args_chunk)
            if name:
                return name, parsed if parsed is not None else args_chunk.strip()
    parsed = _parse_tool_call_payload(text)
    if isinstance(parsed, dict):
        name = str(parsed.get("name") or "").strip()
        if name:
            return name, parsed.get("arguments", {})
    return None
import threading
import queue
import itertools
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field, asdict

# Ensure we import the bundled SAM3 package (sam3/sam3) rather than shadowing it
# with the repo root folder name (sam3/). Without this, sam3 becomes a namespace
# that lacks the train.data modules needed for text prompting.
SAM3_SRC_ROOT = (Path(__file__).resolve().parent / "sam3").resolve()
if SAM3_SRC_ROOT.exists():
    sys.path.insert(0, str(SAM3_SRC_ROOT))

from tools.clip_training import train_clip_from_yolo, TrainingError, TrainingArtifacts
try:
    from tools.qwen_training import (
        QwenTrainingConfig,
        QwenTrainingResult,
        train_qwen_model,
        TrainingError as QwenTrainingError,
        DEFAULT_SYSTEM_PROMPT,
    )
except Exception as exc:  # noqa: BLE001
    QWEN_TRAINING_IMPORT_ERROR = exc
    QwenTrainingConfig = None  # type: ignore[assignment]
    QwenTrainingResult = None  # type: ignore[assignment]
    train_qwen_model = None  # type: ignore[assignment]
    QwenTrainingError = TrainingError  # type: ignore[assignment]
    DEFAULT_SYSTEM_PROMPT = (
        "You are an annotation assistant that only returns JSON objects shaped like {\"detections\":[{\"label\":\"class\","
        "\"bbox\":[x1,y1,x2,y2]} or {\"label\":\"class\",\"point\":[x,y]}]}"
    )
else:
    QWEN_TRAINING_IMPORT_ERROR = None

try:
    from transformers import (
        AutoProcessor,
        AutoConfig,
        AutoModelForCausalLM,
        Qwen3VLForConditionalGeneration,
        Qwen3VLMoeForConditionalGeneration,
    )
    from qwen_vl_utils import process_vision_info
except Exception as exc:  # noqa: BLE001
    QWEN_IMPORT_ERROR = exc
    Qwen3VLForConditionalGeneration = None  # type: ignore[assignment]
    Qwen3VLMoeForConditionalGeneration = None  # type: ignore[assignment]
    AutoConfig = None  # type: ignore[assignment]
    AutoModelForCausalLM = None  # type: ignore[assignment]
    AutoProcessor = None  # type: ignore[assignment]
    process_vision_info = None  # type: ignore[assignment]
else:
    QWEN_IMPORT_ERROR = None

def _try_import_qwen_agent() -> Tuple[Optional[Any], Optional[Any], Optional[Any], Optional[Any], Optional[Exception]]:
    try:
        from qwen_agent.agents import FnCallAgent as _QwenFnCallAgent  # type: ignore
        from qwen_agent.llm.schema import Message as QwenAgentMessage, ContentItem as QwenAgentContentItem  # type: ignore
        import qwen_agent.settings as qwen_settings  # type: ignore
        from qwen_agent_llm import LocalQwenVLChatModel  # type: ignore
        from qwen_agent_tools import build_local_agent_tools  # type: ignore
        return _QwenFnCallAgent, QwenAgentMessage, QwenAgentContentItem, (LocalQwenVLChatModel, build_local_agent_tools), None
    except Exception as exc:  # noqa: BLE001
        return None, None, None, None, exc


QwenAgentAssistant, QwenAgentMessage, QwenAgentContentItem, _agent_payload, QWEN_AGENT_IMPORT_ERROR = _try_import_qwen_agent()
if QWEN_AGENT_IMPORT_ERROR is not None:
    local_repo = (Path(__file__).resolve().parent / "Qwen-Agent").resolve()
    if local_repo.exists():
        sys.path.insert(0, str(local_repo))
        QwenAgentAssistant, QwenAgentMessage, QwenAgentContentItem, _agent_payload, QWEN_AGENT_IMPORT_ERROR = _try_import_qwen_agent()
if _agent_payload is not None:
    LocalQwenVLChatModel, build_local_agent_tools = _agent_payload
else:
    LocalQwenVLChatModel = None  # type: ignore[assignment]
    build_local_agent_tools = None  # type: ignore[assignment]

BASE64_IMAGE_MAX_BYTES = int(os.environ.get("IMAGE_MAX_BYTES", str(100 * 1024 * 1024)))
BASE64_IMAGE_MAX_DIM = int(os.environ.get("IMAGE_MAX_DIM", "4096"))

try:
    from sam3.model_builder import build_sam3_image_model
    from sam3.model.sam3_image_processor import Sam3Processor as Sam3ImageProcessor
except Exception as exc:  # noqa: BLE001
    SAM3_NATIVE_IMAGE_IMPORT_ERROR = exc
    build_sam3_image_model = None  # type: ignore[assignment]
    Sam3ImageProcessor = None  # type: ignore[assignment]
else:
    SAM3_NATIVE_IMAGE_IMPORT_ERROR = None

try:
    from peft import PeftModel
except Exception as exc:  # noqa: BLE001
    PEFT_IMPORT_ERROR = exc
    PeftModel = None  # type: ignore[assignment]
else:
    PEFT_IMPORT_ERROR = None


def _env_int(name: str, default: int) -> int:
    raw = os.environ.get(name)
    if not raw:
        return default
    try:
        return int(raw)
    except (TypeError, ValueError):
        return default


def _env_bool(name: str, default: bool) -> bool:
    raw = os.environ.get(name)
    if raw is None:
        return default
    raw = raw.strip().lower()
    if raw in {"1", "true", "yes", "on"}:
        return True
    if raw in {"0", "false", "no", "off"}:
        return False
    return default


def _env_float(name: str, default: float) -> float:
    raw = os.environ.get(name)
    if not raw:
        return default
    try:
        return float(raw)
    except (TypeError, ValueError):
        return default


MAX_PREDICTOR_SLOTS = 3
DATASET_ZIP_MAX_BYTES = _env_int("DATASET_ZIP_MAX_BYTES", 100 * 1024 * 1024 * 1024)
DATASET_ZIP_ENTRY_MAX_BYTES = _env_int("DATASET_ZIP_ENTRY_MAX_BYTES", 50 * 1024 * 1024 * 1024)
CLIP_DATASET_CHUNK_MAX_BYTES = _env_int("CLIP_DATASET_CHUNK_MAX_BYTES", 10 * 1024 * 1024 * 1024)
CLIP_DATASET_UPLOAD_QUOTA_BYTES = _env_int("CLIP_DATASET_UPLOAD_QUOTA_BYTES", 100 * 1024 * 1024 * 1024)
QWEN_DATASET_CHUNK_MAX_BYTES = _env_int("QWEN_DATASET_CHUNK_MAX_BYTES", 10 * 1024 * 1024 * 1024)
QWEN_DATASET_UPLOAD_QUOTA_BYTES = _env_int("QWEN_DATASET_UPLOAD_QUOTA_BYTES", 100 * 1024 * 1024 * 1024)
ASSET_MAX_BYTES = _env_int("ASSET_MAX_BYTES", 10 * 1024 * 1024 * 1024)
ASSET_UPLOAD_QUOTA_BYTES = _env_int("ASSET_UPLOAD_QUOTA_BYTES", 100 * 1024 * 1024 * 1024)
CLASSIFIER_ALLOWED_EXTS = {".pkl", ".joblib"}
LABELMAP_ALLOWED_EXTS = {".txt", ".pkl"}

SAM_PRELOAD_MAX_BYTES = _env_int("SAM_PRELOAD_MAX_BYTES", 2 * 1024 * 1024 * 1024)
MAX_RESPONSE_DETECTIONS = _env_int("MAX_RESPONSE_DETECTIONS", 5000)
MAX_RESPONSE_MASKS = _env_int("MAX_RESPONSE_MASKS", 2000)
MASK_ENCODE_MAX_BYTES = _env_int("MASK_ENCODE_MAX_BYTES", 64 * 1024 * 1024)
AGENT_MINING_CACHE_MAX_BYTES = _env_int("AGENT_MINING_CACHE_MAX_BYTES", 80 * 1024 * 1024 * 1024)
AGENT_MINING_CACHE_TTL_HOURS = _env_int("AGENT_MINING_CACHE_TTL_HOURS", 0)  # 0 = no TTL purge by default
AGENT_RECIPE_MAX_CROPS = _env_int("AGENT_RECIPE_MAX_CROPS", 1000)
AGENT_RECIPE_MAX_CROP_BYTES = _env_int("AGENT_RECIPE_MAX_CROP_BYTES", 512 * 1024 * 1024)
AGENT_RECIPE_MAX_CLIP_HEAD_BYTES = _env_int("AGENT_RECIPE_MAX_CLIP_HEAD_BYTES", 256 * 1024 * 1024)
AGENT_RECIPE_MAX_JSON_BYTES = _env_int("AGENT_RECIPE_MAX_JSON_BYTES", 10 * 1024 * 1024)
AGENT_RECIPE_MAX_BYTES = _env_int("AGENT_RECIPE_MAX_BYTES", 2 * 1024 * 1024 * 1024)
AGENT_CASCADE_MAX_JSON_BYTES = _env_int("AGENT_CASCADE_MAX_JSON_BYTES", 10 * 1024 * 1024)
AGENT_CASCADE_MAX_BYTES = _env_int("AGENT_CASCADE_MAX_BYTES", 8 * 1024 * 1024 * 1024)
CLIP_TRAIN_UPLOAD_MAX_BYTES = _env_int("CLIP_TRAIN_UPLOAD_MAX_BYTES", 10 * 1024 * 1024 * 1024)
CLIP_TRAIN_UPLOAD_QUOTA_BYTES = _env_int("CLIP_TRAIN_UPLOAD_QUOTA_BYTES", 100 * 1024 * 1024 * 1024)

QWEN_MODEL_NAME = os.environ.get("QWEN_MODEL_NAME", "Qwen/Qwen3-VL-4B-Instruct")
QWEN_MIN_TRANSFORMERS = "4.57.0"
QWEN_MIN_PIXELS = _env_int("QWEN_MIN_PIXELS", 256 * 28 * 28)
QWEN_MAX_PIXELS = _env_int("QWEN_MAX_PIXELS", 1280 * 28 * 28)
QWEN_MAX_NEW_TOKENS = _env_int("QWEN_MAX_NEW_TOKENS", 2000)
QWEN_DO_SAMPLE = _env_bool("QWEN_DO_SAMPLE", False)
QWEN_TEMPERATURE = _env_float("QWEN_TEMPERATURE", 0.2)
QWEN_TOP_P = _env_float("QWEN_TOP_P", 0.9)
QWEN_DEVICE_PREF = os.environ.get("QWEN_DEVICE", "auto").strip().lower()
QWEN_TRUST_REMOTE_CODE = _env_bool("QWEN_TRUST_REMOTE_CODE", False)
QWEN_CAPTION_CACHE_LIMIT = _env_int("QWEN_CAPTION_CACHE_LIMIT", 0)
QWEN_WINDOW_DEFAULT_SIZE = _env_int("QWEN_WINDOW_SIZE", 672)
QWEN_WINDOW_DEFAULT_OVERLAP = _env_float("QWEN_WINDOW_OVERLAP", 0.2)

# Rough VRAM estimates (GB) for Qwen3 training defaults (batch=1, default pixel budget).
# These are approximate and should be treated as guidance, not a hard guarantee.
QWEN_VRAM_ESTIMATE_GB = {
    "official_lora": {
        "2B": 12.0,
        "4B": 20.0,
        "8B": 96.0,
        "32B": 192.0,
    },
    "trl_qlora": {
        "2B": 8.0,
        "4B": 10.0,
        "8B": 16.0,
        "32B": 48.0,
    },
}
QWEN_VRAM_THINKING_SCALE = 1.08
QWEN_VRAM_PIXEL_BASE = 451584
QWEN_VRAM_PIXEL_SCALE_MIN = 0.6
QWEN_VRAM_PIXEL_SCALE_MAX = 1.6


def _resolve_qwen_max_seq_len(model: Any) -> Optional[int]:
    config = getattr(model, "config", None)
    if config is None:
        return None

    def _read_seq_len(cfg: Any) -> Optional[int]:
        if cfg is None:
            return None
        for attr in ("max_position_embeddings", "max_sequence_length", "seq_length"):
            val = getattr(cfg, attr, None)
            if isinstance(val, int) and val > 0:
                return val
        # Some configs expose max_length as a generation hint (often tiny); treat as fallback only.
        val = getattr(cfg, "max_length", None)
        if isinstance(val, int) and val > 0:
            return val
        return None

    for cfg in (getattr(config, "text_config", None), getattr(config, "language_config", None), config):
        val = _read_seq_len(cfg)
        if isinstance(val, int) and val >= 256:
            return val
    return None


def _qwen_estimate_vision_tokens(preview_inputs: Any) -> Optional[int]:
    grid = None
    if isinstance(preview_inputs, dict):
        grid = preview_inputs.get("image_grid_thw")
    else:
        grid = getattr(preview_inputs, "image_grid_thw", None)
    if grid is None:
        return None
    try:
        grid_vals = grid if isinstance(grid, torch.Tensor) else torch.as_tensor(grid)
        if grid_vals.ndim == 3:
            grid_vals = grid_vals[0]
        if grid_vals.ndim == 2 and grid_vals.shape[-1] == 3:
            tokens = (grid_vals[:, 0] * grid_vals[:, 1] * grid_vals[:, 2]).sum()
            return int(tokens.item())
        if grid_vals.ndim == 1 and grid_vals.numel() == 3:
            tokens = grid_vals[0] * grid_vals[1] * grid_vals[2]
            return int(tokens.item())
    except Exception:
        return None
    return None


def _qwen_effective_input_len(preview_inputs: Any, input_len: int, num_images: int) -> Tuple[int, Optional[int]]:
    vision_tokens = _qwen_estimate_vision_tokens(preview_inputs)
    if vision_tokens is None or num_images <= 0:
        return input_len, vision_tokens
    effective_len = max(1, input_len - num_images + vision_tokens)
    return effective_len, vision_tokens


def _qwen_supports_presence_penalty(model: Any) -> bool:
    gen_config = getattr(model, "generation_config", None)
    if gen_config is None:
        return False
    if hasattr(gen_config, "to_dict"):
        try:
            return "presence_penalty" in gen_config.to_dict()
        except Exception:
            pass
    return hasattr(gen_config, "presence_penalty")


class ThinkingEffortProcessor(_BASE_LOGITS_PROCESSOR):
    """Scale the </think> token logit to reduce or increase chain-of-thought length."""

    def __init__(self, end_thinking_token_id: int, thinking_effort: float = 1.0, scale_factor: float = 2.0):
        super().__init__()
        self.end_thinking_token_id = int(end_thinking_token_id)
        self.thinking_effort = float(thinking_effort)
        self.scale_factor = float(scale_factor)
        self.finished_sequences: Set[int] = set()

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        if self.end_thinking_token_id >= scores.size(1):
            return scores
        scale = self.scale_factor ** (1.0 - self.thinking_effort)
        batch_size = input_ids.size(0)
        for i in range(batch_size):
            if i in self.finished_sequences:
                continue
            if (input_ids[i] == self.end_thinking_token_id).any():
                self.finished_sequences.add(i)
                continue
            scores[i, self.end_thinking_token_id] *= scale
        return scores


class ImmediateActionBiasProcessor(_BASE_LOGITS_PROCESSOR):
    """Boost </think> when 'wait' appears inside a think block after a minimum threshold."""

    def __init__(
        self,
        tokenizer: Any,
        end_thinking_token_id: int,
        *,
        min_think_chars: int = 200,
        min_think_seconds: float = 2.0,
        logit_bias: float = 6.0,
    ):
        super().__init__()
        self._tokenizer = tokenizer
        self.end_thinking_token_id = int(end_thinking_token_id)
        self.min_think_chars = max(1, int(min_think_chars))
        self.min_think_seconds = max(0.0, float(min_think_seconds))
        self.logit_bias = float(logit_bias)
        self._think_started_at: Dict[int, float] = {}
        self._wait_seen: Set[int] = set()

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        if self.logit_bias <= 0:
            return scores
        if self.end_thinking_token_id >= scores.size(1):
            return scores
        now = time.time()
        batch_size = input_ids.size(0)
        for i in range(batch_size):
            if i in self._wait_seen:
                scores[i, self.end_thinking_token_id] += self.logit_bias
                continue
            try:
                text = self._tokenizer.decode(input_ids[i].tolist(), skip_special_tokens=False)
            except Exception:
                continue
            think_text = self._extract_open_think_text(text)
            if think_text is None:
                if i in self._think_started_at:
                    del self._think_started_at[i]
                continue
            if i not in self._think_started_at:
                self._think_started_at[i] = now
            if len(think_text) < self.min_think_chars:
                continue
            if (now - self._think_started_at.get(i, now)) < self.min_think_seconds:
                continue
            if re.search(r"\bwait\b", think_text, flags=re.IGNORECASE):
                self._wait_seen.add(i)
                scores[i, self.end_thinking_token_id] += self.logit_bias
        return scores


def _qwen_find_end_think_token_id(tokenizer: Any) -> Optional[int]:
    if tokenizer is None:
        return None
    vocab_size = getattr(tokenizer, "vocab_size", None)
    vocab_size = int(vocab_size) if isinstance(vocab_size, int) and vocab_size > 0 else None
    candidates = [
        "</think>",
        "<|endofthink|>",
        "<|end_of_thought|>",
        "<|end_of_thinking|>",
    ]
    unk_id = getattr(tokenizer, "unk_token_id", None)
    for token in candidates:
        try:
            tok_id = tokenizer.convert_tokens_to_ids(token)
        except Exception:
            tok_id = None
        if tok_id is not None and tok_id != unk_id:
            if vocab_size is not None and int(tok_id) >= vocab_size:
                tok_id = None
            else:
                return int(tok_id)
        try:
            ids = tokenizer.encode(token, add_special_tokens=False)
        except Exception:
            ids = []
        if isinstance(ids, list) and len(ids) == 1:
            tok_id = int(ids[0])
            if vocab_size is None or tok_id < vocab_size:
                return tok_id
    return None


def _qwen_build_thinking_effort_processor(
    tokenizer: Any,
    thinking_effort: Optional[float],
    scale_factor: Optional[float],
) -> Optional[ThinkingEffortProcessor]:
    if LogitsProcessorList is None or LogitsProcessor is None:
        return None
    if thinking_effort is None:
        return None
    try:
        effort_val = float(thinking_effort)
    except (TypeError, ValueError):
        return None
    end_token_id = _qwen_find_end_think_token_id(tokenizer)
    if end_token_id is None:
        return None
    scale_val = 2.0
    if scale_factor is not None:
        try:
            scale_val = float(scale_factor)
        except (TypeError, ValueError):
            scale_val = 2.0
    return ThinkingEffortProcessor(end_token_id, thinking_effort=effort_val, scale_factor=scale_val)


def _qwen_build_immediate_action_processor(
    tokenizer: Any,
    immediate_action_bias: Optional[bool],
    min_think_chars: Optional[int],
    min_think_seconds: Optional[float],
    logit_bias: Optional[float],
) -> Optional[ImmediateActionBiasProcessor]:
    if LogitsProcessorList is None or LogitsProcessor is None:
        return None
    if not immediate_action_bias:
        return None
    end_token_id = _qwen_find_end_think_token_id(tokenizer)
    if end_token_id is None:
        return None
    chars_val = 200 if min_think_chars is None else int(min_think_chars)
    secs_val = 2.0 if min_think_seconds is None else float(min_think_seconds)
    bias_val = 6.0 if logit_bias is None else float(logit_bias)
    return ImmediateActionBiasProcessor(
        tokenizer,
        end_token_id,
        min_think_chars=chars_val,
        min_think_seconds=secs_val,
        logit_bias=bias_val,
    )


def _qwen_append_logits_processor(
    gen_kwargs: Dict[str, Any],
    processor: Optional[_BASE_LOGITS_PROCESSOR],
) -> None:
    if processor is None:
        return
    processors = gen_kwargs.get("logits_processor")
    if processors is None:
        gen_kwargs["logits_processor"] = LogitsProcessorList([processor])
    elif isinstance(processors, LogitsProcessorList):
        processors.append(processor)
    else:
        try:
            gen_kwargs["logits_processor"] = LogitsProcessorList(list(processors) + [processor])
        except Exception:
            gen_kwargs["logits_processor"] = LogitsProcessorList([processor])

def _is_qwen_moe_model_id(model_id: str) -> bool:
    lowered = model_id.lower()
    return "a3b" in lowered or "moe" in lowered


def _infer_qwen_model_size(model_id: str) -> Optional[str]:
    for size in ("2B", "4B", "8B", "32B"):
        if size in model_id:
            return size
    return None


def _estimate_qwen_vram_mb(
    model_id: str,
    training_mode: str,
    *,
    max_pixels: Optional[int] = None,
    batch_size: Optional[int] = None,
) -> Tuple[Optional[float], Optional[str]]:
    size = _infer_qwen_model_size(model_id)
    if not size:
        return None, None
    mode = training_mode if training_mode in QWEN_VRAM_ESTIMATE_GB else "official_lora"
    base_gb = QWEN_VRAM_ESTIMATE_GB.get(mode, {}).get(size)
    if base_gb is None:
        return None, None
    scale = 1.0
    if "Thinking" in model_id:
        scale *= QWEN_VRAM_THINKING_SCALE
    if max_pixels:
        try:
            pixel_scale = max_pixels / max(1, QWEN_VRAM_PIXEL_BASE)
        except Exception:
            pixel_scale = 1.0
        pixel_scale = min(QWEN_VRAM_PIXEL_SCALE_MAX, max(QWEN_VRAM_PIXEL_SCALE_MIN, pixel_scale))
        scale *= pixel_scale
    if batch_size and batch_size > 1:
        scale *= float(batch_size)
    estimate_mb = base_gb * 1024.0 * scale
    note = None
    if mode == "official_lora" and size in {"8B", "32B"}:
        note = "Official LoRA is very VRAM-hungry; 8B/32B often exceed 48GB even at smaller pixel budgets."
    if mode == "trl_qlora" and size == "32B" and "Thinking" in model_id:
        note = "32B Thinking QLoRA is experimental; some setups hit device-map gradient issues."
    return estimate_mb, note

qwen_model = None
qwen_processor = None
qwen_device: Optional[str] = None
qwen_last_error: Optional[str] = None
qwen_lock = threading.RLock()
qwen_config_lock = threading.RLock()
qwen_caption_cache: Dict[str, Tuple[Any, Any]] = {}
qwen_caption_order: deque[str] = deque()
_HF_OFFLINE_AUTO_ENABLED = False
_CAPTION_WINDOW_HOOK: ContextVar[Optional[Callable[[int, int, int, str], None]]] = ContextVar(
    "caption_window_hook",
    default=None,
)

QWEN_METADATA_FILENAME = "metadata.json"


def _default_qwen_metadata() -> Dict[str, Any]:
    return {
        "id": "default",
        "label": "Base Qwen 3",
        "system_prompt": DEFAULT_SYSTEM_PROMPT,
        "dataset_context": "",
        "classes": [],
        "model_id": QWEN_MODEL_NAME,
        "model_family": "qwen3",
        "source": "huggingface",
        "min_pixels": QWEN_MIN_PIXELS,
        "max_pixels": QWEN_MAX_PIXELS,
    }


def _enable_hf_offline_defaults() -> None:
    global _HF_OFFLINE_AUTO_ENABLED
    if _HF_OFFLINE_AUTO_ENABLED:
        return
    if not os.environ.get("HF_HUB_OFFLINE"):
        os.environ["HF_HUB_OFFLINE"] = "1"
    if not os.environ.get("TRANSFORMERS_OFFLINE"):
        os.environ["TRANSFORMERS_OFFLINE"] = "1"
    _HF_OFFLINE_AUTO_ENABLED = True
    logger.info("[qwen] HF offline mode enabled after initial download")


def _hf_offline_enabled() -> bool:
    return os.environ.get("HF_HUB_OFFLINE") == "1" or os.environ.get("TRANSFORMERS_OFFLINE") == "1"


def _emit_caption_window(x0: int, y0: int, size: int, caption: str) -> None:
    hook = _CAPTION_WINDOW_HOOK.get()
    if hook is None:
        return
    try:
        hook(int(x0), int(y0), int(size), str(caption))
    except Exception:
        return


def _set_hf_offline(enabled: bool) -> None:
    if enabled:
        os.environ["HF_HUB_OFFLINE"] = "1"
        os.environ["TRANSFORMERS_OFFLINE"] = "1"
    else:
        os.environ["HF_HUB_OFFLINE"] = "0"
        os.environ["TRANSFORMERS_OFFLINE"] = "0"


active_qwen_model_id = "default"
active_qwen_model_path: Optional[Path] = None
active_qwen_metadata: Dict[str, Any] = _default_qwen_metadata()
loaded_qwen_model_id: Optional[str] = None


def _reset_qwen_runtime() -> None:
    global qwen_model, qwen_processor, qwen_last_error, loaded_qwen_model_id, qwen_device
    qwen_model = None
    qwen_processor = None
    qwen_device = None
    loaded_qwen_model_id = None
    qwen_last_error = None
    if torch.cuda.is_available():
        try:
            torch.cuda.empty_cache()
        except Exception:  # noqa: BLE001
            pass


def _unload_sam3_text_runtime() -> None:
    """Release SAM3 text prompt model to free device memory."""
    global sam3_text_model, sam3_text_processor, sam3_text_device
    with sam3_text_lock:
        try:
            del sam3_text_model
        except Exception:
            pass
        try:
            del sam3_text_processor
        except Exception:
            pass
        sam3_text_model = None
        sam3_text_processor = None
        sam3_text_device = None


def _unload_dinov3_backbone() -> None:
    """Release DINOv3 encoder + per-device caches."""
    global dinov3_model, dinov3_processor, dinov3_model_name, dinov3_initialized, dinov3_model_device
    with dinov3_lock:
        try:
            del dinov3_model
        except Exception:
            pass
        try:
            del dinov3_processor
        except Exception:
            pass
        dinov3_model = None
        dinov3_processor = None
        dinov3_model_name = None
        dinov3_model_device = None
        dinov3_initialized = False
    try:
        with _agent_dinov3_backbones_lock:
            _agent_dinov3_backbones.clear()
            _agent_dinov3_locks.clear()
    except Exception:
        pass


def _unload_detector_inference() -> None:
    """Release detector inference models (YOLO/RF-DETR) to free GPU memory."""
    global yolo_infer_model, yolo_infer_path, yolo_infer_labelmap, yolo_infer_task
    global rfdetr_infer_model, rfdetr_infer_path, rfdetr_infer_labelmap, rfdetr_infer_task, rfdetr_infer_variant
    try:
        del yolo_infer_model
    except Exception:
        pass
    yolo_infer_model = None
    yolo_infer_path = None
    yolo_infer_labelmap = []
    yolo_infer_task = None
    try:
        del rfdetr_infer_model
    except Exception:
        pass
    rfdetr_infer_model = None
    rfdetr_infer_path = None
    rfdetr_infer_labelmap = []
    rfdetr_infer_task = None
    rfdetr_infer_variant = None


def _unload_non_qwen_runtimes() -> None:
    """Free heavy inference runtimes except Qwen (SAM, detectors, classifier backbones)."""
    try:
        predictor_manager.unload_all()
    except Exception as exc:  # noqa: BLE001
        logger.warning("Failed to unload SAM predictors: %s", exc)
    _unload_sam3_text_runtime()
    _suspend_clip_backbone()
    _unload_dinov3_backbone()
    _unload_detector_inference()
    if torch.cuda.is_available():
        try:
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
        except Exception:
            pass


def _unload_inference_runtimes() -> None:
    """Free heavy inference runtimes (SAM, detectors, Qwen, classifier backbones)."""
    _unload_non_qwen_runtimes()
    _unload_qwen_runtime()
    if torch.cuda.is_available():
        try:
            device_count = torch.cuda.device_count()
            if device_count > 1:
                current = torch.cuda.current_device()
                for idx in range(device_count):
                    try:
                        torch.cuda.set_device(idx)
                        torch.cuda.empty_cache()
                        torch.cuda.ipc_collect()
                    except Exception:
                        continue
                try:
                    torch.cuda.set_device(current)
                except Exception:
                    pass
            else:
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
        except Exception:  # noqa: BLE001
            pass


def _prepare_for_training() -> None:
    """Free heavy inference runtimes before starting a training job."""
    _unload_inference_runtimes()


def _finalize_training_environment() -> None:
    _resume_classifier_backbone()
    if torch.cuda.is_available():
        try:
            torch.cuda.empty_cache()
        except Exception:  # noqa: BLE001
            pass


sam3_text_model = None
sam3_text_processor = None
sam3_text_device: Optional[torch.device] = None
sam3_text_lock = threading.RLock()


def _set_active_qwen_model_default() -> None:
    global active_qwen_model_id, active_qwen_model_path, active_qwen_metadata
    active_qwen_model_id = "default"
    active_qwen_model_path = None
    active_qwen_metadata = _default_qwen_metadata()
    _reset_qwen_runtime()


def _set_active_qwen_model_custom(model_id: str, ckpt_path: Path, metadata: Dict[str, Any]) -> None:
    global active_qwen_model_id, active_qwen_model_path, active_qwen_metadata
    active_qwen_model_id = model_id
    active_qwen_model_path = ckpt_path
    active_qwen_metadata = metadata or {}
    active_qwen_metadata.setdefault("id", model_id)
    _reset_qwen_runtime()


def _prepare_for_qwen_training() -> None:
    _prepare_for_training()


def _finalize_qwen_training_environment() -> None:
    _finalize_training_environment()


def _bytes_to_mb(value: int) -> float:
    return round(value / (1024 * 1024), 2)

# ----------------------------------------------------------------
# 1) Define a global error message and a global load-flag for CLIP
ERROR_MESSAGE = 0 # messy hack, making this an int because of the way we parse it later... the message has actually just been moved to the JS and appears when bbox uuid is None
clip_initialized = True
clip_last_error: Optional[str] = None
# ----------------------------------------------------------------

# 2) Attempt to load the logistic regression model (.pkl)
MODEL_PATH = "./my_logreg_model.pkl"
clf = None
if os.path.exists(MODEL_PATH):
    try:
        print("Loading logistic regression...")
        clf = joblib.load(MODEL_PATH)
        clip_last_error = None
    except Exception as e:
        print(f"Failed to load logistic regression model: {e}")
        clip_initialized = False
        clip_last_error = str(e)
else:
    print(f"File {MODEL_PATH} not found.")
    clip_initialized = False
    clip_last_error = "classifier_not_found"

LABELMAP_DEFAULT_PATH = "./my_label_list.pkl"
active_classifier_path: Optional[str] = MODEL_PATH if clf is not None else None
active_labelmap_path: Optional[str] = LABELMAP_DEFAULT_PATH if os.path.exists(LABELMAP_DEFAULT_PATH) else None
active_label_list: List[str] = []
active_encoder_type: str = "clip"
active_encoder_model: Optional[str] = None
active_classifier_meta: Dict[str, Any] = {}
active_head_normalize_embeddings: bool = True
active_classifier_head: Optional[Dict[str, Any]] = None
if active_labelmap_path:
    try:
        if active_labelmap_path.lower().endswith(".pkl"):
            loaded = joblib.load(active_labelmap_path)
            if isinstance(loaded, list):
                active_label_list = [str(item) for item in loaded]
            else:
                active_labelmap_path = None
        else:
            with open(active_labelmap_path, "r", encoding="utf-8") as handle:
                active_label_list = [line.strip() for line in handle if line.strip()]
    except Exception as exc:  # noqa: BLE001
        print(f"Failed to load labelmap {active_labelmap_path}: {exc}")
        active_labelmap_path = None
        active_label_list = []

try:
    if active_classifier_path and os.path.isfile(active_classifier_path):
        meta_path = os.path.splitext(active_classifier_path)[0] + ".meta.pkl"
        if os.path.exists(meta_path):
            meta_obj = joblib.load(meta_path)
            if isinstance(meta_obj, dict):
                active_classifier_meta = dict(meta_obj)
                active_encoder_type = meta_obj.get("encoder_type") or "clip"
                active_encoder_model = meta_obj.get("encoder_model") or meta_obj.get("clip_model")
                active_head_normalize_embeddings = _resolve_active_head_normalize_embeddings(meta_obj, clf, default=True)
except Exception:
    active_encoder_type = "clip"
    active_encoder_model = None
    active_classifier_head = None

# Keep default CLIP artifacts usable with path allowlists by mirroring them into uploads/.
# This preserves older workflows that write my_logreg_model.pkl/my_label_list.pkl at repo root.
try:
    _uploads_root_early = Path("uploads")
    _uploads_root_early.mkdir(exist_ok=True)
    _classifiers_root_early = (_uploads_root_early / "classifiers").resolve()
    _labelmaps_root_early = (_uploads_root_early / "labelmaps").resolve()
    _classifiers_root_early.mkdir(parents=True, exist_ok=True)
    _labelmaps_root_early.mkdir(parents=True, exist_ok=True)

    if active_classifier_path and os.path.isfile(active_classifier_path):
        src = Path(active_classifier_path).resolve()
        if not str(src).startswith(str(_classifiers_root_early)):
            dst = _classifiers_root_early / src.name
            try:
                if not dst.exists() or dst.stat().st_mtime < src.stat().st_mtime or dst.stat().st_size != src.stat().st_size:
                    shutil.copy2(src, dst)
                active_classifier_path = str(dst)
            except Exception:
                pass

    if active_labelmap_path and os.path.isfile(active_labelmap_path):
        src = Path(active_labelmap_path).resolve()
        if not str(src).startswith(str(_labelmaps_root_early)):
            dst = _labelmaps_root_early / src.name
            try:
                if not dst.exists() or dst.stat().st_mtime < src.stat().st_mtime or dst.stat().st_size != src.stat().st_size:
                    shutil.copy2(src, dst)
                active_labelmap_path = str(dst)
            except Exception:
                pass
except Exception:
    pass

# 3) Attempt to load the CLIP model (only when the active classifier uses CLIP)
device = "cuda" if torch.cuda.is_available() else "cpu"
if torch.cuda.is_available():
    try:
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
    except Exception as exc:  # noqa: BLE001
        logger.warning("Failed to enable TF32: %s", exc)
SUPPORTED_CLIP_MODELS = [
    "ViT-L/14",
    "ViT-B/16",
    "ViT-B/32",
]
DEFAULT_CLIP_MODEL = SUPPORTED_CLIP_MODELS[0]

def _infer_clip_model_from_embedding_dim(embedding_dim: Optional[int], *, active_name: Optional[str] = None) -> Optional[str]:
    try:
        emb = int(embedding_dim or 0)
    except Exception:
        return None
    if emb == 768:
        return "ViT-L/14"
    if emb == 512:
        active = str(active_name or "").strip()
        if active in {"ViT-B/32", "ViT-B/16"}:
            return active
        return "ViT-B/32"
    return None

clip_model = None
clip_preprocess = None
clip_model_name: Optional[str] = None
_clip_reload_needed = False
_skip_clip_load = os.environ.get("TATOR_SKIP_CLIP_LOAD", "").strip() == "1"
if _skip_clip_load:
    print("Skipping CLIP model load (TATOR_SKIP_CLIP_LOAD=1).")
    clip_initialized = False
    clip_model_name = None
else:
    encoder_type_norm = str(active_encoder_type or "clip").strip().lower()
    if encoder_type_norm != "clip" or not active_classifier_path:
        print(f"Skipping CLIP model load (active encoder={active_encoder_type}).")
        clip_initialized = False
        clip_model_name = None
    else:
        try:
            clip_name = active_encoder_model or DEFAULT_CLIP_MODEL
            print(f"Loading CLIP model ({clip_name})...")
            clip_model, clip_preprocess = clip.load(clip_name, device=device)
            clip_model_name = clip_name
        except Exception as e:
            print(f"Failed to load CLIP model: {e}")
            clip_initialized = False
            clip_model_name = None

clip_lock = threading.Lock()
if clip_model is None or clf is None:
    clip_initialized = False

# Agent Mining often evaluates across multiple GPUs. The global CLIP backbone is pinned to
# `device` (typically "cuda" == GPU0) and guarded by `clip_lock`. To avoid serializing all CLIP
# embedding work onto a single GPU during mining/eval, we maintain per-device CLIP backbones
# (raw CLIP, not trained heads) with per-device locks.
_agent_clip_backbones: Dict[Tuple[str, str], Tuple[Any, Any]] = {}
_agent_clip_locks: Dict[Tuple[str, str], threading.Lock] = {}
_agent_clip_backbones_lock = threading.Lock()

# Optional DINOv3 image encoder (frozen) for classifier heads.
dinov3_model: Optional[Any] = None
dinov3_processor: Optional[Any] = None
dinov3_model_name: Optional[str] = None
dinov3_model_device: Optional[str] = None
dinov3_initialized = False
dinov3_cuda_disabled = False
dinov3_lock = threading.Lock()
_agent_dinov3_backbones: Dict[Tuple[str, str], Tuple[Any, Any]] = {}
_agent_dinov3_locks: Dict[Tuple[str, str], threading.Lock] = {}
_agent_dinov3_backbones_lock = threading.Lock()


def _dinov3_resolve_device(requested: str) -> str:
    normalized = str(requested or "").strip() or "cpu"
    if dinov3_cuda_disabled and normalized.startswith("cuda"):
        return "cpu"
    return normalized


def _suspend_clip_backbone() -> None:
    global clip_model, clip_preprocess, clip_initialized, _clip_reload_needed
    with clip_lock:
        if clip_model is None:
            return
        if str(active_encoder_type or "clip").strip().lower() == "clip":
            logger.info("Suspending CLIP backbone to free GPU memory for training.")
        else:
            logger.debug("Suspending CLIP backbone (inactive classifier) to free GPU memory for training.")
        clip_model = None
        clip_preprocess = None
        clip_initialized = False
        _clip_reload_needed = True
    # Also clear any per-device mining CLIP backbones to free VRAM.
    try:
        with _agent_clip_backbones_lock:
            _agent_clip_backbones.clear()
            _agent_clip_locks.clear()
    except Exception:
        pass


def _resume_clip_backbone() -> None:
    global clip_model, clip_preprocess, clip_initialized, _clip_reload_needed
    if not _clip_reload_needed:
        return
    with clip_lock:
        if clip_model is not None:
            _clip_reload_needed = False
            clip_initialized = True
            return
        clip_name = clip_model_name or DEFAULT_CLIP_MODEL
        try:
            clip_model, clip_preprocess = clip.load(clip_name, device=device)
            clip_initialized = bool(clf is not None and clip_model is not None)
            logger.info("Reloaded CLIP backbone %s after training.", clip_name)
        except Exception as exc:  # noqa: BLE001
            clip_model = None
            clip_preprocess = None
            clip_initialized = False
            logger.warning("Failed to reload CLIP backbone %s: %s", clip_name, exc)
        finally:
            _clip_reload_needed = False


def _resume_classifier_backbone() -> None:
    """Reload the active encoder backbone after training, based on user-selected classifier."""
    global dinov3_model, dinov3_processor, dinov3_model_name, dinov3_initialized, dinov3_model_device
    global clip_model_name, _clip_reload_needed
    encoder_type = str(active_encoder_type or "clip").strip().lower()
    if encoder_type == "dinov3":
        model_name = str(active_encoder_model or "").strip()
        if not model_name:
            dinov3_initialized = False
            return
        target_device = _dinov3_resolve_device(device)
        with dinov3_lock:
            if dinov3_model is not None and dinov3_processor is not None and dinov3_model_name == model_name:
                if dinov3_cuda_disabled and not dinov3_model_device:
                    pass
                elif dinov3_model_device and dinov3_model_device != target_device:
                    pass
                else:
                    dinov3_initialized = True
                    return
            model, processor = _load_dinov3_backbone(model_name, target_device)
            if model is None or processor is None:
                dinov3_model = None
                dinov3_processor = None
                dinov3_model_name = None
                dinov3_model_device = None
                dinov3_initialized = False
                return
            dinov3_model = model
            dinov3_processor = processor
            dinov3_model_name = model_name
            dinov3_model_device = target_device
            dinov3_initialized = True
        return
    if encoder_type != "clip":
        _clip_reload_needed = False
        return
    if active_encoder_model:
        clip_model_name = active_encoder_model
    _resume_clip_backbone()


def _ensure_clip_backbone_for_mining() -> Tuple[Optional[Any], Optional[Any]]:
    """Ensure a CLIP backbone is available for exemplar embedding/fp guard (raw CLIP, no classifier required)."""
    global clip_model, clip_preprocess, clip_model_name, clip_initialized
    if clip is None:
        return None, None
    with clip_lock:
        if clip_model is None or clip_preprocess is None:
            clip_name = clip_model_name or DEFAULT_CLIP_MODEL
            try:
                clip_model, clip_preprocess = clip.load(clip_name, device=device)
                clip_model_name = clip_name
                clip_initialized = bool(clip_model is not None)
            except Exception as exc:  # noqa: BLE001
                logger.warning("Agent mining could not load CLIP backbone: %s", exc)
                clip_model = None
                clip_preprocess = None
                clip_initialized = False
                return None, None
    return clip_model, clip_preprocess

# 4) Load the SAM model (segment-anything) as normal:
MODEL_TYPE = os.environ.get("SAM_MODEL_TYPE", "vit_h")
CHECKPOINT_PATH = os.environ.get("SAM_CHECKPOINT_PATH", "./sam_vit_h_4b8939.pth")
SAM3_MODEL_ID = os.environ.get("SAM3_MODEL_ID", "facebook/sam3")
SAM3_PROCESSOR_ID = os.environ.get("SAM3_PROCESSOR_ID", SAM3_MODEL_ID)
SAM3_CHECKPOINT_PATH = os.environ.get("SAM3_CHECKPOINT_PATH")
SAM3_DEVICE_PREF = os.environ.get("SAM3_DEVICE", "auto").strip().lower()
active_sam3_model_id = "default"
active_sam3_checkpoint = SAM3_CHECKPOINT_PATH
active_sam3_enable_segmentation = True
active_sam3_metadata: Dict[str, Any] = {
    "id": "default",
    "label": "Base SAM3",
    "checkpoint": SAM3_CHECKPOINT_PATH,
    "source": "env",
    "enable_segmentation": True,
}


def _resolve_sam3_device() -> torch.device:
    if SAM3_DEVICE_PREF in {"", "auto"}:
        return torch.device("cuda" if torch.cuda.is_available() else "cpu")
    try:
        return torch.device(SAM3_DEVICE_PREF)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"invalid_sam3_device:{SAM3_DEVICE_PREF}:{exc}") from exc


def _resolve_sam3_mining_devices() -> List[torch.device]:
    """
    Resolve the list of devices to use for agent mining. If SAM3_DEVICE specifies an explicit device
    (or comma-separated list), honor it; otherwise fan out across all available CUDA devices, falling
    back to CPU when needed.
    """
    devices: List[torch.device] = []
    if SAM3_DEVICE_PREF not in {"", "auto"}:
        for part in SAM3_DEVICE_PREF.split(","):
            name = part.strip()
            if not name:
                continue
            try:
                devices.append(torch.device(name))
            except Exception:
                logger.warning("Invalid SAM3 device in SAM3_DEVICE=%s", name)
    if not devices and torch.cuda.is_available():
        try:
            for idx in range(torch.cuda.device_count()):
                devices.append(torch.device(f"cuda:{idx}"))
        except Exception as exc:  # noqa: BLE001
            logger.warning("Failed to enumerate CUDA devices for mining: %s", exc)
            devices = []
    if not devices:
        devices = [torch.device("cpu")]
    return devices


def _require_sam3_for_prepass(enable_text: bool, enable_similarity: bool) -> None:
    if not (enable_text or enable_similarity):
        return
    if (
        SAM3_NATIVE_IMAGE_IMPORT_ERROR is not None
        or build_sam3_image_model is None
        or Sam3ImageProcessor is None
    ):
        detail = f"sam3_unavailable:{SAM3_NATIVE_IMAGE_IMPORT_ERROR}"
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=detail)


def _reset_sam3_runtime() -> None:
    global sam3_text_model, sam3_text_processor, sam3_text_device
    sam3_text_model = None
    sam3_text_processor = None
    sam3_text_device = None
    try:
        predictor_manager.unload_all()
    except Exception:
        pass
    if torch.cuda.is_available():
        try:
            torch.cuda.empty_cache()
        except Exception:  # noqa: BLE001
            pass


def _resolve_sam1_devices() -> List[torch.device]:
    devices: List[torch.device] = []
    if torch.cuda.is_available():
        try:
            for idx in range(torch.cuda.device_count()):
                devices.append(torch.device(f"cuda:{idx}"))
        except Exception as exc:  # noqa: BLE001
            logger.warning("Failed to enumerate CUDA devices for SAM1: %s", exc)
            devices = []
    if not devices:
        devices = [torch.device("cpu")]
    return devices


class _Sam1Backend:
    def __init__(self):
        self.predictor = SamPredictor(sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH))

    def set_image(self, np_img: np.ndarray) -> None:
        self.predictor.set_image(np_img)

    def predict(self, **kwargs):
        return self.predictor.predict(**kwargs)

    def unload(self) -> None:
        try:
            del self.predictor
        except Exception:  # noqa: BLE001
            pass
        self.predictor = None


class _Sam3Backend:
    def __init__(self):
        if SAM3_NATIVE_IMAGE_IMPORT_ERROR is not None or build_sam3_image_model is None:
            raise RuntimeError(f"sam3_unavailable:{SAM3_NATIVE_IMAGE_IMPORT_ERROR}")
        self.device = _resolve_sam3_device()
        device_str = "cuda" if self.device.type == "cuda" else "cpu"
        source = active_sam3_metadata.get("source") if isinstance(active_sam3_metadata, dict) else None
        try:
            model = build_sam3_image_model(
                device=device_str,
                checkpoint_path=active_sam3_checkpoint,
                load_from_HF=active_sam3_checkpoint is None,
                enable_inst_interactivity=True,
                enable_segmentation=active_sam3_enable_segmentation,
                bpe_path=str(SAM3_BPE_PATH),
            )
            if self.device:
                model = model.to(self.device)
            predictor = getattr(model, "inst_interactive_predictor", None)
            if predictor is None:
                raise RuntimeError("sam3_interactive_predictor_missing")
            tracker = getattr(predictor, "model", None)
            if tracker is None:
                raise RuntimeError("sam3_tracker_missing")
            if getattr(tracker, "backbone", None) is None:
                tracker.backbone = model.backbone
        except Exception as exc:  # noqa: BLE001
            raise RuntimeError(f"sam3_load_failed:{exc}") from exc
        self.model = model
        self.predictor = predictor

    def set_image(self, np_img: np.ndarray) -> None:
        arr = np.ascontiguousarray(np_img)
        self.predictor.set_image(arr)

    def predict(self, **kwargs):
        point_coords = kwargs.get("point_coords")
        point_labels = kwargs.get("point_labels")
        box = kwargs.get("box")
        mask_input = kwargs.get("mask_input")
        multimask_output = kwargs.get("multimask_output", True)
        return self.predictor.predict(
            point_coords=point_coords,
            point_labels=point_labels,
            box=box,
            mask_input=mask_input,
            multimask_output=multimask_output,
        )

    def unload(self) -> None:
        try:
            del self.predictor
        except Exception:  # noqa: BLE001
            pass
        try:
            del self.model
        except Exception:  # noqa: BLE001
            pass
        self.predictor = None
        self.model = None
        if torch.cuda.is_available():
            try:
                torch.cuda.empty_cache()
            except Exception:  # noqa: BLE001
                pass


def _build_backend_for_variant(variant: str):
    normalized = (variant or "sam1").lower()
    if normalized == "sam3":
        return _Sam3Backend()
    # default to classic SAM1 backend
    return _Sam1Backend()


def _sam3_clear_device_pinned_caches(model: Any) -> None:
    """
    SAM3 upstream precomputes some internal caches on `cuda` (i.e. cuda:0) during module
    construction to help torch.compile. Those caches are stored in plain Python containers and
    are NOT moved by `model.to(cuda:N)`, which can break multi-GPU inference (device mismatch).

    We don't rely on torch.compile in this server path, so it's safe to clear these caches after
    moving the model to its target device.
    """
    if model is None:
        return
    try:
        modules = model.modules()
    except Exception:
        return
    for m in modules:
        try:
            cls_name = getattr(m, "__class__", type(m)).__name__
            cls_mod = getattr(getattr(m, "__class__", type(m)), "__module__", "")
        except Exception:
            cls_name = ""
            cls_mod = ""
        # Position encoding cache: dict of tensors keyed by shape (upstream).
        if cls_name == "PositionEmbeddingSine" or str(cls_mod).endswith("position_encoding"):
            try:
                cache = getattr(m, "cache", None)
                if isinstance(cache, dict):
                    cache.clear()
            except Exception:
                pass
        # Decoder cache: precomputed coord cache tuple (upstream).
        if hasattr(m, "compilable_cord_cache"):
            try:
                setattr(m, "compilable_cord_cache", None)
            except Exception:
                pass
        if hasattr(m, "coord_cache"):
            try:
                cache = getattr(m, "coord_cache", None)
                if isinstance(cache, dict):
                    cache.clear()
            except Exception:
                pass


def _ensure_sam3_text_runtime():
    global sam3_text_model, sam3_text_processor, sam3_text_device
    with sam3_text_lock:
        if sam3_text_model is not None and sam3_text_processor is not None and sam3_text_device is not None:
            return sam3_text_model, sam3_text_processor, sam3_text_device
        device = _resolve_sam3_device()
        if SAM3_NATIVE_IMAGE_IMPORT_ERROR is not None or build_sam3_image_model is None or Sam3ImageProcessor is None:
            detail = f"sam3_text_unavailable:{SAM3_NATIVE_IMAGE_IMPORT_ERROR}"
            raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=detail)
        try:
            # Preserve explicit CUDA indices (e.g. "cuda:1") so the processor's internal tensors
            # and transforms land on the same device as the model.
            device_str = str(device) if device.type == "cuda" else "cpu"
            # Force segmentation head on for text prompting so pred_masks and related keys exist.
            enable_seg = True
            if active_sam3_checkpoint:
                model = build_sam3_image_model(
                    checkpoint_path=active_sam3_checkpoint,
                    device=device_str,
                    load_from_HF=False,
                    enable_segmentation=enable_seg,
                    bpe_path=str(SAM3_BPE_PATH),
                ).to(device)
            else:
                model = build_sam3_image_model(
                    device=device_str,
                    enable_segmentation=enable_seg,
                    bpe_path=str(SAM3_BPE_PATH),
                ).to(device)
            _sam3_clear_device_pinned_caches(model)
            processor = Sam3ImageProcessor(model, device=device_str)
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"sam3_text_load_failed:{exc}") from exc
        sam3_text_model = model
        sam3_text_processor = processor
        sam3_text_device = device
        return sam3_text_model, sam3_text_processor, sam3_text_device


class PredictorSlot:
    def __init__(self, name: str):
        self.name = name
        self.backends: Dict[str, Any] = {}
        self.token: Optional[str] = None
        self.variant: Optional[str] = None
        self.image_shape: Optional[Tuple[int, int, int]] = None
        self.image_name: Optional[str] = None
        self.last_loaded: float = 0.0
        self.lock = threading.RLock()
        self._busy = threading.Event()
        self.image_memory_bytes: int = 0

    def set_image(self, np_img: np.ndarray, token: Optional[str], variant: Optional[str], image_name: Optional[str]) -> None:
        variant_name = (variant or "sam1").lower()
        with self.lock:
            self._busy.set()
            try:
                backend = self._ensure_backend(variant_name)
                backend.set_image(np_img)
                self.token = token
                self.variant = variant_name
                self.image_shape = np_img.shape
                self.image_name = image_name
                self.last_loaded = time.time()
                self.image_memory_bytes = int(np_img.nbytes)
            finally:
                self._busy.clear()

    def predict(self, **kwargs):
        with self.lock:
            self._busy.set()
            try:
                backend = self._ensure_backend((self.variant or "sam1").lower())
                return backend.predict(**kwargs)
            finally:
                self._busy.clear()

    def is_busy(self) -> bool:
        return self._busy.is_set()

    def clear(self) -> None:
        with self.lock:
            self.token = None
            self.variant = None
            self.image_shape = None
            self.image_name = None
            self.last_loaded = 0.0
            self.image_memory_bytes = 0

    def unload(self) -> None:
        with self.lock:
            self.clear()
            for backend in self.backends.values():
                try:
                    backend.unload()
                except Exception:  # noqa: BLE001
                    pass
            self.backends.clear()

    def _ensure_backend(self, variant: str):
        backend = self.backends.get(variant)
        if backend is None:
            backend = _build_backend_for_variant(variant)
            self.backends[variant] = backend
        return backend


class PredictorManager:
    def __init__(self):
        self.slots: Dict[str, PredictorSlot] = {
            "current": PredictorSlot("current"),
            "next": PredictorSlot("next"),
            "previous": PredictorSlot("previous"),
        }
        self.slot_order: List[str] = ["current", "next", "previous"]
        self.capacity_lock = threading.RLock()
        self.capacity: int = min(MAX_PREDICTOR_SLOTS, len(self.slot_order))
        self.enabled_slots: set[str] = set(self.slot_order[: self.capacity])
        self.token_index: Dict[Tuple[str, str], PredictorSlot] = {}
        self.image_index: Dict[Tuple[str, str], PredictorSlot] = {}
        self.queue: "queue.Queue[Tuple[str, Dict[str, Any]]]" = queue.Queue()
        self.stop_event = threading.Event()
        self.worker = threading.Thread(target=self._worker, name="predictor-preload-worker", daemon=True)
        self.worker.start()

    def _slot_key(self, token: Optional[str], variant: Optional[str]) -> Optional[Tuple[str, str]]:
        if not token or not variant:
            return None
        return (token, variant)

    def _image_key(self, image_name: Optional[str], variant: Optional[str]) -> Optional[Tuple[str, str]]:
        if not image_name or not variant:
            return None
        return (variant, image_name)

    def is_slot_enabled(self, slot_name: str) -> bool:
        return slot_name in self.enabled_slots

    def resolve_slot(self, slot_name: Optional[str], *, allow_disabled_fallback: bool = True) -> str:
        """Return a normalised slot name.

        When ``allow_disabled_fallback`` is False we fail fast if the requested
        slot is currently disabled instead of silently falling back to the
        "current" slot. This prevents background preloads from clobbering the
        user's active predictor when the capacity shrinks.
        """

        candidate = (slot_name or "current").lower()
        if candidate not in self.slots:
            return "current"
        if self.is_slot_enabled(candidate):
            return candidate
        if allow_disabled_fallback:
            return "current"
        raise ValueError(f"slot_disabled:{candidate}")

    def capacity_limits(self) -> Tuple[int, int]:
        return (1, min(MAX_PREDICTOR_SLOTS, len(self.slot_order)))

    def get_capacity(self) -> int:
        with self.capacity_lock:
            return self.capacity

    def set_capacity(self, capacity: int) -> None:
        minimum, maximum = self.capacity_limits()
        normalized = max(minimum, min(maximum, capacity))
        with self.capacity_lock:
            if normalized == self.capacity:
                return
            self.capacity = normalized
            new_enabled = set(self.slot_order[: normalized])
            disabled = self.enabled_slots - new_enabled
            self.enabled_slots = new_enabled
            for slot_name in disabled:
                slot = self.slots.get(slot_name)
                if slot:
                    self._clear_slot_refs(slot)
                    slot.clear()

    def active_slot_count(self) -> int:
        return len(self.enabled_slots)

    def loaded_slot_count(self) -> int:
        return sum(1 for name, slot in self.slots.items() if name in self.enabled_slots and slot.token)

    def total_image_memory_bytes(self) -> int:
        return sum(slot.image_memory_bytes for name, slot in self.slots.items() if name in self.enabled_slots)

    def _clear_slot_refs(self, slot: PredictorSlot) -> None:
        remove_keys = [key for key, value in self.token_index.items() if value is slot]
        for key in remove_keys:
            self.token_index.pop(key, None)
        remove_image_keys = [key for key, value in self.image_index.items() if value is slot]
        for key in remove_image_keys:
            self.image_index.pop(key, None)

    def unload_all(self) -> None:
        with self.capacity_lock:
            for slot in self.slots.values():
                self._clear_slot_refs(slot)
                slot.unload()

    def set_slot(self, slot_name: str, np_img: np.ndarray, token: Optional[str], variant: Optional[str], image_name: Optional[str]) -> None:
        slot_name = self.resolve_slot(slot_name, allow_disabled_fallback=False)
        slot = self.slots[slot_name]
        self._clear_slot_refs(slot)
        slot.set_image(np_img, token, variant, image_name)
        key = self._slot_key(token, variant)
        if key:
            self.token_index[key] = slot
        image_key = self._image_key(image_name, variant)
        if image_key:
            self.image_index[image_key] = slot

    def ensure_current(self, np_img: np.ndarray, token: Optional[str], variant: Optional[str], image_name: Optional[str]) -> PredictorSlot:
        slot = self.token_index.get(self._slot_key(token, variant)) if token and variant else None
        if slot and slot.name == "current":
            return slot
        self.set_slot("current", np_img, token, variant, image_name)
        return self.slots["current"]

    def get_slot_for_token(self, token: Optional[str], variant: Optional[str]) -> Optional[PredictorSlot]:
        key = self._slot_key(token, variant)
        if key is None:
            return None
        return self.token_index.get(key)

    def get_slot_for_image(self, image_name: Optional[str], variant: Optional[str]) -> Optional[PredictorSlot]:
        key = self._image_key(image_name, variant)
        if key is None:
            return None
        return self.image_index.get(key)

    def promote_slot(self, slot_name: str) -> bool:
        if slot_name not in self.slots or slot_name == "current" or not self.is_slot_enabled(slot_name):
            return False
        if slot_name == "next":
            prev_slot = self.slots["previous"]
            curr_slot = self.slots["current"]
            next_slot = self.slots["next"]
            self.slots["previous"] = curr_slot
            self.slots["current"] = next_slot
            self.slots["next"] = prev_slot
        elif slot_name == "previous":
            prev_slot = self.slots["previous"]
            curr_slot = self.slots["current"]
            next_slot = self.slots["next"]
            self.slots["next"] = curr_slot
            self.slots["current"] = prev_slot
            self.slots["previous"] = next_slot
        else:
            return False
        self.slots["previous"].name = "previous"
        self.slots["current"].name = "current"
        self.slots["next"].name = "next"
        return True

    def predict(self, np_img: np.ndarray, token: Optional[str], variant: Optional[str], image_name: Optional[str], **predict_kwargs):
        slot = self.get_slot_for_token(token, variant)
        if slot is None:
            slot = self.ensure_current(np_img, token, variant, image_name)
        return slot.predict(**predict_kwargs)

    def set_slot_with_wait(self, slot_name: str, np_img: np.ndarray, token: Optional[str], variant: Optional[str], image_name: Optional[str]) -> None:
        slot_name = self.resolve_slot(slot_name, allow_disabled_fallback=False)
        if slot_name != "current":
            waited = 0.0
            # Give the "current" slot a brief head start so the active image always begins loading first,
            # but do not block background slots for the full duration of set_image.
            while (
                not self.stop_event.is_set()
                and waited < 0.2
                and not self.slots["current"].is_busy()
                and not self.slots["current"].token
            ):
                time.sleep(0.01)
                waited += 0.01
        self.set_slot(slot_name, np_img, token, variant, image_name)

    def stop(self) -> None:
        self.stop_event.set()
        self.worker.join(timeout=1.0)

    def schedule_slot(self, slot_name: str, payload: Dict[str, Any]) -> None:
        self.queue.put((slot_name, payload))

    def status(self) -> List[Dict[str, Any]]:
        info = []
        for name, slot in self.slots.items():
            entry: Dict[str, Any] = {
                "slot": name,
                "token": slot.token,
                "variant": slot.variant,
                "image_name": slot.image_name,
                "last_loaded": slot.last_loaded,
                "busy": slot.is_busy(),
                "enabled": self.is_slot_enabled(name),
                "memory_bytes": slot.image_memory_bytes,
            }
            if slot.image_shape:
                entry["height"] = slot.image_shape[0]
                entry["width"] = slot.image_shape[1]
            info.append(entry)
        return info

    def _materialize(self, payload: Dict[str, Any]) -> Tuple[np.ndarray, str, str, Optional[str]]:
        variant = _default_variant(payload.get("sam_variant"))
        image_name = payload.get("image_name")
        token = payload.get("image_token")
        if token:
            cached = _fetch_preloaded_image(token, variant)
            if cached is not None:
                return cached, token, variant, image_name
        base64_data = payload.get("image_base64")
        if not base64_data:
            raise HTTPException(status_code=HTTP_428_PRECONDITION_REQUIRED, detail="image_payload_missing")
        _, np_img = _decode_image_base64(base64_data)
        token = hashlib.md5(np_img.tobytes()).hexdigest()
        _store_preloaded_image(token, np_img, variant)
        return np_img, token, variant, image_name

    def _worker(self) -> None:
        while not self.stop_event.is_set():
            try:
                slot_name, payload = self.queue.get(timeout=0.5)
            except queue.Empty:
                continue
            try:
                np_img, token, variant, image_name = self._materialize(payload)
                try:
                    self.set_slot_with_wait(slot_name, np_img, token, variant, image_name)
                except ValueError:
                    # Slot was disabled while this job was in flight; skip.
                    continue
            except Exception as exc:  # noqa: BLE001
                print(f"predictor preload failed: {exc}")


predictor_manager = PredictorManager()

# 5) Threading lock for SAM usage:
sam_lock = threading.Lock()

job_store: Dict[str, List["CropImage"]] = {}

app = FastAPI(title="Local Inference API (Multi-Predictor)")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger("localinferenceapi")
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(message)s"))
    logger.addHandler(handler)
logger.propagate = False

# Cache for repeated calls
SAM_CACHE_LIMIT = 8
sam_cache_lock = threading.Lock()
sam_preload_cache: "OrderedDict[str, Tuple[np.ndarray, str]]" = OrderedDict()
sam_preload_cache_bytes = 0


def _store_preloaded_image(token: str, np_img: np.ndarray, variant: str) -> None:
    global sam_preload_cache_bytes
    arr = np.ascontiguousarray(np_img)
    arr_bytes = arr.nbytes
    if SAM_PRELOAD_MAX_BYTES > 0 and arr_bytes > SAM_PRELOAD_MAX_BYTES:
        logger.warning("Skipping preload store: image too large (%d bytes > %d)", arr_bytes, SAM_PRELOAD_MAX_BYTES)
        return
    with sam_cache_lock:
        # Remove existing entry bytes
        if token in sam_preload_cache:
            old_arr, _ = sam_preload_cache[token]
            sam_preload_cache_bytes -= getattr(old_arr, "nbytes", 0)
        sam_preload_cache[token] = (arr, variant)
        sam_preload_cache.move_to_end(token)
        sam_preload_cache_bytes += arr_bytes
        while len(sam_preload_cache) > SAM_CACHE_LIMIT or (
            SAM_PRELOAD_MAX_BYTES > 0 and sam_preload_cache_bytes > SAM_PRELOAD_MAX_BYTES
        ):
            _, (evicted_arr, _) = sam_preload_cache.popitem(last=False)
            sam_preload_cache_bytes -= getattr(evicted_arr, "nbytes", 0)


def _fetch_preloaded_image(token: str, variant: str) -> Optional[np.ndarray]:
    with sam_cache_lock:
        item = sam_preload_cache.get(token)
        if not item:
            return None
        arr, stored_variant = item
        # Allow reuse across variants; cache holds raw image arrays only.
        sam_preload_cache.move_to_end(token)
        return arr


_job_id_counter = itertools.count(1)


@dataclass
class SamPreloadJob:
    request_id: int
    variant: str
    slot: str
    generation: Optional[int]
    image_token: Optional[str]
    image_base64: Optional[str]
    image_name: Optional[str]
    event: threading.Event
    result: Optional[SamPreloadResponse] = None
    error: Optional[Exception] = None


class SamPreloadManager:
    def __init__(self):
        self.queue: "queue.Queue[SamPreloadJob]" = queue.Queue()
        self.lock = threading.Lock()
        self.latest_request_id: Dict[Tuple[str, str], int] = {}
        self.latest_generation: Dict[Tuple[str, str], int] = {}
        self.worker = threading.Thread(target=self._worker, name="sam-preload-worker", daemon=True)
        self.worker.start()

    def submit(
        self,
        *,
        variant: str,
        slot: str,
        generation: Optional[int],
        image_token: Optional[str],
        image_base64: Optional[str],
        image_name: Optional[str],
    ) -> SamPreloadResponse:
        job = SamPreloadJob(
            request_id=next(_job_id_counter),
            variant=variant,
            slot=slot,
            generation=generation,
            image_token=image_token,
            image_base64=image_base64,
            image_name=image_name,
            event=threading.Event(),
        )
        key = (variant, slot)
        with self.lock:
            self.latest_request_id[key] = job.request_id
            if generation is not None:
                prev = self.latest_generation.get(key)
                if prev is None or generation > prev:
                    self.latest_generation[key] = generation
        self.queue.put(job)
        job.event.wait()
        if job.error:
            raise job.error
        return job.result  # type: ignore[return-value]

    def _worker(self) -> None:
        while True:
            job = self.queue.get()
            try:
                if self._is_superseded(job):
                    job.result = self._superseded_response(job)
                else:
                    job.result = self._process_job(job)
            except Exception as exc:  # noqa: BLE001
                job.error = exc
            finally:
                job.event.set()
                self.queue.task_done()

    def _key(self, job: SamPreloadJob) -> Tuple[str, str]:
        return (job.variant, job.slot)

    def _is_superseded(self, job: SamPreloadJob) -> bool:
        with self.lock:
            latest_id = self.latest_request_id.get(self._key(job))
            latest_generation = self.latest_generation.get(self._key(job))
        if latest_id is not None and job.request_id < latest_id:
            return True
        if job.generation is not None and latest_generation is not None and job.generation < latest_generation:
            return True
        return False

    def _superseded_response(self, job: SamPreloadJob) -> SamPreloadResponse:
        width = 0
        height = 0
        if job.image_token:
            cached = _fetch_preloaded_image(job.image_token, job.variant)
            if cached is not None:
                height, width = cached.shape[:2]
        return SamPreloadResponse(status="superseded", width=int(width), height=int(height), token=job.image_token or "")

    def _process_job(self, job: SamPreloadJob) -> SamPreloadResponse:
        variant = job.variant
        slot = job.slot
        image_name = job.image_name

        if job.image_token:
            cached = _fetch_preloaded_image(job.image_token, variant)
            if cached is not None:
                if self._is_superseded(job):
                    height, width = cached.shape[:2]
                    return SamPreloadResponse(
                        status="superseded",
                        width=int(width),
                        height=int(height),
                        token=job.image_token,
                    )
                predictor_manager.set_slot_with_wait(slot, cached, job.image_token, variant, image_name)
                height, width = cached.shape[:2]
                return SamPreloadResponse(status="ready", width=int(width), height=int(height), token=job.image_token)
            if not job.image_base64:
                raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="image_token_not_found")

        if not job.image_base64:
            raise HTTPException(status_code=HTTP_428_PRECONDITION_REQUIRED, detail="image_base64_required")

        np_img = self._decode_base64(job.image_base64)
        token = hashlib.md5(np_img.tobytes()).hexdigest()
        _store_preloaded_image(token, np_img, variant)

        if self._is_superseded(job):
            height, width = np_img.shape[:2]
            return SamPreloadResponse(status="superseded", width=int(width), height=int(height), token=token)

        predictor_manager.set_slot_with_wait(slot, np_img, token, variant, image_name)
        height, width = np_img.shape[:2]
        return SamPreloadResponse(status="ready", width=int(width), height=int(height), token=token)

    @staticmethod
    def _decode_base64(image_base64: str) -> np.ndarray:
        _, np_img = _decode_image_base64(image_base64)
        return np_img


sam_preload_manager = SamPreloadManager()


def _predict_with_cache(
    np_img: np.ndarray,
    token: Optional[str],
    variant: str,
    *,
    image_name: Optional[str] = None,
    **predict_kwargs: Any,
):
    normalized = _default_variant(variant)
    if normalized == "sam3" and not active_sam3_enable_segmentation:
        height, width = np_img.shape[:2]
        mask = np.zeros((height, width), dtype=np.uint8)
        box = predict_kwargs.get("box")
        point_coords = predict_kwargs.get("point_coords")
        if box is not None and len(box) >= 4:
            try:
                x1 = int(round(float(box[0])))
                y1 = int(round(float(box[1])))
                x2 = int(round(float(box[2])))
                y2 = int(round(float(box[3])))
            except (TypeError, ValueError):
                x1 = y1 = x2 = y2 = 0
            x1 = max(0, min(x1, width))
            x2 = max(0, min(x2, width))
            y1 = max(0, min(y1, height))
            y2 = max(0, min(y2, height))
            if x2 > x1 and y2 > y1:
                mask[y1:y2, x1:x2] = 1
        elif point_coords is not None:
            try:
                px = int(round(float(point_coords[0][0])))
                py = int(round(float(point_coords[0][1])))
            except Exception:
                px = py = 0
            px = max(0, min(px, width - 1))
            py = max(0, min(py, height - 1))
            size = 2
            x1 = max(0, px - size)
            x2 = min(width, px + size)
            y1 = max(0, py - size)
            y2 = min(height, py + size)
            mask[y1:y2, x1:x2] = 1
        masks = np.asarray([mask], dtype=np.uint8)
        return masks, None, None
    return predictor_manager.predict(np_img, token, variant, image_name=image_name, **predict_kwargs)


def _default_variant(value: Optional[str]) -> str:
    return (value or "sam1").lower()


_job_id_counter = itertools.count(1)


@dataclass
class SamPreloadJob:
    request_id: int
    variant: str
    generation: Optional[int]
    image_token: Optional[str]
    image_base64: Optional[str]
    image_name: Optional[str]
    slot: str
    event: threading.Event
    result: Optional['SamPreloadResponse'] = None
    error: Optional[Exception] = None


class SamPreloadManager:
    def __init__(self):
        self.queue: "queue.Queue[SamPreloadJob]" = queue.Queue()
        self.lock = threading.Lock()
        self.latest_request_id: Dict[str, int] = {}
        self.latest_generation: Dict[str, int] = {}
        self.worker = threading.Thread(target=self._worker, name="sam-preload-worker", daemon=True)
        self.worker.start()

    def submit(
        self,
        *,
        variant: str,
        generation: Optional[int],
        image_token: Optional[str],
        image_base64: Optional[str],
        image_name: Optional[str],
        slot: str,
    ) -> 'SamPreloadResponse':
        job = SamPreloadJob(
            request_id=next(_job_id_counter),
            variant=variant,
            generation=generation,
            image_token=image_token,
            image_base64=image_base64,
            image_name=image_name,
            slot=slot,
            event=threading.Event(),
        )
        with self.lock:
            self.latest_request_id[variant] = job.request_id
            if generation is not None:
                prev = self.latest_generation.get(variant)
                if prev is None or generation > prev:
                    self.latest_generation[variant] = generation
        self.queue.put(job)
        job.event.wait()
        if job.error:
            raise job.error
        return job.result  # type: ignore[return-value]

    def _worker(self) -> None:
        while True:
            job = self.queue.get()
            try:
                if self._is_superseded(job):
                    job.result = SamPreloadResponse(status="superseded", width=0, height=0, token=job.image_token or "")
                else:
                    job.result = self._process_job(job)
            except Exception as exc:  # noqa: BLE001 - propagate to caller
                job.error = exc
            finally:
                job.event.set()
                self.queue.task_done()

    def _is_superseded(self, job: SamPreloadJob) -> bool:
        with self.lock:
            latest_id = self.latest_request_id.get(job.variant)
            latest_generation = self.latest_generation.get(job.variant)
        if latest_id is not None and job.request_id < latest_id:
            return True
        if job.generation is not None and latest_generation is not None and job.generation < latest_generation:
            return True
        return False

    def _process_job(self, job: SamPreloadJob) -> 'SamPreloadResponse':
        variant = job.variant
        try:
            slot_name = predictor_manager.resolve_slot(job.slot, allow_disabled_fallback=False)
        except ValueError:
            return SamPreloadResponse(status="slot_disabled", width=0, height=0, token=job.image_token or "")
        image_name = job.image_name

        if job.image_token:
            cached = _fetch_preloaded_image(job.image_token, variant)
            if cached is not None:
                if self._is_superseded(job):
                    return SamPreloadResponse(
                        status="superseded",
                        width=int(cached.shape[1]),
                        height=int(cached.shape[0]),
                        token=job.image_token,
                    )
                predictor_manager.set_slot_with_wait(slot_name, cached, job.image_token, variant, image_name)
                height, width = cached.shape[:2]
                return SamPreloadResponse(status="ready", width=int(width), height=int(height), token=job.image_token)
            if not job.image_base64:
                raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="image_token_not_found")

        if not job.image_base64:
            raise HTTPException(status_code=HTTP_428_PRECONDITION_REQUIRED, detail="image_base64_required")

        np_img = self._decode_base64(job.image_base64)
        token = hashlib.md5(np_img.tobytes()).hexdigest()
        _store_preloaded_image(token, np_img, variant)

        if self._is_superseded(job):
            return SamPreloadResponse(status="superseded", width=int(np_img.shape[1]), height=int(np_img.shape[0]), token=token)

        predictor_manager.set_slot_with_wait(slot_name, np_img, token, variant, image_name)
        height, width = np_img.shape[:2]
        return SamPreloadResponse(status="ready", width=int(width), height=int(height), token=token)

    @staticmethod
    def _decode_base64(image_base64: str) -> np.ndarray:
        _, np_img = _decode_image_base64(image_base64)
        return np_img


def _resolve_qwen_device() -> str:
    if QWEN_DEVICE_PREF and QWEN_DEVICE_PREF != "auto":
        if QWEN_DEVICE_PREF.startswith("cuda") and not torch.cuda.is_available():
            raise RuntimeError("cuda_requested_but_unavailable")
        if QWEN_DEVICE_PREF.startswith("mps"):
            mps_backend = getattr(torch.backends, "mps", None)
            if not mps_backend or not mps_backend.is_available():  # type: ignore[attr-defined]
                raise RuntimeError("mps_requested_but_unavailable")
        return QWEN_DEVICE_PREF
    if torch.cuda.is_available():
        return "cuda"
    mps_backend = getattr(torch.backends, "mps", None)
    if mps_backend and mps_backend.is_available():  # type: ignore[attr-defined]
        return "mps"
    return "cpu"


def _get_qwen_prompt_config() -> QwenPromptConfig:
    with qwen_config_lock:
        return qwen_prompt_config.copy(deep=True)


def _set_qwen_prompt_config(config: QwenPromptConfig) -> None:
    global qwen_prompt_config
    with qwen_config_lock:
        qwen_prompt_config = config.copy(deep=True)


def _render_qwen_prompt(
    prompt_type: str,
    *,
    items: Optional[str],
    image_type: Optional[str],
    extra_context: Optional[str],
) -> str:
    if not items:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="qwen_items_required")
    config = _get_qwen_prompt_config()
    section_name = "bbox" if prompt_type in {"bbox", "bbox_sam"} else prompt_type
    section = getattr(config, section_name)
    template = (section.base_prompt or "{items}").strip()
    image_value = (image_type or section.default_image_type or "image").strip() or "image"
    extra_value = extra_context if extra_context is not None and extra_context.strip() else section.default_extra_context
    formatted = template.format(
        image_type=image_value,
        items=items.strip(),
        extra_context=(extra_value or "").strip(),
    )
    return formatted.strip()


def _extract_qwen_json_block(text: str) -> Tuple[str, List[Dict[str, Any]]]:
    def _attempt_parse(raw: str) -> Optional[Tuple[str, List[Dict[str, Any]]]]:
        snippet = (raw or "").strip()
        if not snippet:
            return None
        snippet = snippet.strip("`").strip()

        parsed: Any = None
        try:
            parsed = json.loads(snippet)
        except json.JSONDecodeError:
            parsed = None

        if parsed is None:
            for start_char, end_char in (("{", "}"), ("[", "]")):
                start = snippet.find(start_char)
                end = snippet.rfind(end_char)
                if start < 0 or end < 0 or end <= start:
                    continue
                candidate = snippet[start : end + 1]
                try:
                    parsed = json.loads(candidate)
                    snippet = candidate
                    break
                except json.JSONDecodeError:
                    parsed = None

        if parsed is None:
            return None

        if isinstance(parsed, dict):
            if "detections" in parsed and isinstance(parsed["detections"], list):
                return snippet, [item for item in parsed["detections"] if isinstance(item, dict)]
            return snippet, [parsed]
        if isinstance(parsed, list):
            return snippet, [item for item in parsed if isinstance(item, dict)]
        return None

    fenced = re.findall(r"```(?:[a-zA-Z0-9_-]+)?\s*(.*?)```", text, flags=re.DOTALL)
    for raw in [*fenced, text]:
        parsed = _attempt_parse(raw)
        if parsed is not None:
            return parsed

    raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="qwen_parse_error:no_json_block_found")


def _extract_numeric_sequence(value: Any, *, length: int) -> Optional[List[float]]:
    if isinstance(value, str):
        try:
            value = json.loads(value)
        except json.JSONDecodeError:
            return None
    if not isinstance(value, (list, tuple)) or len(value) < length:
        return None
    numbers: List[float] = []
    for idx in range(length):
        try:
            numbers.append(float(value[idx]))
        except (TypeError, ValueError):
            return None
    return numbers


def _scale_coord(value: float, src: int, dst: int) -> float:
    if src <= 0:
        return float(value)
    return float(value) * (float(dst) / float(src))


def _scale_bbox_to_image(
    bbox: List[float],
    proc_w: int,
    proc_h: int,
    full_w: int,
    full_h: int,
) -> Optional[Tuple[int, int, int, int]]:
    if len(bbox) < 4:
        return None
    left = _scale_coord(bbox[0], proc_w, full_w)
    top = _scale_coord(bbox[1], proc_h, full_h)
    right = _scale_coord(bbox[2], proc_w, full_w)
    bottom = _scale_coord(bbox[3], proc_h, full_h)
    left_i = max(0, min(full_w, int(round(left))))
    top_i = max(0, min(full_h, int(round(top))))
    right_i = max(0, min(full_w, int(round(right))))
    bottom_i = max(0, min(full_h, int(round(bottom))))
    if right_i <= left_i or bottom_i <= top_i:
        return None
    return left_i, top_i, right_i, bottom_i


def _scale_point_to_image(
    point: List[float],
    proc_w: int,
    proc_h: int,
    full_w: int,
    full_h: int,
) -> Optional[Tuple[float, float]]:
    if len(point) < 2:
        return None
    x = _scale_coord(point[0], proc_w, full_w)
    y = _scale_coord(point[1], proc_h, full_h)
    x = float(min(max(x, 0.0), float(full_w)))
    y = float(min(max(y, 0.0), float(full_h)))
    return x, y


def _qwen_items_from_payload(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    return [item for item in items if isinstance(item, dict)]


def _qwen_bbox_results(
    items: List[Dict[str, Any]],
    proc_w: int,
    proc_h: int,
    full_w: int,
    full_h: int,
    *,
    limit: int,
) -> List['QwenDetection']:
    results: List[QwenDetection] = []
    for item in items:
        bbox = (
            _extract_numeric_sequence(item.get("bbox_2d"), length=4)
            or _extract_numeric_sequence(item.get("bbox"), length=4)
            or _extract_numeric_sequence(item.get("box"), length=4)
        )
        if not bbox:
            continue
        scaled = _scale_bbox_to_image(bbox, proc_w, proc_h, full_w, full_h)
        if not scaled:
            continue
        left, top, right, bottom = scaled
        yolo_box = to_yolo(full_w, full_h, left, top, right, bottom)
        label = item.get("label") or item.get("class") or item.get("name")
        results.append(QwenDetection(bbox=yolo_box, qwen_label=str(label) if label else None, source="bbox"))
        if len(results) >= limit:
            break
    return results


def _qwen_bbox_sam_results(
    items: List[Dict[str, Any]],
    proc_w: int,
    proc_h: int,
    pil_img: Image.Image,
    np_img: np.ndarray,
    token: Optional[str],
    variant: str,
    *,
    image_name: Optional[str],
    limit: int,
) -> List['QwenDetection']:
    results: List[QwenDetection] = []
    for item in items:
        bbox = (
            _extract_numeric_sequence(item.get("bbox_2d"), length=4)
            or _extract_numeric_sequence(item.get("bbox"), length=4)
            or _extract_numeric_sequence(item.get("box"), length=4)
        )
        if not bbox:
            continue
        scaled = _scale_bbox_to_image(bbox, proc_w, proc_h, pil_img.width, pil_img.height)
        if not scaled:
            continue
        sub_box = np.array(list(scaled), dtype=np.float32)
        masks, _, _ = _predict_with_cache(
            np_img,
            token,
            variant,
            image_name=image_name,
            box=sub_box,
            multimask_output=False,
        )
        mask = masks[0]
        left, top, right, bottom = mask_to_bounding_box(mask)
        if right <= left or bottom <= top:
            continue
        yolo_box = to_yolo(pil_img.width, pil_img.height, left, top, right, bottom)
        label = item.get("label") or item.get("class") or item.get("name")
        results.append(QwenDetection(bbox=yolo_box, qwen_label=str(label) if label else None, source="bbox_sam"))
        if len(results) >= limit:
            break
    return results


def _qwen_point_results(
    items: List[Dict[str, Any]],
    proc_w: int,
    proc_h: int,
    pil_img: Image.Image,
    np_img: np.ndarray,
    token: Optional[str],
    variant: str,
    *,
    image_name: Optional[str],
    limit: int,
) -> List['QwenDetection']:
    results: List[QwenDetection] = []
    for item in items:
        point = _extract_numeric_sequence(item.get("point_2d") or item.get("point"), length=2)
        if not point:
            continue
        scaled_point = _scale_point_to_image(point, proc_w, proc_h, pil_img.width, pil_img.height)
        if not scaled_point:
            continue
        coords = np.array([[scaled_point[0], scaled_point[1]]], dtype=np.float32)
        labels = np.array([1], dtype=np.int64)
        masks, _, _ = _predict_with_cache(
            np_img,
            token,
            variant,
            image_name=image_name,
            point_coords=coords,
            point_labels=labels,
            multimask_output=False,
        )
        mask = masks[0]
        left, top, right, bottom = mask_to_bounding_box(mask)
        if right <= left or bottom <= top:
            continue
        yolo_box = to_yolo(pil_img.width, pil_img.height, left, top, right, bottom)
        label = item.get("label") or item.get("class") or item.get("name")
        results.append(QwenDetection(bbox=yolo_box, qwen_label=str(label) if label else None, source="point"))
        if len(results) >= limit:
            break
    return results


def _sam3_text_detections(
    pil_img: Image.Image,
    payload: Dict[str, Any],
    text_prompt: str,
    limit: Optional[int],
    *,
    min_score: Optional[float] = None,
    masks_arr: Optional[np.ndarray] = None,
    min_size: Optional[float] = None,
    simplify_epsilon: Optional[float] = None,
    collected_masks: Optional[List[np.ndarray]] = None,
) -> List[QwenDetection]:
    width, height = pil_img.width, pil_img.height
    boxes_source = payload.get("boxes")
    scores_source = payload.get("scores")
    masks = payload.get("masks")
    if isinstance(boxes_source, torch.Tensor):
        boxes_iter: Sequence[Any] = boxes_source.cpu().numpy()
    elif boxes_source is None:
        boxes_iter = []
    else:
        boxes_iter = boxes_source
    if isinstance(scores_source, torch.Tensor):
        scores_iter: Sequence[Any] = scores_source.cpu().numpy().tolist()
    elif scores_source is None:
        scores_iter = []
    else:
        scores_iter = scores_source
    if masks_arr is None and masks is not None:
        if isinstance(masks, torch.Tensor):
            masks_arr = masks.cpu().numpy()
        else:
            masks_arr = np.asarray(masks)
    detections: List[QwenDetection] = []
    if limit is None:
        numeric_limit: Optional[int] = None
    else:
        try:
            numeric_limit = int(limit)
        except (TypeError, ValueError):
            numeric_limit = None
        else:
            if numeric_limit <= 0:
                numeric_limit = None
    # Prefer highest-score boxes first when scores are available, since we may be limiting outputs.
    order = list(range(len(boxes_iter)))
    try:
        if scores_iter is not None and len(scores_iter) >= len(order):
            order.sort(
                key=lambda i: float(scores_iter[i]) if i < len(scores_iter) and scores_iter[i] is not None else -1e9,
                reverse=True,
            )
    except Exception:
        order = list(range(len(boxes_iter)))

    for idx in order:
        box = boxes_iter[idx]
        coords = np.asarray(box, dtype=np.float32).tolist()
        if len(coords) < 4:
            continue
        x_min, y_min, x_max, y_max = coords[:4]
        if x_max <= x_min or y_max <= y_min:
            continue
        yolo_box = to_yolo(width, height, x_min, y_min, x_max, y_max)
        score_val = None
        if idx < len(scores_iter):
            try:
                score_val = float(scores_iter[idx])
            except (TypeError, ValueError):
                score_val = None
        if min_score is not None and score_val is not None and score_val < min_score:
            continue
        area = max(0.0, (x_max - x_min) * (y_max - y_min))
        if masks_arr is not None and idx < len(masks_arr):
            try:
                area = float(np.count_nonzero(masks_arr[idx]))
            except Exception:
                area = area
        if min_size is not None:
            try:
                if area < float(min_size):
                    continue
            except Exception:
                pass
        mask_payload = None
        mask_value = None
        if masks_arr is not None and idx < len(masks_arr):
            mask_value = masks_arr[idx]
            mask_payload = encode_binary_mask(mask_value)
        if collected_masks is not None:
            collected_masks.append(mask_value)
        detections.append(
            QwenDetection(
                bbox=yolo_box,
                qwen_label=text_prompt,
                source="sam3_text",
                score=score_val,
                mask=mask_payload,
                simplify_epsilon=simplify_epsilon,
            )
        )
        if numeric_limit is not None and len(detections) >= numeric_limit:
            break
    if detections or masks_arr is None:
        return detections
    for idx, mask in enumerate(masks_arr):
        x_min, y_min, x_max, y_max = mask_to_bounding_box(mask)
        if x_max <= x_min or y_max <= y_min:
            continue
        yolo_box = to_yolo(width, height, x_min, y_min, x_max, y_max)
        score_val = None
        if idx < len(scores_iter):
            try:
                score_val = float(scores_iter[idx])
            except (TypeError, ValueError):
                score_val = None
        if min_score is not None and score_val is not None and score_val < min_score:
            continue
        area = max(0.0, (x_max - x_min) * (y_max - y_min))
        try:
            area = float(np.count_nonzero(mask))
        except Exception:
            area = area
        if min_size is not None:
            try:
                if area < float(min_size):
                    continue
            except Exception:
                pass
        detections.append(
            QwenDetection(
                bbox=yolo_box,
                qwen_label=text_prompt,
                source="sam3_text",
                score=score_val,
                mask=encode_binary_mask(mask),
                simplify_epsilon=simplify_epsilon,
            )
        )
        if collected_masks is not None:
            collected_masks.append(mask)
        if numeric_limit is not None and len(detections) >= numeric_limit:
            break
    return detections


def _run_sam3_text_inference(
    pil_img: Image.Image,
    text_prompt: str,
    threshold: float,
    mask_threshold: float,
    limit: Optional[int],
    *,
    return_masks: bool = False,
    min_size: Optional[float] = None,
    simplify_epsilon: Optional[float] = None,
    processor_override: Optional[Any] = None,
    state: Optional[Any] = None,
) -> List[QwenDetection] | Tuple[List[QwenDetection], Optional[List[np.ndarray]]]:
    """
    Run SAM3 text inference. By default returns detections list; callers that need masks should
    inspect the second element when `return_masks=True`.
    """
    if processor_override is not None:
        processor = processor_override
    else:
        _, processor, _ = _ensure_sam3_text_runtime()
    try:
        processor.set_confidence_threshold(float(threshold))
    except Exception:
        # If the processor refuses the threshold, continue with its default.
        pass
    normalized_limit: Optional[int]
    if limit is None:
        normalized_limit = None
    else:
        try:
            normalized_limit = max(1, int(limit))
        except (TypeError, ValueError):
            normalized_limit = None
    img_state = state if state is not None else processor.set_image(pil_img)
    masks_arr: Optional[np.ndarray] = None
    try:
        output = processor.set_text_prompt(state=img_state, prompt=text_prompt)
    except KeyError:
        # Box-only checkpoints (enable_segmentation=False) do not emit pred_masks.
        # Fall back to raw model output and extract boxes/scores manually.
        try:
            raw = processor.model.forward_grounding(
                backbone_out=img_state.get("backbone_out", {}),
                find_input=processor.find_stage,
                find_target=None,
                geometric_prompt=img_state.get("geometric_prompt", processor.model._get_dummy_prompt()),
            )
            boxes_xyxy = raw.get("pred_boxes_xyxy")
            if boxes_xyxy is None:
                boxes_xyxy = raw.get("pred_boxes")
            scores = None
            logits = raw.get("pred_logits")
            if logits is not None:
                try:
                    scores = torch.sigmoid(logits.squeeze(-1))
                except Exception:  # noqa: BLE001
                    try:
                        scores = torch.sigmoid(logits)
                    except Exception:  # noqa: BLE001
                        scores = None
            output = {
                "boxes": boxes_xyxy,
                "scores": scores,
                # no masks for box-only checkpoints
            }
        except Exception as exc:  # noqa: BLE001
            logger.warning("SAM3 box-only text prompt fallback failed: %s", exc)
            raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"sam3_text_grounding_failed:{exc}") from exc
    try:
        if output is not None and hasattr(output, "pred_masks"):
            masks = output.pred_masks
            if masks is not None:
                try:
                    masks_arr = masks.cpu().numpy()
                except Exception:
                    try:
                        masks_arr = np.asarray(masks)
                    except Exception:
                        masks_arr = None
        if masks_arr is not None:
            try:
                masks_arr = (masks_arr >= float(mask_threshold)).astype(np.uint8)
            except Exception:
                # If thresholding fails, keep the raw masks.
                pass
    except Exception:
        masks_arr = None
    collected_masks: Optional[List[np.ndarray]] = [] if return_masks else None
    preds = _sam3_text_detections(
        pil_img,
        output,
        text_prompt,
        normalized_limit,
        min_score=float(threshold),
        masks_arr=masks_arr,
        min_size=min_size,
        simplify_epsilon=simplify_epsilon,
        collected_masks=collected_masks,
    )
    aligned_masks: Optional[List[np.ndarray]]
    if collected_masks is None:
        aligned_masks = None
    else:
        aligned_masks = collected_masks
    return (preds, aligned_masks) if return_masks else preds


def _run_sam3_visual_inference(
    pil_img: Image.Image,
    bbox_xywh: Tuple[float, float, float, float],
    threshold: float,
    mask_threshold: float,
    limit: Optional[int],
    *,
    return_masks: bool = False,
    min_size: Optional[float] = None,
    simplify_epsilon: Optional[float] = None,
    processor_override: Optional[Any] = None,
    state: Optional[Any] = None,
) -> List[QwenDetection] | Tuple[List[QwenDetection], Optional[List[np.ndarray]]]:
    """
    Run SAM3 with a single positive visual (box) prompt. By default returns detections list;
    callers that need masks should inspect the second element when `return_masks=True`.
    """
    if processor_override is not None:
        processor = processor_override
    else:
        _, processor, _ = _ensure_sam3_text_runtime()
    try:
        processor.set_confidence_threshold(float(threshold))
    except Exception:
        pass
    normalized_limit: Optional[int]
    if limit is None:
        normalized_limit = None
    else:
        try:
            normalized_limit = max(1, int(limit))
        except (TypeError, ValueError):
            normalized_limit = None
    img_state = state if state is not None else processor.set_image(pil_img)
    img_w, img_h = float(pil_img.width), float(pil_img.height)
    x, y, w, h = bbox_xywh
    cx = (x + w / 2.0) / img_w
    cy = (y + h / 2.0) / img_h
    w_norm = w / img_w
    h_norm = h / img_h
    try:
        output = processor.add_geometric_prompt([cx, cy, w_norm, h_norm], True, state=img_state)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"sam3_visual_prompt_failed:{exc}") from exc
    masks_arr: Optional[np.ndarray] = None
    mask_logits = None
    if isinstance(output, Mapping):
        if "masks_logits" in output and output.get("masks_logits") is not None:
            mask_logits = output.get("masks_logits")
        elif "masks" in output and output.get("masks") is not None:
            mask_logits = output.get("masks")
    if mask_logits is None and isinstance(img_state, Mapping):
        if "masks_logits" in img_state and img_state.get("masks_logits") is not None:
            mask_logits = img_state.get("masks_logits")
        elif "masks" in img_state and img_state.get("masks") is not None:
            mask_logits = img_state.get("masks")
    try:
        threshold_val = float(mask_threshold)
    except Exception:
        threshold_val = 0.5
    threshold_val = max(0.0, min(1.0, threshold_val))
    # Normalize mask logits into a numpy array before thresholding.
    try:
        def _sigmoid_np(arr: np.ndarray) -> np.ndarray:
            try:
                return 1.0 / (1.0 + np.exp(-np.clip(arr, -50, 50)))
            except Exception:
                return 1.0 / (1.0 + np.exp(-arr))

        if isinstance(mask_logits, (list, tuple)):
            if any(isinstance(m, torch.Tensor) for m in mask_logits):
                stacked = [m.detach().cpu().numpy() if isinstance(m, torch.Tensor) else np.asarray(m) for m in mask_logits]
                mask_logits = np.stack(stacked)
            else:
                mask_logits = np.asarray(mask_logits)
        if isinstance(mask_logits, torch.Tensor):
            try:
                probs = mask_logits
                try:
                    min_v = float(probs.min())
                    max_v = float(probs.max())
                    if not (0.0 <= min_v <= 1.0 and 0.0 <= max_v <= 1.0):
                        probs = torch.sigmoid(probs)
                except Exception:
                    probs = torch.sigmoid(probs)
                masks_arr = (probs > threshold_val).cpu().numpy()
            except Exception:
                masks_arr = mask_logits.detach().cpu().numpy()
        elif mask_logits is not None:
            masks_np = np.asarray(mask_logits)
            if masks_np.dtype == bool or (
                np.issubdtype(masks_np.dtype, np.floating)
                and np.nanmin(masks_np) >= 0.0
                and np.nanmax(masks_np) <= 1.0
            ):
                probs_np = masks_np
            else:
                probs_np = _sigmoid_np(masks_np)
            masks_arr = probs_np > threshold_val
        # Normalize mask shape to (N, H, W) where possible
        if masks_arr is not None:
            masks_arr = np.asarray(masks_arr)
            if masks_arr.dtype == object:
                flattened = [np.asarray(m) for m in masks_arr]
                masks_arr = np.stack(flattened)
            if masks_arr.ndim == 2:
                masks_arr = masks_arr[None, ...]
            elif masks_arr.ndim == 4 and masks_arr.shape[1] == 1:
                masks_arr = masks_arr[:, 0, ...]
            elif masks_arr.ndim == 4 and masks_arr.shape[-1] == 1:
                masks_arr = masks_arr[..., 0]
    except Exception:
        masks_arr = None
    def _to_numpy_safe(val: Any) -> Optional[np.ndarray]:
        if val is None:
            return None
        if isinstance(val, torch.Tensor):
            try:
                return val.detach().cpu().numpy()
            except Exception:
                return None
        try:
            return np.asarray(val)
        except Exception:
            return None

    payload_for_detection: Dict[str, Any] = {}
    if isinstance(output, Mapping):
        boxes_val = _to_numpy_safe(output.get("boxes"))
        scores_val = _to_numpy_safe(output.get("scores"))
        masks_val = _to_numpy_safe(output.get("masks"))
        if boxes_val is not None:
            payload_for_detection["boxes"] = boxes_val
        if scores_val is not None:
            payload_for_detection["scores"] = scores_val
        if masks_val is not None:
            payload_for_detection["masks"] = masks_val
    collected_masks: Optional[List[np.ndarray]] = [] if return_masks else None
    detections = _sam3_text_detections(
        pil_img,
        payload_for_detection,
        "visual",
        normalized_limit,
        min_score=float(threshold),
        masks_arr=masks_arr,
        min_size=min_size,
        simplify_epsilon=simplify_epsilon,
        collected_masks=collected_masks,
    )
    # Drop the seed box if SAM returns it again (dedupe by IoU against the input box).
    seed_xyxy = (bbox_xywh[0], bbox_xywh[1], bbox_xywh[0] + bbox_xywh[2], bbox_xywh[1] + bbox_xywh[3])
    def _iou(box_a: Tuple[float, float, float, float], box_b: Tuple[float, float, float, float]) -> float:
        ax1, ay1, ax2, ay2 = box_a
        bx1, by1, bx2, by2 = box_b
        ix1, iy1 = max(ax1, bx1), max(ay1, by1)
        ix2, iy2 = min(ax2, bx2), min(ay2, by2)
        iw, ih = max(0.0, ix2 - ix1), max(0.0, iy2 - iy1)
        inter = iw * ih
        if inter <= 0:
            return 0.0
        area_a = max(0.0, (ax2 - ax1)) * max(0.0, (ay2 - ay1))
        area_b = max(0.0, (bx2 - bx1)) * max(0.0, (by2 - by1))
        denom = area_a + area_b - inter
        return inter / denom if denom > 0 else 0.0

    aligned_masks: Optional[List[np.ndarray]]
    if collected_masks is None:
        aligned_masks = None
    else:
        aligned_masks = collected_masks
    if detections:
        filtered_dets: List[QwenDetection] = []
        filtered_masks: List[np.ndarray] = []
        for det_idx, det in enumerate(detections):
            bbox = det.bbox or []
            if len(bbox) < 4:
                continue
            det_xyxy = yolo_to_corners(bbox, pil_img.width, pil_img.height)
            if _iou(seed_xyxy, det_xyxy) > 0.9:
                continue
            filtered_dets.append(det)
            if aligned_masks is not None and det_idx < len(aligned_masks):
                filtered_masks.append(aligned_masks[det_idx])
        detections = filtered_dets
        if aligned_masks is not None:
            aligned_masks = filtered_masks
    return (detections, aligned_masks) if return_masks else detections


def _run_sam3_visual_inference_multi(
    pil_img: Image.Image,
    bboxes_xywh: List[Tuple[float, float, float, float]],
    bbox_labels: Optional[List[bool]],
    threshold: float,
    mask_threshold: float,
    limit: Optional[int],
    *,
    return_masks: bool = False,
    min_size: Optional[float] = None,
    simplify_epsilon: Optional[float] = None,
    processor_override: Optional[Any] = None,
    state: Optional[Any] = None,
) -> List[QwenDetection] | Tuple[List[QwenDetection], Optional[List[np.ndarray]]]:
    """
    Run SAM3 with multiple positive visual (box) prompts. Uses a shared image state
    to accumulate prompts and returns detections from the combined prompt set.
    """
    if processor_override is not None:
        processor = processor_override
    else:
        _, processor, _ = _ensure_sam3_text_runtime()
    try:
        processor.set_confidence_threshold(float(threshold))
    except Exception:
        pass
    normalized_limit: Optional[int]
    if limit is None:
        normalized_limit = None
    else:
        try:
            normalized_limit = max(1, int(limit))
        except (TypeError, ValueError):
            normalized_limit = None
    if not bboxes_xywh:
        empty = ([], []) if return_masks else []
        return empty
    labels: List[bool]
    if bbox_labels is None:
        labels = [True] * len(bboxes_xywh)
    else:
        labels = list(bbox_labels)
        if len(labels) < len(bboxes_xywh):
            labels.extend([True] * (len(bboxes_xywh) - len(labels)))
        elif len(labels) > len(bboxes_xywh):
            labels = labels[: len(bboxes_xywh)]
    img_state = state if state is not None else processor.set_image(pil_img)
    img_w, img_h = float(pil_img.width), float(pil_img.height)
    output = None
    for bbox_xywh, label in zip(bboxes_xywh, labels):
        x, y, w, h = bbox_xywh
        cx = (x + w / 2.0) / img_w
        cy = (y + h / 2.0) / img_h
        w_norm = w / img_w
        h_norm = h / img_h
        try:
            output = processor.add_geometric_prompt([cx, cy, w_norm, h_norm], bool(label), state=img_state)
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"sam3_visual_prompt_failed:{exc}") from exc
    masks_arr: Optional[np.ndarray] = None
    mask_logits = None
    if isinstance(output, Mapping):
        if "masks_logits" in output and output.get("masks_logits") is not None:
            mask_logits = output.get("masks_logits")
        elif "masks" in output and output.get("masks") is not None:
            mask_logits = output.get("masks")
    if mask_logits is None and isinstance(img_state, Mapping):
        if "masks_logits" in img_state and img_state.get("masks_logits") is not None:
            mask_logits = img_state.get("masks_logits")
        elif "masks" in img_state and img_state.get("masks") is not None:
            mask_logits = img_state.get("masks")
    try:
        threshold_val = float(mask_threshold)
    except Exception:
        threshold_val = 0.5
    threshold_val = max(0.0, min(1.0, threshold_val))
    try:
        def _sigmoid_np(arr: np.ndarray) -> np.ndarray:
            try:
                return 1.0 / (1.0 + np.exp(-np.clip(arr, -50, 50)))
            except Exception:
                return 1.0 / (1.0 + np.exp(-arr))

        if isinstance(mask_logits, (list, tuple)):
            if any(isinstance(m, torch.Tensor) for m in mask_logits):
                stacked = [m.detach().cpu().numpy() if isinstance(m, torch.Tensor) else np.asarray(m) for m in mask_logits]
                mask_logits = np.stack(stacked)
            else:
                mask_logits = np.asarray(mask_logits)
        if isinstance(mask_logits, torch.Tensor):
            try:
                probs = mask_logits
                try:
                    min_v = float(probs.min())
                    max_v = float(probs.max())
                    if not (0.0 <= min_v <= 1.0 and 0.0 <= max_v <= 1.0):
                        probs = torch.sigmoid(probs)
                except Exception:
                    probs = torch.sigmoid(probs)
                masks_arr = (probs > threshold_val).cpu().numpy()
            except Exception:
                masks_arr = mask_logits.detach().cpu().numpy()
        elif mask_logits is not None:
            masks_np = np.asarray(mask_logits)
            if masks_np.dtype == bool or (
                np.issubdtype(masks_np.dtype, np.floating)
                and np.nanmin(masks_np) >= 0.0
                and np.nanmax(masks_np) <= 1.0
            ):
                probs_np = masks_np
            else:
                probs_np = _sigmoid_np(masks_np)
            masks_arr = probs_np > threshold_val
        if masks_arr is not None:
            masks_arr = np.asarray(masks_arr)
            if masks_arr.dtype == object:
                flattened = [np.asarray(m) for m in masks_arr]
                masks_arr = np.stack(flattened)
            if masks_arr.ndim == 2:
                masks_arr = masks_arr[None, ...]
            elif masks_arr.ndim == 4 and masks_arr.shape[1] == 1:
                masks_arr = masks_arr[:, 0, ...]
            elif masks_arr.ndim == 4 and masks_arr.shape[-1] == 1:
                masks_arr = masks_arr[..., 0]
    except Exception:
        masks_arr = None
    def _to_numpy_safe(val: Any) -> Optional[np.ndarray]:
        if val is None:
            return None
        if isinstance(val, torch.Tensor):
            try:
                return val.detach().cpu().numpy()
            except Exception:
                return None
        try:
            return np.asarray(val)
        except Exception:
            return None

    payload_for_detection: Dict[str, Any] = {}
    if isinstance(output, Mapping):
        boxes_val = _to_numpy_safe(output.get("boxes"))
        scores_val = _to_numpy_safe(output.get("scores"))
        masks_val = _to_numpy_safe(output.get("masks"))
        if boxes_val is not None:
            payload_for_detection["boxes"] = boxes_val
        if scores_val is not None:
            payload_for_detection["scores"] = scores_val
        if masks_val is not None:
            payload_for_detection["masks"] = masks_val
    collected_masks: Optional[List[np.ndarray]] = [] if return_masks else None
    detections = _sam3_text_detections(
        pil_img,
        payload_for_detection,
        "visual",
        normalized_limit,
        min_score=float(threshold),
        masks_arr=masks_arr,
        min_size=min_size,
        simplify_epsilon=simplify_epsilon,
        collected_masks=collected_masks,
    )
    seed_boxes_xyxy = [
        (bx[0], bx[1], bx[0] + bx[2], bx[1] + bx[3])
        for bx in bboxes_xywh
    ]
    def _iou(box_a: Tuple[float, float, float, float], box_b: Tuple[float, float, float, float]) -> float:
        ax1, ay1, ax2, ay2 = box_a
        bx1, by1, bx2, by2 = box_b
        ix1, iy1 = max(ax1, bx1), max(ay1, by1)
        ix2, iy2 = min(ax2, bx2), min(ay2, by2)
        iw, ih = max(0.0, ix2 - ix1), max(0.0, iy2 - iy1)
        inter = iw * ih
        if inter <= 0:
            return 0.0
        area_a = max(0.0, (ax2 - ax1)) * max(0.0, (ay2 - ay1))
        area_b = max(0.0, (bx2 - bx1)) * max(0.0, (by2 - by1))
        denom = area_a + area_b - inter
        return inter / denom if denom > 0 else 0.0

    aligned_masks: Optional[List[np.ndarray]]
    if collected_masks is None:
        aligned_masks = None
    else:
        aligned_masks = collected_masks
    if detections:
        filtered_dets: List[QwenDetection] = []
        filtered_masks: List[np.ndarray] = []
        for det_idx, det in enumerate(detections):
            bbox = det.bbox or []
            if len(bbox) < 4:
                continue
            det_xyxy = yolo_to_corners(bbox, pil_img.width, pil_img.height)
            if any(_iou(seed_xyxy, det_xyxy) > 0.9 for seed_xyxy in seed_boxes_xyxy):
                continue
            filtered_dets.append(det)
            if aligned_masks is not None and det_idx < len(aligned_masks):
                filtered_masks.append(aligned_masks[det_idx])
        detections = filtered_dets
        if aligned_masks is not None:
            aligned_masks = filtered_masks
    return (detections, aligned_masks) if return_masks else detections


def _ensure_qwen_ready():
    global qwen_model, qwen_processor, qwen_device, qwen_last_error, loaded_qwen_model_id
    if QWEN_IMPORT_ERROR is not None or Qwen3VLForConditionalGeneration is None or AutoProcessor is None or process_vision_info is None:
        detail = f"qwen_dependencies_missing:{QWEN_IMPORT_ERROR}"
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=detail)
    if packaging_version is not None:
        try:
            import transformers  # local import to avoid import-time failures

            if packaging_version.parse(transformers.__version__) < packaging_version.parse(QWEN_MIN_TRANSFORMERS):
                raise HTTPException(
                    status_code=HTTP_503_SERVICE_UNAVAILABLE,
                    detail=f"qwen_transformers_too_old:{transformers.__version__}<{QWEN_MIN_TRANSFORMERS}",
                )
        except HTTPException:
            raise
        except Exception:
            # If we cannot resolve version info, continue and let the load fail if incompatible.
            pass
    if (
        qwen_model is not None
        and qwen_processor is not None
        and loaded_qwen_model_id == active_qwen_model_id
    ):
        return qwen_model, qwen_processor
    with qwen_lock:
        if (
            qwen_model is not None
            and qwen_processor is not None
            and loaded_qwen_model_id == active_qwen_model_id
        ):
            return qwen_model, qwen_processor
        try:
            device = _resolve_qwen_device()
        except RuntimeError as exc:  # noqa: BLE001
            qwen_last_error = str(exc)
            raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=f"qwen_device_unavailable:{exc}") from exc
        use_auto_map = QWEN_DEVICE_PREF == "auto" and device.startswith("cuda") and torch.cuda.is_available()
        load_kwargs: Dict[str, Any]
        if use_auto_map:
            load_kwargs = {
                "torch_dtype": "auto",
                "device_map": "auto",
            }
        else:
            dtype = torch.float16 if device.startswith(("cuda", "mps")) else torch.float32
            load_kwargs = {
                "torch_dtype": dtype,
                "low_cpu_mem_usage": True,
            }
        adapter_path = active_qwen_model_path
        metadata = active_qwen_metadata or {}
        base_model_id = metadata.get("model_id") or QWEN_MODEL_NAME
        if adapter_path and PeftModel is None:
            detail = "qwen_peft_missing"
            if PEFT_IMPORT_ERROR is not None:
                detail = f"{detail}:{PEFT_IMPORT_ERROR}"
            raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=detail)
        def _load_candidate(candidate_id: str, processor_source: str) -> Tuple[Any, Any]:
            local_only = _hf_offline_enabled()
            model_local = _load_qwen_vl_model(str(candidate_id), load_kwargs, local_files_only=local_only)
            if adapter_path:
                model_local = PeftModel.from_pretrained(model_local, str(adapter_path))
            if not load_kwargs.get("device_map"):
                model_local.to(device)
            model_local.eval()
            processor_local = AutoProcessor.from_pretrained(
                processor_source,
                min_pixels=QWEN_MIN_PIXELS,
                max_pixels=QWEN_MAX_PIXELS,
                local_files_only=local_only,
            )
            return model_local, processor_local

        def _load_with_online_retry(candidate_id: str, processor_source: str) -> Tuple[Any, Any]:
            try:
                return _load_candidate(candidate_id, processor_source)
            except Exception as exc:  # noqa: BLE001
                if _hf_offline_enabled():
                    logger.warning("[qwen] offline load failed; retrying with HF online: %s", exc)
                    _set_hf_offline(False)
                    try:
                        return _load_candidate(candidate_id, processor_source)
                    finally:
                        _enable_hf_offline_defaults()
                raise

        try:
            processor_source = str(adapter_path) if adapter_path else str(base_model_id)
            model, processor = _load_with_online_retry(str(base_model_id), processor_source)
        except Exception as exc:  # noqa: BLE001
            fallback_id = _strip_qwen_model_suffix(str(base_model_id))
            if fallback_id:
                try:
                    logger.warning("Qwen model %s not found; falling back to %s", base_model_id, fallback_id)
                    processor_source = str(adapter_path) if adapter_path else str(fallback_id)
                    model, processor = _load_with_online_retry(str(fallback_id), processor_source)
                except Exception as fallback_exc:  # noqa: BLE001
                    qwen_last_error = str(fallback_exc)
                    detail = _format_qwen_load_error(fallback_exc)
                    raise HTTPException(
                        status_code=HTTP_503_SERVICE_UNAVAILABLE,
                        detail=f"qwen_load_failed:{detail}",
                    ) from fallback_exc
            else:
                qwen_last_error = str(exc)
                detail = _format_qwen_load_error(exc)
                raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=f"qwen_load_failed:{detail}") from exc
        qwen_model = model
        qwen_processor = processor
        qwen_device = device
        qwen_last_error = None
        loaded_qwen_model_id = active_qwen_model_id
        _enable_hf_offline_defaults()
        return model, processor


def _unload_qwen_runtime() -> None:
    """Release Qwen model/processor to free device memory."""
    global qwen_model, qwen_processor, qwen_device, loaded_qwen_model_id
    global qwen_caption_cache, qwen_caption_order
    try:
        del qwen_model
    except Exception:
        pass
    try:
        del qwen_processor
    except Exception:
        pass
    qwen_model = None
    qwen_processor = None
    loaded_qwen_model_id = None
    qwen_caption_cache = {}
    qwen_caption_order = deque()
    cuda_alloc = None
    cuda_reserved = None
    if torch.cuda.is_available():
        try:
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            cuda_alloc = int(torch.cuda.memory_allocated())
            cuda_reserved = int(torch.cuda.memory_reserved())
        except Exception:
            pass
    qwen_device = None
    try:
        gc.collect()
    except Exception:
        pass
    if cuda_alloc is not None or cuda_reserved is not None:
        logger.info(
            "[qwen] after unload: cuda_alloc=%s bytes cuda_reserved=%s bytes",
            cuda_alloc,
            cuda_reserved,
        )


def _evict_qwen_caption_entry(cache_key: str, cache_entry: Optional[Tuple[Any, Any]]) -> None:
    if not cache_entry:
        return
    try:
        model, processor = cache_entry
        try:
            del model
        except Exception:
            pass
        try:
            del processor
        except Exception:
            pass
    except Exception:
        pass
    if torch.cuda.is_available():
        try:
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
        except Exception:
            pass
    try:
        gc.collect()
    except Exception:
        pass


def _ensure_qwen_ready_for_caption(model_id_override: str) -> Tuple[Any, Any]:
    global qwen_device, qwen_last_error
    global qwen_caption_cache, qwen_caption_order
    if QWEN_IMPORT_ERROR is not None or Qwen3VLForConditionalGeneration is None or AutoProcessor is None or process_vision_info is None:
        detail = f"qwen_dependencies_missing:{QWEN_IMPORT_ERROR}"
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=detail)
    if packaging_version is not None:
        try:
            import transformers  # local import to avoid import-time failures

            if packaging_version.parse(transformers.__version__) < packaging_version.parse(QWEN_MIN_TRANSFORMERS):
                raise HTTPException(
                    status_code=HTTP_503_SERVICE_UNAVAILABLE,
                    detail=f"qwen_transformers_too_old:{transformers.__version__}<{QWEN_MIN_TRANSFORMERS}",
                )
        except HTTPException:
            raise
        except Exception:
            pass
    cache_key = f"caption:{model_id_override}"
    cache_limit = max(0, int(QWEN_CAPTION_CACHE_LIMIT or 0))
    if cache_limit == 0 and qwen_caption_cache:
        for key, entry in list(qwen_caption_cache.items()):
            _evict_qwen_caption_entry(key, entry)
        qwen_caption_cache.clear()
        qwen_caption_order.clear()
    cached = qwen_caption_cache.get(cache_key)
    if cached and cache_limit:
        try:
            qwen_caption_order.remove(cache_key)
        except ValueError:
            pass
        qwen_caption_order.append(cache_key)
        return cached
    with qwen_lock:
        cached = qwen_caption_cache.get(cache_key)
        if cached and cache_limit:
            try:
                qwen_caption_order.remove(cache_key)
            except ValueError:
                pass
            qwen_caption_order.append(cache_key)
            return cached
        try:
            device = _resolve_qwen_device()
        except RuntimeError as exc:  # noqa: BLE001
            qwen_last_error = str(exc)
            raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=f"qwen_device_unavailable:{exc}") from exc
        use_auto_map = QWEN_DEVICE_PREF == "auto" and device.startswith("cuda") and torch.cuda.is_available()
        if use_auto_map:
            load_kwargs = {
                "torch_dtype": "auto",
                "device_map": "auto",
            }
        else:
            dtype = torch.float16 if device.startswith(("cuda", "mps")) else torch.float32
            load_kwargs = {
                "torch_dtype": dtype,
                "low_cpu_mem_usage": True,
            }
        def _load_candidate(candidate_id: str) -> Tuple[Any, Any]:
            local_only = _hf_offline_enabled()
            model_local = _load_qwen_vl_model(str(candidate_id), load_kwargs, local_files_only=local_only)
            if not load_kwargs.get("device_map"):
                model_local.to(device)
            model_local.eval()
            processor_local = AutoProcessor.from_pretrained(
                str(candidate_id),
                min_pixels=QWEN_MIN_PIXELS,
                max_pixels=QWEN_MAX_PIXELS,
                local_files_only=local_only,
            )
            return model_local, processor_local

        def _load_with_online_retry(candidate_id: str) -> Tuple[Any, Any]:
            try:
                return _load_candidate(candidate_id)
            except Exception as exc:  # noqa: BLE001
                if _hf_offline_enabled():
                    logger.warning("[qwen] offline load failed; retrying with HF online: %s", exc)
                    _set_hf_offline(False)
                    try:
                        return _load_candidate(candidate_id)
                    finally:
                        _enable_hf_offline_defaults()
                raise

        try:
            model, processor = _load_with_online_retry(str(model_id_override))
        except Exception as exc:  # noqa: BLE001
            fallback_id = _strip_qwen_model_suffix(str(model_id_override))
            if fallback_id:
                try:
                    logger.warning("Qwen model %s not found; falling back to %s", model_id_override, fallback_id)
                    model, processor = _load_with_online_retry(str(fallback_id))
                    qwen_caption_cache[cache_key] = (model, processor)
                    qwen_caption_order.append(cache_key)
                except Exception as fallback_exc:  # noqa: BLE001
                    qwen_last_error = str(fallback_exc)
                    detail = _format_qwen_load_error(fallback_exc)
                    raise HTTPException(
                        status_code=HTTP_503_SERVICE_UNAVAILABLE,
                        detail=f"qwen_load_failed:{detail}",
                    ) from fallback_exc
            else:
                qwen_last_error = str(exc)
                detail = _format_qwen_load_error(exc)
                raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=f"qwen_load_failed:{detail}") from exc
        qwen_device = device
        qwen_last_error = None
        if cache_limit:
            qwen_caption_cache[cache_key] = (model, processor)
            qwen_caption_order.append(cache_key)
            while len(qwen_caption_order) > cache_limit:
                evict_key = qwen_caption_order.popleft()
                evict_model = qwen_caption_cache.pop(evict_key, None)
                _evict_qwen_caption_entry(evict_key, evict_model)
        if torch.cuda.is_available():
            try:
                torch.cuda.empty_cache()
            except Exception:
                pass
        _enable_hf_offline_defaults()
        return model, processor


def _resolve_qwen_variant_model_id(base_model_id: str, variant: Optional[str]) -> str:
    if not variant or variant == "auto":
        return base_model_id
    if "Thinking" in base_model_id:
        if variant == "Thinking":
            return base_model_id
        if variant == "Instruct":
            return base_model_id.replace("Thinking", "Instruct")
    if "Instruct" in base_model_id:
        if variant == "Instruct":
            return base_model_id
        if variant == "Thinking":
            return base_model_id.replace("Instruct", "Thinking")
    return base_model_id


def _strip_qwen_model_suffix(model_id: str) -> Optional[str]:
    if model_id.endswith("-2507"):
        return model_id[: -len("-2507")]
    return None


def _caption_glossary_map(labelmap_glossary: Optional[str], labels: Sequence[str]) -> Dict[str, List[str]]:
    if not labelmap_glossary:
        return {}
    return _parse_glossary_mapping(_normalize_labelmap_glossary(labelmap_glossary), list(labels))


def _caption_preferred_label(label: str, glossary_map: Optional[Dict[str, List[str]]] = None) -> str:
    label = str(label or "").strip()
    if not label:
        return ""
    terms = (glossary_map or {}).get(label) or []
    for term in terms:
        if term and "_" not in term:
            return str(term)
    if "_" in label:
        return label.replace("_", " ")
    return label


def _build_qwen_caption_prompt(
    user_prompt: str,
    label_hints: Sequence[QwenCaptionHint],
    image_width: int,
    image_height: int,
    include_counts: bool,
    include_coords: bool,
    max_boxes: int,
    detailed_mode: bool,
    restrict_to_labels: bool = True,
    labelmap_glossary: Optional[str] = None,
) -> Tuple[str, Dict[str, int], int, bool]:
    safe_width = max(1, int(image_width))
    safe_height = max(1, int(image_height))
    counts: Dict[str, int] = dict(Counter([hint.label for hint in label_hints if hint.label]))
    def _bbox_to_qwen_2d(bbox: Sequence[float]) -> List[int]:
        x1, y1, x2, y2 = bbox
        scale = 1000.0
        nx1 = int(round((x1 / safe_width) * scale))
        ny1 = int(round((y1 / safe_height) * scale))
        nx2 = int(round((x2 / safe_width) * scale))
        ny2 = int(round((y2 / safe_height) * scale))
        return [
            max(0, min(1000, nx1)),
            max(0, min(1000, ny1)),
            max(0, min(1000, nx2)),
            max(0, min(1000, ny2)),
        ]
    hints_payload = []
    for hint in label_hints:
        bbox = hint.bbox or []
        if len(bbox) == 4:
            x1, y1, x2, y2 = bbox
            x1 = max(0.0, min(float(x1), safe_width))
            y1 = max(0.0, min(float(y1), safe_height))
            x2 = max(0.0, min(float(x2), safe_width))
            y2 = max(0.0, min(float(y2), safe_height))
            if x2 <= x1 or y2 <= y1:
                continue
        else:
            x1 = y1 = x2 = y2 = None
        hints_payload.append(
            {
                "label": hint.label,
                "bbox": [x1, y1, x2, y2] if x1 is not None else None,
                "bbox_2d": _bbox_to_qwen_2d([x1, y1, x2, y2]) if x1 is not None else None,
                "confidence": hint.confidence if hint.confidence is not None else None,
                "area": (x2 - x1) * (y2 - y1) if x1 is not None else 0.0,
            }
        )
    if max_boxes <= 0:
        selected = sorted(
            hints_payload,
            key=lambda entry: (
                -(entry["confidence"] if entry["confidence"] is not None else 0.0),
                -entry["area"],
            ),
        )
        truncated = False
    else:
        sorted_hints = sorted(
            hints_payload,
            key=lambda entry: (
                -(entry["confidence"] if entry["confidence"] is not None else 0.0),
                -entry["area"],
            ),
        )
        selected = sorted_hints[:max_boxes]
        truncated = len(sorted_hints) > len(selected)
    lines: List[str] = []
    if user_prompt:
        lines.append(f"User hint: {user_prompt}")
        if "style inspirations" in user_prompt.lower():
            lines.append(
                "Style guidance: use inspirations for tone/angle only. Rephrase, do not copy wording."
            )
    lines.append(f"Image size: {safe_width}x{safe_height} pixels.")
    glossary_map = _caption_glossary_map(
        labelmap_glossary,
        list(counts.keys()) or [hint.label for hint in label_hints if hint.label],
    )

    # Build a forbidden token list for labelmap tags that should not appear verbatim.
    forbidden_labels: List[str] = []
    for lbl in sorted(set([hint.label for hint in label_hints if hint.label])):
        preferred = _caption_preferred_label(lbl, glossary_map)
        if "_" in lbl or (preferred and preferred.lower() != str(lbl).lower()):
            forbidden_labels.append(str(lbl))

    if include_counts and counts:
        counts_text = ", ".join(
            f"{_caption_preferred_label(label, glossary_map)}: {count}" for label, count in counts.items()
        )
        if restrict_to_labels:
            lines.append(f"COUNTS (use exactly): {counts_text}.")
            lines.append(
                "State these counts without qualifiers (avoid words like 'visible', 'roughly', or 'approximately')."
            )
        else:
            lines.append(f"COUNTS (use as hints; may be incomplete): {counts_text}.")
    elif counts:
        lines.append("Use the label hints to mention the main objects you see.")
    if counts and restrict_to_labels:
        allowed = ", ".join(sorted(_caption_preferred_label(lbl, glossary_map) for lbl in counts.keys()))
        if allowed:
            lines.append(
                f"Only mention these classes if they appear: {allowed}. Do not invent other entity types."
            )
    elif counts and not restrict_to_labels:
        lines.append("Label hints are suggestions; you may mention other visible objects too.")
    if selected:
        if include_coords:
            lines.append(
                "Labeled boxes (bbox_2d=[x1,y1,x2,y2], coords 0–1000 relative to this image/window):"
            )
            compact = [
                {"label": entry["label"], "bbox_2d": entry["bbox_2d"]}
                for entry in selected
                if entry["bbox_2d"] is not None
            ]
            if compact:
                lines.append(json.dumps(compact, separators=(",", ":")))
            lines.append("Use relative positions (e.g., top-left, center) when describing layout.")
        else:
            labels_only = ", ".join(_caption_preferred_label(entry["label"], glossary_map) for entry in selected)
            lines.append(f"Labeled objects (one per box): {labels_only}.")
    if forbidden_labels:
        lines.append(
            "Forbidden label tokens (do NOT output these exact strings): "
            + ", ".join(forbidden_labels)
            + ". Use natural synonyms instead."
        )
    elif hints_payload and not include_counts:
        lines.append("Labels provided but box details omitted.")
    if truncated:
        lines.append("Note: Only a subset of boxes is shown; counts reflect all hints.")
    lines.append(
        "Write a detailed caption. Use the image as truth and incorporate the label hints; "
        "if hints conflict with the image, mention the uncertainty briefly."
    )
    lines.append("Describe what the main objects are doing or how they are arranged when it is visible.")
    lines.append(
        "Be maximally descriptive: longer captions are acceptable when there is a lot to see. "
        "The labeled boxes are especially important and should be mentioned explicitly unless counts are overwhelming "
        "(e.g., summarize many cars as a parking lot)."
    )
    return "\n".join(lines), counts, len(selected), truncated


def _collapse_whitespace(text: str) -> str:
    return " ".join(text.split())


def _extract_caption_from_text(text: str, marker: Optional[str] = None) -> Tuple[str, bool]:
    cleaned = text.strip()
    marker_found = False
    if marker:
        match = re.search(rf"{marker}\\s*:?\\s*(.+)", cleaned, re.IGNORECASE | re.DOTALL)
        if match:
            cleaned = match.group(1)
            marker_found = True
    if not marker_found:
        match = re.search(r"FINAL\\s*:?\\s*(.+)", cleaned, re.IGNORECASE | re.DOTALL)
        if match:
            cleaned = match.group(1)
            marker_found = True
    cleaned = _collapse_whitespace(cleaned) if cleaned else text.strip()
    return cleaned, marker_found


def _caption_needs_english_rewrite(text: str) -> bool:
    return bool(re.search(r"[^\x00-\x7F]", text))

_CAPTION_GENERIC_OPENERS = (
    "an aerial view",
    "aerial view",
    "from a high angle",
    "a drone image",
    "a bird's-eye view",
    "overhead view",
)


def _caption_starts_generic(text: str) -> bool:
    lowered = text.strip().lower()
    return any(lowered.startswith(prefix) for prefix in _CAPTION_GENERIC_OPENERS)


def _caption_missing_labels(
    text: str,
    counts: Dict[str, int],
    glossary_map: Optional[Dict[str, List[str]]] = None,
) -> List[str]:
    if not text:
        return list(counts.keys())
    lowered = text.lower()
    missing = []
    for label, count in counts.items():
        if count <= 0:
            continue
        label_terms = [str(label)]
        if "_" in label:
            label_terms.append(label.replace("_", " "))
        if glossary_map and glossary_map.get(label):
            label_terms.extend(glossary_map[label])
        label_terms = [term.strip() for term in label_terms if term and term.strip()]
        if not any(term.lower() in lowered for term in label_terms):
            missing.append(label)
    return missing


def _caption_needs_refine(
    caption: str,
    counts: Dict[str, int],
    detailed_mode: bool,
    include_counts: bool,
    glossary_map: Optional[Dict[str, List[str]]] = None,
) -> Tuple[bool, List[str]]:
    words = caption.split() if caption else []
    min_words = 12 if detailed_mode else 8
    if len(words) < min_words:
        return True, []
    missing = _caption_missing_labels(caption, counts, glossary_map) if include_counts else []
    if missing:
        return True, missing
    if _caption_starts_generic(caption) and detailed_mode:
        return True, []
    return False, []

def _format_qwen_load_error(exc: Exception) -> str:
    msg = str(exc)
    if "FP8" in msg and "compute capability" in msg:
        cc = None
        if torch.cuda.is_available():
            try:
                major, minor = torch.cuda.get_device_capability(torch.cuda.current_device())
                cc = f"{major}.{minor}"
            except Exception:
                cc = None
        cc_note = f" Current GPU compute capability: {cc}." if cc else ""
        return (
            f"{msg} FP8 models require GPU compute capability >= 8.9 (e.g., 4090/H100)."
            f"{cc_note} Use a non-FP8 model on lower-capability GPUs."
        )
    return msg


def _sanitize_qwen_caption(text: str) -> str:
    if not text:
        return text
    cleaned = text.strip()
    final_match = re.search(r"<final>(.*?)</final>", cleaned, flags=re.IGNORECASE | re.DOTALL)
    if final_match:
        cleaned = final_match.group(1).strip()
    if re.search(r"</think>", cleaned, flags=re.IGNORECASE):
        parts = re.split(r"</think>", cleaned, flags=re.IGNORECASE)
        cleaned = parts[-1].strip()
    if re.search(r"\bFINAL\b", cleaned, flags=re.IGNORECASE):
        cleaned, _ = _extract_caption_from_text(cleaned, marker="FINAL")
    cleaned = cleaned.strip()
    if cleaned.startswith(":"):
        cleaned = cleaned.lstrip(":").strip()
    return cleaned


_QWEN_THINKING_REASONING_RE = re.compile(
    r"(?:\bgot it\b|\blet'?s\b|\bfirst\b|\bsecond\b|\bthird\b|\bstep\b|\bi need\b|\bnow\b|\bthe task\b)",
    re.IGNORECASE,
)
_QWEN_CAPTION_META_RE = re.compile(
    r"(authoritative|as indicated|label hint|bounding box|bbox|coordinates|hinted|counts are provided)",
    re.IGNORECASE,
)


def _thinking_caption_needs_cleanup(cleaned: str, raw: Optional[str]) -> bool:
    if not cleaned:
        return True
    if len(cleaned.split()) < 6:
        return True
    if raw and not re.search(r"<final>|\bFINAL\b", raw, re.IGNORECASE):
        return True
    if _QWEN_THINKING_REASONING_RE.search(cleaned):
        return True
    return False


def _caption_needs_completion(caption: str) -> bool:
    if not caption:
        return True
    trimmed = caption.strip()
    if not trimmed:
        return True
    return trimmed[-1] not in ".!?"


def _caption_has_meta(caption: str) -> bool:
    if not caption:
        return False
    return _QWEN_CAPTION_META_RE.search(caption) is not None


def _caption_needs_short_form(caption: str, max_words: int = 80, max_sentences: int = 2) -> bool:
    if not caption:
        return False
    words = caption.split()
    if len(words) > max_words:
        return True
    sentences = [s.strip() for s in re.split(r"[.!?]+", caption) if s.strip()]
    return len(sentences) > max_sentences


def _resolve_qwen_caption_decode(payload: QwenCaptionRequest, is_thinking: bool) -> Dict[str, Any]:
    use_sampling = payload.use_sampling if payload.use_sampling is not None else True
    if not use_sampling:
        return {"do_sample": False}
    defaults = {
        "temperature": 1.0 if is_thinking else 0.7,
        "top_p": 0.95 if is_thinking else 0.8,
        "top_k": 20,
        "presence_penalty": 0.0 if is_thinking else 1.5,
    }
    temperature = payload.temperature if payload.temperature is not None else defaults["temperature"]
    top_p = payload.top_p if payload.top_p is not None else defaults["top_p"]
    top_k = payload.top_k if payload.top_k is not None else defaults["top_k"]
    presence_penalty = (
        payload.presence_penalty if payload.presence_penalty is not None else defaults["presence_penalty"]
    )
    return {
        "do_sample": True,
        "temperature": temperature,
        "top_p": top_p,
        "top_k": top_k,
        "presence_penalty": presence_penalty,
    }


def _adjust_prompt_for_thinking(prompt_text: str) -> str:
    if not prompt_text:
        return prompt_text
    lines = prompt_text.splitlines()
    filtered = [line for line in lines if not line.startswith("Write a concise caption")]
    return "\n".join(filtered)


def _run_qwen_caption_cleanup(
    prompt: str,
    pil_img: Image.Image,
    max_new_tokens: int,
    base_model_id: str,
    use_caption_cache: bool,
    model_id_override: Optional[str] = None,
    runtime_override: Optional[Tuple[Any, Any]] = None,
    allowed_labels: Optional[List[str]] = None,
    strict: bool = False,
    minimal_edit: bool = False,
) -> str:
    allowed_note = ""
    if allowed_labels:
        allowed_note = (
            f"Only mention these classes if they appear: {', '.join(sorted(set(allowed_labels)))}. "
            "Do not introduce any other entity types. "
        )
    strict_note = "Return exactly one complete sentence. " if strict else ""
    minimal_note = (
        "Edit the draft with minimal changes. Do not introduce new objects or actions. "
        if minimal_edit
        else ""
    )
    cleanup_system = (
        "You are a captioning assistant. Respond in English only. "
        "Return only <final>...</final> and nothing else."
    )
    cleanup_prompt = (
        f"{strict_note}{allowed_note}{minimal_note}"
        "Remove repetition, avoid coordinates, and remove any mention of labels, hints, or counts being provided. "
        "Keep the caption grounded in the image.\n"
        f"Draft caption: {prompt}"
    )
    cleanup_model = model_id_override or _resolve_qwen_variant_model_id(base_model_id, "Instruct")
    qwen_text, _, _ = _run_qwen_inference(
        cleanup_prompt,
        pil_img,
        max_new_tokens=max_new_tokens,
        system_prompt_override=cleanup_system,
        model_id_override=cleanup_model if use_caption_cache and runtime_override is None else None,
        runtime_override=runtime_override,
        decode_override={"do_sample": False},
    )
    caption_text, _ = _extract_caption_from_text(qwen_text, marker=None)
    return _sanitize_qwen_caption(caption_text)


def _run_qwen_caption_merge(
    draft_caption: str,
    windowed_captions: Sequence[Tuple[int, int, int, str]],
    *,
    pil_img: Image.Image,
    base_model_id: str,
    runtime_resolver: Callable[[str], Tuple[Any, Any]],
    max_new_tokens: int,
    glossary_line: Optional[str] = None,
) -> str:
    if not draft_caption or not windowed_captions:
        return draft_caption
    window_lines = ["Window observations (do NOT invent objects):"]
    for x0, y0, size, caption in windowed_captions:
        if caption:
            window_lines.append(f"- {caption}")
    if len(window_lines) == 1:
        return draft_caption
    merge_prompt = (
        "Revise the draft caption so it includes all distinct object details "
        "from the window observations that are missing in the draft. "
        "Do not invent new objects. Use multiple sentences if needed. "
        "Preserve specific counts, actions, and notable attributes from the windows; "
        "do not drop any concrete window detail unless it clearly conflicts with the full image. "
        "Do not mention labels, hints, or coordinates.\n"
        f"Draft caption: {draft_caption}\n"
        + "\n".join(window_lines)
    )
    if glossary_line:
        merge_prompt = f"{merge_prompt}\n{glossary_line}"
    merge_system = (
        "You are a caption editor. Return only the revised caption in English."
    )
    merge_model = _resolve_qwen_variant_model_id(base_model_id, "Instruct")
    qwen_text, _, _ = _run_qwen_inference(
        merge_prompt,
        pil_img,
        max_new_tokens=max_new_tokens,
        system_prompt_override=merge_system,
        runtime_override=runtime_resolver(merge_model),
        decode_override={"do_sample": False},
    )
    merged, _ = _extract_caption_from_text(qwen_text, marker=None)
    merged = _sanitize_qwen_caption(merged)
    return merged or draft_caption


def _resolve_qwen_window_size(
    requested: Optional[int],
    image_width: int,
    image_height: int,
    *,
    overlap: Optional[float] = None,
) -> int:
    if requested is None:
        overlap_val = _resolve_qwen_window_overlap(overlap)
        base_dim = max(1, min(int(image_width), int(image_height)))
        # 2x2 grid with overlap -> window = dim / (2 - overlap)
        base = base_dim / max(1.0, 2.0 - overlap_val)
    else:
        base = requested
    try:
        base = int(base)
    except (TypeError, ValueError):
        base = QWEN_WINDOW_DEFAULT_SIZE
    base = max(128, min(base, 4096))
    return max(64, min(base, int(image_width), int(image_height)))


def _resolve_qwen_window_overlap(requested: Optional[float]) -> float:
    try:
        overlap = float(requested) if requested is not None else QWEN_WINDOW_DEFAULT_OVERLAP
    except (TypeError, ValueError):
        overlap = QWEN_WINDOW_DEFAULT_OVERLAP
    return max(0.0, min(overlap, 0.2))


def _window_positions(
    total: int,
    window: int,
    overlap: float,
    *,
    force_two: bool = False,
) -> List[int]:
    if total <= window:
        return [0]
    if force_two:
        return [0, max(0, total - window)]
    step = max(1, int(round(window * (1.0 - overlap))))
    positions = list(range(0, max(1, total - window + 1), step))
    last = total - window
    if positions[-1] != last:
        positions.append(last)
    return sorted(set(positions))


def _allowed_caption_labels(label_hints: Sequence[QwenCaptionHint]) -> List[str]:
    labels = [hint.label for hint in label_hints if hint.label]
    return sorted(set(str(label) for label in labels if str(label).strip()))


def _caption_is_degenerate(caption: str) -> bool:
    if not caption:
        return True
    trimmed = caption.strip()
    if not trimmed:
        return True
    if _QWEN_THINKING_REASONING_RE.search(trimmed):
        return True
    compact = re.sub(r"\\s+", "", trimmed)
    if compact:
        alnum = re.findall(r"[A-Za-z0-9]", compact)
        if not alnum and len(compact) > 10:
            return True
        if len(compact) >= 20:
            ratio = len(alnum) / max(1, len(compact))
            if ratio < 0.2:
                return True
        if re.fullmatch(r"[^A-Za-z0-9]+", compact):
            return True
        if re.search(r"([!?.<>\\-_=])\\1{20,}", compact):
            return True
    words = caption.split()
    if len(words) < 8:
        return True
    sentences = [s.strip().lower() for s in re.split(r"[.!?]+", caption) if s.strip()]
    if sentences:
        counts = Counter(sentences)
        most_common = counts.most_common(1)[0][1]
        if most_common >= 3:
            return True
        if most_common / max(1, len(sentences)) > 0.45:
            return True
    if len(words) > 40:
        tokens = [w.lower() for w in words]
        bigrams = list(zip(tokens, tokens[1:]))
        if bigrams:
            unique_ratio = len(set(bigrams)) / len(bigrams)
            if unique_ratio < 0.55:
                return True
    return False


def _group_hints_by_window(
    label_hints: Sequence[QwenCaptionHint],
    x_positions: Sequence[int],
    y_positions: Sequence[int],
    window: int,
) -> Dict[Tuple[int, int], List[QwenCaptionHint]]:
    grouped: Dict[Tuple[int, int], List[QwenCaptionHint]] = {(x0, y0): [] for y0 in y_positions for x0 in x_positions}
    for hint in label_hints:
        if not hint.bbox or len(hint.bbox) != 4:
            continue
        bx1, by1, bx2, by2 = hint.bbox
        try:
            cx = (float(bx1) + float(bx2)) * 0.5
            cy = (float(by1) + float(by2)) * 0.5
        except (TypeError, ValueError):
            continue
        x0_match = None
        for x0 in x_positions:
            if x0 <= cx <= x0 + window:
                x0_match = x0
                break
        if x0_match is None:
            continue
        y0_match = None
        for y0 in y_positions:
            if y0 <= cy <= y0 + window:
                y0_match = y0
                break
        if y0_match is None:
            continue
        nx1 = max(0.0, min(float(bx1) - x0_match, window))
        ny1 = max(0.0, min(float(by1) - y0_match, window))
        nx2 = max(0.0, min(float(bx2) - x0_match, window))
        ny2 = max(0.0, min(float(by2) - y0_match, window))
        if nx2 <= nx1 or ny2 <= ny1:
            continue
        grouped[(x0_match, y0_match)].append(
            QwenCaptionHint(
                label=hint.label,
                bbox=[nx1, ny1, nx2, ny2],
                confidence=hint.confidence,
            )
        )
    return grouped


def _filter_hints_for_window(
    label_hints: Sequence[QwenCaptionHint],
    x0: int,
    y0: int,
    window: int,
    image_width: int,
    image_height: int,
) -> List[QwenCaptionHint]:
    # Deprecated: use _group_hints_by_window to avoid duplicates.
    window_hints: List[QwenCaptionHint] = []
    x1 = x0 + window
    y1 = y0 + window
    for hint in label_hints:
        if not hint.bbox or len(hint.bbox) != 4:
            continue
        bx1, by1, bx2, by2 = hint.bbox
        try:
            cx = (float(bx1) + float(bx2)) * 0.5
            cy = (float(by1) + float(by2)) * 0.5
        except (TypeError, ValueError):
            continue
        if cx < x0 or cx > x1 or cy < y0 or cy > y1:
            continue
        nx1 = max(0.0, min(float(bx1) - x0, window))
        ny1 = max(0.0, min(float(by1) - y0, window))
        nx2 = max(0.0, min(float(bx2) - x0, window))
        ny2 = max(0.0, min(float(by2) - y0, window))
        if nx2 <= nx1 or ny2 <= ny1:
            continue
        window_hints.append(
            QwenCaptionHint(
                label=hint.label,
                bbox=[nx1, ny1, nx2, ny2],
                confidence=hint.confidence,
            )
        )
    return window_hints


def _run_qwen_inference(
    prompt: str,
    pil_img: Image.Image,
    max_new_tokens: Optional[int] = None,
    system_prompt_override: Optional[str] = None,
    model_id_override: Optional[str] = None,
    runtime_override: Optional[Tuple[Any, Any]] = None,
    decode_override: Optional[Dict[str, Any]] = None,
) -> Tuple[str, int, int]:
    """Execute a Qwen 3 VL inference following the reference recipe."""
    if runtime_override is not None:
        model, processor = runtime_override
    elif model_id_override:
        model, processor = _ensure_qwen_ready_for_caption(model_id_override)
    else:
        model, processor = _ensure_qwen_ready()
    messages: List[Dict[str, Any]] = []
    sys_prompt = system_prompt_override if system_prompt_override is not None else (active_qwen_metadata or {}).get("system_prompt")
    if sys_prompt:
        messages.append(
            {
                "role": "system",
                "content": [{"type": "text", "text": sys_prompt}],
            }
        )
    messages.append(
        {
            "role": "user",
            "content": [
                {"type": "image", "image": pil_img},
                {"type": "text", "text": prompt},
            ],
        }
    )
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    _agent_full_trace_write(
        {
            "type": "llm_input",
            "source": "qwen_inference",
            "model_id": model_id_override or (active_qwen_metadata or {}).get("model_id") or QWEN_MODEL_NAME,
            "messages": messages,
            "prompt_text": text,
            "max_new_tokens": int(max_new_tokens) if max_new_tokens is not None else None,
            "decode_override": decode_override,
        }
    )
    image_inputs, video_inputs = process_vision_info(messages)
    max_seq_len = _resolve_qwen_max_seq_len(model)
    max_input_len = None
    requested_max = int(max_new_tokens) if max_new_tokens is not None else QWEN_MAX_NEW_TOKENS
    if max_seq_len:
        if requested_max >= max_seq_len:
            requested_max = max(1, max_seq_len - 1)
    preview_inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        truncation=False,
        return_tensors="pt",
    )
    input_len = int(preview_inputs.input_ids.shape[1])
    num_images = len(image_inputs) if image_inputs is not None else 0
    effective_len, vision_tokens = _qwen_effective_input_len(preview_inputs, input_len, num_images)
    if max_seq_len:
        if effective_len + requested_max > max_seq_len:
            requested_max = max(1, max_seq_len - effective_len)
        if effective_len > max_seq_len:
            logger.warning(
                "[qwen] effective input length %s exceeds max_seq_len %s; truncating prompt.",
                effective_len,
                max_seq_len,
            )
            if vision_tokens is not None:
                max_input_len = max(1, max_seq_len - requested_max - vision_tokens + num_images)
            else:
                max_input_len = max(1, max_seq_len - requested_max)
    max_new_tokens = requested_max
    if max_input_len is None:
        inputs = preview_inputs
    else:
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            truncation=True,
            max_length=max_input_len,
            return_tensors="pt",
        )
    device = qwen_device or _resolve_qwen_device()
    inputs = inputs.to(device)
    gen_kwargs: Dict[str, Any] = {
        "max_new_tokens": int(max_new_tokens) if max_new_tokens is not None else QWEN_MAX_NEW_TOKENS,
    }
    use_sampling = QWEN_DO_SAMPLE
    if decode_override is not None and "do_sample" in decode_override:
        use_sampling = bool(decode_override.get("do_sample"))
    if not use_sampling:
        gen_config = getattr(model, "generation_config", None)
        if gen_config is not None and hasattr(gen_config, "clone"):
            try:
                gen_config = gen_config.clone()
            except Exception:
                gen_config = None
        if gen_config is not None:
            for attr in ("temperature", "top_p", "top_k"):
                if hasattr(gen_config, attr):
                    setattr(gen_config, attr, None)
            if hasattr(gen_config, "do_sample"):
                gen_config.do_sample = False
            gen_kwargs["generation_config"] = gen_config
    if use_sampling:
        temperature = QWEN_TEMPERATURE
        top_p = QWEN_TOP_P
        top_k = None
        presence_penalty = None
        if decode_override is not None:
            temperature = decode_override.get("temperature", temperature)
            top_p = decode_override.get("top_p", top_p)
            top_k = decode_override.get("top_k", top_k)
            presence_penalty = decode_override.get("presence_penalty", presence_penalty)
        gen_kwargs.update(
            {
                "do_sample": True,
                "temperature": temperature,
                "top_p": top_p,
            }
        )
        if top_k is not None:
            gen_kwargs["top_k"] = int(top_k)
        if presence_penalty is not None and _qwen_supports_presence_penalty(model):
            gen_kwargs["presence_penalty"] = float(presence_penalty)
    else:
        gen_kwargs["do_sample"] = False
    with torch.inference_mode():
        try:
            generated_ids = model.generate(**inputs, **gen_kwargs)
        except RuntimeError as exc:
            if QWEN_DO_SAMPLE and "probability tensor" in str(exc).lower():
                fallback_kwargs = {**gen_kwargs}
                fallback_kwargs["do_sample"] = False
                fallback_kwargs.pop("temperature", None)
                fallback_kwargs.pop("top_p", None)
                generated_ids = model.generate(**inputs, **fallback_kwargs)
            else:
                raise
    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False,
    )[0]
    _agent_full_trace_write(
        {
            "type": "llm_output",
            "source": "qwen_inference",
            "model_id": model_id_override or (active_qwen_metadata or {}).get("model_id") or QWEN_MODEL_NAME,
            "output_text": output_text,
        }
    )
    grid = inputs.get("image_grid_thw")
    patch_size = 14
    try:
        vision_cfg = getattr(model, "config", None)
        vision_cfg = getattr(vision_cfg, "vision_config", None)
        if vision_cfg is not None and getattr(vision_cfg, "patch_size", None):
            patch_size = int(vision_cfg.patch_size)
        elif getattr(processor, "image_processor", None) is not None:
            patch = getattr(processor.image_processor, "patch_size", None)
            if patch:
                patch_size = int(patch)
    except Exception:
        patch_size = 14
    if grid is not None:
        grid_values = grid[0]
        input_height = int(grid_values[1].item() * patch_size)
        input_width = int(grid_values[2].item() * patch_size)
    else:
        input_height = pil_img.height
        input_width = pil_img.width
    return output_text, input_width, input_height


def _run_qwen_chat(
    messages: List[Dict[str, Any]],
    *,
    max_new_tokens: Optional[int] = None,
    model_id_override: Optional[str] = None,
    runtime_override: Optional[Tuple[Any, Any]] = None,
    decode_override: Optional[Dict[str, Any]] = None,
    tools: Optional[List[Dict[str, Any]]] = None,
    chat_template_kwargs: Optional[Dict[str, Any]] = None,
    add_generation_prompt: bool = True,
    assistant_prefix: Optional[str] = None,
    thinking_effort: Optional[float] = None,
    thinking_scale_factor: Optional[float] = None,
    immediate_action_bias: Optional[bool] = None,
    immediate_action_min_chars: Optional[int] = None,
    immediate_action_min_seconds: Optional[float] = None,
    immediate_action_logit_bias: Optional[float] = None,
) -> str:
    if runtime_override is not None:
        model, processor = runtime_override
    elif model_id_override:
        model, processor = _ensure_qwen_ready_for_caption(model_id_override)
    else:
        model, processor = _ensure_qwen_ready()
    if assistant_prefix:
        add_generation_prompt = True
    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=bool(add_generation_prompt),
        tools=tools,
        chat_template_kwargs=chat_template_kwargs,
    )
    if assistant_prefix:
        text = f"{text}{assistant_prefix}"
    _agent_full_trace_write(
        {
            "type": "llm_input",
            "model_id": model_id_override or (active_qwen_metadata or {}).get("model_id") or QWEN_MODEL_NAME,
            "messages": messages,
            "prompt_text": text,
            "max_new_tokens": int(max_new_tokens) if max_new_tokens is not None else None,
            "decode_override": decode_override,
            "thinking_effort": thinking_effort,
            "thinking_scale_factor": thinking_scale_factor,
            "immediate_action_bias": immediate_action_bias,
            "immediate_action_min_chars": immediate_action_min_chars,
            "immediate_action_min_seconds": immediate_action_min_seconds,
            "immediate_action_logit_bias": immediate_action_logit_bias,
            "tools": tools,
            "chat_template_kwargs": chat_template_kwargs,
            "assistant_prefix": assistant_prefix,
        }
    )
    tokenizer = getattr(processor, "tokenizer", None)
    image_inputs, video_inputs = process_vision_info(messages)
    max_seq_len = _resolve_qwen_max_seq_len(model)
    requested_max = int(max_new_tokens) if max_new_tokens is not None else QWEN_MAX_NEW_TOKENS
    if max_seq_len:
        if requested_max >= max_seq_len:
            requested_max = max(1, max_seq_len - 1)
    preview_inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        truncation=False,
        return_tensors="pt",
    )
    input_len = int(preview_inputs.input_ids.shape[1])
    num_images = len(image_inputs) if image_inputs is not None else 0
    effective_len, vision_tokens = _qwen_effective_input_len(preview_inputs, input_len, num_images)
    max_input_len = None
    if max_seq_len:
        if effective_len + requested_max > max_seq_len:
            requested_max = max(1, max_seq_len - effective_len)
        if effective_len > max_seq_len:
            if vision_tokens is not None:
                max_input_len = max(1, max_seq_len - requested_max - vision_tokens + num_images)
            else:
                max_input_len = max(1, max_seq_len - requested_max)
    max_new_tokens = requested_max
    if max_input_len is None:
        inputs = preview_inputs
    else:
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            truncation=True,
            max_length=max_input_len,
            return_tensors="pt",
        )
    device = qwen_device or _resolve_qwen_device()
    inputs = inputs.to(device)
    gen_kwargs: Dict[str, Any] = {
        "max_new_tokens": int(max_new_tokens) if max_new_tokens is not None else QWEN_MAX_NEW_TOKENS,
    }
    use_sampling = QWEN_DO_SAMPLE
    if decode_override is not None and "do_sample" in decode_override:
        use_sampling = bool(decode_override.get("do_sample"))
    if use_sampling:
        temperature = QWEN_TEMPERATURE
        top_p = QWEN_TOP_P
        top_k = None
        presence_penalty = None
        if decode_override is not None:
            temperature = decode_override.get("temperature", temperature)
            top_p = decode_override.get("top_p", top_p)
            top_k = decode_override.get("top_k", top_k)
            presence_penalty = decode_override.get("presence_penalty", presence_penalty)
        gen_kwargs.update(
            {
                "do_sample": True,
                "temperature": temperature,
                "top_p": top_p,
            }
        )
        if top_k is not None:
            gen_kwargs["top_k"] = int(top_k)
        if presence_penalty is not None and _qwen_supports_presence_penalty(model):
            gen_kwargs["presence_penalty"] = float(presence_penalty)
    else:
        gen_kwargs["do_sample"] = False
    thinking_processor = _qwen_build_thinking_effort_processor(tokenizer, thinking_effort, thinking_scale_factor)
    immediate_processor = _qwen_build_immediate_action_processor(
        tokenizer,
        immediate_action_bias,
        immediate_action_min_chars,
        immediate_action_min_seconds,
        immediate_action_logit_bias,
    )
    _qwen_append_logits_processor(gen_kwargs, thinking_processor)
    _qwen_append_logits_processor(gen_kwargs, immediate_processor)
    with torch.inference_mode():
        output_ids = model.generate(**inputs, **gen_kwargs)
    output_ids = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, output_ids)]
    output_text = processor.batch_decode(
        output_ids,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False,
    )[0]
    _agent_full_trace_write(
        {
            "type": "llm_output",
            "model_id": model_id_override or (active_qwen_metadata or {}).get("model_id") or QWEN_MODEL_NAME,
            "output_text": output_text,
        }
    )
    return output_text


def _run_qwen_chat_stream(
    messages: List[Dict[str, Any]],
    *,
    max_new_tokens: Optional[int] = None,
    model_id_override: Optional[str] = None,
    runtime_override: Optional[Tuple[Any, Any]] = None,
    decode_override: Optional[Dict[str, Any]] = None,
    tools: Optional[List[Dict[str, Any]]] = None,
    chat_template_kwargs: Optional[Dict[str, Any]] = None,
    add_generation_prompt: bool = True,
    assistant_prefix: Optional[str] = None,
    thinking_effort: Optional[float] = None,
    thinking_scale_factor: Optional[float] = None,
    immediate_action_bias: Optional[bool] = None,
    immediate_action_min_chars: Optional[int] = None,
    immediate_action_min_seconds: Optional[float] = None,
    immediate_action_logit_bias: Optional[float] = None,
) -> Iterator[str]:
    if runtime_override is not None:
        model, processor = runtime_override
    elif model_id_override:
        model, processor = _ensure_qwen_ready_for_caption(model_id_override)
    else:
        model, processor = _ensure_qwen_ready()
    if assistant_prefix:
        add_generation_prompt = True
    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=bool(add_generation_prompt),
        tools=tools,
        chat_template_kwargs=chat_template_kwargs,
    )
    if assistant_prefix:
        text = f"{text}{assistant_prefix}"
    _agent_full_trace_write(
        {
            "type": "llm_input",
            "model_id": model_id_override or (active_qwen_metadata or {}).get("model_id") or QWEN_MODEL_NAME,
            "messages": messages,
            "prompt_text": text,
            "max_new_tokens": int(max_new_tokens) if max_new_tokens is not None else None,
            "decode_override": decode_override,
            "thinking_effort": thinking_effort,
            "thinking_scale_factor": thinking_scale_factor,
            "tools": tools,
            "chat_template_kwargs": chat_template_kwargs,
            "assistant_prefix": assistant_prefix,
        }
    )
    tokenizer = getattr(processor, "tokenizer", None)
    if tokenizer is None:
        output_text = _run_qwen_chat(
            messages,
            max_new_tokens=max_new_tokens,
            model_id_override=model_id_override,
            runtime_override=(model, processor),
            decode_override=decode_override,
            tools=tools,
            chat_template_kwargs=chat_template_kwargs,
            add_generation_prompt=add_generation_prompt,
            assistant_prefix=assistant_prefix,
            thinking_effort=thinking_effort,
            thinking_scale_factor=thinking_scale_factor,
            immediate_action_bias=immediate_action_bias,
            immediate_action_min_chars=immediate_action_min_chars,
            immediate_action_min_seconds=immediate_action_min_seconds,
            immediate_action_logit_bias=immediate_action_logit_bias,
        )
        yield output_text
        return
    try:
        from transformers import TextIteratorStreamer
    except Exception:
        output_text = _run_qwen_chat(
            messages,
            max_new_tokens=max_new_tokens,
            model_id_override=model_id_override,
            runtime_override=(model, processor),
            decode_override=decode_override,
            tools=tools,
            chat_template_kwargs=chat_template_kwargs,
            add_generation_prompt=add_generation_prompt,
            assistant_prefix=assistant_prefix,
            thinking_effort=thinking_effort,
            thinking_scale_factor=thinking_scale_factor,
            immediate_action_bias=immediate_action_bias,
            immediate_action_min_chars=immediate_action_min_chars,
            immediate_action_min_seconds=immediate_action_min_seconds,
            immediate_action_logit_bias=immediate_action_logit_bias,
        )
        yield output_text
        return
    image_inputs, video_inputs = process_vision_info(messages)
    max_seq_len = _resolve_qwen_max_seq_len(model)
    requested_max = int(max_new_tokens) if max_new_tokens is not None else QWEN_MAX_NEW_TOKENS
    if max_seq_len:
        if requested_max >= max_seq_len:
            requested_max = max(1, max_seq_len - 1)
    preview_inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        truncation=False,
        return_tensors="pt",
    )
    input_len = int(preview_inputs.input_ids.shape[1])
    num_images = len(image_inputs) if image_inputs is not None else 0
    effective_len, vision_tokens = _qwen_effective_input_len(preview_inputs, input_len, num_images)
    max_input_len = None
    if max_seq_len:
        if effective_len + requested_max > max_seq_len:
            requested_max = max(1, max_seq_len - effective_len)
        if effective_len > max_seq_len:
            if vision_tokens is not None:
                max_input_len = max(1, max_seq_len - requested_max - vision_tokens + num_images)
            else:
                max_input_len = max(1, max_seq_len - requested_max)
            max_input_len = max(1, max_seq_len - requested_max)
    max_new_tokens = requested_max
    if max_input_len is None:
        inputs = preview_inputs
    else:
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            truncation=True,
            max_length=max_input_len,
            return_tensors="pt",
        )
    device = qwen_device or _resolve_qwen_device()
    inputs = inputs.to(device)
    gen_kwargs: Dict[str, Any] = {
        "max_new_tokens": int(max_new_tokens) if max_new_tokens is not None else QWEN_MAX_NEW_TOKENS,
    }
    use_sampling = QWEN_DO_SAMPLE
    if decode_override is not None and "do_sample" in decode_override:
        use_sampling = bool(decode_override.get("do_sample"))
    if use_sampling:
        temperature = QWEN_TEMPERATURE
        top_p = QWEN_TOP_P
        top_k = None
        presence_penalty = None
        if decode_override is not None:
            temperature = decode_override.get("temperature", temperature)
            top_p = decode_override.get("top_p", top_p)
            top_k = decode_override.get("top_k", top_k)
            presence_penalty = decode_override.get("presence_penalty", presence_penalty)
        gen_kwargs.update(
            {
                "do_sample": True,
                "temperature": temperature,
                "top_p": top_p,
            }
        )
        if top_k is not None:
            gen_kwargs["top_k"] = int(top_k)
        if presence_penalty is not None and _qwen_supports_presence_penalty(model):
            gen_kwargs["presence_penalty"] = float(presence_penalty)
    else:
        gen_kwargs["do_sample"] = False
    thinking_processor = _qwen_build_thinking_effort_processor(tokenizer, thinking_effort, thinking_scale_factor)
    immediate_processor = _qwen_build_immediate_action_processor(
        tokenizer,
        immediate_action_bias,
        immediate_action_min_chars,
        immediate_action_min_seconds,
        immediate_action_logit_bias,
    )
    _qwen_append_logits_processor(gen_kwargs, thinking_processor)
    _qwen_append_logits_processor(gen_kwargs, immediate_processor)
    streamer = TextIteratorStreamer(
        tokenizer,
        skip_prompt=True,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False,
        timeout=600.0,
    )
    gen_kwargs["streamer"] = streamer
    generated_text = ""

    def _generate() -> None:
        with torch.inference_mode():
            model.generate(**inputs, **gen_kwargs)

    thread = threading.Thread(target=_generate, name="qwen-chat-streamer", daemon=True)
    thread.start()
    try:
        for new_text in streamer:
            generated_text += new_text
            yield generated_text
    except queue.Empty:
        _agent_full_trace_write(
            {
                "type": "llm_stream_timeout",
                "model_id": model_id_override or (active_qwen_metadata or {}).get("model_id") or QWEN_MODEL_NAME,
                "generated_chars": len(generated_text),
            }
        )
    thread.join()
    _agent_full_trace_write(
        {
            "type": "llm_output",
            "model_id": model_id_override or (active_qwen_metadata or {}).get("model_id") or QWEN_MODEL_NAME,
            "output_text": generated_text,
        }
    )


def _load_qwen_vl_model(model_id: str, load_kwargs: Dict[str, Any], local_files_only: bool = False) -> Any:
    if AutoConfig is None or AutoModelForCausalLM is None:
        return Qwen3VLForConditionalGeneration.from_pretrained(
            str(model_id), local_files_only=local_files_only, **load_kwargs
        )
    if not QWEN_TRUST_REMOTE_CODE:
        if _is_qwen_moe_model_id(str(model_id)) and Qwen3VLMoeForConditionalGeneration is not None:
            return Qwen3VLMoeForConditionalGeneration.from_pretrained(
                str(model_id), local_files_only=local_files_only, **load_kwargs
            )
        try:
            config = AutoConfig.from_pretrained(
                str(model_id), trust_remote_code=False, local_files_only=local_files_only
            )
            model_type = getattr(config, "model_type", None)
            if model_type == "qwen3_vl_moe" and Qwen3VLMoeForConditionalGeneration is not None:
                return Qwen3VLMoeForConditionalGeneration.from_pretrained(
                    str(model_id), local_files_only=local_files_only, **load_kwargs
                )
            if model_type not in (None, "qwen3_vl", "qwen3_vl_moe"):
                logging.warning(
                    "Qwen model_type=%s may require trust_remote_code; set QWEN_TRUST_REMOTE_CODE=1 to enable.",
                    model_type,
                )
        except Exception as exc:
            logging.warning(
                "Qwen config load failed without trust_remote_code; set QWEN_TRUST_REMOTE_CODE=1 if needed. (%s)",
                exc,
            )
        return Qwen3VLForConditionalGeneration.from_pretrained(
            str(model_id), local_files_only=local_files_only, **load_kwargs
        )
    try:
        config = AutoConfig.from_pretrained(
            str(model_id), trust_remote_code=True, local_files_only=local_files_only
        )
    except Exception:
        config = None
    if config is not None:
        model_type = getattr(config, "model_type", None)
        if model_type == "qwen3_vl_moe" and Qwen3VLMoeForConditionalGeneration is not None:
            return Qwen3VLMoeForConditionalGeneration.from_pretrained(
                str(model_id), local_files_only=local_files_only, **load_kwargs
            )
        if model_type not in (None, "qwen3_vl", "qwen3_vl_moe"):
            return AutoModelForCausalLM.from_pretrained(
                str(model_id), trust_remote_code=True, local_files_only=local_files_only, **load_kwargs
            )
    return Qwen3VLForConditionalGeneration.from_pretrained(
        str(model_id), local_files_only=local_files_only, **load_kwargs
    )


def _generate_qwen_text(
    prompt: str,
    *,
    max_new_tokens: int = 128,
    use_system_prompt: bool = True,
) -> str:
    """Text-only generation with Qwen for small helper tasks (no images)."""
    model, processor = _ensure_qwen_ready()
    messages: List[Dict[str, Any]] = []
    sys_prompt = (active_qwen_metadata or {}).get("system_prompt")
    if use_system_prompt and sys_prompt:
        messages.append(
            {
                "role": "system",
                "content": [{"type": "text", "text": sys_prompt}],
            }
        )
    messages.append({"role": "user", "content": [{"type": "text", "text": prompt}]})
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = processor(
        text=[text],
        padding=True,
        return_tensors="pt",
    )
    device = qwen_device or _resolve_qwen_device()
    inputs = inputs.to(device)
    with torch.inference_mode():
        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, temperature=0.0, top_p=1.0)
    input_len = inputs["input_ids"].shape[1]
    generated_ids = outputs[:, input_len:]
    decoded = processor.batch_decode(
        generated_ids,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False,
    )[0]
    return decoded.strip()


def _parse_prompt_candidates(raw: str, seen: set[str], limit: int) -> List[str]:
    """Parse and validate a comma/list output into cleaned candidates; returns [] if invalid."""
    if not raw:
        return []
    parts = re.split(r"[,;\n]+", raw)
    parsed: List[str] = []
    for part in parts:
        cand = part.strip().strip('"').strip("'")
        cand = re.sub(r"(?i)^assistant\s+final[:\s]+", "", cand)
        if not cand:
            continue
        if cand.upper() == "STOP":
            break
        # Must be letters/spaces/hyphens only.
        if re.search(r"[^A-Za-z\s\-]", cand):
            continue
        words = cand.split()
        if not (1 <= len(words) <= 4):
            continue
        if any(len(w) < 2 for w in words):
            continue
        key = cand.lower()
        if key in seen:
            continue
        seen.add(key)
        parsed.append(cand)
        if limit and len(parsed) >= limit:
            break
    return parsed


def _generate_prompt_text(
    prompt: str,
    *,
    max_new_tokens: int = 128,
) -> str:
    """
    Text-only helper for prompt brainstorming/critique.
    Uses Qwen (text-only) and returns empty string on failure.
    """
    try:
        helper_prompt = (
            "You generate short noun-phrase candidates for open-vocabulary detection. "
            "Respond with a comma-separated list only (no prose). "
            "Each candidate: 1-3 words, letters/spaces/hyphens only, no numbers, no quotes, no JSON. "
            "If no valid candidates, return an empty list.\n\n"
            f"Task: {prompt}"
        )
        text = _generate_qwen_text(helper_prompt, max_new_tokens=max_new_tokens, use_system_prompt=False)
        return text.strip()
    except Exception:
        pass
    return ""


def resolve_image_payload(
    image_base64: Optional[str],
    image_token: Optional[str],
    sam_variant: Optional[str],
) -> Tuple[Image.Image, np.ndarray, str]:
    variant = _default_variant(sam_variant)
    if image_token:
        cached = _fetch_preloaded_image(image_token, variant)
        if cached is None:
            raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="image_token_not_found")
        pil_img = Image.fromarray(cached)
        return pil_img, cached, image_token
    pil_img, np_img = _decode_image_base64(image_base64)
    token = hashlib.md5(np_img.tobytes()).hexdigest()
    _store_preloaded_image(token, np_img, variant)
    return pil_img, np_img, token


class Base64Payload(BaseModel):
    image_base64: str
    uuid: Optional[str] = None
    background_guard: Optional[bool] = None


class PredictResponse(BaseModel):
    prediction: str
    proba: Optional[float] = None
    second_label: Optional[str] = None
    second_proba: Optional[float] = None
    margin: Optional[float] = None
    error: Optional[str] = None
    uuid: Optional[str] = None


class BboxModel(BaseModel):
    className: str
    x: float
    y: float
    width: float
    height: float


class CropImage(BaseModel):
    image_base64: str
    originalName: str
    bboxes: List[BboxModel]


class CropZipRequest(BaseModel):
    images: List[CropImage]


class PointPrompt(BaseModel):
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    point_x: float
    point_y: float
    uuid: Optional[str] = None
    sam_variant: Optional[str] = None
    image_name: Optional[str] = None

    @root_validator(skip_on_failure=True)
    def _ensure_point_payload(cls, values):  # noqa: N805
        if not values.get("image_base64") and not values.get("image_token"):
            raise ValueError("image_payload_missing")
        return values


class BboxPrompt(BaseModel):
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    bbox_left: float
    bbox_top: float
    bbox_width: float
    bbox_height: float
    uuid: Optional[str] = None
    sam_variant: Optional[str] = None
    image_name: Optional[str] = None

    @root_validator(skip_on_failure=True)
    def _ensure_bbox_payload(cls, values):  # noqa: N805
        if not values.get("image_base64") and not values.get("image_token"):
            raise ValueError("image_payload_missing")
        return values


class SamPreloadRequest(BaseModel):
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    sam_variant: Optional[str] = None
    preload_generation: Optional[int] = None
    image_name: Optional[str] = None
    slot: Optional[str] = "current"

    @root_validator(skip_on_failure=True)
    def _ensure_preload_payload(cls, values):  # noqa: N805
        if not values.get("image_base64") and not values.get("image_token"):
            raise ValueError("image_payload_missing")
        if values.get("slot") and values.get("slot") != "current" and not values.get("image_name"):
            raise ValueError("image_name_required_for_slot")
        return values


class SamPreloadResponse(BaseModel):
    status: str = "ready"
    width: int
    height: int
    token: str


sam_preload_manager = SamPreloadManager()


class SamSlotStatus(BaseModel):
    slot: str
    image_name: Optional[str]
    token: Optional[str]
    variant: Optional[str]
    width: Optional[int]
    height: Optional[int]
    busy: bool
    last_loaded: float
    enabled: bool = True
    memory_bytes: Optional[int] = None


class SamActivateRequest(BaseModel):
    image_name: str
    sam_variant: Optional[str] = None


class SamActivateResponse(BaseModel):
    status: str
    slot: Optional[str] = None
    token: Optional[str] = None


class PredictorSettings(BaseModel):
    max_predictors: int
    min_predictors: int
    max_supported_predictors: int
    active_predictors: int
    loaded_predictors: int
    process_ram_mb: float
    total_ram_mb: float
    available_ram_mb: float
    image_ram_mb: float
    gpu_total_mb: Optional[float] = None
    gpu_free_mb: Optional[float] = None
    gpu_compute_capability: Optional[str] = None
    gpu_device_count: Optional[int] = None


class PredictorSettingsUpdate(BaseModel):
    max_predictors: int


class MultiPointPrompt(BaseModel):
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    positive_points: List[List[float]] = []
    negative_points: List[List[float]] = []
    uuid: Optional[str] = None
    sam_variant: Optional[str] = None
    image_name: Optional[str] = None

    @root_validator(skip_on_failure=True)
    def _ensure_multi_payload(cls, values):  # noqa: N805
        if not values.get("image_base64") and not values.get("image_token"):
            raise ValueError("image_payload_missing")
        return values


class YoloBboxOutput(BaseModel):
    class_id: str
    bbox: List[float]
    uuid: Optional[str] = None
    image_token: Optional[str] = None
    mask: Optional[Dict[str, Any]] = None
    simplify_epsilon: Optional[float] = None


class Sam3TextPrompt(BaseModel):
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    text_prompt: str
    threshold: float = 0.5
    mask_threshold: float = 0.5
    simplify_epsilon: Optional[float] = None
    sam_variant: Optional[str] = None
    image_name: Optional[str] = None
    max_results: Optional[int] = None
    min_size: Optional[int] = None

    @root_validator(skip_on_failure=True)
    def _ensure_text_payload(cls, values):  # noqa: N805
        if not values.get("image_base64") and not values.get("image_token"):
            raise ValueError("image_payload_missing")
        if not values.get("text_prompt"):
            raise ValueError("text_prompt_required")
        min_size = values.get("min_size")
        if min_size is not None:
            try:
                min_size_int = max(0, int(min_size))
            except (TypeError, ValueError):
                min_size_int = 0
            values["min_size"] = min_size_int
        eps = values.get("simplify_epsilon")
        if eps is not None:
            try:
                eps_val = float(eps)
            except (TypeError, ValueError):
                eps_val = None
            values["simplify_epsilon"] = eps_val if eps_val is None or eps_val >= 0 else 0.0
        return values


class Sam3VisualPrompt(BaseModel):
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    bbox_left: Optional[float] = None
    bbox_top: Optional[float] = None
    bbox_width: Optional[float] = None
    bbox_height: Optional[float] = None
    bboxes: Optional[List[List[float]]] = None
    bbox_labels: Optional[List[bool]] = None
    threshold: float = 0.5
    mask_threshold: float = 0.5
    simplify_epsilon: Optional[float] = None
    sam_variant: Optional[str] = None
    image_name: Optional[str] = None
    max_results: Optional[int] = None
    min_size: Optional[int] = None

    @root_validator(skip_on_failure=True)
    def _validate_visual_payload(cls, values):  # noqa: N805
        if not values.get("image_base64") and not values.get("image_token"):
            raise ValueError("image_payload_missing")
        raw_bboxes = values.get("bboxes")
        cleaned_bboxes: List[Tuple[float, float, float, float]] = []
        if isinstance(raw_bboxes, list) and raw_bboxes:
            for entry in raw_bboxes:
                coords = None
                if isinstance(entry, Mapping):
                    coords = [
                        entry.get("left", entry.get("x")),
                        entry.get("top", entry.get("y")),
                        entry.get("width", entry.get("w")),
                        entry.get("height", entry.get("h")),
                    ]
                else:
                    coords = entry
                if not isinstance(coords, (list, tuple)) or len(coords) < 4:
                    continue
                try:
                    cleaned = tuple(float(coords[idx]) for idx in range(4))
                except (TypeError, ValueError):
                    continue
                if cleaned[2] <= 0 or cleaned[3] <= 0:
                    continue
                cleaned_bboxes.append(cleaned)
        if cleaned_bboxes:
            values["bboxes"] = cleaned_bboxes
        else:
            values["bboxes"] = None
            for key in ("bbox_left", "bbox_top", "bbox_width", "bbox_height"):
                raw = values.get(key)
                try:
                    values[key] = float(raw)
                except (TypeError, ValueError):
                    raise ValueError(f"invalid_{key}")
            if values["bbox_width"] <= 0 or values["bbox_height"] <= 0:
                raise ValueError("invalid_bbox_dims")
        raw_labels = values.get("bbox_labels")
        cleaned_labels: Optional[List[bool]] = None
        if isinstance(raw_labels, (list, tuple)):
            cleaned_labels = []
            for entry in raw_labels:
                if isinstance(entry, bool):
                    cleaned_labels.append(entry)
                elif isinstance(entry, (int, float)):
                    cleaned_labels.append(bool(entry))
                elif isinstance(entry, str):
                    cleaned_labels.append(entry.strip().lower() in {"1", "true", "yes", "pos", "positive"})
                else:
                    cleaned_labels.append(True)
        if values.get("bboxes"):
            if cleaned_labels is None:
                values["bbox_labels"] = None
            else:
                if len(cleaned_labels) < len(values["bboxes"]):
                    cleaned_labels.extend([True] * (len(values["bboxes"]) - len(cleaned_labels)))
                values["bbox_labels"] = cleaned_labels[: len(values["bboxes"])]
        else:
            values["bbox_labels"] = None
        min_size = values.get("min_size")
        if min_size is not None:
            try:
                values["min_size"] = max(0, int(min_size))
            except (TypeError, ValueError):
                values["min_size"] = 0
        eps = values.get("simplify_epsilon")
        if eps is not None:
            try:
                eps_val = float(eps)
            except (TypeError, ValueError):
                eps_val = None
            values["simplify_epsilon"] = eps_val if eps_val is None or eps_val >= 0 else 0.0
        return values


class SamPointAutoResponse(BaseModel):
    prediction: Optional[str] = None
    proba: Optional[float] = None
    second_label: Optional[str] = None
    second_proba: Optional[float] = None
    margin: Optional[float] = None
    bbox: List[float]
    uuid: Optional[str] = None
    error: Optional[str] = None
    image_token: Optional[str] = None
    score: Optional[float] = None
    mask: Optional[Dict[str, Any]] = None
    simplify_epsilon: Optional[float] = None


class QwenDetection(BaseModel):
    bbox: List[float]
    qwen_label: Optional[str] = None
    source: Literal["bbox", "point", "bbox_sam", "sam3_text"]
    score: Optional[float] = None
    mask: Optional[Dict[str, Any]] = None
    simplify_epsilon: Optional[float] = None
    class_id: Optional[int] = None
    class_name: Optional[str] = None
    clip_head_prob: Optional[float] = None
    clip_head_margin: Optional[float] = None
    clip_head_bg_prob: Optional[float] = None
    clip_head_bg_margin: Optional[float] = None


class QwenInferenceRequest(BaseModel):
    prompt: Optional[str] = None
    item_list: Optional[str] = None
    image_type: Optional[str] = None
    extra_context: Optional[str] = None
    prompt_type: Literal["bbox", "point", "bbox_sam"] = "bbox"
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    sam_variant: Optional[str] = None
    image_name: Optional[str] = None
    max_results: Optional[int] = 8

    @root_validator(skip_on_failure=True)
    def _validate_qwen_payload(cls, values):  # noqa: N805
        if not values.get("image_base64") and not values.get("image_token"):
            raise ValueError("image_payload_missing")
        prompt = (values.get("prompt") or "").strip()
        items = (values.get("item_list") or "").strip()
        if prompt:
            values["prompt"] = prompt
        elif items:
            values["item_list"] = items
        else:
            raise ValueError("prompt_or_items_required")
        max_results = values.get("max_results")
        if max_results is not None:
            try:
                max_int = int(max_results)
            except (TypeError, ValueError):
                max_int = 8
            values["max_results"] = max(1, min(max_int, 50))
        else:
            values["max_results"] = 8
        return values


class QwenInferenceResponse(BaseModel):
    boxes: List[QwenDetection] = Field(default_factory=list)
    raw_response: str
    prompt: str
    prompt_type: Literal["bbox", "point", "bbox_sam"]
    warnings: List[str] = Field(default_factory=list)
    image_token: Optional[str] = None


class QwenCaptionHint(BaseModel):
    label: str
    bbox: Optional[List[float]] = None
    confidence: Optional[float] = None

    @root_validator(skip_on_failure=True)
    def _validate_hint(cls, values):  # noqa: N805
        label = (values.get("label") or "").strip()
        if not label:
            raise ValueError("label_required")
        values["label"] = label
        bbox = values.get("bbox")
        if bbox is not None:
            if not isinstance(bbox, (list, tuple)) or len(bbox) != 4:
                raise ValueError("invalid_bbox")
            cleaned = []
            for val in bbox:
                try:
                    cleaned.append(float(val))
                except (TypeError, ValueError):
                    cleaned.append(0.0)
            values["bbox"] = cleaned
        return values


class QwenCaptionRequest(BaseModel):
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    user_prompt: Optional[str] = None
    label_hints: Optional[List[QwenCaptionHint]] = None
    image_width: Optional[int] = None
    image_height: Optional[int] = None
    include_counts: Optional[bool] = True
    include_coords: Optional[bool] = True
    max_boxes: Optional[int] = 0
    max_new_tokens: Optional[int] = 256
    model_variant: Optional[Literal["auto", "Instruct", "Thinking"]] = "auto"
    model_id: Optional[str] = None
    final_answer_only: Optional[bool] = True
    two_stage_refine: Optional[bool] = False
    caption_mode: Optional[Literal["full", "windowed"]] = "full"
    window_size: Optional[int] = None
    window_overlap: Optional[float] = None
    restrict_to_labels: Optional[bool] = True
    caption_all_windows: Optional[bool] = True
    unload_others: Optional[bool] = False
    force_unload: Optional[bool] = None
    multi_model_cache: Optional[bool] = False
    fast_mode: Optional[bool] = False
    use_sampling: Optional[bool] = True
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    top_k: Optional[int] = None
    presence_penalty: Optional[float] = None
    labelmap_glossary: Optional[str] = None

    @root_validator(skip_on_failure=True)
    def _validate_caption_payload(cls, values):  # noqa: N805
        if not values.get("image_base64") and not values.get("image_token"):
            raise ValueError("image_payload_missing")
        user_prompt = (values.get("user_prompt") or "").strip()
        values["user_prompt"] = user_prompt
        max_boxes = values.get("max_boxes")
        if max_boxes is not None:
            try:
                max_boxes_val = int(max_boxes)
            except (TypeError, ValueError):
                max_boxes_val = 0
            values["max_boxes"] = max(0, min(max_boxes_val, 200))
        else:
            values["max_boxes"] = 0
        max_tokens = values.get("max_new_tokens")
        if max_tokens is not None:
            try:
                max_tokens_val = int(max_tokens)
            except (TypeError, ValueError):
                max_tokens_val = 256
            values["max_new_tokens"] = max(32, min(max_tokens_val, 2000))
        else:
            values["max_new_tokens"] = 256
        temp = values.get("temperature")
        if temp is not None:
            try:
                values["temperature"] = float(temp)
            except (TypeError, ValueError):
                values["temperature"] = None
        top_p = values.get("top_p")
        if top_p is not None:
            try:
                values["top_p"] = float(top_p)
            except (TypeError, ValueError):
                values["top_p"] = None
        top_k = values.get("top_k")
        if top_k is not None:
            try:
                values["top_k"] = int(top_k)
            except (TypeError, ValueError):
                values["top_k"] = None
        presence = values.get("presence_penalty")
        if presence is not None:
            try:
                values["presence_penalty"] = float(presence)
            except (TypeError, ValueError):
                values["presence_penalty"] = None
        for key in ("image_width", "image_height"):
            val = values.get(key)
            if val is None:
                continue
            try:
                values[key] = max(1, int(val))
            except (TypeError, ValueError):
                values[key] = None
        caption_mode = (values.get("caption_mode") or "full").strip().lower()
        if caption_mode == "hybrid":
            caption_mode = "windowed"
        if caption_mode not in {"full", "windowed"}:
            caption_mode = "full"
        values["caption_mode"] = caption_mode
        window_size = values.get("window_size")
        if window_size is not None:
            try:
                values["window_size"] = max(64, int(window_size))
            except (TypeError, ValueError):
                values["window_size"] = None
        window_overlap = values.get("window_overlap")
        if window_overlap is not None:
            try:
                values["window_overlap"] = float(window_overlap)
            except (TypeError, ValueError):
                values["window_overlap"] = None
        model_id = (values.get("model_id") or "").strip()
        values["model_id"] = model_id or None
        glossary = values.get("labelmap_glossary")
        if glossary is not None:
            values["labelmap_glossary"] = str(glossary).strip() or None
        values["final_answer_only"] = bool(values.get("final_answer_only", True))
        values["two_stage_refine"] = bool(values.get("two_stage_refine", False))
        return values


class QwenCaptionResponse(BaseModel):
    caption: str
    used_counts: Dict[str, int] = Field(default_factory=dict)
    used_boxes: int
    truncated: bool


class QwenPrepassRequest(BaseModel):
    dataset_id: Optional[str] = None
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    image_name: Optional[str] = None
    model_id: Optional[str] = None
    model_variant: Optional[Literal["auto", "Instruct", "Thinking"]] = "auto"
    labelmap: Optional[List[str]] = None
    labelmap_glossary: Optional[str] = None
    detector_mode: Optional[Literal["yolo", "rfdetr"]] = "yolo"
    detector_id: Optional[str] = None
    enable_yolo: Optional[bool] = True
    enable_rfdetr: Optional[bool] = True
    yolo_id: Optional[str] = None
    rfdetr_id: Optional[str] = None
    sahi_window_size: Optional[int] = None
    sahi_overlap_ratio: Optional[float] = None
    classifier_id: Optional[str] = None
    sam_variant: Optional[str] = "sam3"
    enable_sam3_text: Optional[bool] = True
    sam3_text_synonym_budget: Optional[int] = 10
    sam3_text_window_extension: Optional[bool] = True
    sam3_text_window_mode: Optional[Literal["grid", "sahi"]] = "grid"
    sam3_text_window_size: Optional[int] = None
    sam3_text_window_overlap: Optional[float] = None
    enable_sam3_similarity: Optional[bool] = True
    similarity_min_exemplar_score: Optional[float] = 0.6
    similarity_mid_conf_low: Optional[float] = None
    similarity_mid_conf_high: Optional[float] = None
    similarity_mid_conf_class_count: Optional[int] = None
    similarity_window_mode: Optional[Literal["grid", "sahi"]] = "grid"
    similarity_window_size: Optional[int] = None
    similarity_window_overlap: Optional[float] = None
    similarity_window_extension: Optional[bool] = False
    prepass_mode: Optional[str] = "ensemble_sahi_sam3_text_similarity"
    prepass_only: Optional[bool] = True
    prepass_finalize: Optional[bool] = True
    prepass_keep_all: Optional[bool] = False
    prepass_sam3_text_thr: Optional[float] = 0.2
    prepass_similarity_score: Optional[float] = 0.3
    prepass_similarity_per_class: Optional[int] = None
    prepass_inspect_topk: Optional[int] = None
    prepass_inspect_score: Optional[float] = None
    prepass_inspect_quadrants: Optional[bool] = None
    prepass_caption: Optional[bool] = True
    prepass_caption_profile: Optional[str] = "light"
    prepass_caption_model_id: Optional[str] = None
    prepass_caption_variant: Optional[Literal["auto", "Instruct", "Thinking"]] = None
    prepass_caption_max_tokens: Optional[int] = None
    use_detection_overlay: Optional[bool] = True
    grid_cols: Optional[int] = 2
    grid_rows: Optional[int] = 2
    grid_overlap_ratio: Optional[float] = None
    overlay_dot_radius: Optional[int] = None
    tighten_fp: Optional[bool] = True
    detector_conf: Optional[float] = 0.45
    detector_iou: Optional[float] = None
    detector_merge_iou: Optional[float] = None
    sam3_score_thr: Optional[float] = 0.2
    sam3_mask_threshold: Optional[float] = 0.2
    classifier_min_prob: Optional[float] = 0.35
    classifier_margin: Optional[float] = 0.05
    classifier_bg_margin: Optional[float] = 0.05
    scoreless_iou: Optional[float] = 0.0
    ensemble_enabled: Optional[bool] = False
    ensemble_job_id: Optional[str] = None
    max_detections: Optional[int] = 2000
    iou: Optional[float] = 0.75
    cross_iou: Optional[float] = None
    max_new_tokens: Optional[int] = 4096
    thinking_effort: Optional[float] = None
    thinking_scale_factor: Optional[float] = None
    immediate_action_bias: Optional[bool] = True
    immediate_action_min_chars: Optional[int] = 200
    immediate_action_min_seconds: Optional[float] = 2.0
    immediate_action_logit_bias: Optional[float] = 6.0
    trace_verbose: Optional[bool] = False


class CalibrationRequest(BaseModel):
    dataset_id: str
    max_images: Optional[int] = 2000
    seed: Optional[int] = 42
    base_fp_ratio: Optional[float] = 0.2
    relax_fp_ratio: Optional[float] = 0.2
    recall_floor: Optional[float] = 0.6
    per_class_thresholds: Optional[bool] = True
    threshold_steps: Optional[int] = 200
    optimize_metric: Optional[str] = "f1"
    sam3_text_synonym_budget: Optional[int] = 10
    sam3_text_window_extension: Optional[bool] = True
    sam3_text_window_mode: Optional[Literal["grid", "sahi"]] = "grid"
    sam3_text_window_size: Optional[int] = None
    sam3_text_window_overlap: Optional[float] = None
    prepass_sam3_text_thr: Optional[float] = 0.2
    prepass_similarity_score: Optional[float] = 0.3
    sam3_score_thr: Optional[float] = 0.2
    sam3_mask_threshold: Optional[float] = 0.2
    similarity_min_exemplar_score: Optional[float] = 0.6
    similarity_window_extension: Optional[bool] = False
    detector_conf: Optional[float] = 0.45
    sahi_window_size: Optional[int] = None
    sahi_overlap_ratio: Optional[float] = None
    classifier_id: Optional[str] = None
    support_iou: Optional[float] = 0.5
    context_radius: Optional[float] = 0.075
    label_iou: Optional[float] = 0.9
    eval_iou: Optional[float] = 0.5
    eval_iou_grid: Optional[str] = None
    dedupe_iou: Optional[float] = 0.75
    dedupe_iou_grid: Optional[str] = None
    scoreless_iou: Optional[float] = 0.0
    model_hidden: Optional[str] = "256,128"
    model_dropout: Optional[float] = 0.1
    model_epochs: Optional[int] = 20
    model_lr: Optional[float] = 1e-3
    model_weight_decay: Optional[float] = 1e-4
    model_seed: Optional[int] = 42


class QwenPrepassResponse(BaseModel):
    detections: List[Dict[str, Any]]
    trace: List[AgentTraceEvent]
    warnings: Optional[List[str]] = None
    caption: Optional[str] = None
    trace_path: Optional[str] = None
    trace_full_path: Optional[str] = None


class AgentToolCall(BaseModel):
    name: str
    arguments: Dict[str, Any] = Field(default_factory=dict)
    call_id: Optional[str] = None


class AgentToolResult(BaseModel):
    name: str
    result: Dict[str, Any] = Field(default_factory=dict)
    error: Optional[str] = None


class AgentTraceEvent(BaseModel):
    step_id: int
    phase: Literal["intent", "tool_call", "tool_result", "merge"] = "intent"
    tool_name: Optional[str] = None
    summary: Optional[str] = None
    counts: Optional[Dict[str, int]] = None
    windows: Optional[List[Dict[str, Any]]] = None
    timestamp: Optional[float] = None
    tool_input: Optional[Dict[str, Any]] = None
    tool_output: Optional[Dict[str, Any]] = None
    model_output: Optional[str] = None


AGENT_TOOL_REGISTRY: Dict[str, Any] = {}
_AGENT_ACTIVE_IMAGE_TOKEN: Optional[str] = None
_AGENT_ACTIVE_IMAGE_BASE64: Optional[str] = None
_AGENT_ACTIVE_DATASET_ID: Optional[str] = None
_AGENT_ACTIVE_LABELMAP: Optional[List[str]] = None
_AGENT_ACTIVE_GLOSSARY: Optional[str] = None
_AGENT_ACTIVE_OVERALL_CAPTION: Optional[str] = None
_AGENT_ACTIVE_WINDOWED_CAPTIONS: Optional[List[Dict[str, Any]]] = None
_AGENT_ACTIVE_INSPECTED_WINDOWS: Optional[Set[Tuple[int, int, int, int]]] = None
_AGENT_ACTIVE_CLASSIFIER_ID: Optional[str] = None
_AGENT_ACTIVE_TIGHTEN_FP: bool = False
_AGENT_ACTIVE_DETECTOR_CONF: Optional[float] = None
_AGENT_ACTIVE_SAM3_SCORE_THR: Optional[float] = None
_AGENT_ACTIVE_SAM3_MASK_THR: Optional[float] = None
_AGENT_ACTIVE_CLASSIFIER_MIN_PROB: Optional[float] = None
_AGENT_ACTIVE_CLASSIFIER_MARGIN: Optional[float] = None
_AGENT_ACTIVE_CLASSIFIER_BG_MARGIN: Optional[float] = None
_AGENT_ACTIVE_SCORELESS_IOU: Optional[float] = None
_AGENT_ACTIVE_DETECTIONS: Optional[List[Dict[str, Any]]] = None
_AGENT_ACTIVE_CANDIDATES: Optional[List[Dict[str, Any]]] = None
_AGENT_ACTIVE_CANDIDATE_INDEX: Dict[int, Dict[str, Any]] = {}
_AGENT_ACTIVE_CLUSTERS: Optional[List[Dict[str, Any]]] = None
_AGENT_ACTIVE_CLUSTER_INDEX: Dict[int, Dict[str, Any]] = {}
_AGENT_ACTIVE_GRID: Optional[Dict[str, Any]] = None
_AGENT_ACTIVE_GRID_IMAGE: Optional[Image.Image] = None
_AGENT_ACTIVE_OVERLAY_IMAGE: Optional[Image.Image] = None
_AGENT_ACTIVE_LABEL_COLORS: Optional[Dict[str, str]] = None
_AGENT_ACTIVE_LABEL_PREFIXES: Optional[Dict[str, str]] = None
_AGENT_ACTIVE_OVERLAY_DOT_RADIUS: Optional[int] = None
_AGENT_GRID_TOOL_USAGE: Dict[str, Dict[str, int]] = {}
_AGENT_GRID_TOOL_LAST: Dict[str, Dict[str, Any]] = {}
_AGENT_HANDLE_INDEX: Dict[str, int] = {}
_AGENT_NEXT_CANDIDATE_ID = 1
_AGENT_NEXT_CLUSTER_ID = 1
_AGENT_LAST_SUBMIT_DETECTIONS: Optional[List[Dict[str, Any]]] = None
_AGENT_PENDING_CLASSIFY_IDS: List[int] = []
_AGENT_CLASSIFIER_CLASS_CACHE: Dict[str, List[str]] = {}
_AGENT_TRACE_FULL_WRITER: Optional[Callable[[Dict[str, Any]], None]] = None
_AGENT_TRACE_READABLE_WRITER: Optional[Callable[[str], None]] = None
_AGENT_TILE_CONTEXT_STORE: Dict[str, Dict[str, Any]] = {}
_AGENT_GLOBAL_CONTEXT_STORE: Dict[str, Dict[str, Any]] = {}
_AGENT_TILE_SUMMARIES: List[Dict[str, Any]] = []
_AGENT_PREPASS_COMPLETE: bool = False

PREPASS_TIGHT_DEFAULT_DETECTOR_CONF = 0.45
PREPASS_TIGHT_DEFAULT_SAM3_SCORE = 0.5
PREPASS_TIGHT_DEFAULT_SAM3_MASK = 0.5
PREPASS_TIGHT_DEFAULT_CLASSIFIER_MIN_PROB = 0.35
PREPASS_TIGHT_DEFAULT_CLASSIFIER_MARGIN = 0.05
PREPASS_TIGHT_DEFAULT_CLASSIFIER_BG_MARGIN = 0.05
PREPASS_TIGHT_DEFAULT_SCORELESS_IOU = 0.3
PREPASS_CLASSIFIER_STRICT_MIN_PROB = 0.65
PREPASS_CLASSIFIER_STRICT_MARGIN = 0.15
PREPASS_CLASSIFIER_STRICT_BG_MARGIN = 0.15
PREPASS_STRICT_SAM3_MIN_SCORE = 0.7
PREPASS_GRID_OVERLAP_RATIO = 0.2
PREPASS_CONTEXT_CHUNK_BYTES = 5 * 1024 * 1024
PREPASS_MIN_ZOOM_WINDOW_PX = 200
PREPASS_READABLE_TO_CONSOLE = str(os.environ.get("PREPASS_READABLE_TO_CONSOLE", "1")).lower() not in {"0", "false", "no"}
PREPASS_CLUSTER_IOU = 0.75
PREPASS_INSPECT_MAX_OBJECTS = 0


def _register_agent_tool(name: str):
    def _wrap(func):
        AGENT_TOOL_REGISTRY[name] = func
        return func
    return _wrap


def _agent_set_active_detections(detections: Optional[Sequence[Dict[str, Any]]]) -> None:
    global _AGENT_ACTIVE_DETECTIONS
    if not detections:
        _AGENT_ACTIVE_DETECTIONS = []
        return
    _AGENT_ACTIVE_DETECTIONS = [dict(det) for det in detections if isinstance(det, dict)]


def _agent_set_active_clusters(clusters: Optional[Sequence[Dict[str, Any]]]) -> None:
    global _AGENT_ACTIVE_CLUSTERS, _AGENT_ACTIVE_DETECTIONS, _AGENT_ACTIVE_CLUSTER_INDEX
    if not clusters:
        _AGENT_ACTIVE_CLUSTERS = []
        _AGENT_ACTIVE_DETECTIONS = []
        _AGENT_ACTIVE_CLUSTER_INDEX = {}
        _agent_refresh_handle_index()
        return
    cluster_list = [dict(item) for item in clusters if isinstance(item, dict)]
    _AGENT_ACTIVE_CLUSTERS = cluster_list
    _AGENT_ACTIVE_DETECTIONS = cluster_list
    _AGENT_ACTIVE_CLUSTER_INDEX = {
        int(item.get("cluster_id")): item for item in cluster_list if item.get("cluster_id") is not None
    }
    _agent_refresh_handle_index()


def _agent_reset_registries() -> None:
    global _AGENT_ACTIVE_CANDIDATES, _AGENT_ACTIVE_CANDIDATE_INDEX
    global _AGENT_ACTIVE_CLUSTERS, _AGENT_ACTIVE_CLUSTER_INDEX
    global _AGENT_NEXT_CANDIDATE_ID, _AGENT_NEXT_CLUSTER_ID
    global _AGENT_LAST_SUBMIT_DETECTIONS
    global _AGENT_PENDING_CLASSIFY_IDS
    global _AGENT_ACTIVE_LABEL_COLORS, _AGENT_ACTIVE_LABEL_PREFIXES, _AGENT_ACTIVE_OVERLAY_DOT_RADIUS
    global _AGENT_GRID_TOOL_USAGE, _AGENT_GRID_TOOL_LAST
    global _AGENT_TILE_CONTEXT_STORE, _AGENT_GLOBAL_CONTEXT_STORE
    global _AGENT_ACTIVE_OVERALL_CAPTION, _AGENT_ACTIVE_WINDOWED_CAPTIONS
    global _AGENT_TILE_SUMMARIES, _AGENT_HANDLE_INDEX, _AGENT_PREPASS_COMPLETE
    _AGENT_ACTIVE_CANDIDATES = []
    _AGENT_ACTIVE_CANDIDATE_INDEX = {}
    _AGENT_ACTIVE_CLUSTERS = []
    _AGENT_ACTIVE_CLUSTER_INDEX = {}
    _AGENT_NEXT_CANDIDATE_ID = 1
    _AGENT_NEXT_CLUSTER_ID = 1
    _AGENT_LAST_SUBMIT_DETECTIONS = None
    _AGENT_PENDING_CLASSIFY_IDS = []
    _AGENT_ACTIVE_LABEL_COLORS = None
    _AGENT_ACTIVE_LABEL_PREFIXES = None
    _AGENT_ACTIVE_OVERLAY_DOT_RADIUS = None
    _AGENT_GRID_TOOL_USAGE = {}
    _AGENT_GRID_TOOL_LAST = {}
    _AGENT_HANDLE_INDEX = {}
    _AGENT_TILE_CONTEXT_STORE = {}
    _AGENT_GLOBAL_CONTEXT_STORE = {}
    _AGENT_ACTIVE_OVERALL_CAPTION = None
    _AGENT_ACTIVE_WINDOWED_CAPTIONS = None
    _AGENT_TILE_SUMMARIES = []
    _AGENT_PREPASS_COMPLETE = False


def _agent_default_classifier_for_dataset(dataset_id: Optional[str]) -> Optional[str]:
    if not dataset_id:
        return None
    labelmap = _agent_load_labelmap(dataset_id)
    if not labelmap:
        return None
    root = UPLOAD_ROOT / "classifiers"
    candidates = [
        "DinoV3_best_model_large.pkl",
        f"{dataset_id}_clip_vitl14_bg5.pkl",
        f"{dataset_id}_clip_vitb16_bg5.pkl",
        f"{dataset_id}_clip_vitb32_bg5.pkl",
    ]
    for name in candidates:
        path = root / name
        if path.exists() and _agent_classifier_matches_labelmap(path, labelmap):
            return str(path)
    return None


def _agent_classifier_classes_for_path(path: Path) -> List[str]:
    key = str(path.resolve())
    cached = _AGENT_CLASSIFIER_CLASS_CACHE.get(key)
    if cached is not None:
        return cached
    classes: List[str] = []
    try:
        obj = joblib.load(str(path))
        if isinstance(obj, dict):
            raw = obj.get("classes")
        else:
            raw = getattr(obj, "classes_", None)
        if raw is not None:
            try:
                classes = [str(c) for c in list(raw)]
            except Exception:
                classes = [str(raw)]
    except Exception:
        classes = []
    _AGENT_CLASSIFIER_CLASS_CACHE[key] = classes
    return classes


def _agent_classifier_matches_labelmap(path: Path, labelmap: Sequence[str]) -> bool:
    if not labelmap:
        return False
    classes = _agent_classifier_classes_for_path(path)
    if not classes:
        return False
    bg_indices = _clip_head_background_indices(classes)
    filtered = [cls for idx, cls in enumerate(classes) if idx not in bg_indices]
    label_norm = {_normalize_class_name_for_match(n) for n in labelmap if n}
    clf_norm = {_normalize_class_name_for_match(n) for n in filtered if n}
    return bool(label_norm) and label_norm == clf_norm


_AgentToolRunner = None  # legacy tool runner removed


def _agent_error_payload(code: str, message: Optional[str] = None, fix_hint: Optional[str] = None) -> Dict[str, Any]:
    return {
        "error": {
            "code": str(code),
            "message": str(message or code),
            "fix_hint": str(fix_hint) if fix_hint else None,
        }
    }


def _agent_error_from_detail(detail: str, tool_name: Optional[str] = None) -> Tuple[str, str, Optional[str]]:
    text = str(detail or "").strip()
    lower = text.lower()
    if "grid_cell_required" in lower or "grid_cell_invalid" in lower:
        return "missing_grid_cell", text, "Provide a valid grid_cell (e.g., C2)."
    if "grid_unavailable" in lower:
        return "missing_grid_cell", text, "Enable grid overlay or provide window_bbox_2d."
    if "sam3_similarity_label_required" in lower or "missing_label" in lower:
        return "missing_label", text, "Provide a canonical labelmap class."
    if "sam3_similarity_exemplar_required" in lower or "invalid_exemplar" in lower:
        return "invalid_exemplar", text, "Provide exemplar handles from list_candidates/get_tile_context."
    if "classifier_unavailable" in lower:
        return "classifier_unavailable", text, "Classifier unavailable; skip verification."
    if "prompt_type_invalid" in lower or "items_required" in lower or "agent_detector_mode_invalid" in lower:
        return "invalid_prompt", text, "Provide a valid prompt or items list."
    if "bbox_required" in lower or "invalid_bbox" in lower or "window_bbox_required" in lower:
        return "invalid_prompt", text, "Provide a valid bbox or grid_cell."
    if "context_handle_required" in lower or "context_chunk_index_required" in lower:
        return "invalid_prompt", text, "Provide context_handle and chunk_index."
    if "context_handle_missing" in lower or "context_chunk_index_invalid" in lower:
        return "invalid_prompt", text, "Use a valid context_handle and chunk_index."
    if "cluster_not_found" in lower:
        return "invalid_prompt", text, "Provide a valid cluster_id from list_candidates."
    if "tool_not_found" in lower:
        return "tool_failed", text, "Use a supported tool name."
    return "tool_failed", text, "Check tool arguments and retry once."


def _dispatch_agent_tool(call: AgentToolCall) -> AgentToolResult:
    handler = AGENT_TOOL_REGISTRY.get(call.name)
    if handler is None:
        _agent_full_trace_write(
            {
                "type": "tool_dispatch_error",
                "tool": call.name,
                "error": "tool_not_found",
                "ts": time.time(),
            }
        )
        return AgentToolResult(
            name=call.name,
            result=_agent_error_payload("tool_failed", "tool_not_found", "Use a supported tool name."),
        )
    try:
        args = dict(call.arguments or {})
        try:
            import inspect
            handler_params = inspect.signature(handler).parameters
        except Exception:
            handler_params = {}
        if ("image_token" in handler_params or "image_base64" in handler_params) and not args.get("image_token") and not args.get("image_base64"):
            if _AGENT_ACTIVE_IMAGE_TOKEN:
                args["image_token"] = _AGENT_ACTIVE_IMAGE_TOKEN
            elif _AGENT_ACTIVE_IMAGE_BASE64:
                args["image_base64"] = _AGENT_ACTIVE_IMAGE_BASE64
        grid_cell = args.get("grid_cell")
        if grid_cell is not None:
            grid_cell = str(grid_cell).strip().upper()
            if grid_cell:
                args["grid_cell"] = grid_cell
        grid_tools = {
            "look_and_inspect",
            "image_zoom_in_tool",
            "zoom_and_detect",
            "run_detector",
            "sam3_text",
            "sam3_similarity",
            "qwen_infer",
            "view_cell_raw",
            "view_cell_overlay",
        }
        inspect_window_key: Optional[Tuple[int, int, int, int]] = None
        if grid_cell and call.name in grid_tools:
            if not _AGENT_ACTIVE_GRID:
                return AgentToolResult(
                    name=call.name,
                    result=_agent_error_payload(
                        "missing_grid_cell",
                        "grid_unavailable",
                        "Enable grid overlay or provide window_bbox_2d.",
                    ),
                )
            cell_xyxy = _agent_grid_cell_xyxy(
                _AGENT_ACTIVE_GRID,
                str(grid_cell),
                overlap_ratio=PREPASS_GRID_OVERLAP_RATIO,
            )
            if not cell_xyxy:
                cols = int(_AGENT_ACTIVE_GRID.get("cols") or 0)
                rows = int(_AGENT_ACTIVE_GRID.get("rows") or 0)
                labels = _AGENT_ACTIVE_GRID.get("col_labels") or []
                if labels:
                    col_range = f"{labels[0]}-{labels[-1]}"
                else:
                    col_range = "unknown"
                return AgentToolResult(
                    name=call.name,
                    result=_agent_error_payload(
                        "missing_grid_cell",
                        f"grid_cell_invalid:valid_cols={col_range},rows=1-{rows or 0}",
                        "Provide a valid grid_cell (e.g., C2).",
                    ),
                )
            img_w = int(_AGENT_ACTIVE_GRID.get("img_w") or 0)
            img_h = int(_AGENT_ACTIVE_GRID.get("img_h") or 0)
            if img_w <= 0 or img_h <= 0:
                return AgentToolResult(
                    name=call.name,
                    result=_agent_error_payload(
                        "missing_grid_cell",
                        "grid_unavailable",
                        "Enable grid overlay or provide window_bbox_2d.",
                    ),
                )
            cell_bbox_2d = list(_xyxy_to_qwen_bbox(img_w, img_h, *cell_xyxy))
            if call.name in {"view_cell_raw", "view_cell_overlay"}:
                pass
            elif call.name == "image_zoom_in_tool":
                if not args.get("bbox_2d") and not args.get("bbox_xyxy_px") and not args.get("window_bbox_2d"):
                    args["bbox_2d"] = cell_bbox_2d
            else:
                if not args.get("window_bbox_2d"):
                    args["window_bbox_2d"] = cell_bbox_2d
                if call.name == "look_and_inspect" and not args.get("bbox_space"):
                    args["bbox_space"] = "window"
        if call.name == "run_detector":
            if args.get("conf") is None and _AGENT_ACTIVE_DETECTOR_CONF is not None:
                args["conf"] = _AGENT_ACTIVE_DETECTOR_CONF
        bbox_2d = args.get("bbox_2d")
        if isinstance(bbox_2d, (list, tuple)) and len(bbox_2d) < 4:
            args.pop("bbox_2d", None)
        bbox_xyxy_px = args.get("bbox_xyxy_px")
        if isinstance(bbox_xyxy_px, (list, tuple)) and len(bbox_xyxy_px) < 4:
            args.pop("bbox_xyxy_px", None)
        window_bbox_2d = args.get("window_bbox_2d")
        if isinstance(window_bbox_2d, (list, tuple)) and len(window_bbox_2d) < 4:
            args.pop("window_bbox_2d", None)
        if call.name == "look_and_inspect":
            if _AGENT_ACTIVE_IMAGE_TOKEN and not args.get("image_base64"):
                args["image_token"] = _AGENT_ACTIVE_IMAGE_TOKEN
            if _AGENT_ACTIVE_LABELMAP:
                args["labelmap"] = list(_AGENT_ACTIVE_LABELMAP)
            if _AGENT_PREPASS_COMPLETE and "register" not in args:
                args["register"] = False
            if _AGENT_ACTIVE_GLOSSARY:
                args["labelmap_glossary"] = str(_AGENT_ACTIVE_GLOSSARY)
                model_id = str(args.get("model_id") or "").strip().lower()
                if model_id in {"qwen", "default", "auto"}:
                    args.pop("model_id", None)
                window_vals = args.get("window_bbox_2d") or args.get("bbox_2d")
                if isinstance(window_vals, (list, tuple)) and len(window_vals) >= 4:
                    try:
                        window_key = tuple(int(round(float(v))) for v in window_vals[:4])
                    except Exception:
                        window_key = None
                    if window_key and _AGENT_ACTIVE_INSPECTED_WINDOWS is not None:
                        if window_key in _AGENT_ACTIVE_INSPECTED_WINDOWS:
                            already_result = {
                                "candidates": [],
                                "window_bbox_2d": list(window_vals[:4]),
                                "already_inspected": True,
                            }
                            _agent_full_trace_write(
                                {
                                    "type": "tool_dispatch",
                                    "tool": call.name,
                                    "args": args,
                                    "ts": time.time(),
                                }
                            )
                            _agent_full_trace_write(
                                {
                                    "type": "tool_dispatch_result",
                                    "tool": call.name,
                                    "result": already_result,
                                    "ts": time.time(),
                                }
                            )
                            return AgentToolResult(
                                name=call.name,
                                result=already_result,
                            )
                        inspect_window_key = window_key
        if call.name == "qwen_infer":
            if not args.get("items") and _AGENT_ACTIVE_LABELMAP:
                args["items"] = list(_AGENT_ACTIVE_LABELMAP)
            if not args.get("extra_context") and _AGENT_ACTIVE_GLOSSARY:
                args["extra_context"] = str(_AGENT_ACTIVE_GLOSSARY)
        if call.name == "sam3_text":
            label = str(args.get("label") or "").strip()
            prompt = str(args.get("prompt") or "").strip()
            if not label and prompt and _AGENT_ACTIVE_LABELMAP:
                aligned = _agent_fuzzy_align_label(prompt, _AGENT_ACTIVE_LABELMAP)
                if aligned:
                    label = aligned
                    args["label"] = aligned
            if not label and not prompt and _AGENT_ACTIVE_LABELMAP:
                label = str(_AGENT_ACTIVE_LABELMAP[0]).strip()
                if label:
                    args["label"] = label
            if not prompt and label:
                synonym_map, _ = _agent_generate_sam3_synonyms(
                    _AGENT_ACTIVE_LABELMAP or [],
                    _AGENT_ACTIVE_GLOSSARY or "",
                    max_synonyms=10,
                )
                prompts = _sam3_prompt_variants(label, synonym_map, max_prompts=1)
                if prompts:
                    args["prompt"] = prompts[0]
                else:
                    args["prompt"] = label
            if args.get("score_thr") is None and _AGENT_ACTIVE_SAM3_SCORE_THR is not None:
                args["score_thr"] = _AGENT_ACTIVE_SAM3_SCORE_THR
            if args.get("mask_threshold") is None and _AGENT_ACTIVE_SAM3_MASK_THR is not None:
                args["mask_threshold"] = _AGENT_ACTIVE_SAM3_MASK_THR
        if call.name == "sam3_similarity":
            if args.get("score_thr") is None and _AGENT_ACTIVE_SAM3_SCORE_THR is not None:
                args["score_thr"] = _AGENT_ACTIVE_SAM3_SCORE_THR
        if call.name == "classify_crop":
            classifier_id = args.get("classifier_id")
            if isinstance(classifier_id, str):
                classifier_norm = classifier_id.strip().lower()
                if classifier_norm in {"default", "auto", "best"}:
                    classifier_id = None
                    args.pop("classifier_id", None)
                    if _AGENT_ACTIVE_CLASSIFIER_ID and not isinstance(active_classifier_head, dict):
                        args["classifier_id"] = _AGENT_ACTIVE_CLASSIFIER_ID
                elif Path(classifier_id).suffix.lower() not in CLASSIFIER_ALLOWED_EXTS:
                    args.pop("classifier_id", None)
            if not args.get("classifier_id") and not isinstance(active_classifier_head, dict):
                if _AGENT_ACTIVE_CLASSIFIER_ID:
                    args["classifier_id"] = _AGENT_ACTIVE_CLASSIFIER_ID
                else:
                    dataset_id = args.get("dataset_id") or _AGENT_ACTIVE_DATASET_ID
                    fallback = _agent_default_classifier_for_dataset(dataset_id)
                    if fallback:
                        args["classifier_id"] = fallback
                    else:
                        skipped = _agent_error_payload(
                            "classifier_unavailable",
                            "classifier_unavailable",
                            "Classifier unavailable; skip verification.",
                        )
                        _agent_full_trace_write(
                            {
                                "type": "tool_dispatch",
                                "tool": call.name,
                                "args": args,
                                "ts": time.time(),
                            }
                        )
                        _agent_full_trace_write(
                            {
                                "type": "tool_dispatch_result",
                                "tool": call.name,
                                "result": skipped,
                                "ts": time.time(),
                            }
                        )
                        return AgentToolResult(
                            name=call.name,
                            result=skipped,
                        )
        if not args.get("dataset_id") and _AGENT_ACTIVE_DATASET_ID:
            try:
                import inspect
                sig = inspect.signature(handler)
                if "dataset_id" in sig.parameters:
                    args["dataset_id"] = _AGENT_ACTIVE_DATASET_ID
            except Exception:
                # Best-effort: only inject when we can confirm the signature accepts it.
                pass
        _agent_full_trace_write(
            {
                "type": "tool_dispatch",
                "tool": call.name,
                "args": args,
                "ts": time.time(),
            }
        )
        result = handler(**args)
        if call.name == "look_and_inspect" and inspect_window_key and _AGENT_ACTIVE_INSPECTED_WINDOWS is not None:
            if not (isinstance(result, dict) and result.get("error")):
                _AGENT_ACTIVE_INSPECTED_WINDOWS.add(inspect_window_key)
        _agent_record_grid_tool_usage(call.name, args, result)
    except HTTPException as exc:
        detail = exc.detail if isinstance(exc.detail, str) else str(exc.detail)
        code, message, fix_hint = _agent_error_from_detail(detail, call.name)
        _agent_full_trace_write(
            {
                "type": "tool_dispatch_error",
                "tool": call.name,
                "error": message,
                "ts": time.time(),
            }
        )
        return AgentToolResult(
            name=call.name,
            result=_agent_error_payload(code, message, fix_hint),
        )
    except Exception as exc:  # noqa: BLE001
        if call.name == "look_and_inspect":
            return AgentToolResult(
                name=call.name,
                result={"skipped": True, "reason": f"inspect_failed:{exc}"},
            )
        _agent_full_trace_write(
            {
                "type": "tool_dispatch_error",
                "tool": call.name,
                "error": str(exc),
                "ts": time.time(),
            }
        )
        return AgentToolResult(
            name=call.name,
            result=_agent_error_payload("tool_failed", f"tool_failed:{exc}", "Check tool arguments and retry once."),
        )
    _agent_full_trace_write(
        {
            "type": "tool_dispatch_result",
            "tool": call.name,
            "result": result,
            "ts": time.time(),
        }
    )
    return AgentToolResult(name=call.name, result=result or {})


def _agent_extract_json_array(text: str) -> Optional[List[Any]]:
    if not text:
        return None
    start = text.find("[")
    end = text.rfind("]")
    if start == -1 or end == -1 or end <= start:
        return None
    snippet = text[start : end + 1]
    try:
        parsed = json.loads(snippet)
    except Exception:
        return None
    return parsed if isinstance(parsed, list) else None


def _agent_background_classes_from_head(head: Optional[Dict[str, Any]]) -> List[str]:
    if not isinstance(head, dict):
        return []
    classes = [str(c) for c in list(head.get("classes") or [])]
    indices = _clip_head_background_indices(classes)
    return [classes[idx] for idx in indices if 0 <= idx < len(classes)]


def _agent_load_labelmap(dataset_id: Optional[str]) -> List[str]:
    labelmap, _ = _agent_load_labelmap_meta(dataset_id)
    return labelmap


def _normalize_labelmap_glossary(raw_glossary: Any) -> str:
    if raw_glossary is None:
        return ""
    if isinstance(raw_glossary, str):
        return raw_glossary.strip()
    if isinstance(raw_glossary, list):
        lines = [str(item).strip() for item in raw_glossary if str(item).strip()]
        return "\n".join(lines)
    if isinstance(raw_glossary, dict):
        lines = []
        for key, value in raw_glossary.items():
            if value is None:
                lines.append(f"{key}".strip())
                continue
            if isinstance(value, (list, tuple)):
                joined = ", ".join([str(item) for item in value if str(item).strip()])
                lines.append(f"{key}: {joined}".strip())
            else:
                lines.append(f"{key}: {value}".strip())
        return "\n".join([line for line in lines if line.strip()])
    return str(raw_glossary).strip()


_DEFAULT_GLOSSARY_MAP: Dict[str, List[str]] = {
    "bike": ["bike", "motorbike", "scooter", "motorcycle"],
    "boat": ["boat", "canoe", "kayak", "surfboard", "ship"],
    "building": ["building", "house", "store", "office building", "residential building", "warehouse"],
    "bus": ["bus", "omnibus", "autobus", "coach"],
    "container": ["container", "truck container", "shipping container"],
    "digger": [
        "digger",
        "excavator",
        "tractor",
        "backhoe",
        "construction vehicle",
        "bulldozer",
        "steam shovel",
        "loader excavator",
        "dozer",
        "earthmover",
        "heavy machinery",
    ],
    "gastank": [
        "silos",
        "tank",
        "storage tank",
        "barrel",
        "pressure vessel",
        "oil silo",
        "storage silo",
        "oil tank",
    ],
    "light_vehicle": [
        "car",
        "light vehicle",
        "light_vehicle",
        "pickup truck",
        "sedan",
        "suv",
        "van",
        "4x4",
        "family car",
        "passenger vehicle",
        "automobile",
        "hatchback",
    ],
    "person": [
        "cyclist",
        "person",
        "swimmer",
        "human",
        "passenger",
        "pedestrian",
        "walker",
        "hiker",
        "individual",
    ],
    "solarpanels": ["array", "solar panel", "solarpanels"],
    "truck": [
        "truck",
        "lorry",
        "commercial vehicle",
        "semi truck",
        "articulated truck",
        "heavy-duty vehicle",
        "big rig",
        "18-wheeler",
        "semi-trailer truck",
    ],
    "utility_pole": [
        "antenna",
        "pole",
        "utility pole",
        "utility_pole",
        "street fixture",
        "drying rack",
        "streetlight",
        "street lamp",
        "electricity pylon",
        "power pylon",
        "transmission tower",
        "high-voltage pole",
        "lattice tower",
        "mast",
        "comms mast",
        "aerial mast",
        "satellite dish",
        "mounting pole",
        "light fixture",
    ],
}

_DEFAULT_SAM3_SYNONYMS: Dict[str, List[str]] = {
    key: list(value) for key, value in _DEFAULT_GLOSSARY_MAP.items()
}
_SAM3_SYNONYM_CACHE: Dict[str, Dict[str, List[str]]] = {}
_SAM3_SYNONYM_CACHE_ORDER: deque[str] = deque()
_SAM3_SYNONYM_CACHE_LIMIT = 8


def _glossary_label_key(label: str) -> str:
    return _normalize_class_name_for_match(label)


def _extract_glossary_synonyms(text: str) -> List[str]:
    if not text:
        return []
    cleaned = re.sub(r"[()]", " ", str(text))
    parts = re.split(r"[;,/]|\\band\\b|\\bor\\b", cleaned, flags=re.IGNORECASE)
    synonyms: List[str] = []
    for part in parts:
        term = part.strip()
        if not term:
            continue
        term = re.sub(
            r"^(all kinds of|all kind of|all|including|include|including the|including those|such as|other)\\s+",
            "",
            term,
            flags=re.IGNORECASE,
        ).strip()
        term = term.strip(" .")
        if term:
            synonyms.append(term)
    return synonyms


def _parse_glossary_synonyms(glossary: str, labelmap: Sequence[str]) -> Dict[str, List[str]]:
    if not glossary:
        return {}
    norm_to_label = {_glossary_label_key(lbl): lbl for lbl in labelmap if str(lbl).strip()}
    mapping: Dict[str, List[str]] = {}
    for line in str(glossary).splitlines():
        raw = line.strip()
        if not raw:
            continue
        key = None
        rest = None
        if "->" in raw:
            key, rest = raw.split("->", 1)
        elif ":" in raw:
            key, rest = raw.split(":", 1)
        elif "=" in raw:
            key, rest = raw.split("=", 1)
        elif " - " in raw:
            key, rest = raw.split(" - ", 1)
        else:
            match = re.match(r"^(\\w+)\\s*\\((.+)\\)$", raw)
            if match:
                key, rest = match.group(1), match.group(2)
        if not key or rest is None:
            continue
        label = norm_to_label.get(_glossary_label_key(key))
        if not label:
            continue
        synonyms = _extract_glossary_synonyms(rest)
        if synonyms:
            mapping.setdefault(label, []).extend(synonyms)
    return mapping


def _parse_glossary_mapping(glossary: str, labelmap: Sequence[str]) -> Dict[str, List[str]]:
    text = str(glossary or "").strip()
    if not text:
        return {}
    norm_to_label = {_glossary_label_key(lbl): lbl for lbl in labelmap if str(lbl).strip()}
    parsed: Any = None
    if text.startswith("{") or text.startswith("["):
        try:
            parsed = json.loads(text)
        except Exception:
            parsed = None
    mapping: Dict[str, List[str]] = {}
    if isinstance(parsed, dict):
        for key, value in parsed.items():
            label = norm_to_label.get(_glossary_label_key(key))
            if not label:
                continue
            terms: List[str] = []
            if isinstance(value, (list, tuple)):
                terms = [str(item) for item in value]
            elif isinstance(value, str):
                terms = _split_synonym_terms(value)
            cleaned = _normalize_synonym_list(terms)
            if cleaned:
                mapping[label] = cleaned
    elif isinstance(parsed, list):
        for item in parsed:
            if not isinstance(item, dict):
                continue
            key = item.get("label") or item.get("name")
            if not key:
                continue
            label = norm_to_label.get(_glossary_label_key(key))
            if not label:
                continue
            raw_terms = item.get("terms") or item.get("synonyms") or item.get("glossary")
            terms: List[str] = []
            if isinstance(raw_terms, (list, tuple)):
                terms = [str(term) for term in raw_terms]
            elif isinstance(raw_terms, str):
                terms = _split_synonym_terms(raw_terms)
            cleaned = _normalize_synonym_list(terms)
            if cleaned:
                mapping[label] = cleaned
    if mapping:
        return mapping
    return _parse_glossary_synonyms(glossary, labelmap)


def _sam3_synonym_cache_key(labelmap: Sequence[str], glossary: str, max_synonyms: Optional[int]) -> str:
    joined = ",".join([str(lbl).strip() for lbl in labelmap if str(lbl).strip()])
    limit = "none" if max_synonyms is None else str(int(max_synonyms))
    payload = f"{joined}\n{glossary or ''}\nmax={limit}".encode("utf-8", errors="ignore")
    return hashlib.md5(payload).hexdigest()


def _get_cached_sam3_synonyms(cache_key: str) -> Optional[Dict[str, List[str]]]:
    cached = _SAM3_SYNONYM_CACHE.get(cache_key)
    if cached is None:
        return None
    try:
        _SAM3_SYNONYM_CACHE_ORDER.remove(cache_key)
    except ValueError:
        pass
    _SAM3_SYNONYM_CACHE_ORDER.append(cache_key)
    return cached


def _set_cached_sam3_synonyms(cache_key: str, mapping: Dict[str, List[str]]) -> None:
    _SAM3_SYNONYM_CACHE[cache_key] = mapping
    try:
        _SAM3_SYNONYM_CACHE_ORDER.remove(cache_key)
    except ValueError:
        pass
    _SAM3_SYNONYM_CACHE_ORDER.append(cache_key)
    while len(_SAM3_SYNONYM_CACHE_ORDER) > _SAM3_SYNONYM_CACHE_LIMIT:
        evict = _SAM3_SYNONYM_CACHE_ORDER.popleft()
        _SAM3_SYNONYM_CACHE.pop(evict, None)


def _split_synonym_terms(text: str) -> List[str]:
    if not text:
        return []
    parts = re.split(r"[;,/]|\\band\\b|\\bor\\b", str(text), flags=re.IGNORECASE)
    return [part.strip() for part in parts if part.strip()]


def _clean_sam3_synonym(term: str) -> str:
    if not term:
        return ""
    cleaned = str(term).strip()
    cleaned = re.sub(r"[\"']", "", cleaned).strip()
    cleaned = re.sub(
        r"^(all kinds of|all kind of|all|including|include|including the|including those|such as|other|and|or)\\s+",
        "",
        cleaned,
        flags=re.IGNORECASE,
    ).strip()
    cleaned = cleaned.strip(" .")
    if not cleaned:
        return ""
    if len(cleaned) > 40:
        return ""
    if "->" in cleaned or ":" in cleaned:
        return ""
    return cleaned


def _normalize_synonym_list(terms: Sequence[str]) -> List[str]:
    normalized: List[str] = []
    seen: Set[str] = set()
    for term in terms:
        for part in _split_synonym_terms(term):
            cleaned = _clean_sam3_synonym(part)
            if not cleaned:
                continue
            key = cleaned.lower()
            if key in seen:
                continue
            seen.add(key)
            normalized.append(cleaned)
    return normalized


def _dedupe_synonyms(terms: Sequence[str]) -> List[str]:
    output: List[str] = []
    seen: Set[str] = set()
    for term in terms:
        key = str(term).strip().lower()
        if not key or key in seen:
            continue
        seen.add(key)
        output.append(term)
    return output


def _agent_generate_sam3_synonyms(
    labelmap: Sequence[str],
    glossary: str,
    *,
    max_synonyms: Optional[int] = 10,
) -> Tuple[Dict[str, List[str]], Dict[str, Dict[str, List[str]]]]:
    labels = [str(lbl).strip() for lbl in labelmap if str(lbl).strip()]
    if not labels:
        return {}, {}
    glossary_terms = _parse_glossary_mapping(glossary, labels)
    limit: Optional[int]
    if max_synonyms is None:
        limit = None
    else:
        limit = max(0, int(max_synonyms))
    mapping: Dict[str, List[str]] = {}
    if limit is None or limit > 0:
        cache_key = _sam3_synonym_cache_key(labels, glossary or "", limit)
        cached = _get_cached_sam3_synonyms(cache_key)
        if cached is None:
            if limit is None:
                limit_text = "as many short noun phrases (1-3 words) as are useful"
            else:
                limit_text = f"up to {limit} short noun phrases (1-3 words)"
            prompt = (
                "You generate short text prompts for a segmentation model. "
                f"Return ONLY JSON. For each label, provide {limit_text}. "
                "Use lowercase. No extra text, no markdown, no explanation. "
                "Avoid filler like 'including' or 'all kinds of'. "
                "If unsure, return an empty list for that label.\n"
                f"Labelmap: {', '.join(labels)}\n"
                f"Glossary hints:\n{glossary or 'none'}\n"
                "JSON example:\n"
                "{\"light_vehicle\": [\"car\", \"van\", \"pickup truck\", \"suv\", \"sedan\"]}"
            )
            raw = ""
            try:
                raw = _generate_qwen_text(prompt, max_new_tokens=384, use_system_prompt=False)
            except Exception:
                raw = ""
            data: Dict[str, Any] = {}
            json_text = _extract_balanced_json(raw, "{", "}")
            if json_text:
                try:
                    data = json.loads(json_text)
                except Exception:
                    data = {}
            if not data and raw:
                for line in raw.splitlines():
                    if ":" not in line:
                        continue
                    key, rest = line.split(":", 1)
                    data[key.strip()] = [item.strip() for item in rest.split(",") if item.strip()]
            norm_to_label = {_normalize_class_name_for_match(lbl): lbl for lbl in labels}
            for key, value in (data or {}).items():
                label = norm_to_label.get(_normalize_class_name_for_match(key))
                if not label:
                    continue
                terms: List[str] = []
                if isinstance(value, (list, tuple)):
                    terms = [str(item) for item in value]
                elif isinstance(value, str):
                    terms = [value]
                cleaned = _normalize_synonym_list(terms)
                if cleaned:
                    mapping[label] = cleaned if limit is None else cleaned[:limit]
            for label in labels:
                if label not in mapping:
                    mapping[label] = []
            _set_cached_sam3_synonyms(cache_key, mapping)
        else:
            mapping = cached
    else:
        mapping = {label: [] for label in labels}

    term_meta: Dict[str, Dict[str, List[str]]] = {}
    for label in labels:
        base_terms = _normalize_synonym_list([label, str(label).replace("_", " ")])
        glossary_list = _normalize_synonym_list(glossary_terms.get(label, []))
        expanded_terms = _normalize_synonym_list(mapping.get(label, []))
        expanded_terms = _dedupe_synonyms(glossary_list + expanded_terms)
        if not expanded_terms:
            fallback = _DEFAULT_SAM3_SYNONYMS.get(_glossary_label_key(label), [])
            expanded_terms = _normalize_synonym_list(fallback)
        if expanded_terms and limit is not None and limit > 0 and not glossary_list:
            expanded_terms = expanded_terms[:limit]
        term_meta[label] = {
            "base_terms": base_terms,
            "expanded_terms": expanded_terms,
        }
        mapping[label] = expanded_terms
    return mapping, term_meta


def _sam3_prompt_variants(
    label: str,
    synonym_map: Mapping[str, List[str]],
    *,
    max_prompts: int = 6,
) -> List[str]:
    if not label:
        return []
    canonical = str(label).strip()
    label_norm = _glossary_label_key(canonical)
    prompts: List[str] = []
    seen: Set[str] = set()

    def add(term: str) -> None:
        text = str(term).strip()
        if not text:
            return
        key = text.lower()
        if key in seen:
            return
        seen.add(key)
        prompts.append(text)

    if "_" in canonical:
        add(canonical.replace("_", " "))
    add(canonical)
    expanded_terms = synonym_map.get(canonical, [])
    for term in expanded_terms:
        add(term)
    if not expanded_terms:
        for term in _DEFAULT_SAM3_SYNONYMS.get(label_norm, []):
            add(term)
    if max_prompts > 0:
        prompts = prompts[:max_prompts]
    return prompts


@_register_agent_tool("look_and_inspect")
def _agent_tool_look_and_inspect(
    image_base64: Optional[str] = None,
    image_token: Optional[str] = None,
    bbox_2d: Optional[Sequence[float]] = None,
    bbox_xyxy_px: Optional[Sequence[float]] = None,
    bbox_space: Optional[str] = None,
    window_bbox_2d: Optional[Sequence[float]] = None,
    grid_cell: Optional[str] = None,
    intent: Optional[str] = None,
    labelmap: Optional[List[str]] = None,
    labelmap_glossary: Optional[str] = None,
    max_objects: Optional[int] = None,
    model_id: Optional[str] = None,
    register: Optional[bool] = True,
    include_caption: Optional[bool] = True,
) -> Dict[str, Any]:
    try:
        if not model_id or str(model_id).strip().lower() in {"default", "auto"}:
            model_id = (active_qwen_metadata or {}).get("model_id") or QWEN_MODEL_NAME
        pil_img, _, _ = _agent_resolve_image(image_base64, image_token)
        img_w, img_h = pil_img.size
        ann: Dict[str, Any] = {"bbox_space": bbox_space or "full"}
        if intent:
            ann["intent"] = str(intent)
        if bbox_2d is not None:
            ann["bbox_2d"] = list(bbox_2d)
        if bbox_xyxy_px is not None:
            ann["bbox_xyxy_px"] = list(bbox_xyxy_px)
        window_xyxy = None
        if ann.get("bbox_2d") is not None or ann.get("bbox_xyxy_px") is not None:
            window_xyxy = _resolve_agent_bbox_xyxy(ann, img_w, img_h, window_bbox_2d=window_bbox_2d)
        if window_xyxy is None and window_bbox_2d is not None:
            window_xyxy = _normalize_window_xyxy({"bbox_2d": window_bbox_2d}, img_w, img_h)
        if window_xyxy is None:
            window_xyxy = (0.0, 0.0, float(img_w), float(img_h))
        x1, y1, x2, y2 = window_xyxy
        crop = pil_img.crop((x1, y1, x2, y2))
        labels = [str(x) for x in (labelmap or [])]
        if not labels and _AGENT_ACTIVE_LABELMAP:
            labels = [str(x) for x in _AGENT_ACTIVE_LABELMAP]
        glossary = _normalize_labelmap_glossary(labelmap_glossary)
        if not glossary and _AGENT_ACTIVE_GLOSSARY:
            glossary = _normalize_labelmap_glossary(_AGENT_ACTIVE_GLOSSARY)
        max_items = PREPASS_INSPECT_MAX_OBJECTS
        prompt_lines = [
            "Inspect this window and list ALL visible objects from the labelmap.",
            "Return ONLY a JSON array. Each item: {\"label\": <labelmap label>, \"bbox_2d\": [x1,y1,x2,y2]}",
            "bbox_2d uses 0-1000 coordinates RELATIVE TO THIS WINDOW (not the full image).",
            "Only include labels from the labelmap. If none, return [].",
        ]
        if labels:
            prompt_lines.append("Labelmap: " + ", ".join(labels))
        if glossary:
            prompt_lines.append("Glossary:\n" + glossary)
        if max_items > 0:
            prompt_lines.append(f"Max objects: {max_items}.")
        messages = [
            {"role": "system", "content": [{"type": "text", "text": "You are a vision inspector. Output JSON only."}]},
            {"role": "user", "content": [{"type": "image", "image": crop}, {"type": "text", "text": "\n".join(prompt_lines)}]},
        ]
        response = _run_qwen_chat(
            messages,
            max_new_tokens=512,
            decode_override={"temperature": 0.2, "top_p": 0.9},
            model_id_override=model_id,
        )
        items = _agent_extract_json_array(response) or []
        results = []
        label_set = set(labels)
        for item in items:
            if not isinstance(item, dict):
                continue
            label = str(item.get("label") or "").strip()
            if label and label_set and label not in label_set:
                continue
            bbox = item.get("bbox_2d")
            if not isinstance(bbox, (list, tuple)) or len(bbox) < 4:
                continue
            results.append(
                {
                    "label": label,
                    "bbox_2d": [float(bbox[0]), float(bbox[1]), float(bbox[2]), float(bbox[3])],
                    "bbox_space": "window",
                    "window_bbox_2d": [float(v) for v in _xyxy_to_qwen_bbox(img_w, img_h, *window_xyxy)],
                    "score": None,
                    "score_source": "qwen_inspect",
                }
            )
            if max_items > 0 and len(results) >= max_items:
                break
        candidate_counts: Dict[str, int] = {}
        for item in results:
            label = str(item.get("label") or "").strip()
            if not label:
                continue
            candidate_counts[label] = candidate_counts.get(label, 0) + 1
        caption_text = ""
        if include_caption:
            try:
                caption_prompt = (
                    "Describe this window in 1 short sentence. "
                    "Mention visible objects using common terms, no coordinates."
                )
                caption_messages = [
                    {"role": "system", "content": [{"type": "text", "text": "You are a visual captioner."}]},
                    {"role": "user", "content": [{"type": "image", "image": crop}, {"type": "text", "text": caption_prompt}]},
                ]
                caption_raw = _run_qwen_chat(
                    caption_messages,
                    max_new_tokens=120,
                    decode_override={"temperature": 0.3, "top_p": 0.9},
                    model_id_override=model_id,
                )
                caption_text = _sanitize_qwen_caption(caption_raw)
            except Exception:
                caption_text = ""
        register_summary: Optional[Dict[str, Any]] = None
        if register:
            owner_cell = grid_cell or _agent_grid_cell_for_window_bbox(
                _AGENT_ACTIVE_GRID or {},
                _xyxy_to_qwen_bbox(img_w, img_h, *window_xyxy),
            )
            register_summary = _agent_register_detections(
                results,
                img_w=img_w,
                img_h=img_h,
                grid=_AGENT_ACTIVE_GRID,
                labelmap=labelmap,
                background=None,
                source_override="qwen_inspect",
                owner_cell=owner_cell,
            )
        new_cluster_ids = register_summary.get("new_cluster_ids") if isinstance(register_summary, dict) else []
        updated_cluster_ids = register_summary.get("updated_cluster_ids") if isinstance(register_summary, dict) else []
        new_summary = _agent_cluster_summaries(new_cluster_ids, include_ids=False)
        new_handles = _agent_handles_from_cluster_ids(new_cluster_ids or [])
        updated_handles = _agent_handles_from_cluster_ids(updated_cluster_ids or [])
        agent_view = {
            "grid_cell": grid_cell or _agent_grid_cell_for_window_bbox(_AGENT_ACTIVE_GRID or {}, _xyxy_to_qwen_bbox(img_w, img_h, *window_xyxy)),
            "caption": caption_text or None,
            "new_clusters": register_summary.get("new_clusters") if isinstance(register_summary, dict) else 0,
            "new_handles": new_handles,
            "updated_clusters": len(updated_cluster_ids or []),
            "updated_handles": updated_handles,
            "new_items": new_summary.get("items"),
            "new_items_total": new_summary.get("total"),
            "new_items_truncated": new_summary.get("truncated"),
            "label_counts": _agent_cluster_label_counts(new_cluster_ids or []),
            "candidate_count": len(results),
            "candidate_label_counts": candidate_counts,
        }
        return {
            "candidates": results,
            "window_xyxy_px": list(window_xyxy),
            "caption": caption_text or None,
            "register_summary": register_summary,
            "__agent_view__": agent_view,
        }
    except Exception as exc:  # noqa: BLE001
        reason = f"inspect_failed:{exc}"
        grid_text = None
        try:
            grid_text = grid_cell or _agent_grid_cell_for_window_bbox(
                _AGENT_ACTIVE_GRID or {}, _xyxy_to_qwen_bbox(img_w, img_h, *window_xyxy)  # type: ignore[arg-type]
            )
        except Exception:
            grid_text = grid_cell
        agent_view = {
            "grid_cell": grid_text,
            "caption": None,
            "new_clusters": 0,
            "new_handles": [],
            "updated_clusters": 0,
            "updated_handles": [],
            "new_items": [],
            "new_items_total": 0,
            "new_items_truncated": False,
            "label_counts": {},
            "candidate_count": 0,
            "skipped": True,
            "reason": reason,
        }
        return {
            "candidates": [],
            "window_xyxy_px": list(window_xyxy) if "window_xyxy" in locals() else None,
            "caption": None,
            "skipped": True,
            "reason": reason,
            "__agent_view__": agent_view,
        }


def _agent_load_labelmap_meta(dataset_id: Optional[str]) -> Tuple[List[str], str]:
    labelmap: List[str] = []
    glossary = ""
    if dataset_id:
        dataset_root = _resolve_sam3_or_qwen_dataset(dataset_id)
        yolo_labels = _discover_yolo_labelmap(dataset_root)
        if yolo_labels:
            labelmap = yolo_labels
        else:
            labelmap = _load_qwen_labelmap(dataset_root)
        meta = _load_sam3_dataset_metadata(dataset_root) or _load_qwen_dataset_metadata(dataset_root) or {}
        glossary = _normalize_labelmap_glossary(
            meta.get("labelmap_glossary") or meta.get("glossary") or meta.get("labelmap_ontology")
        )
        if not glossary and labelmap:
            glossary = _default_agent_glossary_for_labelmap(labelmap)
        return labelmap, glossary
    if active_label_list:
        labelmap = list(active_label_list)
    if not glossary and labelmap:
        glossary = _default_agent_glossary_for_labelmap(labelmap)
    return labelmap, glossary


def _default_agent_glossary_for_labelmap(labelmap: Sequence[str]) -> str:
    def _normalize(name: str) -> str:
        return "".join(ch for ch in name.lower().strip() if ch.isalnum() or ch == "_")
    mapped: Dict[str, List[str]] = {}
    for label in labelmap:
        norm = _normalize(label)
        synonyms = _DEFAULT_GLOSSARY_MAP.get(norm)
        if synonyms:
            mapped[label] = synonyms
    if not mapped:
        return ""
    return json.dumps(mapped, indent=2, ensure_ascii=True)


def _agent_compact_tool_result(result: Dict[str, Any], max_items: int = 0) -> Dict[str, Any]:
    if not isinstance(result, dict):
        return {"summary": "tool_result_invalid"}
    if max_items <= 0:
        return result
    detections = result.get("detections")
    if not isinstance(detections, list):
        candidates = result.get("candidates")
        if not isinstance(candidates, list):
            return result
        total = len(candidates)
        if total <= max_items:
            return result
        trimmed = candidates[:max_items]
        return {
            **{k: v for k, v in result.items() if k != "candidates"},
            "candidates": trimmed,
            "candidate_count": total,
            "truncated": True,
        }
    total = len(detections)
    if total <= max_items:
        return result
    classes = {}
    for det in detections:
        label = str(det.get("label") or det.get("class") or "unknown")
        classes[label] = classes.get(label, 0) + 1
    trimmed = detections[:max_items]
    return {
        **{k: v for k, v in result.items() if k != "detections"},
        "detections": trimmed,
        "detection_count": total,
        "class_counts": classes,
        "truncated": True,
    }


def _agent_compact_tool_response(tool_result: AgentToolResult) -> Dict[str, Any]:
    compact = _agent_compact_tool_result(tool_result.result)
    if tool_result.error:
        return {"error": tool_result.error, "result": compact}
    return compact


def _agent_label_color_map(labelmap: Sequence[str]) -> Dict[str, str]:
    colors: Dict[str, str] = {}
    if not labelmap:
        return colors
    golden = 0.61803398875
    for idx, label in enumerate(labelmap):
        name = str(label).strip()
        if not name:
            continue
        hue = (idx * golden) % 1.0
        r, g, b = colorsys.hsv_to_rgb(hue, 0.85, 0.9)
        colors[name] = f"#{int(r * 255):02X}{int(g * 255):02X}{int(b * 255):02X}"
    return colors


def _agent_grid_col_label(index: int) -> str:
    label = ""
    idx = int(index) + 1
    while idx > 0:
        idx, rem = divmod(idx - 1, 26)
        label = chr(ord("A") + rem) + label
    return label


def _agent_grid_col_index(label: str) -> Optional[int]:
    if not label:
        return None
    label = "".join(ch for ch in label.strip().upper() if "A" <= ch <= "Z")
    if not label:
        return None
    idx = 0
    for ch in label:
        idx = idx * 26 + (ord(ch) - ord("A") + 1)
    return idx - 1


def _agent_grid_spec(
    img_w: int,
    img_h: int,
    *,
    target: int = 200,
    min_cell: int = 160,
    max_cell: int = 260,
) -> Dict[str, Any]:
    def _fit(count: int, length: int) -> Tuple[int, float]:
        count = max(1, int(count))
        while count > 1 and length / count < min_cell:
            count -= 1
        while length / count > max_cell:
            count += 1
        return count, length / count

    cols_guess = max(1, int(round(img_w / float(target))))
    rows_guess = max(1, int(round(img_h / float(target))))
    cols, cell_w = _fit(cols_guess, img_w)
    rows, cell_h = _fit(rows_guess, img_h)
    col_labels = [_agent_grid_col_label(idx) for idx in range(cols)]
    return {
        "cols": cols,
        "rows": rows,
        "cell_w": float(cell_w),
        "cell_h": float(cell_h),
        "col_labels": col_labels,
        "img_w": int(img_w),
        "img_h": int(img_h),
    }


def _agent_grid_spec_for_payload(payload: QwenPrepassRequest, img_w: int, img_h: int) -> Dict[str, Any]:
    cols = payload.grid_cols if payload.grid_cols is not None else None
    rows = payload.grid_rows if payload.grid_rows is not None else None
    try:
        cols = int(cols) if cols is not None else None
    except (TypeError, ValueError):
        cols = None
    try:
        rows = int(rows) if rows is not None else None
    except (TypeError, ValueError):
        rows = None
    if cols and cols < 1:
        cols = None
    if rows and rows < 1:
        rows = None
    if cols or rows:
        if not cols and rows:
            cell_h = float(img_h) / float(rows)
            cols = max(1, int(round(float(img_w) / cell_h)))
        if not rows and cols:
            cell_w = float(img_w) / float(cols)
            rows = max(1, int(round(float(img_h) / cell_w)))
        cols = max(1, int(cols or 1))
        rows = max(1, int(rows or 1))
        cell_w = float(img_w) / float(cols)
        cell_h = float(img_h) / float(rows)
        col_labels = [_agent_grid_col_label(idx) for idx in range(cols)]
        return {
            "cols": cols,
            "rows": rows,
            "cell_w": float(cell_w),
            "cell_h": float(cell_h),
            "col_labels": col_labels,
            "img_w": int(img_w),
            "img_h": int(img_h),
        }
    return _agent_grid_spec(img_w, img_h)


def _agent_grid_cell_xyxy(
    grid: Mapping[str, Any],
    cell_label: str,
    *,
    overlap_ratio: float = 0.0,
) -> Optional[Tuple[float, float, float, float]]:
    if not grid or not cell_label:
        return None
    text = str(cell_label).strip()
    match = re.search(r"([A-Za-z]+)\s*(\d+)", text)
    col_label = None
    row_text = None
    if match:
        col_label, row_text = match.groups()
    else:
        match = re.search(r"(\d+)\s*([A-Za-z]+)", text)
        if match:
            row_text, col_label = match.groups()
    if not col_label or not row_text:
        return None
    col_idx = _agent_grid_col_index(col_label)
    try:
        row_idx = int(row_text) - 1
    except ValueError:
        return None
    cols = int(grid.get("cols") or 0)
    rows = int(grid.get("rows") or 0)
    if col_idx is None or col_idx < 0 or col_idx >= cols or row_idx < 0 or row_idx >= rows:
        return None
    cell_w = float(grid.get("cell_w") or 0.0)
    cell_h = float(grid.get("cell_h") or 0.0)
    img_w = int(grid.get("img_w") or 0)
    img_h = int(grid.get("img_h") or 0)
    x1 = col_idx * cell_w
    y1 = row_idx * cell_h
    x2 = img_w if col_idx == cols - 1 else (col_idx + 1) * cell_w
    y2 = img_h if row_idx == rows - 1 else (row_idx + 1) * cell_h
    ratio = max(0.0, float(overlap_ratio or 0.0))
    if ratio > 0.0:
        expand_x = (x2 - x1) * ratio * 0.5
        expand_y = (y2 - y1) * ratio * 0.5
        x1 = max(0.0, x1 - expand_x)
        y1 = max(0.0, y1 - expand_y)
        x2 = min(float(img_w), x2 + expand_x)
        y2 = min(float(img_h), y2 + expand_y)
    return (x1, y1, x2, y2)


def _agent_grid_cell_for_window_bbox(
    grid: Mapping[str, Any],
    window_bbox_2d: Sequence[float],
) -> Optional[str]:
    if not grid or not window_bbox_2d or len(window_bbox_2d) < 4:
        return None
    img_w = int(grid.get("img_w") or 0)
    img_h = int(grid.get("img_h") or 0)
    if img_w <= 0 or img_h <= 0:
        return None
    x1, y1, x2, y2 = _qwen_bbox_to_xyxy(img_w, img_h, window_bbox_2d)
    cx = (x1 + x2) / 2.0
    cy = (y1 + y2) / 2.0
    cell_w = float(grid.get("cell_w") or 0.0)
    cell_h = float(grid.get("cell_h") or 0.0)
    cols = int(grid.get("cols") or 0)
    rows = int(grid.get("rows") or 0)
    if cell_w <= 0 or cell_h <= 0 or cols <= 0 or rows <= 0:
        return None
    col_idx = min(cols - 1, max(0, int(cx / cell_w)))
    row_idx = min(rows - 1, max(0, int(cy / cell_h)))
    col_labels = grid.get("col_labels") or []
    if col_idx >= len(col_labels):
        return None
    return f"{col_labels[col_idx]}{row_idx + 1}"


def _agent_grid_prompt_text(grid: Optional[Mapping[str, Any]]) -> str:
    if not grid:
        return ""
    cols = int(grid.get("cols") or 0)
    rows = int(grid.get("rows") or 0)
    labels = grid.get("col_labels") or []
    if not cols or not rows or not labels:
        return ""
    first = str(labels[0])
    last = str(labels[-1])
    cell_w = float(grid.get("cell_w") or 0.0)
    cell_h = float(grid.get("cell_h") or 0.0)
    return (
        f"Grid: columns {first}-{last}, rows 1-{rows}. "
        f"Cell size ~{cell_w:.0f}x{cell_h:.0f} px. "
        "Use grid_cell like C2 (column C, row 2, top-left origin) for windowed tools; "
        "do not use numeric coordinates when the grid is enabled."
    )


def _agent_label_prefix_candidates(label: str) -> List[str]:
    if not label:
        return []
    cleaned = re.sub(r"[^A-Za-z0-9]+", " ", str(label)).strip()
    tokens = [tok for tok in cleaned.split() if tok]
    candidates: List[str] = []
    if tokens:
        if len(tokens) >= 2:
            candidates.append(tokens[0][0] + tokens[1][0])
            if len(tokens[1]) >= 2:
                candidates.append(tokens[0][0] + tokens[1][:2])
        if len(tokens[0]) >= 2:
            candidates.append(tokens[0][:2])
        if len(tokens[0]) >= 3:
            candidates.append(tokens[0][:3])
    else:
        flat = re.sub(r"[^A-Za-z0-9]+", "", str(label))
        if len(flat) >= 2:
            candidates.append(flat[:2])
        if len(flat) >= 3:
            candidates.append(flat[:3])
    seen: Set[str] = set()
    uniq: List[str] = []
    for cand in candidates:
        cand = re.sub(r"[^A-Za-z0-9]+", "", cand).upper()
        if len(cand) < 2 or cand in seen:
            continue
        uniq.append(cand)
        seen.add(cand)
    return uniq


def _agent_label_prefix_map(labels: Sequence[str]) -> Dict[str, str]:
    prefixes: Dict[str, str] = {}
    used: Set[str] = set()
    for label in labels:
        label_str = str(label or "").strip()
        if not label_str:
            continue
        candidates = _agent_label_prefix_candidates(label_str)
        chosen = None
        for cand in candidates:
            if cand not in used:
                chosen = cand
                break
        if chosen is None:
            base = candidates[0] if candidates else "X"
            base = re.sub(r"[^A-Za-z0-9]+", "", base).upper() or "X"
            idx = 1
            chosen = f"{base}{idx}"
            while chosen in used:
                idx += 1
                chosen = f"{base}{idx}"
        prefixes[label_str] = chosen
        used.add(chosen)
    return prefixes


def _agent_current_label_colors(labels: Sequence[str]) -> Dict[str, str]:
    global _AGENT_ACTIVE_LABEL_COLORS
    if _AGENT_ACTIVE_LABEL_COLORS:
        if all(lbl in _AGENT_ACTIVE_LABEL_COLORS for lbl in labels):
            return _AGENT_ACTIVE_LABEL_COLORS
    label_colors = _agent_label_color_map(labels)
    _AGENT_ACTIVE_LABEL_COLORS = label_colors
    return label_colors


def _agent_current_label_prefixes(labels: Sequence[str]) -> Dict[str, str]:
    global _AGENT_ACTIVE_LABEL_PREFIXES
    if _AGENT_ACTIVE_LABEL_PREFIXES:
        if all(lbl in _AGENT_ACTIVE_LABEL_PREFIXES for lbl in labels):
            return _AGENT_ACTIVE_LABEL_PREFIXES
    label_prefixes = _agent_label_prefix_map(labels)
    _AGENT_ACTIVE_LABEL_PREFIXES = label_prefixes
    return label_prefixes


def _agent_refresh_handle_index() -> None:
    global _AGENT_HANDLE_INDEX
    handle_index: Dict[str, int] = {}
    for cluster in list(_AGENT_ACTIVE_CLUSTERS or []):
        if not isinstance(cluster, dict):
            continue
        handle = _agent_cluster_handle(cluster)
        cid = cluster.get("cluster_id")
        if handle and cid is not None:
            handle_index[str(handle)] = int(cid)
    _AGENT_HANDLE_INDEX = handle_index


def _agent_cluster_handle(cluster: Mapping[str, Any]) -> Optional[str]:
    cluster_id = cluster.get("cluster_id") or cluster.get("candidate_id")
    if cluster_id is None:
        return None
    label = str(cluster.get("label") or "").strip()
    prefix = None
    if label:
        prefix = (_AGENT_ACTIVE_LABEL_PREFIXES or {}).get(label)
        if prefix is None:
            labels = list(_AGENT_ACTIVE_LABELMAP or [])
            if not labels:
                labels = [label]
            prefix_map = _agent_current_label_prefixes(labels)
            prefix = prefix_map.get(label)
    if prefix:
        return f"{prefix}{int(cluster_id)}"
    return str(cluster_id)


def _agent_cluster_id_from_handle(handle: Optional[str]) -> Optional[int]:
    if not handle:
        return None
    text = str(handle).strip()
    if not text:
        return None
    if text in _AGENT_HANDLE_INDEX:
        return int(_AGENT_HANDLE_INDEX[text])
    if text.isdigit():
        cid = int(text)
        if cid in _AGENT_ACTIVE_CLUSTER_INDEX:
            return cid
    match = re.search(r"(\\d+)$", text)
    if match:
        cid = int(match.group(1))
        cluster = _AGENT_ACTIVE_CLUSTER_INDEX.get(cid)
        if cluster and _agent_cluster_handle(cluster) == text:
            return cid
    return None


def _agent_handles_from_cluster_ids(cluster_ids: Sequence[int]) -> List[str]:
    handles: List[str] = []
    for cid in cluster_ids:
        cluster = _AGENT_ACTIVE_CLUSTER_INDEX.get(int(cid))
        if not cluster:
            continue
        handle = _agent_cluster_handle(cluster)
        if handle:
            handles.append(handle)
    return handles


def _agent_cluster_ids_from_handles(handles: Sequence[str]) -> List[int]:
    ids: List[int] = []
    for handle in handles:
        cid = _agent_cluster_id_from_handle(handle)
        if cid is not None:
            ids.append(int(cid))
    return ids


def _agent_overlay_key_text(
    label_colors: Mapping[str, str],
    label_prefixes: Optional[Mapping[str, str]] = None,
) -> str:
    if not label_colors:
        return ""
    lines: List[str] = []
    for label, color in label_colors.items():
        prefix = label_prefixes.get(label) if label_prefixes else None
        if prefix:
            lines.append(f"{label} ({prefix}) -> {color}")
        else:
            lines.append(f"{label} -> {color}")
    return "\n".join(lines)


def _agent_detection_center_px(det: Dict[str, Any], img_w: int, img_h: int) -> Optional[Tuple[float, float]]:
    bbox = det.get("bbox_xyxy_px")
    if isinstance(bbox, (list, tuple)) and len(bbox) >= 4:
        x1, y1, x2, y2 = [float(v) for v in bbox[:4]]
        return (x1 + x2) / 2.0, (y1 + y2) / 2.0
    bbox_2d = det.get("bbox_2d")
    if isinstance(bbox_2d, (list, tuple)) and len(bbox_2d) >= 4:
        x1, y1, x2, y2 = _qwen_bbox_to_xyxy(img_w, img_h, bbox_2d)
        return (x1 + x2) / 2.0, (y1 + y2) / 2.0
    bbox_yolo = det.get("bbox_yolo")
    if isinstance(bbox_yolo, (list, tuple)) and len(bbox_yolo) >= 4:
        x1, y1, x2, y2 = _yolo_to_xyxy(img_w, img_h, bbox_yolo)
        return (x1 + x2) / 2.0, (y1 + y2) / 2.0
    return None


def _agent_cluster_match(
    det: Dict[str, Any],
    clusters: Sequence[Dict[str, Any]],
    *,
    iou_thr: float,
) -> Optional[Dict[str, Any]]:
    label = str(det.get("label") or "").strip()
    bbox = det.get("bbox_xyxy_px")
    if not label or not isinstance(bbox, (list, tuple)) or len(bbox) < 4:
        return None
    for cluster in clusters:
        if not isinstance(cluster, dict):
            continue
        cluster_label = str(cluster.get("label") or "").strip()
        if cluster_label and cluster_label != label:
            continue
        cluster_bbox = cluster.get("bbox_xyxy_px")
        if not isinstance(cluster_bbox, (list, tuple)) or len(cluster_bbox) < 4:
            continue
        if _agent_iou_xyxy(bbox, cluster_bbox) >= iou_thr:
            return cluster
    return None


def _agent_det_score(det: Dict[str, Any]) -> Optional[float]:
    raw = det.get("score")
    if raw is None:
        return None
    try:
        return float(raw)
    except (TypeError, ValueError):
        return None


def _agent_register_detections(
    detections: Sequence[Dict[str, Any]],
    *,
    img_w: int,
    img_h: int,
    grid: Optional[Mapping[str, Any]] = None,
    labelmap: Optional[Sequence[str]] = None,
    background: Optional[Sequence[str]] = None,
    source_override: Optional[str] = None,
    owner_cell: Optional[str] = None,
    iou_thr: Optional[float] = None,
) -> Dict[str, Any]:
    global _AGENT_ACTIVE_CANDIDATES, _AGENT_ACTIVE_CANDIDATE_INDEX
    global _AGENT_ACTIVE_CLUSTERS, _AGENT_ACTIVE_CLUSTER_INDEX
    global _AGENT_NEXT_CANDIDATE_ID, _AGENT_NEXT_CLUSTER_ID

    if not detections:
        return {"candidate_ids": [], "cluster_ids": [], "new_clusters": 0, "updated_clusters": 0, "rejected": 0}
    labelmap = list(labelmap or _AGENT_ACTIVE_LABELMAP or [])
    cleaned, rejected = _agent_sanitize_detection_items(
        list(detections),
        pil_img=None,
        classifier_head=None,
        img_w=img_w,
        img_h=img_h,
        labelmap=labelmap,
        background=background,
    )
    if _AGENT_ACTIVE_CANDIDATES is None:
        _AGENT_ACTIVE_CANDIDATES = []
    if _AGENT_ACTIVE_CLUSTERS is None:
        _AGENT_ACTIVE_CLUSTERS = []
    if iou_thr is None:
        iou_thr = PREPASS_CLUSTER_IOU
    new_cluster_ids: List[int] = []
    updated_cluster_ids: List[int] = []
    candidate_ids: List[int] = []
    for det in cleaned:
        candidate_id = _AGENT_NEXT_CANDIDATE_ID
        _AGENT_NEXT_CANDIDATE_ID += 1
        candidate_ids.append(candidate_id)
        source = source_override or det.get("source") or det.get("score_source") or "agent"
        source_primary = det.get("source_primary") or source
        source_prompt = det.get("source_prompt")
        source_exemplar_handles = det.get("source_exemplar_handles")
        source_detector_run_id = det.get("source_detector_run_id")
        source_list = set(det.get("source_list") or [])
        if source:
            source_list.add(str(source))
        cell = _agent_grid_cell_for_detection(det, img_w, img_h, grid)
        owner = owner_cell or cell
        candidate = {
            "candidate_id": candidate_id,
            "label": det.get("label"),
            "class_id": det.get("class_id"),
            "bbox_xyxy_px": det.get("bbox_xyxy_px"),
            "bbox_2d": det.get("bbox_2d"),
            "bbox_yolo": det.get("bbox_yolo"),
            "score": det.get("score"),
            "score_source": det.get("score_source") or det.get("source"),
            "source": source,
            "source_primary": source_primary,
            "source_prompt": source_prompt,
            "source_exemplar_handles": source_exemplar_handles,
            "source_detector_run_id": source_detector_run_id,
            "source_list": sorted(source_list) if source_list else None,
            "grid_cell": cell,
            "owner_cell": owner,
            "window_bbox_2d": det.get("window_bbox_2d"),
        }
        _AGENT_ACTIVE_CANDIDATES.append(candidate)
        _AGENT_ACTIVE_CANDIDATE_INDEX[candidate_id] = candidate
        cluster = _agent_cluster_match(det, _AGENT_ACTIVE_CLUSTERS, iou_thr=iou_thr)
        if cluster is None:
            cluster_id = _AGENT_NEXT_CLUSTER_ID
            _AGENT_NEXT_CLUSTER_ID += 1
            origin_tag = "prepass"
            cluster_entry = {
                "cluster_id": cluster_id,
                "label": det.get("label"),
                "class_id": det.get("class_id"),
                "bbox_xyxy_px": det.get("bbox_xyxy_px"),
                "bbox_2d": det.get("bbox_2d"),
                "bbox_yolo": det.get("bbox_yolo"),
                "score": det.get("score"),
                "score_source": det.get("score_source") or det.get("source"),
                "source": source,
                "source_primary": source_primary,
                "source_prompt": source_prompt,
                "source_exemplar_handles": source_exemplar_handles,
                "source_detector_run_id": source_detector_run_id,
                "source_list": sorted(source_list) if source_list else None,
                "origin": origin_tag,
                "candidate_ids": [candidate_id],
                "grid_cell": cell,
                "owner_cell": owner,
                "classifier_best": det.get("classifier_best"),
                "classifier_prob": det.get("classifier_prob"),
                "classifier_accept": det.get("classifier_accept"),
            }
            _AGENT_ACTIVE_CLUSTERS.append(cluster_entry)
            _AGENT_ACTIVE_CLUSTER_INDEX[cluster_id] = cluster_entry
            new_cluster_ids.append(cluster_id)
            candidate["cluster_id"] = cluster_id
            continue
        cluster_id = int(cluster.get("cluster_id"))
        candidate["cluster_id"] = cluster_id
        if owner and not cluster.get("owner_cell"):
            cluster["owner_cell"] = owner
        cluster.setdefault("candidate_ids", [])
        cluster["candidate_ids"] = list(cluster.get("candidate_ids") or []) + [candidate_id]
        cluster_sources = set(cluster.get("source_list") or [])
        if cluster.get("source"):
            cluster_sources.add(str(cluster.get("source")))
        cluster_sources.update(source_list)
        if cluster_sources:
            cluster["source_list"] = sorted(cluster_sources)
        cluster_score = _agent_det_score(cluster)
        det_score = _agent_det_score(det)
        replace = False
        if det_score is not None and (cluster_score is None or det_score > cluster_score):
            replace = True
        elif det_score is None and cluster_score is None and not cluster.get("bbox_xyxy_px"):
            replace = True
        if replace:
            keep_ids = list(cluster.get("candidate_ids") or [])
            keep_sources = list(cluster.get("source_list") or [])
            keep_classifier = {
                "classifier_best": cluster.get("classifier_best"),
                "classifier_prob": cluster.get("classifier_prob"),
                "classifier_accept": cluster.get("classifier_accept"),
            }
            cluster.update(
                {
                    "label": det.get("label"),
                    "class_id": det.get("class_id"),
                    "bbox_xyxy_px": det.get("bbox_xyxy_px"),
                    "bbox_2d": det.get("bbox_2d"),
                    "bbox_yolo": det.get("bbox_yolo"),
                    "score": det.get("score"),
                    "score_source": det.get("score_source") or det.get("source"),
                    "source": source,
                    "source_primary": source_primary,
                    "source_prompt": source_prompt,
                    "source_exemplar_handles": source_exemplar_handles,
                    "source_detector_run_id": source_detector_run_id,
                    "grid_cell": cell,
                }
            )
            cluster["candidate_ids"] = keep_ids
            if keep_sources:
                cluster["source_list"] = keep_sources
            for key, value in keep_classifier.items():
                if value is not None and cluster.get(key) is None:
                    cluster[key] = value
        else:
            if source_primary and cluster.get("source_primary") is None:
                cluster["source_primary"] = source_primary
            if source_prompt and cluster.get("source_prompt") is None:
                cluster["source_prompt"] = source_prompt
            if source_exemplar_handles and cluster.get("source_exemplar_handles") is None:
                cluster["source_exemplar_handles"] = list(source_exemplar_handles)
            if source_detector_run_id and cluster.get("source_detector_run_id") is None:
                cluster["source_detector_run_id"] = source_detector_run_id
        updated_cluster_ids.append(cluster_id)
    _agent_set_active_clusters(_AGENT_ACTIVE_CLUSTERS)
    return {
        "candidate_ids": candidate_ids,
        "cluster_ids": new_cluster_ids + [cid for cid in updated_cluster_ids if cid not in new_cluster_ids],
        "new_cluster_ids": new_cluster_ids,
        "updated_cluster_ids": updated_cluster_ids,
        "new_clusters": len(new_cluster_ids),
        "updated_clusters": len(updated_cluster_ids),
        "rejected": int(rejected),
    }


def _agent_cluster_label_counts(cluster_ids: Sequence[int]) -> Dict[str, int]:
    counts: Dict[str, int] = {}
    for cid in cluster_ids:
        cluster = _AGENT_ACTIVE_CLUSTER_INDEX.get(int(cid))
        if not cluster:
            continue
        label = str(cluster.get("label") or "").strip()
        if not label:
            continue
        counts[label] = counts.get(label, 0) + 1
    return counts


def _agent_round_bbox_2d(bbox: Any) -> Optional[List[float]]:
    if not isinstance(bbox, (list, tuple)) or len(bbox) < 4:
        return None
    try:
        return [round(float(v), 1) for v in bbox[:4]]
    except (TypeError, ValueError):
        return None


def _agent_cluster_summaries(
    cluster_ids: Sequence[int],
    *,
    max_items: int = 0,
    include_ids: bool = True,
) -> Dict[str, Any]:
    items: List[Dict[str, Any]] = []
    for cid in cluster_ids:
        cluster = _AGENT_ACTIVE_CLUSTER_INDEX.get(int(cid))
        if not cluster:
            continue
        item = {
            "handle": _agent_cluster_handle(cluster),
            "label": cluster.get("label"),
            "grid_cell": cluster.get("grid_cell"),
            "score": cluster.get("score"),
            "score_source": cluster.get("score_source") or cluster.get("source"),
            "bbox_2d": _agent_round_bbox_2d(cluster.get("bbox_2d")),
        }
        if include_ids:
            item["cluster_id"] = int(cluster.get("cluster_id"))
        items.append(item)
    total = len(items)
    if max_items <= 0:
        return {"items": items, "total": total, "truncated": False}
    truncated = total > max_items
    return {"items": items[:max_items], "total": total, "truncated": truncated}


def _agent_grid_label_counts(
    *,
    grid: Optional[Mapping[str, Any]],
    clusters: Sequence[Dict[str, Any]],
    label: Optional[str] = None,
) -> List[Dict[str, Any]]:
    if not grid:
        return []
    label_filter = _agent_fuzzy_align_label(label, _AGENT_ACTIVE_LABELMAP or []) if label else None
    counts: Dict[str, Dict[str, Any]] = {}
    for cluster in clusters:
        if not isinstance(cluster, dict):
            continue
        cluster_label = str(cluster.get("label") or "").strip()
        if label_filter and cluster_label != label_filter:
            continue
        cell = cluster.get("owner_cell") or cluster.get("grid_cell")
        if not cell:
            bbox_2d = cluster.get("bbox_2d")
            if isinstance(bbox_2d, (list, tuple)) and len(bbox_2d) >= 4:
                cell = _agent_grid_cell_for_window_bbox(grid, bbox_2d)
        if not cell:
            continue
        entry = counts.setdefault(str(cell), {"grid_cell": str(cell), "counts": {}, "cluster_ids": []})
        entry["cluster_ids"].append(int(cluster.get("cluster_id")))
        entry["counts"][cluster_label] = entry["counts"].get(cluster_label, 0) + 1
    summary: List[Dict[str, Any]] = []
    for cell, entry in sorted(counts.items()):
        counts_map = entry.get("counts") or {}
        total = sum(int(v) for v in counts_map.values())
        summary.append(
            {
                "grid_cell": cell,
                "total": total,
                "counts": counts_map,
                "cluster_ids": entry.get("cluster_ids") or [],
            }
        )
    return summary


def _agent_render_detection_overlay(
    pil_img: Image.Image,
    detections: Sequence[Dict[str, Any]],
    label_colors: Mapping[str, str],
    *,
    dot_radius: Optional[int] = None,
    label_prefixes: Optional[Mapping[str, str]] = None,
) -> Image.Image:
    if not detections:
        return pil_img
    overlay = pil_img.convert("RGB").copy()
    draw = ImageDraw.Draw(overlay)
    img_w, img_h = overlay.size
    if dot_radius is None or dot_radius <= 0:
        dot_radius = max(2, int(round(min(img_w, img_h) * 0.004)))
    try:
        id_font = ImageFont.load_default()
    except Exception:
        id_font = None
    for det in detections:
        if not isinstance(det, dict):
            continue
        center = _agent_detection_center_px(det, img_w, img_h)
        if not center:
            continue
        cx, cy = center
        label = str(det.get("label") or det.get("class_name") or "").strip()
        color = label_colors.get(label, "#FFFFFF")
        try:
            r = int(color[1:3], 16)
            g = int(color[3:5], 16)
            b = int(color[5:7], 16)
            luminance = 0.299 * r + 0.587 * g + 0.114 * b
            outline = "#000000" if luminance > 140 else "#FFFFFF"
        except Exception:
            outline = "#000000"
        draw.ellipse(
            (cx - dot_radius, cy - dot_radius, cx + dot_radius, cy + dot_radius),
            fill=color,
            outline=outline,
            width=1,
        )
        cluster_id = det.get("cluster_id") or det.get("candidate_id")
        if cluster_id is not None:
            text = str(cluster_id)
            if label_prefixes is not None and label:
                prefix = label_prefixes.get(label)
                if prefix:
                    text = f"{prefix}{cluster_id}"
            tx = min(max(0, int(cx + dot_radius + 2)), img_w - 1)
            ty = min(max(0, int(cy - dot_radius - 2)), img_h - 1)
            draw.text((tx + 1, ty + 1), text, fill="#000000", font=id_font)
            draw.text((tx, ty), text, fill=outline, font=id_font)
    return overlay


def _agent_render_grid_overlay(
    pil_img: Image.Image,
    grid: Mapping[str, Any],
    *,
    line_color: Tuple[int, int, int, int] = (255, 255, 255, 200),
    text_color: Tuple[int, int, int, int] = (255, 255, 255, 90),
) -> Image.Image:
    if not grid:
        return pil_img
    cols = int(grid.get("cols") or 0)
    rows = int(grid.get("rows") or 0)
    if cols <= 0 or rows <= 0:
        return pil_img
    cell_w = float(grid.get("cell_w") or 0.0)
    cell_h = float(grid.get("cell_h") or 0.0)
    labels = grid.get("col_labels") or []
    base = pil_img.convert("RGBA")
    overlay = Image.new("RGBA", base.size, (0, 0, 0, 0))
    draw = ImageDraw.Draw(overlay)
    img_w, img_h = base.size
    for col in range(1, cols):
        x = int(round(col * cell_w))
        draw.line([(x, 0), (x, img_h)], fill=line_color, width=1)
    for row in range(1, rows):
        y = int(round(row * cell_h))
        draw.line([(0, y), (img_w, y)], fill=line_color, width=1)
    try:
        font = ImageFont.load_default()
    except Exception:
        font = None
    if labels:
        for col_idx, label in enumerate(labels):
            x = int(round(col_idx * cell_w + 4))
            draw.text((x, 2), str(label), fill=text_color, font=font)
    for row_idx in range(rows):
        y = int(round(row_idx * cell_h + 2))
        draw.text((2, y), str(row_idx + 1), fill=text_color, font=font)
    combined = Image.alpha_composite(base, overlay)
    return combined.convert("RGB")


def _agent_image_to_data_uri(pil_img: Image.Image) -> str:
    buf = BytesIO()
    pil_img.save(buf, format="PNG")
    return "data:image/png;base64," + base64.b64encode(buf.getvalue()).decode("ascii")


def _agent_overlay_labels(clusters: Sequence[Dict[str, Any]]) -> List[str]:
    labels = list(_AGENT_ACTIVE_LABELMAP or [])
    if labels:
        return labels
    label_set = {
        str(cluster.get("label") or "").strip()
        for cluster in clusters
        if isinstance(cluster, dict) and cluster.get("label")
    }
    return sorted(label for label in label_set if label)


def _agent_overlay_base_image() -> Optional[Image.Image]:
    if _AGENT_ACTIVE_GRID_IMAGE is not None:
        return _AGENT_ACTIVE_GRID_IMAGE
    if _AGENT_ACTIVE_IMAGE_BASE64 or _AGENT_ACTIVE_IMAGE_TOKEN:
        base_img, _, _ = _agent_resolve_image(_AGENT_ACTIVE_IMAGE_BASE64, _AGENT_ACTIVE_IMAGE_TOKEN)
        return base_img
    return None


def _agent_grid_cells(grid: Optional[Mapping[str, Any]]) -> List[str]:
    if not grid:
        return []
    labels = list(grid.get("col_labels") or [])
    rows = int(grid.get("rows") or 0)
    if not labels or rows <= 0:
        return []
    cells: List[str] = []
    for row in range(1, rows + 1):
        for col in labels:
            cells.append(f"{col}{row}")
    return cells


def _agent_tool_grid_cell_from_args(
    tool_args: Mapping[str, Any],
    tool_result: Any,
) -> Optional[str]:
    if not _AGENT_ACTIVE_GRID:
        return None
    grid_cell = tool_args.get("grid_cell")
    if grid_cell:
        return str(grid_cell)
    cluster_id = tool_args.get("cluster_id")
    if cluster_id is not None:
        cluster = _AGENT_ACTIVE_CLUSTER_INDEX.get(int(cluster_id))
        if cluster and cluster.get("grid_cell"):
            return str(cluster.get("grid_cell"))
    if isinstance(tool_result, dict):
        agent_view = tool_result.get("__agent_view__")
        if isinstance(agent_view, dict) and agent_view.get("grid_cell"):
            return str(agent_view.get("grid_cell"))
    window_bbox_2d = tool_args.get("window_bbox_2d")
    if isinstance(window_bbox_2d, (list, tuple)) and len(window_bbox_2d) >= 4:
        cell = _agent_grid_cell_for_window_bbox(_AGENT_ACTIVE_GRID, window_bbox_2d)
        if cell:
            return str(cell)
    window_arg = tool_args.get("window")
    if isinstance(window_arg, dict):
        window_bbox_2d = window_arg.get("bbox_2d")
        if isinstance(window_bbox_2d, (list, tuple)) and len(window_bbox_2d) >= 4:
            cell = _agent_grid_cell_for_window_bbox(_AGENT_ACTIVE_GRID, window_bbox_2d)
            if cell:
                return str(cell)
    return None


def _agent_record_grid_tool_usage(
    tool_name: str,
    tool_args: Mapping[str, Any],
    tool_result: Any,
) -> None:
    if not _AGENT_ACTIVE_GRID:
        return
    track_tools = {
        "look_and_inspect",
        "image_zoom_in_tool",
        "zoom_and_detect",
        "run_detector",
        "sam3_text",
        "sam3_similarity",
        "qwen_infer",
        "classify_crop",
        "view_cell_raw",
        "view_cell_overlay",
    }
    if tool_name not in track_tools:
        return
    cell = _agent_tool_grid_cell_from_args(tool_args, tool_result)
    if not cell:
        return
    usage = _AGENT_GRID_TOOL_USAGE.setdefault(cell, {})
    usage[tool_name] = int(usage.get(tool_name, 0)) + 1
    _AGENT_GRID_TOOL_LAST[cell] = {"tool": tool_name, "ts": time.time()}


def _agent_grid_usage_rows(grid: Optional[Mapping[str, Any]]) -> List[Dict[str, Any]]:
    cells = _agent_grid_cells(grid)
    rows: List[Dict[str, Any]] = []
    for cell in cells:
        tool_counts = dict(_AGENT_GRID_TOOL_USAGE.get(cell, {}))
        total = sum(int(v) for v in tool_counts.values())
        last = _AGENT_GRID_TOOL_LAST.get(cell, {})
        rows.append(
            {
                "grid_cell": cell,
                "total_calls": total,
                "tools": tool_counts,
                "last_tool": last.get("tool"),
                "last_ts": last.get("ts"),
            }
        )
    return rows


def _agent_grid_usage_text(rows: Sequence[Dict[str, Any]]) -> str:
    if not rows:
        return ""
    tool_short = {
        "look_and_inspect": "inspect",
        "image_zoom_in_tool": "zoom",
        "zoom_and_detect": "zoom_detect",
        "run_detector": "detector",
        "sam3_text": "sam3_text",
        "sam3_similarity": "sam3_sim",
        "qwen_infer": "qwen_infer",
        "classify_crop": "classify",
        "view_cell_raw": "view_raw",
        "view_cell_overlay": "view_overlay",
    }
    parts: List[str] = []
    for row in rows:
        cell = row.get("grid_cell")
        tools = row.get("tools") or {}
        last_tool = row.get("last_tool")
        if not tools:
            suffix = f" last={last_tool}" if last_tool else ""
            parts.append(f"{cell}: none{suffix}")
            continue
        tool_bits = []
        for tool_name, count in sorted(tools.items()):
            short = tool_short.get(tool_name, tool_name)
            tool_bits.append(f"{short}={int(count)}")
        if last_tool:
            tool_bits.append(f"last={last_tool}")
        parts.append(f"{cell}: " + ",".join(tool_bits))
    return "; ".join(parts)


def _agent_cluster_owner_cell(cluster: Mapping[str, Any]) -> Optional[str]:
    owner = cluster.get("owner_cell")
    if owner:
        return str(owner)
    cell = cluster.get("grid_cell")
    if cell:
        return str(cell)
    return None


def _agent_tile_clusters(grid_cell: str) -> List[Dict[str, Any]]:
    clusters = list(_AGENT_ACTIVE_CLUSTERS or [])
    if not grid_cell:
        return clusters
    tile = str(grid_cell)
    return [
        cluster
        for cluster in clusters
        if isinstance(cluster, dict) and _agent_cluster_owner_cell(cluster) == tile
    ]


def _agent_tile_cluster_payload(grid_cell: str) -> List[Dict[str, Any]]:
    items: List[Dict[str, Any]] = []
    for cluster in _agent_tile_clusters(grid_cell):
        items.append(
            {
                "cluster_id": int(cluster.get("cluster_id")),
                "handle": _agent_cluster_handle(cluster),
                "label": cluster.get("label"),
                "score": cluster.get("score"),
                "sources": cluster.get("source_list") or [cluster.get("source")],
                "grid_cell": cluster.get("grid_cell"),
                "owner_cell": cluster.get("owner_cell"),
                "verified": bool(cluster.get("classifier_accept")),
            }
        )
    return items


def _agent_tile_caption_hint(grid_cell: str) -> Optional[str]:
    if not _AGENT_ACTIVE_WINDOWED_CAPTIONS or not _AGENT_ACTIVE_GRID:
        return None
    hints: List[str] = []
    for entry in _AGENT_ACTIVE_WINDOWED_CAPTIONS:
        if not isinstance(entry, dict):
            continue
        bbox = entry.get("bbox_2d")
        if not isinstance(bbox, (list, tuple)) or len(bbox) < 4:
            continue
        cell = _agent_grid_cell_for_window_bbox(_AGENT_ACTIVE_GRID, bbox)
        if cell and str(cell) == str(grid_cell):
            text = str(entry.get("caption") or "").strip()
            if text:
                hints.append(text)
    if not hints:
        return None
    return " ".join(hints)


def _agent_context_store(
    payload: Dict[str, Any],
    *,
    kind: str,
    max_bytes: Optional[int] = None,
) -> Dict[str, Any]:
    raw = json.dumps(payload, ensure_ascii=True)
    raw_bytes = raw.encode("utf-8", errors="ignore")
    byte_size = len(raw_bytes)
    limit = int(max_bytes or PREPASS_CONTEXT_CHUNK_BYTES)
    if limit <= 0 or byte_size <= limit:
        return {"payload": payload, "byte_size": byte_size, "chunked": False}
    chunk_size = max(1, limit)
    chunks: List[str] = []
    for idx in range(0, byte_size, chunk_size):
        chunk = raw_bytes[idx : idx + chunk_size].decode("utf-8", errors="ignore")
        chunks.append(chunk)
    handle = f"{kind}_{uuid.uuid4().hex[:10]}"
    store = {"chunks": chunks, "byte_size": byte_size}
    if kind == "tile":
        _AGENT_TILE_CONTEXT_STORE[handle] = store
    else:
        _AGENT_GLOBAL_CONTEXT_STORE[handle] = store
    return {
        "chunked": True,
        "context_handle": handle,
        "chunk_total": len(chunks),
        "byte_size": byte_size,
    }


def _agent_context_chunk(
    handle: str,
    *,
    chunk_index: int,
    kind: str,
) -> Dict[str, Any]:
    store = _AGENT_TILE_CONTEXT_STORE if kind == "tile" else _AGENT_GLOBAL_CONTEXT_STORE
    entry = store.get(handle)
    if not entry:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="context_handle_missing")
    chunks = entry.get("chunks") or []
    total = len(chunks)
    idx = int(chunk_index)
    if idx < 0 or idx >= total:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="context_chunk_index_invalid")
    return {
        "context_handle": handle,
        "chunk_index": idx,
        "chunk_total": total,
        "payload_chunk": chunks[idx],
        "byte_size": entry.get("byte_size"),
    }


def _agent_clip_xyxy(
    xyxy: Optional[Tuple[float, float, float, float]],
    img_w: int,
    img_h: int,
) -> Optional[Tuple[float, float, float, float]]:
    if not xyxy:
        return None
    x1, y1, x2, y2 = xyxy
    x1 = max(0.0, min(float(img_w), float(x1)))
    y1 = max(0.0, min(float(img_h), float(y1)))
    x2 = max(0.0, min(float(img_w), float(x2)))
    y2 = max(0.0, min(float(img_h), float(y2)))
    if x2 <= x1 or y2 <= y1:
        return None
    return x1, y1, x2, y2


def _agent_overlay_crop_xyxy(
    tool_args: Mapping[str, Any],
    tool_result: Any,
    img_w: int,
    img_h: int,
) -> Optional[Tuple[float, float, float, float]]:
    agent_view = tool_result.get("__agent_view__") if isinstance(tool_result, dict) else None
    grid_cell = tool_args.get("grid_cell") or (
        agent_view.get("grid_cell") if isinstance(agent_view, dict) else None
    )
    if grid_cell and _AGENT_ACTIVE_GRID:
        cell_xyxy = _agent_grid_cell_xyxy(
            _AGENT_ACTIVE_GRID,
            str(grid_cell),
            overlap_ratio=PREPASS_GRID_OVERLAP_RATIO,
        )
        return _agent_clip_xyxy(cell_xyxy, img_w, img_h)

    window_xyxy = None
    for source in (tool_result, tool_args):
        if isinstance(source, dict):
            win = source.get("window_xyxy_px")
            if isinstance(win, (list, tuple)) and len(win) >= 4:
                window_xyxy = tuple(float(v) for v in win[:4])
                break
    if window_xyxy:
        return _agent_clip_xyxy(window_xyxy, img_w, img_h)

    window_bbox_2d = None
    if isinstance(tool_result, dict):
        window_bbox_2d = tool_result.get("window_bbox_2d")
    if window_bbox_2d is None:
        window_bbox_2d = tool_args.get("window_bbox_2d")
    if isinstance(window_bbox_2d, (list, tuple)) and len(window_bbox_2d) >= 4:
        return _agent_clip_xyxy(_qwen_bbox_to_xyxy(img_w, img_h, window_bbox_2d), img_w, img_h)

    window_arg = tool_args.get("window")
    if isinstance(window_arg, dict):
        if isinstance(window_arg.get("bbox_xyxy_px"), (list, tuple)) and len(window_arg.get("bbox_xyxy_px")) >= 4:
            xyxy = tuple(float(v) for v in window_arg.get("bbox_xyxy_px")[:4])
            return _agent_clip_xyxy(xyxy, img_w, img_h)
        if isinstance(window_arg.get("bbox_2d"), (list, tuple)) and len(window_arg.get("bbox_2d")) >= 4:
            return _agent_clip_xyxy(_qwen_bbox_to_xyxy(img_w, img_h, window_arg.get("bbox_2d")), img_w, img_h)

    cluster_id = tool_args.get("cluster_id")
    if cluster_id is not None:
        cluster = _AGENT_ACTIVE_CLUSTER_INDEX.get(int(cluster_id))
        if cluster:
            bbox_xyxy = cluster.get("bbox_xyxy_px")
            if isinstance(bbox_xyxy, (list, tuple)) and len(bbox_xyxy) >= 4:
                xyxy = tuple(float(v) for v in bbox_xyxy[:4])
                return _agent_clip_xyxy(xyxy, img_w, img_h)

    bbox_2d = tool_args.get("bbox_2d")
    bbox_space = str(tool_args.get("bbox_space") or "full").strip().lower()
    if isinstance(bbox_2d, (list, tuple)) and len(bbox_2d) >= 4:
        if bbox_space == "window":
            window_bbox_2d = tool_args.get("window_bbox_2d")
            if isinstance(window_bbox_2d, (list, tuple)) and len(window_bbox_2d) >= 4:
                xyxy = _window_local_bbox_2d_to_full_xyxy(img_w, img_h, window_bbox_2d, bbox_2d)
                return _agent_clip_xyxy(xyxy, img_w, img_h) if xyxy else None
        return _agent_clip_xyxy(_qwen_bbox_to_xyxy(img_w, img_h, bbox_2d), img_w, img_h)

    bbox_xyxy_px = tool_args.get("bbox_xyxy_px")
    if isinstance(bbox_xyxy_px, (list, tuple)) and len(bbox_xyxy_px) >= 4:
        xyxy = tuple(float(v) for v in bbox_xyxy_px[:4])
        return _agent_clip_xyxy(xyxy, img_w, img_h)

    return None


def _agent_expand_window_xyxy(
    x1: float,
    y1: float,
    x2: float,
    y2: float,
    img_w: int,
    img_h: int,
    min_size: int,
) -> Tuple[Tuple[float, float, float, float], bool]:
    width = max(0.0, x2 - x1)
    height = max(0.0, y2 - y1)
    target_w = max(width, float(min_size))
    target_h = max(height, float(min_size))
    if target_w > img_w:
        target_w = float(img_w)
    if target_h > img_h:
        target_h = float(img_h)
    expanded = target_w > width or target_h > height
    if not expanded:
        return (x1, y1, x2, y2), False
    cx = (x1 + x2) / 2.0
    cy = (y1 + y2) / 2.0
    nx1 = cx - target_w / 2.0
    nx2 = cx + target_w / 2.0
    ny1 = cy - target_h / 2.0
    ny2 = cy + target_h / 2.0
    if nx1 < 0.0:
        nx2 -= nx1
        nx1 = 0.0
    if nx2 > img_w:
        nx1 -= nx2 - img_w
        nx2 = float(img_w)
    if ny1 < 0.0:
        ny2 -= ny1
        ny1 = 0.0
    if ny2 > img_h:
        ny1 -= ny2 - img_h
        ny2 = float(img_h)
    nx1 = max(0.0, min(float(img_w), nx1))
    nx2 = max(0.0, min(float(img_w), nx2))
    ny1 = max(0.0, min(float(img_h), ny1))
    ny2 = max(0.0, min(float(img_h), ny2))
    return (nx1, ny1, nx2, ny2), True


def _agent_xyxy_to_xywh(x1: float, y1: float, x2: float, y2: float) -> List[float]:
    return [float(x1), float(y1), float(max(0.0, x2 - x1)), float(max(0.0, y2 - y1))]


def _agent_merge_detections(
    detections: List[Dict[str, Any]],
    *,
    iou_thr: float,
    max_det: Optional[int],
    cross_iou: Optional[float],
) -> List[Dict[str, Any]]:
    if not detections:
        return []
    by_class: Dict[int, List[int]] = {}
    for idx, det in enumerate(detections):
        class_id = int(det.get("class_id", -1))
        by_class.setdefault(class_id, []).append(idx)
    kept: List[int] = []
    for idxs in by_class.values():
        boxes = [detections[i]["bbox_xywh_px"] for i in idxs]
        scores = [float(detections[i].get("score") or 0.0) for i in idxs]
        if iou_thr <= 0:
            keep = list(range(len(idxs)))
        else:
            keep = _nms_indices(boxes, scores, iou_thr)
        kept.extend([idxs[k] for k in keep])
    merged = [detections[i] for i in kept]
    if cross_iou and cross_iou > 0:
        boxes = [det["bbox_xywh_px"] for det in merged]
        scores = [float(det.get("score") or 0.0) for det in merged]
        keep = _nms_indices(boxes, scores, cross_iou)
        merged = [merged[i] for i in keep]
    merged.sort(key=lambda det: float(det.get("score") or 0.0), reverse=True)
    if max_det:
        return merged[:max_det]
    return merged


def _agent_iou_xyxy(box_a: Sequence[float], box_b: Sequence[float]) -> float:
    try:
        ax1, ay1, ax2, ay2 = [float(v) for v in box_a[:4]]
        bx1, by1, bx2, by2 = [float(v) for v in box_b[:4]]
    except Exception:
        return 0.0
    ix1, iy1 = max(ax1, bx1), max(ay1, by1)
    ix2, iy2 = min(ax2, bx2), min(ay2, by2)
    iw, ih = max(0.0, ix2 - ix1), max(0.0, iy2 - iy1)
    inter = iw * ih
    if inter <= 0:
        return 0.0
    area_a = max(0.0, ax2 - ax1) * max(0.0, ay2 - ay1)
    area_b = max(0.0, bx2 - bx1) * max(0.0, by2 - by1)
    denom = area_a + area_b - inter
    return inter / denom if denom > 0 else 0.0


def _agent_source_counts(detections: Sequence[Dict[str, Any]]) -> Dict[str, int]:
    counts: Dict[str, int] = {}
    for det in detections:
        sources = det.get("source_list")
        if isinstance(sources, (list, tuple)) and sources:
            for src in sources:
                source = str(src or "unknown")
                counts[source] = counts.get(source, 0) + 1
            continue
        source = str(det.get("source") or det.get("score_source") or "unknown")
        counts[source] = counts.get(source, 0) + 1
    return counts


def _agent_format_source_counts(counts: Mapping[str, int]) -> str:
    if not counts:
        return "none"
    parts = [f"{key}={counts[key]}" for key in sorted(counts.keys())]
    return ", ".join(parts)


def _agent_detection_has_source(det: Dict[str, Any], sources: Set[str]) -> bool:
    if not det or not sources:
        return False
    source = str(det.get("source") or det.get("score_source") or "")
    if source and source in sources:
        return True
    source_list = det.get("source_list")
    if isinstance(source_list, (list, tuple)):
        for item in source_list:
            if str(item) in sources:
                return True
    return False


def _agent_merge_prepass_detections(
    detections: List[Dict[str, Any]],
    *,
    iou_thr: float = 0.85,
) -> Tuple[List[Dict[str, Any]], int]:
    if not detections or iou_thr <= 0:
        return detections, 0
    def det_score(det: Dict[str, Any]) -> float:
        try:
            return float(det.get("score") or 0.0)
        except (TypeError, ValueError):
            return 0.0

    merged: List[Dict[str, Any]] = []
    removed = 0
    ordered = sorted(detections, key=det_score, reverse=True)
    for det in ordered:
        label = str(det.get("label") or det.get("class_name") or "").strip()
        box = det.get("bbox_xyxy_px")
        if not isinstance(box, (list, tuple)) or len(box) < 4:
            merged.append(det)
            continue
        matched_idx = None
        for idx, kept in enumerate(merged):
            kept_label = str(kept.get("label") or kept.get("class_name") or "").strip()
            if label and kept_label and label != kept_label:
                continue
            kept_box = kept.get("bbox_xyxy_px")
            if not isinstance(kept_box, (list, tuple)) or len(kept_box) < 4:
                continue
            if _agent_iou_xyxy(box, kept_box) >= iou_thr:
                matched_idx = idx
                break
        if matched_idx is None:
            entry = dict(det)
            source_list = set(entry.get("source_list") or [])
            if entry.get("source"):
                source_list.add(entry.get("source"))
            if source_list:
                entry["source_list"] = sorted(source_list)
            merged.append(entry)
        else:
            kept = merged[matched_idx]
            source_list = set(kept.get("source_list") or [])
            if kept.get("source"):
                source_list.add(kept.get("source"))
            if det.get("source"):
                source_list.add(det.get("source"))
            keep_det = kept
            if det_score(det) > det_score(kept):
                keep_det = dict(det)
            keep_det["source_list"] = sorted(source_list) if source_list else keep_det.get("source_list")
            merged[matched_idx] = keep_det
            removed += 1
    return merged, removed


def _agent_filter_scoreless_detections(
    detections: List[Dict[str, Any]],
    *,
    iou_thr: float,
) -> Tuple[List[Dict[str, Any]], int]:
    if not detections or iou_thr <= 0:
        return detections, 0
    anchors = [
        det
        for det in detections
        if det.get("score") is not None and (det.get("score_source") or det.get("source")) != "unknown"
    ]
    if not anchors:
        return detections, 0
    filtered: List[Dict[str, Any]] = []
    removed = 0
    for det in detections:
        score = det.get("score")
        score_source = det.get("score_source") or det.get("source") or "unknown"
        if score is None or score_source == "unknown":
            bbox = det.get("bbox_xyxy_px") or []
            has_overlap = False
            for anchor in anchors:
                anchor_bbox = anchor.get("bbox_xyxy_px") or []
                if _agent_iou_xyxy(bbox, anchor_bbox) >= iou_thr:
                    has_overlap = True
                    break
            if not has_overlap:
                removed += 1
                continue
        filtered.append(det)
    return filtered, removed


def _resolve_classifier_batch_size() -> int:
    raw = os.environ.get("AGENT_CLASSIFIER_BATCH_SIZE")
    try:
        if raw is not None and str(raw).strip():
            return max(1, min(int(raw), 512))
    except Exception:
        pass
    return 64


def _predict_proba_batched(
    crops: Sequence[Image.Image],
    head: Dict[str, Any],
    *,
    batch_size: int,
) -> Optional[np.ndarray]:
    if not crops:
        return None
    results: List[np.ndarray] = []
    idx = 0
    bs = max(1, batch_size)
    while idx < len(crops):
        chunk = list(crops[idx : idx + bs])
        feats = _encode_pil_batch_for_head(chunk, head=head, batch_size_override=bs)
        if feats is None:
            if bs > 1:
                bs = max(1, bs // 2)
                if torch.cuda.is_available():
                    try:
                        torch.cuda.empty_cache()
                    except Exception:
                        pass
                continue
            return None
        proba = _clip_head_predict_proba(feats, head)
        if proba is None or proba.ndim != 2 or proba.shape[0] != len(chunk):
            if bs > 1:
                bs = max(1, bs // 2)
                if torch.cuda.is_available():
                    try:
                        torch.cuda.empty_cache()
                    except Exception:
                        pass
                continue
            return None
        results.append(proba)
        idx += len(chunk)
    if not results:
        return None
    return np.vstack(results)


def _agent_classifier_review(
    detections: List[Dict[str, Any]],
    *,
    pil_img: Optional[Image.Image],
    classifier_head: Optional[Dict[str, Any]],
) -> Tuple[List[Dict[str, Any]], Dict[str, int]]:
    counts = {
        "classifier_checked": 0,
        "classifier_rejected": 0,
        "classifier_errors": 0,
        "classifier_unavailable": 0,
    }
    if not detections:
        return detections, counts
    if pil_img is None or not isinstance(classifier_head, dict):
        counts["classifier_unavailable"] = len(detections)
        for det in detections:
            det["classifier_accept"] = None
            det["classifier_error"] = "unavailable"
        return detections, counts
    classes = [str(c) for c in list(classifier_head.get("classes") or [])]
    bg_indices = _clip_head_background_indices(classes)
    min_prob = float(classifier_head.get("min_prob") or 0.5)
    margin = float(classifier_head.get("margin") or 0.0)
    background_margin = float(classifier_head.get("background_margin") or 0.0)
    accepted: List[Dict[str, Any]] = []

    pending: List[Tuple[Dict[str, Any], int, Sequence[float]]] = []
    crops: List[Image.Image] = []
    for det in detections:
        bbox = det.get("bbox_xyxy_px")
        label = str(det.get("label") or "").strip()
        if not bbox or len(bbox) < 4:
            counts["classifier_errors"] += 1
            det["classifier_accept"] = False
            det["classifier_error"] = "missing_bbox"
            continue
        target_idx = _find_clip_head_target_index(classes, label)
        if target_idx is None:
            counts["classifier_errors"] += 1
            det["classifier_accept"] = False
            det["classifier_error"] = "label_not_in_classifier"
            continue
        x1, y1, x2, y2 = bbox[:4]
        crop = pil_img.crop((x1, y1, x2, y2))
        pending.append((det, target_idx, bbox[:4]))
        crops.append(crop)

    if pending:
        proba_arr = _predict_proba_batched(crops, classifier_head, batch_size=_resolve_classifier_batch_size())
        if proba_arr is None or proba_arr.ndim != 2:
            for det, _target_idx, _bbox in pending:
                counts["classifier_errors"] += 1
                det["classifier_accept"] = False
                det["classifier_error"] = "predict_failed"
            return [], counts
        for row, (det, target_idx, bbox) in zip(proba_arr, pending):
            order = sorted(range(len(classes)), key=lambda idx: float(row[idx]), reverse=True)
            best_idx = order[0] if order else None
            best_label = classes[best_idx] if best_idx is not None else "unknown"
            best_prob = float(row[best_idx]) if best_idx is not None else None
            det["classifier_best"] = best_label
            det["classifier_prob"] = best_prob
            keep_mask = _clip_head_keep_mask(
                row.reshape(1, -1),
                target_index=target_idx,
                min_prob=min_prob,
                margin=margin,
                background_indices=bg_indices,
                background_guard=True,
                background_margin=background_margin,
            )
            accept = bool(keep_mask[0]) if keep_mask is not None and len(keep_mask) else False
            det["classifier_accept"] = accept
            counts["classifier_checked"] += 1
            summary_bbox = _agent_readable_format_bbox(bbox)
            prob_text = f"{best_prob:.3f}" if isinstance(best_prob, float) else "n/a"
            _agent_readable_write(
                f"classifier check label={det.get('label')} bbox={summary_bbox} "
                f"best={best_label} prob={prob_text} accept={'yes' if accept else 'no'}"
            )
            if accept:
                accepted.append(det)
            else:
                counts["classifier_rejected"] += 1
    return accepted, counts


def _agent_finalize_detections(
    detections: List[Dict[str, Any]],
    *,
    pil_img: Optional[Image.Image] = None,
    classifier_head: Optional[Dict[str, Any]] = None,
    img_w: int,
    img_h: int,
    labelmap: List[str],
    background: Optional[Sequence[str]],
    iou_thr: float,
    cross_iou: Optional[float],
    max_det: Optional[int],
) -> Tuple[List[Dict[str, Any]], Dict[str, int]]:
    cleaned, rejected = _agent_sanitize_detection_items(
        detections,
        pil_img=pil_img,
        classifier_head=None,
        img_w=img_w,
        img_h=img_h,
        labelmap=labelmap,
        background=background,
    )
    filtered_scoreless = 0
    scoreless_iou = _AGENT_ACTIVE_SCORELESS_IOU or 0.0
    if scoreless_iou > 0:
        cleaned, filtered_scoreless = _agent_filter_scoreless_detections(
            cleaned,
            iou_thr=scoreless_iou,
        )
        if filtered_scoreless:
            _agent_readable_write(
                f"scoreless_filter: removed={filtered_scoreless} "
                f"iou>={scoreless_iou:.2f}"
            )
    reviewed, classifier_counts = _agent_classifier_review(
        cleaned,
        pil_img=pil_img,
        classifier_head=classifier_head,
    )
    merged = _agent_merge_detections(
        reviewed,
        iou_thr=iou_thr,
        max_det=max_det,
        cross_iou=cross_iou,
    )
    if classifier_counts.get("classifier_checked") or classifier_counts.get("classifier_unavailable"):
        _agent_readable_write(
            "final_review: "
            f"accepted={len(merged)} "
            f"rejected_classifier={classifier_counts.get('classifier_rejected', 0)} "
            f"classifier_unavailable={classifier_counts.get('classifier_unavailable', 0)}"
        )
    counts = {
        "input": len(detections),
        "accepted": len(merged),
        "rejected": int(rejected),
        "filtered_scoreless": int(filtered_scoreless),
        **classifier_counts,
    }
    return merged, counts


def _agent_fuzzy_align_label(label: Optional[str], labelmap: Sequence[str]) -> Optional[str]:
    if not label:
        return None
    label_norm = _normalize_class_name_for_match(label)
    if not label_norm:
        return None
    norm_map: Dict[str, str] = {}
    for lbl in labelmap:
        norm = _normalize_class_name_for_match(lbl)
        if norm:
            norm_map.setdefault(norm, lbl)
    if label_norm in norm_map:
        return norm_map[label_norm]
    try:
        import difflib
    except Exception:
        return None
    # Fuzzy matching is deliberately strict to avoid re-mapping to the wrong class.
    # Only allow near-exact matches with small edit distance and no ambiguity.
    keys = list(norm_map.keys())
    if not keys:
        return None
    best = None
    best_score = 0.0
    second = 0.0
    for key in keys:
        score = difflib.SequenceMatcher(None, label_norm, key).ratio()
        if score > best_score:
            second = best_score
            best_score = score
            best = key
        elif score > second:
            second = score
    if best is None:
        return None
    if best_score < 0.92:
        return None
    if abs(len(best) - len(label_norm)) > 2:
        return None
    if best[0] != label_norm[0]:
        return None
    if second and abs(best_score - second) <= 0.02:
        return None
    return norm_map.get(best)
    return None


def _agent_sanitize_detection_items(
    items: List[Dict[str, Any]],
    *,
    pil_img: Optional[Image.Image] = None,
    classifier_head: Optional[Dict[str, Any]] = None,
    img_w: int,
    img_h: int,
    labelmap: List[str],
    background: Optional[Sequence[str]] = None,
) -> Tuple[List[Dict[str, Any]], int]:
    background_set = set(background or [])
    label_index = {label: idx for idx, label in enumerate(labelmap)}
    cleaned: List[Dict[str, Any]] = []
    rejected = 0
    pending: List[Dict[str, Any]] = []
    pending_crops: List[Image.Image] = []
    for ann in items:
        label = str(ann.get("label") or ann.get("class_name") or "").strip()
        aligned = _agent_fuzzy_align_label(label, labelmap)
        if not aligned or aligned not in label_index or aligned in background_set:
            rejected += 1
            continue
        raw_score = ann.get("score")
        score: Optional[float]
        if raw_score is None:
            score = None
        else:
            try:
                score = float(raw_score)
            except (TypeError, ValueError):
                score = None
        score_source = ann.get("score_source")
        if not score_source:
            score_source = ann.get("source") or ("unknown" if score is None else None)
        window_bbox = ann.get("window_bbox_2d")
        xyxy = _resolve_agent_bbox_xyxy(ann, img_w, img_h, window_bbox_2d=window_bbox)
        if xyxy is None:
            rejected += 1
            continue
        x1, y1, x2, y2 = xyxy
        x1 = max(0.0, min(float(img_w), x1))
        y1 = max(0.0, min(float(img_h), y1))
        x2 = max(0.0, min(float(img_w), x2))
        y2 = max(0.0, min(float(img_h), y2))
        if x2 <= x1 or y2 <= y1:
            rejected += 1
            continue
        base_entry = {
            "bbox_xyxy_px": [x1, y1, x2, y2],
            "bbox_xywh_px": _agent_xyxy_to_xywh(x1, y1, x2, y2),
            "bbox_2d": list(_xyxy_to_qwen_bbox(img_w, img_h, x1, y1, x2, y2)),
            "bbox_yolo": list(_xyxy_to_yolo_norm(img_w, img_h, x1, y1, x2, y2)),
            "label": aligned,
            "class_id": label_index[aligned],
            "score": score,
            "score_source": score_source,
            "sam3_prompt_term": ann.get("sam3_prompt_term"),
            "sam3_prompt_label": ann.get("sam3_prompt_label"),
            "sam3_prompt_source": ann.get("sam3_prompt_source"),
            "source": ann.get("source") or "agent",
        }
        source_list = ann.get("source_list")
        if isinstance(source_list, (list, tuple)):
            base_entry["source_list"] = [str(item) for item in source_list if item]

        if pil_img is not None and isinstance(classifier_head, dict):
            classes = [str(c) for c in list(classifier_head.get("classes") or [])]
            target_idx = _find_clip_head_target_index(classes, aligned)
            if target_idx is None:
                rejected += 1
                continue
            source = str(score_source or ann.get("source") or "").strip().lower()
            strict_sources = {
                "sam3_similarity",
                "qwen_inspect",
                "qwen_infer",
                "yolo_zoom",
                "rfdetr_zoom",
            }
            strict = source in strict_sources
            if not strict and source == "sam3_text":
                if score is None or score < PREPASS_STRICT_SAM3_MIN_SCORE:
                    strict = True
            min_prob = float(classifier_head.get("min_prob") or 0.5)
            margin = float(classifier_head.get("margin") or 0.0)
            background_margin = float(classifier_head.get("background_margin") or 0.0)
            if strict:
                min_prob = max(min_prob, PREPASS_CLASSIFIER_STRICT_MIN_PROB)
                margin = max(margin, PREPASS_CLASSIFIER_STRICT_MARGIN)
                background_margin = max(background_margin, PREPASS_CLASSIFIER_STRICT_BG_MARGIN)
            pending.append(
                {
                    "entry": base_entry,
                    "target_idx": target_idx,
                    "min_prob": min_prob,
                    "margin": margin,
                    "background_margin": background_margin,
                }
            )
            pending_crops.append(pil_img.crop((x1, y1, x2, y2)))
        else:
            cleaned.append(base_entry)

    if pending and pil_img is not None and isinstance(classifier_head, dict):
        proba_arr = _predict_proba_batched(
            pending_crops,
            classifier_head,
            batch_size=_resolve_classifier_batch_size(),
        )
        if proba_arr is None or proba_arr.ndim != 2 or proba_arr.shape[0] != len(pending):
            rejected += len(pending)
            return cleaned, rejected
        classes = [str(c) for c in list(classifier_head.get("classes") or [])]
        bg_indices = _clip_head_background_indices(classes)
        for row, pending_entry in zip(proba_arr, pending):
            entry = pending_entry["entry"]
            target_idx = pending_entry["target_idx"]
            min_prob = pending_entry["min_prob"]
            margin = pending_entry["margin"]
            background_margin = pending_entry["background_margin"]
            order = sorted(range(len(classes)), key=lambda idx: float(row[idx]), reverse=True)
            best_idx = order[0] if order else None
            best_label = classes[best_idx] if best_idx is not None else None
            best_prob = float(row[best_idx]) if best_idx is not None else None
            keep_mask = _clip_head_keep_mask(
                row.reshape(1, -1),
                target_index=target_idx,
                min_prob=min_prob,
                margin=margin,
                background_indices=bg_indices,
                background_guard=True,
                background_margin=background_margin,
            )
            entry["classifier_best"] = best_label
            entry["classifier_prob"] = best_prob
            entry["classifier_accept"] = bool(keep_mask[0]) if keep_mask is not None and len(keep_mask) else False
            if keep_mask is None or not bool(keep_mask[0]):
                rejected += 1
                continue
            cleaned.append(entry)
    return cleaned, rejected


def _agent_resolve_image(
    image_base64: Optional[str],
    image_token: Optional[str],
    sam_variant: Optional[str] = None,
) -> Tuple[Image.Image, np.ndarray, str]:
    if not image_base64 and not image_token:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="image_payload_missing")
    if image_token:
        variant = _default_variant(sam_variant)
        cached = _fetch_preloaded_image(image_token, variant)
        if cached is None:
            fallback_variant = "sam3" if variant == "sam1" else "sam1"
            cached = _fetch_preloaded_image(image_token, fallback_variant)
            if cached is not None:
                _store_preloaded_image(image_token, cached, variant)
        if cached is not None:
            pil_img = Image.fromarray(cached)
            return pil_img, cached, image_token
    return resolve_image_payload(image_base64, image_token, sam_variant)


def _normalize_window_xyxy(window: Optional[Any], img_w: int, img_h: int) -> Optional[Tuple[float, float, float, float]]:
    if not window:
        return None
    if isinstance(window, (list, tuple)) and len(window) >= 4:
        x1, y1, x2, y2 = map(float, window[:4])
        return max(0.0, x1), max(0.0, y1), min(float(img_w), x2), min(float(img_h), y2)
    if isinstance(window, dict):
        if "bbox_2d" in window:
            x1, y1, x2, y2 = _qwen_bbox_to_xyxy(img_w, img_h, window.get("bbox_2d") or [])
            return max(0.0, x1), max(0.0, y1), min(float(img_w), x2), min(float(img_h), y2)
        if all(k in window for k in ("x1", "y1", "x2", "y2")):
            x1 = float(window.get("x1"))
            y1 = float(window.get("y1"))
            x2 = float(window.get("x2"))
            y2 = float(window.get("y2"))
            return max(0.0, x1), max(0.0, y1), min(float(img_w), x2), min(float(img_h), y2)
    return None


def _window_bbox_2d_to_full_xyxy(
    img_w: int,
    img_h: int,
    window_bbox_2d: Optional[Sequence[float]],
) -> Optional[Tuple[float, float, float, float]]:
    if not window_bbox_2d:
        return None
    x1, y1, x2, y2 = _qwen_bbox_to_xyxy(img_w, img_h, window_bbox_2d)
    return x1, y1, x2, y2


def _window_local_bbox_2d_to_full_xyxy(
    img_w: int,
    img_h: int,
    window_bbox_2d: Optional[Sequence[float]],
    local_bbox_2d: Optional[Sequence[float]],
) -> Optional[Tuple[float, float, float, float]]:
    if not window_bbox_2d or not local_bbox_2d:
        return None
    window_xyxy = _window_bbox_2d_to_full_xyxy(img_w, img_h, window_bbox_2d)
    if not window_xyxy:
        return None
    wx1, wy1, wx2, wy2 = window_xyxy
    win_w = max(1.0, wx2 - wx1)
    win_h = max(1.0, wy2 - wy1)
    lx1, ly1, lx2, ly2 = _qwen_bbox_to_xyxy(int(win_w), int(win_h), local_bbox_2d)
    return lx1 + wx1, ly1 + wy1, lx2 + wx1, ly2 + wy1


def _window_local_xyxy_to_full_xyxy(
    window_xyxy: Optional[Tuple[float, float, float, float]],
    local_xyxy: Optional[Sequence[float]],
) -> Optional[Tuple[float, float, float, float]]:
    if not window_xyxy or not local_xyxy:
        return None
    wx1, wy1, _, _ = window_xyxy
    x1, y1, x2, y2 = map(float, local_xyxy[:4])
    return x1 + wx1, y1 + wy1, x2 + wx1, y2 + wy1


def _resolve_agent_bbox_xyxy(
    ann: Dict[str, Any],
    img_w: int,
    img_h: int,
    *,
    window_bbox_2d: Optional[Sequence[float]] = None,
) -> Optional[Tuple[float, float, float, float]]:
    bbox_space = str(ann.get("bbox_space") or "full").strip().lower()
    if bbox_space == "window":
        window_xyxy = _window_bbox_2d_to_full_xyxy(img_w, img_h, window_bbox_2d)
        if window_xyxy is None:
            return None
        if "bbox_2d" in ann:
            return _window_local_bbox_2d_to_full_xyxy(img_w, img_h, window_bbox_2d, ann.get("bbox_2d"))
        if "bbox_xyxy_px" in ann:
            coords = ann.get("bbox_xyxy_px")
            if isinstance(coords, (list, tuple)) and len(coords) >= 4:
                return _window_local_xyxy_to_full_xyxy(window_xyxy, coords)
            return None
        return None
    if "bbox_xyxy_px" in ann:
        try:
            coords = ann.get("bbox_xyxy_px")
            if isinstance(coords, (list, tuple)) and len(coords) >= 4:
                return tuple(map(float, coords[:4]))  # type: ignore[return-value]
            return None
        except Exception:
            return None
    if "bbox_2d" in ann:
        return _qwen_bbox_to_xyxy(img_w, img_h, ann.get("bbox_2d") or [])
    return None


def _agent_det_payload(
    img_w: int,
    img_h: int,
    xyxy: Tuple[float, float, float, float],
    *,
    label: Optional[str],
    class_id: Optional[int],
    score: Optional[float],
    source: str,
    window: Optional[Tuple[float, float, float, float]] = None,
) -> Dict[str, Any]:
    x1, y1, x2, y2 = xyxy
    bbox_xywh = _agent_xyxy_to_xywh(x1, y1, x2, y2)
    bbox_2d = _xyxy_to_qwen_bbox(img_w, img_h, x1, y1, x2, y2)
    payload = {
        "bbox_2d": list(bbox_2d),
        "bbox_xyxy_px": [float(x1), float(y1), float(x2), float(y2)],
        "bbox_xywh_px": bbox_xywh,
        "bbox_yolo": list(_xyxy_to_yolo_norm(img_w, img_h, x1, y1, x2, y2)),
        "label": label,
        "class_id": class_id,
        "score": score,
        "score_source": source if score is not None else "unknown",
        "source": source,
        "bbox_space": "full",
    }
    if window:
        payload["window_xyxy_px"] = [float(v) for v in window]
        payload["window_bbox_2d"] = list(_xyxy_to_qwen_bbox(img_w, img_h, *window))
    return payload


def _agent_quadrant_windows_qwen(overlap_ratio: float = 0.1) -> List[Dict[str, Any]]:
    overlap_ratio = max(0.0, min(float(overlap_ratio), 0.4))
    base = 1000.0
    if overlap_ratio <= 0.0:
        win = 500.0
    else:
        win = base / (2.0 - overlap_ratio)
    start = base - win
    x0 = 0.0
    y0 = 0.0
    x1 = max(0.0, min(base, start))
    y1 = max(0.0, min(base, start))
    win = max(1.0, min(base, win))
    windows = [
        {"name": "top_left", "bbox_2d": [x0, y0, x0 + win, y0 + win]},
        {"name": "top_right", "bbox_2d": [x1, y0, x1 + win, y0 + win]},
        {"name": "bottom_left", "bbox_2d": [x0, y1, x0 + win, y1 + win]},
        {"name": "bottom_right", "bbox_2d": [x1, y1, x1 + win, y1 + win]},
    ]
    clipped = []
    for window in windows:
        x1w, y1w, x2w, y2w = window["bbox_2d"]
        clipped.append(
            {
                "name": window["name"],
                "bbox_2d": [
                    max(0.0, min(base, x1w)),
                    max(0.0, min(base, y1w)),
                    max(0.0, min(base, x2w)),
                    max(0.0, min(base, y2w)),
                ],
            }
        )
    return clipped


@_register_agent_tool("run_detector")
def _agent_tool_run_detector(
    image_base64: Optional[str] = None,
    image_token: Optional[str] = None,
    detector_id: Optional[str] = None,
    mode: Optional[str] = "yolo",
    conf: Optional[float] = None,
    sahi: Optional[Dict[str, Any]] = None,
    window: Optional[Any] = None,
    window_bbox_2d: Optional[Sequence[float]] = None,
    grid_cell: Optional[str] = None,
    max_det: Optional[int] = None,
    iou: Optional[float] = None,
    merge_iou: Optional[float] = None,
    expected_labelmap: Optional[Sequence[str]] = None,
    register: Optional[bool] = True,
) -> Dict[str, Any]:
    pil_img, _, _ = _agent_resolve_image(image_base64, image_token)
    img_w, img_h = pil_img.size
    mode_norm = (mode or "yolo").strip().lower()
    if mode_norm not in {"yolo", "rfdetr"}:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_detector_mode_invalid")
    window_xyxy = _normalize_window_xyxy(window, img_w, img_h)
    if window_xyxy is None and window_bbox_2d is not None:
        window_xyxy = _normalize_window_xyxy({"bbox_2d": window_bbox_2d}, img_w, img_h)
    crop_img = pil_img
    offset_x = 0.0
    offset_y = 0.0
    if window_xyxy:
        x1, y1, x2, y2 = window_xyxy
        crop_img = pil_img.crop((x1, y1, x2, y2))
        offset_x, offset_y = x1, y1
    detections: List[Dict[str, Any]] = []
    warnings: List[str] = []
    if mode_norm == "yolo":
        model, labelmap, task = _ensure_yolo_inference_runtime()
        expected = list(expected_labelmap or (_AGENT_ACTIVE_LABELMAP or []))
        _raise_on_labelmap_mismatch(expected=expected or None, actual=labelmap, context="yolo")
        conf_val = _clamp_conf_value(float(conf) if conf is not None else 0.25, warnings)
        iou_val = _clamp_iou_value(float(iou) if iou is not None else 0.45, warnings)
        max_det_val = _clamp_max_det_value(int(max_det) if max_det is not None else 300, warnings)
        raw: List[Dict[str, Any]] = []
        if sahi and sahi.get("enabled"):
            try:
                slice_size = int(sahi.get("slice_size") or 640)
                overlap = float(sahi.get("overlap") or 0.2)
                merge_iou_val = float(merge_iou or sahi.get("merge_iou") or 0.5)
                slice_size, overlap, merge_iou_val = _clamp_slice_params(
                    slice_size, overlap, merge_iou_val, crop_img.width, crop_img.height, warnings
                )
                slices, starts = _slice_image_sahi(crop_img, slice_size, overlap)
                for tile, start in zip(slices, starts):
                    tile_offset_x = float(start[0]) + offset_x
                    tile_offset_y = float(start[1]) + offset_y
                    with YOLO_INFER_LOCK:
                        results = model.predict(
                            Image.fromarray(tile),
                            conf=conf_val,
                            iou=iou_val,
                            max_det=max_det_val,
                            verbose=False,
                        )
                    raw.extend(_yolo_extract_detections(results, labelmap, tile_offset_x, tile_offset_y, img_w, img_h))
                raw = _merge_detections_nms(raw, merge_iou_val, max_det_val)
            except HTTPException as exc:
                if "sahi_unavailable" in str(exc.detail):
                    warnings.append(str(exc.detail))
                else:
                    warnings.append(f"sahi_failed:{exc.detail}")
                raw = []
        if not raw:
            with YOLO_INFER_LOCK:
                results = model.predict(
                    crop_img,
                    conf=conf_val,
                    iou=iou_val,
                    max_det=max_det_val,
                    verbose=False,
                )
            raw = _yolo_extract_detections(results, labelmap, offset_x, offset_y, img_w, img_h)
        for det in raw:
            x1, y1, x2, y2 = _xywh_to_xyxy(det.get("bbox") or [])
            detections.append(
                _agent_det_payload(
                    img_w,
                    img_h,
                    (x1, y1, x2, y2),
                    label=det.get("class_name"),
                    class_id=det.get("class_id"),
                    score=det.get("score"),
                    source="yolo",
                    window=window_xyxy,
                )
            )
    else:
        model, labelmap, task = _ensure_rfdetr_inference_runtime()
        expected = list(expected_labelmap or (_AGENT_ACTIVE_LABELMAP or []))
        _raise_on_labelmap_mismatch(expected=expected or None, actual=labelmap, context="rfdetr")
        conf_val = _clamp_conf_value(float(conf) if conf is not None else 0.25, warnings)
        max_det_val = _clamp_max_det_value(int(max_det) if max_det is not None else 300, warnings)
        raw: List[Dict[str, Any]] = []
        if sahi and sahi.get("enabled"):
            try:
                slice_size = int(sahi.get("slice_size") or 640)
                overlap = float(sahi.get("overlap") or 0.2)
                merge_iou_val = float(merge_iou or sahi.get("merge_iou") or 0.5)
                slice_size, overlap, merge_iou_val = _clamp_slice_params(
                    slice_size, overlap, merge_iou_val, crop_img.width, crop_img.height, warnings
                )
                slices, starts = _slice_image_sahi(crop_img, slice_size, overlap)
                for tile, start in zip(slices, starts):
                    tile_offset_x = float(start[0]) + offset_x
                    tile_offset_y = float(start[1]) + offset_y
                    try:
                        with RFDETR_INFER_LOCK:
                            results = model.predict(Image.fromarray(tile), threshold=conf_val)
                    except Exception as exc:  # noqa: BLE001
                        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"rfdetr_predict_failed:{exc}") from exc
                    extracted, shifted = _rfdetr_extract_detections(results, labelmap, tile_offset_x, tile_offset_y, img_w, img_h)
                    if shifted:
                        if expected:
                            raise HTTPException(
                                status_code=HTTP_412_PRECONDITION_FAILED,
                                detail="detector_labelmap_shifted:rfdetr",
                            )
                        warnings.append("rfdetr_labelmap_shifted")
                    raw.extend(extracted)
                raw = _merge_detections_nms(raw, merge_iou_val, max_det_val)
            except HTTPException as exc:
                if "sahi_unavailable" in str(exc.detail):
                    warnings.append(str(exc.detail))
                else:
                    warnings.append(f"sahi_failed:{exc.detail}")
                raw = []
        if not raw:
            try:
                with RFDETR_INFER_LOCK:
                    results = model.predict(crop_img, threshold=conf_val)
            except Exception as exc:  # noqa: BLE001
                raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"rfdetr_predict_failed:{exc}") from exc
            raw, shifted = _rfdetr_extract_detections(results, labelmap, offset_x, offset_y, img_w, img_h)
            if shifted:
                if expected:
                    raise HTTPException(
                        status_code=HTTP_412_PRECONDITION_FAILED,
                        detail="detector_labelmap_shifted:rfdetr",
                    )
                warnings.append("rfdetr_labelmap_shifted")
        raw.sort(key=lambda det: float(det.get("score") or 0.0), reverse=True)
        for det in raw[:max_det_val]:
            x1, y1, x2, y2 = _xywh_to_xyxy(det.get("bbox") or [])
            detections.append(
                _agent_det_payload(
                    img_w,
                    img_h,
                    (x1, y1, x2, y2),
                    label=det.get("class_name"),
                    class_id=det.get("class_id"),
                    score=det.get("score"),
                    source="rfdetr",
                    window=window_xyxy,
                )
            )
    register_summary: Optional[Dict[str, Any]] = None
    if register:
        register_summary = _agent_register_detections(
            detections,
            img_w=img_w,
            img_h=img_h,
            grid=_AGENT_ACTIVE_GRID,
            labelmap=_AGENT_ACTIVE_LABELMAP or [],
            background=None,
            source_override=None,
            owner_cell=grid_cell,
        )
    new_cluster_ids = register_summary.get("new_cluster_ids") if isinstance(register_summary, dict) else []
    updated_cluster_ids = register_summary.get("updated_cluster_ids") if isinstance(register_summary, dict) else []
    new_summary = _agent_cluster_summaries(new_cluster_ids, include_ids=False)
    new_handles = _agent_handles_from_cluster_ids(new_cluster_ids or [])
    updated_handles = _agent_handles_from_cluster_ids(updated_cluster_ids or [])
    agent_view = {
        "mode": mode_norm,
        "grid_cell": grid_cell,
        "warnings": warnings or None,
        "new_clusters": register_summary.get("new_clusters") if isinstance(register_summary, dict) else 0,
        "new_handles": new_handles,
        "updated_clusters": len(updated_cluster_ids or []),
        "updated_handles": updated_handles,
        "new_items": new_summary.get("items"),
        "new_items_total": new_summary.get("total"),
        "new_items_truncated": new_summary.get("truncated"),
        "label_counts": _agent_cluster_label_counts(new_cluster_ids or []),
    }
    return {
        "detections": detections,
        "warnings": warnings or None,
        "register_summary": register_summary,
        "__agent_view__": agent_view,
    }


@_register_agent_tool("zoom_and_detect")
def _agent_tool_zoom_and_detect(
    image_base64: Optional[str] = None,
    image_token: Optional[str] = None,
    detector_id: Optional[str] = None,
    mode: Optional[str] = "yolo",
    conf: Optional[float] = None,
    intent: Optional[str] = None,
    bbox_2d: Optional[Sequence[float]] = None,
    window_bbox_2d: Optional[Sequence[float]] = None,
    bbox_space: Optional[str] = None,
    grid_cell: Optional[str] = None,
    confirm_label: Optional[str] = None,
    confirm_topk: Optional[int] = None,
    max_det: Optional[int] = None,
    iou: Optional[float] = None,
    merge_iou: Optional[float] = None,
) -> Dict[str, Any]:
    if window_bbox_2d is None and bbox_2d is not None:
        if bbox_space and str(bbox_space).lower() == "window":
            raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="window_bbox_required")
        window_bbox_2d = bbox_2d
    if window_bbox_2d is None:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="window_bbox_required")
    if conf is None and _AGENT_ACTIVE_DETECTOR_CONF is not None:
        conf = _AGENT_ACTIVE_DETECTOR_CONF
    mode_norm = (mode or "yolo").strip().lower()
    pil_img, _, _ = _agent_resolve_image(image_base64, image_token)
    img_w, img_h = pil_img.size
    window_xyxy = _normalize_window_xyxy({"bbox_2d": window_bbox_2d}, img_w, img_h)
    sahi_cfg = {"enabled": True}
    result = _agent_tool_run_detector(
        image_base64=image_base64,
        image_token=image_token,
        detector_id=detector_id,
        mode=mode_norm,
        conf=conf,
        sahi=sahi_cfg,
        window_bbox_2d=window_bbox_2d,
        max_det=max_det,
        iou=iou,
        merge_iou=merge_iou,
        register=False,
    )
    detections = result.get("detections") if isinstance(result, dict) else None
    if isinstance(detections, list):
        for det in detections:
            if not isinstance(det, dict):
                continue
            if det.get("source") and not det.get("source_list"):
                det["source_list"] = [det.get("source")]
    register_summary = None
    if register and isinstance(detections, list):
        register_summary = _agent_register_detections(
            detections,
            img_w=img_w,
            img_h=img_h,
            grid=_AGENT_ACTIVE_GRID,
            labelmap=_AGENT_ACTIVE_LABELMAP or [],
            background=None,
            source_override=f"{mode_norm}_zoom",
            owner_cell=grid_cell,
        )
        result["register_summary"] = register_summary
    confirmation = None
    if confirm_label or confirm_topk:
        if window_xyxy is None:
            confirmation = {"label": confirm_label, "error": "window_bbox_invalid"}
        else:
            try:
                classify = _agent_tool_classify_crop(
                    image_base64=image_base64,
                    image_token=image_token,
                    bbox_xyxy_px=list(window_xyxy),
                    bbox_space="full",
                    topk=confirm_topk,
                )
                best = classify.get("best") if isinstance(classify, dict) else None
                best_label = best.get("label") if isinstance(best, dict) else None
                confirmation = {
                    "label": confirm_label,
                    "window_xyxy_px": list(window_xyxy),
                    "window_bbox_2d": list(_xyxy_to_qwen_bbox(img_w, img_h, *window_xyxy)),
                    "best": best,
                    "topk": classify.get("topk") if isinstance(classify, dict) else None,
                    "background_topk": classify.get("background_topk") if isinstance(classify, dict) else None,
                    "label_match": bool(best_label == confirm_label) if confirm_label and best_label else None,
                }
            except HTTPException as exc:
                detail = exc.detail if isinstance(exc.detail, str) else str(exc.detail)
                confirmation = {"label": confirm_label, "error": detail}
    if window_xyxy is not None:
        result["window_xyxy_px"] = list(window_xyxy)
        result["window_bbox_2d"] = list(window_bbox_2d or [])
    if confirmation is not None:
        result["confirmation"] = confirmation
    register_summary = result.get("register_summary") if isinstance(result, dict) else register_summary
    new_cluster_ids = register_summary.get("new_cluster_ids") if isinstance(register_summary, dict) else []
    updated_cluster_ids = register_summary.get("updated_cluster_ids") if isinstance(register_summary, dict) else []
    new_summary = _agent_cluster_summaries(new_cluster_ids, include_ids=False)
    new_handles = _agent_handles_from_cluster_ids(new_cluster_ids or [])
    updated_handles = _agent_handles_from_cluster_ids(updated_cluster_ids or [])
    agent_view = {
        "mode": mode_norm,
        "grid_cell": grid_cell,
        "intent": (intent or "").strip() or None,
        "new_clusters": register_summary.get("new_clusters") if isinstance(register_summary, dict) else 0,
        "new_handles": new_handles,
        "updated_clusters": len(updated_cluster_ids or []),
        "updated_handles": updated_handles,
        "new_items": new_summary.get("items"),
        "new_items_total": new_summary.get("total"),
        "new_items_truncated": new_summary.get("truncated"),
        "label_counts": _agent_cluster_label_counts(new_cluster_ids or []),
    }
    result["__agent_view__"] = agent_view
    return result


@_register_agent_tool("sam3_text")
def _sam3_text_payloads_from_state(
    *,
    full_img: Image.Image,
    crop_img: Image.Image,
    prompt: str,
    label: Optional[str],
    score_thr: Optional[float],
    mask_threshold: Optional[float],
    max_results: Optional[int],
    window_xyxy: Optional[Sequence[float]] = None,
    processor_override: Optional[Any] = None,
    state: Optional[Any] = None,
) -> Tuple[List[Dict[str, Any]], Optional[str], Optional[int]]:
    img_w, img_h = full_img.size
    offset_x = 0.0
    offset_y = 0.0
    if window_xyxy:
        offset_x, offset_y = float(window_xyxy[0]), float(window_xyxy[1])
    threshold_val = float(score_thr) if score_thr is not None else 0.2
    mask_val = float(mask_threshold) if mask_threshold is not None else 0.2
    detections = _run_sam3_text_inference(
        crop_img,
        prompt.strip(),
        threshold_val,
        mask_val,
        max_results,
        processor_override=processor_override,
        state=state,
    )
    aligned_label = _agent_fuzzy_align_label(label, _AGENT_ACTIVE_LABELMAP or [])
    assigned_label = aligned_label or (label or "").strip() or None
    class_id = None
    if assigned_label and _AGENT_ACTIVE_LABELMAP:
        for idx, item in enumerate(_AGENT_ACTIVE_LABELMAP):
            if str(item).strip().lower() == str(assigned_label).strip().lower():
                class_id = idx
                break
    payloads: List[Dict[str, Any]] = []
    for det in detections:
        x1, y1, x2, y2 = _yolo_to_xyxy(crop_img.width, crop_img.height, det.bbox)
        x1 += offset_x
        y1 += offset_y
        x2 += offset_x
        y2 += offset_y
        payloads.append(
            _agent_det_payload(
                img_w,
                img_h,
                (x1, y1, x2, y2),
                label=assigned_label or det.qwen_label,
                class_id=class_id if assigned_label else det.class_id,
                score=det.score,
                source="sam3_text",
                window=window_xyxy,
            )
        )
    return payloads, assigned_label, class_id


def _agent_tool_sam3_text(
    image_base64: Optional[str] = None,
    image_token: Optional[str] = None,
    prompt: Optional[str] = None,
    label: Optional[str] = None,
    score_thr: Optional[float] = None,
    mask_threshold: Optional[float] = None,
    max_results: Optional[int] = None,
    window: Optional[Any] = None,
    window_bbox_2d: Optional[Sequence[float]] = None,
    grid_cell: Optional[str] = None,
    register: Optional[bool] = True,
) -> Dict[str, Any]:
    pil_img, _, _ = _agent_resolve_image(image_base64, image_token, "sam3")
    img_w, img_h = pil_img.size
    window_xyxy = _normalize_window_xyxy(window, img_w, img_h)
    if window_xyxy is None and window_bbox_2d is not None:
        window_xyxy = _normalize_window_xyxy({"bbox_2d": window_bbox_2d}, img_w, img_h)
    crop_img = pil_img
    if window_xyxy:
        x1, y1, x2, y2 = window_xyxy
        crop_img = pil_img.crop((x1, y1, x2, y2))
    payloads, assigned_label, _ = _sam3_text_payloads_from_state(
        full_img=pil_img,
        crop_img=crop_img,
        prompt=(prompt or "").strip(),
        label=label,
        score_thr=score_thr,
        mask_threshold=mask_threshold,
        max_results=max_results,
        window_xyxy=window_xyxy,
    )
    register_summary: Optional[Dict[str, Any]] = None
    if register:
        source_override = "sam3_text"
        register_summary = _agent_register_detections(
            payloads,
            img_w=img_w,
            img_h=img_h,
            grid=_AGENT_ACTIVE_GRID,
            labelmap=_AGENT_ACTIVE_LABELMAP or [],
            background=None,
            source_override=source_override,
            owner_cell=grid_cell,
        )
    new_cluster_ids = register_summary.get("new_cluster_ids") if isinstance(register_summary, dict) else []
    updated_cluster_ids = register_summary.get("updated_cluster_ids") if isinstance(register_summary, dict) else []
    new_summary = _agent_cluster_summaries(new_cluster_ids, include_ids=False)
    new_handles = _agent_handles_from_cluster_ids(new_cluster_ids or [])
    updated_handles = _agent_handles_from_cluster_ids(updated_cluster_ids or [])
    agent_view = {
        "label": assigned_label,
        "prompt": (prompt or "").strip() or None,
        "grid_cell": grid_cell,
        "new_clusters": register_summary.get("new_clusters") if isinstance(register_summary, dict) else 0,
        "new_handles": new_handles,
        "updated_clusters": len(updated_cluster_ids or []),
        "updated_handles": updated_handles,
        "new_items": new_summary.get("items"),
        "new_items_total": new_summary.get("total"),
        "new_items_truncated": new_summary.get("truncated"),
        "label_counts": _agent_cluster_label_counts(new_cluster_ids or []),
    }
    return {
        "detections": payloads,
        "prompt": prompt,
        "label": assigned_label,
        "window": window_xyxy,
        "register_summary": register_summary,
        "__agent_view__": agent_view,
    }


@_register_agent_tool("sam3_similarity")
def _agent_tool_sam3_similarity(
    image_base64: Optional[str] = None,
    image_token: Optional[str] = None,
    exemplar_boxes: Optional[List[Dict[str, Any]]] = None,
    exemplar_cluster_ids: Optional[List[int]] = None,
    exemplar_handles: Optional[List[str]] = None,
    label: Optional[str] = None,
    score_thr: Optional[float] = None,
    mask_threshold: Optional[float] = None,
    max_results: Optional[int] = None,
    bbox_labels: Optional[List[bool]] = None,
    window: Optional[Any] = None,
    window_bbox_2d: Optional[Sequence[float]] = None,
    bbox_space: Optional[str] = None,
    grid_cell: Optional[str] = None,
    register: Optional[bool] = True,
) -> Dict[str, Any]:
    pil_img, _, _ = _agent_resolve_image(image_base64, image_token, "sam3")
    img_w, img_h = pil_img.size
    assigned_label = str(label).strip() if label is not None else ""
    if not assigned_label:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="sam3_similarity_label_required")
    window_xyxy = _normalize_window_xyxy(window, img_w, img_h)
    if window_xyxy is None and window_bbox_2d is not None:
        window_xyxy = _normalize_window_xyxy({"bbox_2d": window_bbox_2d}, img_w, img_h)
    crop_img = pil_img
    offset_x = 0.0
    offset_y = 0.0
    if window_xyxy:
        x1, y1, x2, y2 = window_xyxy
        crop_img = pil_img.crop((x1, y1, x2, y2))
        offset_x, offset_y = x1, y1
    exemplar_boxes = list(exemplar_boxes or [])
    exemplar_ids: List[int] = []
    if exemplar_cluster_ids:
        exemplar_ids.extend([int(cid) for cid in exemplar_cluster_ids])
    if exemplar_handles:
        exemplar_ids.extend(_agent_cluster_ids_from_handles(exemplar_handles))
    for cid in exemplar_ids:
        cluster = _AGENT_ACTIVE_CLUSTER_INDEX.get(int(cid))
        if not cluster:
            continue
        bbox = cluster.get("bbox_2d")
        if isinstance(bbox, (list, tuple)) and len(bbox) >= 4:
            exemplar_boxes.append({"bbox_2d": list(bbox[:4]), "bbox_space": "full"})
    boxes_xywh: List[Tuple[float, float, float, float]] = []
    for box in exemplar_boxes:
        ann: Dict[str, Any] = {}
        window_ref = window_bbox_2d
        if isinstance(box, dict):
            ann["bbox_space"] = box.get("bbox_space") or bbox_space or "full"
            if "bbox_2d" in box:
                ann["bbox_2d"] = box.get("bbox_2d")
            if "bbox_xyxy_px" in box:
                ann["bbox_xyxy_px"] = box.get("bbox_xyxy_px")
            if box.get("window_bbox_2d") is not None:
                window_ref = box.get("window_bbox_2d")
        elif isinstance(box, (list, tuple)) and len(box) >= 4:
            ann["bbox_xyxy_px"] = list(box[:4])
            ann["bbox_space"] = bbox_space or "full"
        xyxy_full = _resolve_agent_bbox_xyxy(ann, img_w, img_h, window_bbox_2d=window_ref)
        if xyxy_full is None:
            continue
        x1, y1, x2, y2 = xyxy_full
        if window_xyxy:
            x1 -= offset_x
            y1 -= offset_y
            x2 -= offset_x
            y2 -= offset_y
        w = max(0.0, x2 - x1)
        h = max(0.0, y2 - y1)
        if w <= 0 or h <= 0:
            continue
        boxes_xywh.append((x1, y1, w, h))
    if not boxes_xywh:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="sam3_similarity_exemplar_required")
    threshold_val = float(score_thr) if score_thr is not None else 0.2
    mask_val = float(mask_threshold) if mask_threshold is not None else 0.2
    detections = _run_sam3_visual_inference_multi(
        crop_img,
        boxes_xywh,
        bbox_labels,
        threshold_val,
        mask_val,
        max_results,
    )
    assigned_label = assigned_label
    payloads: List[Dict[str, Any]] = []
    for det in detections:
        x1, y1, x2, y2 = _yolo_to_xyxy(crop_img.width, crop_img.height, det.bbox)
        x1 += offset_x
        y1 += offset_y
        x2 += offset_x
        y2 += offset_y
        payloads.append(
            _agent_det_payload(
                img_w,
                img_h,
                (x1, y1, x2, y2),
                label=assigned_label,
                class_id=None,
                score=det.score,
                source="sam3_similarity",
                window=window_xyxy,
            )
        )
    register_summary: Optional[Dict[str, Any]] = None
    if register:
        register_summary = _agent_register_detections(
            payloads,
            img_w=img_w,
            img_h=img_h,
            grid=_AGENT_ACTIVE_GRID,
            labelmap=_AGENT_ACTIVE_LABELMAP or [],
            background=None,
            source_override="sam3_similarity",
            owner_cell=grid_cell,
        )
    new_cluster_ids = register_summary.get("new_cluster_ids") if isinstance(register_summary, dict) else []
    updated_cluster_ids = register_summary.get("updated_cluster_ids") if isinstance(register_summary, dict) else []
    new_summary = _agent_cluster_summaries(new_cluster_ids, include_ids=False)
    new_handles = _agent_handles_from_cluster_ids(new_cluster_ids or [])
    updated_handles = _agent_handles_from_cluster_ids(updated_cluster_ids or [])
    exemplar_handles_out = exemplar_handles or _agent_handles_from_cluster_ids(exemplar_ids)
    agent_view = {
        "label": assigned_label,
        "grid_cell": grid_cell,
        "exemplar_handles": exemplar_handles_out,
        "new_clusters": register_summary.get("new_clusters") if isinstance(register_summary, dict) else 0,
        "new_handles": new_handles,
        "updated_clusters": len(updated_cluster_ids or []),
        "updated_handles": updated_handles,
        "new_items": new_summary.get("items"),
        "new_items_total": new_summary.get("total"),
        "new_items_truncated": new_summary.get("truncated"),
        "label_counts": _agent_cluster_label_counts(new_cluster_ids or []),
    }
    return {
        "detections": payloads,
        "label": assigned_label,
        "window": window_xyxy,
        "register_summary": register_summary,
        "__agent_view__": agent_view,
    }


@_register_agent_tool("qwen_infer")
def _agent_tool_qwen_infer(
    image_base64: Optional[str] = None,
    image_token: Optional[str] = None,
    prompt: Optional[str] = None,
    item_list: Optional[str] = None,
    items: Optional[List[str]] = None,
    image_type: Optional[str] = None,
    extra_context: Optional[str] = None,
    prompt_type: Optional[str] = None,
    max_results: Optional[int] = None,
    max_new_tokens: Optional[int] = None,
    sam_variant: Optional[str] = None,
    window: Optional[Any] = None,
    window_bbox_2d: Optional[Sequence[float]] = None,
    grid_cell: Optional[str] = None,
    register: Optional[bool] = True,
) -> Dict[str, Any]:
    pil_img, np_img, token = _agent_resolve_image(image_base64, image_token, sam_variant)
    img_w, img_h = pil_img.size
    window_xyxy = _normalize_window_xyxy(window, img_w, img_h)
    if window_xyxy is None and window_bbox_2d is not None:
        window_xyxy = _normalize_window_xyxy({"bbox_2d": window_bbox_2d}, img_w, img_h)
    crop_img = pil_img
    crop_np = np_img
    offset_x = 0.0
    offset_y = 0.0
    crop_token = token
    if window_xyxy:
        x1, y1, x2, y2 = window_xyxy
        crop_img = pil_img.crop((x1, y1, x2, y2))
        crop_np = np.asarray(crop_img)
        crop_token = hashlib.md5(crop_np.tobytes()).hexdigest()
        _store_preloaded_image(crop_token, crop_np, _default_variant(sam_variant))
        offset_x, offset_y = x1, y1
    prompt_type = (prompt_type or "bbox").strip().lower()
    if prompt_type not in {"bbox", "point", "bbox_sam"}:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_qwen_prompt_type_invalid")
    manual_prompt = (prompt or "").strip()
    if not manual_prompt:
        if items:
            item_list = ", ".join([str(item).strip() for item in items if str(item).strip()])
        item_list = (item_list or "").strip()
        if not item_list:
            raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="agent_qwen_items_required")
        manual_prompt = _render_qwen_prompt(
            prompt_type,
            items=item_list,
            image_type=(image_type or "").strip() or None,
            extra_context=(extra_context or "").strip() or None,
        )
    try:
        qwen_text, proc_w, proc_h = _run_qwen_inference(manual_prompt, crop_img, max_new_tokens=max_new_tokens)
    except HTTPException:
        raise
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=f"qwen_infer_failed:{exc}") from exc
    warnings: List[str] = []
    try:
        _, parsed_items = _extract_qwen_json_block(qwen_text)
    except HTTPException as exc:
        detail = exc.detail if isinstance(exc.detail, str) else str(exc.detail)
        warnings.append(f"parse_error:{detail}")
        return {
            "detections": [],
            "warnings": warnings,
            "prompt": manual_prompt,
            "prompt_type": prompt_type,
            "raw_response": qwen_text,
            "__agent_view__": {
                "grid_cell": grid_cell,
                "prompt_type": prompt_type,
                "new_clusters": 0,
                "new_handles": [],
                "updated_clusters": 0,
                "updated_handles": [],
                "new_items": [],
                "new_items_total": 0,
                "new_items_truncated": False,
                "label_counts": {},
                "warnings": warnings,
            },
        }
    normalized_items = _qwen_items_from_payload(parsed_items)
    if not normalized_items:
        warnings.append("no_results")
        return {
            "detections": [],
            "warnings": warnings,
            "prompt": manual_prompt,
            "prompt_type": prompt_type,
            "raw_response": qwen_text,
            "__agent_view__": {
                "grid_cell": grid_cell,
                "prompt_type": prompt_type,
                "new_clusters": 0,
                "new_handles": [],
                "updated_clusters": 0,
                "updated_handles": [],
                "new_items": [],
                "new_items_total": 0,
                "new_items_truncated": False,
                "label_counts": {},
                "warnings": warnings,
            },
        }
    limit = int(max_results) if max_results is not None else 8
    variant = _default_variant(sam_variant)
    if prompt_type == "bbox":
        boxes = _qwen_bbox_results(normalized_items, proc_w, proc_h, crop_img.width, crop_img.height, limit=limit)
    elif prompt_type == "bbox_sam":
        boxes = _qwen_bbox_sam_results(
            normalized_items,
            proc_w,
            proc_h,
            crop_img,
            crop_np,
            crop_token,
            variant,
            image_name=None,
            limit=limit,
        )
    else:
        boxes = _qwen_point_results(
            normalized_items,
            proc_w,
            proc_h,
            crop_img,
            crop_np,
            crop_token,
            variant,
            image_name=None,
            limit=limit,
        )
    payloads: List[Dict[str, Any]] = []
    for det in boxes:
        x1, y1, x2, y2 = yolo_to_corners(det.bbox, crop_img.width, crop_img.height)
        x1 += offset_x
        y1 += offset_y
        x2 += offset_x
        y2 += offset_y
        raw_label = det.qwen_label or det.class_name
        aligned_label = _agent_fuzzy_align_label(raw_label, _AGENT_ACTIVE_LABELMAP or [])
        payloads.append(
            _agent_det_payload(
                img_w,
                img_h,
                (x1, y1, x2, y2),
                label=aligned_label or raw_label,
                class_id=det.class_id,
                score=det.score,
                source=f"qwen_{det.source}",
                window=window_xyxy,
            )
        )
    if not payloads:
        warnings.append("no_results")
    register_summary: Optional[Dict[str, Any]] = None
    if register:
        register_summary = _agent_register_detections(
            payloads,
            img_w=img_w,
            img_h=img_h,
            grid=_AGENT_ACTIVE_GRID,
            labelmap=_AGENT_ACTIVE_LABELMAP or [],
            background=None,
            source_override="qwen_infer",
            owner_cell=grid_cell,
        )
    new_cluster_ids = register_summary.get("new_cluster_ids") if isinstance(register_summary, dict) else []
    updated_cluster_ids = register_summary.get("updated_cluster_ids") if isinstance(register_summary, dict) else []
    new_summary = _agent_cluster_summaries(new_cluster_ids, include_ids=False)
    new_handles = _agent_handles_from_cluster_ids(new_cluster_ids or [])
    updated_handles = _agent_handles_from_cluster_ids(updated_cluster_ids or [])
    agent_view = {
        "grid_cell": grid_cell,
        "prompt_type": prompt_type,
        "new_clusters": register_summary.get("new_clusters") if isinstance(register_summary, dict) else 0,
        "new_handles": new_handles,
        "updated_clusters": len(updated_cluster_ids or []),
        "updated_handles": updated_handles,
        "new_items": new_summary.get("items"),
        "new_items_total": new_summary.get("total"),
        "new_items_truncated": new_summary.get("truncated"),
        "label_counts": _agent_cluster_label_counts(new_cluster_ids or []),
        "warnings": warnings or None,
    }
    return {
        "detections": payloads,
        "warnings": warnings or None,
        "prompt": manual_prompt,
        "prompt_type": prompt_type,
        "raw_response": qwen_text,
        "window": window_xyxy,
        "register_summary": register_summary,
        "__agent_view__": agent_view,
    }


@_register_agent_tool("classify_crop")
def _agent_tool_classify_crop(
    image_base64: Optional[str] = None,
    image_token: Optional[str] = None,
    bbox_2d: Optional[Sequence[float]] = None,
    bbox_xyxy_px: Optional[Sequence[float]] = None,
    window_bbox_2d: Optional[Sequence[float]] = None,
    bbox_space: Optional[str] = None,
    cluster_id: Optional[int] = None,
    handle: Optional[str] = None,
    label_hint: Optional[str] = None,
    classifier_id: Optional[str] = None,
    topk: Optional[int] = None,
) -> Dict[str, Any]:
    pil_img, np_img, _ = _agent_resolve_image(image_base64, image_token)
    img_w, img_h = pil_img.size
    ann = {"bbox_space": bbox_space or "full"}
    cluster = None
    if cluster_id is None and handle:
        cluster_id = _agent_cluster_id_from_handle(handle)
    if cluster_id is not None:
        cluster = _AGENT_ACTIVE_CLUSTER_INDEX.get(int(cluster_id))
        if not cluster:
            raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="cluster_not_found")
        bbox_xyxy_px = cluster.get("bbox_xyxy_px")
        bbox_2d = cluster.get("bbox_2d")
    if bbox_xyxy_px is not None:
        ann["bbox_xyxy_px"] = list(bbox_xyxy_px[:4])
    if bbox_2d is not None:
        ann["bbox_2d"] = list(bbox_2d[:4])
    xyxy = _resolve_agent_bbox_xyxy(ann, img_w, img_h, window_bbox_2d=window_bbox_2d)
    if xyxy is None:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="bbox_required")
    x1, y1, x2, y2 = xyxy
    x1 = max(0.0, min(float(img_w), x1))
    y1 = max(0.0, min(float(img_h), y1))
    x2 = max(0.0, min(float(img_w), x2))
    y2 = max(0.0, min(float(img_h), y2))
    if x2 <= x1 or y2 <= y1:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="invalid_bbox")
    crop = pil_img.crop((x1, y1, x2, y2))
    head: Optional[Dict[str, Any]] = None
    if classifier_id:
        classifier_path = _resolve_agent_clip_classifier_path(classifier_id)
        if classifier_path is not None:
            head = _load_clip_head_from_classifier(classifier_path)
    elif isinstance(active_classifier_head, dict):
        head = active_classifier_head
    if _AGENT_TRACE_FULL_WRITER is not None:
        _trace_write_full(
            {
                "type": "classifier_head",
                "present": bool(head),
                "classifier_id": classifier_id,
                "head_type": type(head).__name__ if head is not None else None,
            }
        )
    if not isinstance(head, dict):
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail="classifier_unavailable")
    feats = _encode_pil_batch_for_head([crop], head=head)
    if feats is None or not isinstance(feats, np.ndarray) or feats.size == 0:
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail="classifier_encode_failed")
    proba_arr = _clip_head_predict_proba(feats, head)
    classes = [str(c) for c in list(head.get("classes") or [])]
    if proba_arr is None or proba_arr.ndim != 2 or proba_arr.shape[0] < 1 or proba_arr.shape[1] != len(classes):
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail="classifier_predict_failed")
    row = proba_arr[0]
    k = max(1, min(int(topk) if topk is not None else 5, len(classes)))
    order = sorted(range(len(classes)), key=lambda idx: float(row[idx]), reverse=True)
    bg_indices = _clip_head_background_indices(classes)
    topk_items = [
        {"label": classes[idx], "prob": float(row[idx])}
        for idx in order[:k]
    ]
    background_topk = [
        {"label": classes[idx], "prob": float(row[idx])}
        for idx in order
        if idx in bg_indices
    ][:k]
    best = topk_items[0] if topk_items else {"label": "unknown", "prob": None}
    accept = None
    label_target = (label_hint or (cluster.get("label") if cluster else None) or "").strip()
    if label_target:
        target_idx = _find_clip_head_target_index(classes, label_target)
        if target_idx is not None:
            keep_mask = _clip_head_keep_mask(
                proba_arr,
                target_index=target_idx,
                min_prob=float(head.get("min_prob") or 0.5),
                margin=float(head.get("margin") or 0.0),
                background_indices=bg_indices,
                background_guard=True,
                background_margin=float(head.get("background_margin") or 0.0),
            )
            accept = bool(keep_mask[0]) if keep_mask is not None and len(keep_mask) else False
    if cluster is not None:
        cluster["classifier_best"] = best.get("label")
        cluster["classifier_prob"] = best.get("prob")
        cluster["classifier_accept"] = accept
    handle_out = _agent_cluster_handle(cluster) if cluster else None
    agent_view = {
        "handle": handle_out,
        "label_hint": label_target or None,
        "best": best,
        "topk": topk_items[:5],
        "accept": accept,
    }
    return {
        "topk": topk_items,
        "background_topk": background_topk,
        "best": best,
        "accept": accept,
        "__agent_view__": agent_view,
    }


@_register_agent_tool("image_zoom_in_tool")
def _agent_tool_image_zoom_in(
    image_base64: Optional[str] = None,
    image_token: Optional[str] = None,
    bbox_2d: Optional[Sequence[float]] = None,
    bbox_xyxy_px: Optional[Sequence[float]] = None,
    window_bbox_2d: Optional[Sequence[float]] = None,
    bbox_space: Optional[str] = None,
    label: Optional[str] = None,
    grid_cell: Optional[str] = None,
    intent: Optional[str] = None,
    cluster_id: Optional[int] = None,
    handle: Optional[str] = None,
) -> Dict[str, Any]:
    pil_img, _, _ = _agent_resolve_image(image_base64, image_token)
    img_w, img_h = pil_img.size
    ann = {"bbox_space": bbox_space or "full"}
    cluster = None
    if cluster_id is None and handle:
        cluster_id = _agent_cluster_id_from_handle(handle)
    if cluster_id is not None:
        cluster = _AGENT_ACTIVE_CLUSTER_INDEX.get(int(cluster_id))
        if not cluster:
            raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="cluster_not_found")
        bbox_xyxy_px = cluster.get("bbox_xyxy_px")
        bbox_2d = cluster.get("bbox_2d")
    if bbox_xyxy_px is not None:
        ann["bbox_xyxy_px"] = list(bbox_xyxy_px[:4])
    if bbox_2d is not None:
        ann["bbox_2d"] = list(bbox_2d[:4])
    xyxy = _resolve_agent_bbox_xyxy(ann, img_w, img_h, window_bbox_2d=window_bbox_2d)
    if xyxy is None:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="bbox_required")
    x1, y1, x2, y2 = xyxy
    x1 = max(0.0, min(float(img_w), x1))
    y1 = max(0.0, min(float(img_h), y1))
    x2 = max(0.0, min(float(img_w), x2))
    y2 = max(0.0, min(float(img_h), y2))
    if x2 <= x1 or y2 <= y1:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="invalid_bbox")
    orig_xyxy = (x1, y1, x2, y2)
    expanded = False
    if not grid_cell:
        (x1, y1, x2, y2), expanded = _agent_expand_window_xyxy(
            x1,
            y1,
            x2,
            y2,
            img_w,
            img_h,
            PREPASS_MIN_ZOOM_WINDOW_PX,
        )
    crop = pil_img.crop((x1, y1, x2, y2))
    crop_np = np.asarray(crop)
    token = hashlib.md5(crop_np.tobytes()).hexdigest()
    _store_preloaded_image(token, crop_np, _default_variant(None))
    window_bbox_2d = _xyxy_to_qwen_bbox(img_w, img_h, x1, y1, x2, y2)
    original_window_bbox_2d = _xyxy_to_qwen_bbox(img_w, img_h, *orig_xyxy)
    handle_out = _agent_cluster_handle(cluster) if cluster else None
    agent_view = {
        "grid_cell": grid_cell,
        "handle": handle_out,
        "label": (label or "").strip() or None,
        "intent": (intent or "").strip() or None,
        "width": int(crop.width),
        "height": int(crop.height),
        "expanded": expanded,
    }
    return {
        "image_token": token,
        "window_xyxy_px": [float(x1), float(y1), float(x2), float(y2)],
        "window_bbox_2d": list(window_bbox_2d),
        "label": (label or "").strip(),
        "width": int(crop.width),
        "height": int(crop.height),
        "expanded": expanded,
        "original_window_xyxy_px": list(orig_xyxy) if expanded else None,
        "original_window_bbox_2d": list(original_window_bbox_2d) if expanded else None,
        "__agent_view__": agent_view,
    }


@_register_agent_tool("view_cell_raw")
def _agent_tool_view_cell_raw(
    image_base64: Optional[str] = None,
    image_token: Optional[str] = None,
    grid_cell: Optional[str] = None,
) -> Dict[str, Any]:
    if not grid_cell:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="grid_cell_required")
    if not _AGENT_ACTIVE_GRID:
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail="grid_unavailable")
    cell_xyxy = _agent_grid_cell_xyxy(
        _AGENT_ACTIVE_GRID,
        str(grid_cell),
        overlap_ratio=PREPASS_GRID_OVERLAP_RATIO,
    )
    if not cell_xyxy:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="grid_cell_invalid")
    base_img = _AGENT_ACTIVE_GRID_IMAGE
    if base_img is None:
        base_img, _, _ = _agent_resolve_image(image_base64, image_token)
    x1, y1, x2, y2 = cell_xyxy
    crop = base_img.crop((x1, y1, x2, y2))
    crop_np = np.asarray(crop)
    token = hashlib.md5(crop_np.tobytes()).hexdigest()
    _store_preloaded_image(token, crop_np, _default_variant(None))
    agent_view = {
        "grid_cell": str(grid_cell),
        "width": int(crop.width),
        "height": int(crop.height),
    }
    return {
        "image_token": token,
        "grid_cell": str(grid_cell),
        "width": int(crop.width),
        "height": int(crop.height),
        "__agent_view__": agent_view,
    }


@_register_agent_tool("view_cell_overlay")
def _agent_tool_view_cell_overlay(
    image_base64: Optional[str] = None,
    image_token: Optional[str] = None,
    grid_cell: Optional[str] = None,
) -> Dict[str, Any]:
    if not grid_cell:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="grid_cell_required")
    if not _AGENT_ACTIVE_GRID:
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail="grid_unavailable")
    cell_xyxy = _agent_grid_cell_xyxy(
        _AGENT_ACTIVE_GRID,
        str(grid_cell),
        overlap_ratio=PREPASS_GRID_OVERLAP_RATIO,
    )
    if not cell_xyxy:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="grid_cell_invalid")
    base_img = _AGENT_ACTIVE_GRID_IMAGE
    if base_img is None:
        base_img, _, _ = _agent_resolve_image(image_base64, image_token)
    overlay_img = base_img
    clusters = list(_AGENT_ACTIVE_CLUSTERS or [])
    if clusters:
        labels = list(_AGENT_ACTIVE_LABELMAP or [])
        if not labels:
            labels = sorted(
                {
                    str(cluster.get("label") or "").strip()
                    for cluster in clusters
                    if isinstance(cluster, dict) and cluster.get("label")
                }
            )
        label_colors = _agent_current_label_colors(labels)
        label_prefixes = _agent_current_label_prefixes(labels)
        overlay_img = _agent_render_detection_overlay(
            base_img,
            clusters,
            label_colors,
            label_prefixes=label_prefixes,
            dot_radius=_AGENT_ACTIVE_OVERLAY_DOT_RADIUS,
        )
        _AGENT_ACTIVE_OVERLAY_IMAGE = overlay_img
    x1, y1, x2, y2 = cell_xyxy
    crop = overlay_img.crop((x1, y1, x2, y2))
    crop_np = np.asarray(crop)
    token = hashlib.md5(crop_np.tobytes()).hexdigest()
    _store_preloaded_image(token, crop_np, _default_variant(None))
    agent_view = {
        "grid_cell": str(grid_cell),
        "width": int(crop.width),
        "height": int(crop.height),
    }
    return {
        "image_token": token,
        "grid_cell": str(grid_cell),
        "width": int(crop.width),
        "height": int(crop.height),
        "__agent_view__": agent_view,
    }


@_register_agent_tool("view_full_overlay")
def _agent_tool_view_full_overlay(
    image_base64: Optional[str] = None,
    image_token: Optional[str] = None,
) -> Dict[str, Any]:
    base_img = _AGENT_ACTIVE_GRID_IMAGE
    if base_img is None:
        base_img, _, _ = _agent_resolve_image(image_base64, image_token)
    clusters = list(_AGENT_ACTIVE_CLUSTERS or [])
    labels = _agent_overlay_labels(clusters)
    label_colors = _agent_current_label_colors(labels) if labels else {}
    label_prefixes = _agent_current_label_prefixes(labels) if labels else {}
    overlay_img = base_img
    if clusters:
        overlay_img = _agent_render_detection_overlay(
            base_img,
            clusters,
            label_colors,
            dot_radius=_AGENT_ACTIVE_OVERLAY_DOT_RADIUS,
            label_prefixes=label_prefixes,
        )
        _AGENT_ACTIVE_OVERLAY_IMAGE = overlay_img
    usage_rows = _agent_grid_usage_rows(_AGENT_ACTIVE_GRID)
    usage_text = _agent_grid_usage_text(usage_rows)
    agent_view = {
        "grid_usage": usage_rows,
        "grid_usage_text": usage_text,
        "total_cells": len(usage_rows),
        "overlay_key": _agent_overlay_key_text(label_colors, label_prefixes),
    }
    return {
        "width": int(overlay_img.width),
        "height": int(overlay_img.height),
        "__agent_view__": agent_view,
    }


@_register_agent_tool("get_tile_context")
def _agent_tool_get_tile_context(
    grid_cell: Optional[str] = None,
    tile_id: Optional[str] = None,
) -> Dict[str, Any]:
    cell = (grid_cell or tile_id or "").strip()
    if not cell:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="grid_cell_required")
    if not _AGENT_ACTIVE_GRID:
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail="grid_unavailable")
    cluster_list = _agent_tile_cluster_payload(cell)
    counts = _agent_cluster_label_counts([item["cluster_id"] for item in cluster_list])
    tool_usage = dict(_AGENT_GRID_TOOL_USAGE.get(cell, {}))
    tool_usage_last = dict(_AGENT_GRID_TOOL_LAST.get(cell, {}))
    caption_hint = _agent_tile_caption_hint(cell)
    labels = _agent_overlay_labels(_AGENT_ACTIVE_CLUSTERS or [])
    label_colors = _agent_current_label_colors(labels) if labels else {}
    label_prefixes = _agent_current_label_prefixes(labels) if labels else {}
    agent_cluster_list = []
    for item in cluster_list:
        if not isinstance(item, dict):
            continue
        agent_cluster_list.append(
            {
                "handle": item.get("handle"),
                "label": item.get("label"),
                "score": item.get("score"),
                "sources": item.get("sources"),
                "grid_cell": item.get("grid_cell"),
                "owner_cell": item.get("owner_cell"),
                "verified": bool(item.get("verified")),
            }
        )
    payload = {
        "tile_id": cell,
        "grid_cell": cell,
        "tile_cluster_list": cluster_list,
        "tile_counts": counts,
        "tool_usage": tool_usage,
        "tool_usage_last": tool_usage_last,
        "caption_hint": caption_hint,
        "overlay_key": _agent_overlay_key_text(label_colors, label_prefixes),
        "cluster_total": len(cluster_list),
    }
    public_payload = {
        "tile_id": cell,
        "grid_cell": cell,
        "tile_cluster_list": agent_cluster_list,
        "tile_counts": counts,
        "tool_usage": tool_usage,
        "tool_usage_last": tool_usage_last,
        "caption_hint": caption_hint,
        "overlay_key": _agent_overlay_key_text(label_colors, label_prefixes),
        "cluster_total": len(cluster_list),
    }
    stored = _agent_context_store(public_payload, kind="tile")
    if stored.get("chunked"):
        preview = {
            "cluster_total": len(cluster_list),
            "tile_counts": counts,
            "tool_usage": tool_usage,
            "tool_usage_last": tool_usage_last,
            "caption_hint": caption_hint,
        }
        return {
            "tile_id": cell,
            "grid_cell": cell,
            "chunked": True,
            "context_handle": stored.get("context_handle"),
            "chunk_total": stored.get("chunk_total"),
            "byte_size": stored.get("byte_size"),
            "preview": preview,
            "__agent_view__": {
                "tile_id": cell,
                "grid_cell": cell,
                "tile_cluster_list": agent_cluster_list,
                "tile_counts": counts,
                "tool_usage": tool_usage,
                "tool_usage_last": tool_usage_last,
                "caption_hint": caption_hint,
                "overlay_key": _agent_overlay_key_text(label_colors, label_prefixes),
                "cluster_total": len(cluster_list),
                "chunked": True,
                "context_handle": stored.get("context_handle"),
                "chunk_total": stored.get("chunk_total"),
            },
        }
    return {
        **payload,
        "byte_size": stored.get("byte_size"),
        "chunked": False,
        "__agent_view__": {
            "tile_id": cell,
            "grid_cell": cell,
            "tile_cluster_list": agent_cluster_list,
            "tile_counts": counts,
            "tool_usage": tool_usage,
            "tool_usage_last": tool_usage_last,
            "caption_hint": caption_hint,
            "overlay_key": _agent_overlay_key_text(label_colors, label_prefixes),
            "cluster_total": len(cluster_list),
            "chunked": False,
        },
    }


@_register_agent_tool("get_tile_context_chunk")
def _agent_tool_get_tile_context_chunk(
    context_handle: Optional[str] = None,
    chunk_index: Optional[int] = None,
) -> Dict[str, Any]:
    if not context_handle:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="context_handle_required")
    if chunk_index is None:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="context_chunk_index_required")
    payload = _agent_context_chunk(context_handle, chunk_index=int(chunk_index), kind="tile")
    return payload


@_register_agent_tool("get_global_context")
def _agent_tool_get_global_context() -> Dict[str, Any]:
    clusters = list(_AGENT_ACTIVE_CLUSTERS or [])
    counts: Dict[str, int] = {}
    for cluster in clusters:
        if not isinstance(cluster, dict):
            continue
        label = str(cluster.get("label") or "").strip()
        if not label:
            continue
        counts[label] = counts.get(label, 0) + 1
    usage_rows = _agent_grid_usage_rows(_AGENT_ACTIVE_GRID)
    labels = _agent_overlay_labels(clusters)
    label_colors = _agent_current_label_colors(labels) if labels else {}
    label_prefixes = _agent_current_label_prefixes(labels) if labels else {}
    payload = {
        "tile_summaries": list(_AGENT_TILE_SUMMARIES or []),
        "global_counts_by_label": counts,
        "tool_usage_heatmap": usage_rows,
        "overall_caption": _AGENT_ACTIVE_OVERALL_CAPTION or "",
        "windowed_captions": list(_AGENT_ACTIVE_WINDOWED_CAPTIONS or []),
        "overlay_key": _agent_overlay_key_text(label_colors, label_prefixes),
    }
    stored = _agent_context_store(payload, kind="global")
    if stored.get("chunked"):
        preview = {
            "tile_summaries_count": len(payload["tile_summaries"]),
            "global_counts_by_label": counts,
        }
        return {
            "chunked": True,
            "context_handle": stored.get("context_handle"),
            "chunk_total": stored.get("chunk_total"),
            "byte_size": stored.get("byte_size"),
            "preview": preview,
        }
    return {
        **payload,
        "byte_size": stored.get("byte_size"),
        "chunked": False,
    }


@_register_agent_tool("get_global_context_chunk")
def _agent_tool_get_global_context_chunk(
    context_handle: Optional[str] = None,
    chunk_index: Optional[int] = None,
) -> Dict[str, Any]:
    if not context_handle:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="context_handle_required")
    if chunk_index is None:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="context_chunk_index_required")
    payload = _agent_context_chunk(context_handle, chunk_index=int(chunk_index), kind="global")
    return payload


@_register_agent_tool("think_missed_objects")
def _agent_tool_think_missed_objects(
    image_base64: Optional[str] = None,
    image_token: Optional[str] = None,
    notes: Optional[str] = None,
) -> Dict[str, Any]:
    overlay = _AGENT_ACTIVE_OVERLAY_IMAGE
    if overlay is None:
        base = _agent_overlay_base_image()
        if base is None:
            base, _, _ = _agent_resolve_image(image_base64, image_token)
        clusters = list(_AGENT_ACTIVE_CLUSTERS or [])
        labels = _agent_overlay_labels(clusters)
        label_colors = _agent_current_label_colors(labels) if labels else {}
        label_prefixes = _agent_current_label_prefixes(labels) if labels else {}
        overlay = base
        if clusters:
            overlay = _agent_render_detection_overlay(
                base,
                clusters,
                label_colors,
                dot_radius=_AGENT_ACTIVE_OVERLAY_DOT_RADIUS,
                label_prefixes=label_prefixes,
            )
    labels = list(_AGENT_ACTIVE_LABELMAP or [])
    usage_rows = _agent_grid_usage_rows(_AGENT_ACTIVE_GRID)
    usage_text = _agent_grid_usage_text(usage_rows)
    prompt_lines = [
        "You are reviewing an annotated aerial image.",
        "Return JSON only: {\"missing_labels\": [...], \"missing_tiles\": [...], \"rationale\": \"...\"}.",
        "Use labelmap classes only. Use grid cells like A1, B2.",
        f"Labelmap: {', '.join(labels) if labels else 'none'}",
    ]
    if _AGENT_ACTIVE_OVERALL_CAPTION:
        prompt_lines.append(f"Overall caption: {_AGENT_ACTIVE_OVERALL_CAPTION}")
    if _AGENT_ACTIVE_WINDOWED_CAPTIONS:
        sample_caps = _AGENT_ACTIVE_WINDOWED_CAPTIONS[:8]
        cap_lines = []
        for entry in sample_caps:
            if isinstance(entry, dict):
                name = entry.get("window") or "window"
                cap = entry.get("caption") or ""
                cap_lines.append(f"{name}: {cap}")
        if cap_lines:
            prompt_lines.append("Windowed captions: " + " | ".join(cap_lines))
    if usage_text:
        prompt_lines.append(f"Tool usage by grid: {usage_text}")
    if notes:
        prompt_lines.append(f"Notes: {notes}")
    prompt = "\n".join(prompt_lines)
    try:
        raw, _, _ = _run_qwen_inference(
            prompt,
            overlay,
            max_new_tokens=256,
            system_prompt_override="Return JSON only.",
        )
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=f"think_missed_failed:{exc}") from exc
    json_text = _extract_balanced_json(raw, "{", "}") or ""
    data: Dict[str, Any] = {}
    if json_text:
        try:
            data = json.loads(json_text)
        except Exception:
            data = {}
    missing_labels = data.get("missing_labels") if isinstance(data, dict) else None
    missing_tiles = data.get("missing_tiles") if isinstance(data, dict) else None
    rationale = data.get("rationale") if isinstance(data, dict) else None
    return {
        "missing_labels": missing_labels if isinstance(missing_labels, list) else [],
        "missing_tiles": missing_tiles if isinstance(missing_tiles, list) else [],
        "rationale": str(rationale or "").strip(),
    }


@_register_agent_tool("grid_label_counts")
def _agent_tool_grid_label_counts(
    image_base64: Optional[str] = None,
    image_token: Optional[str] = None,
    label: Optional[str] = None,
) -> Dict[str, Any]:
    del image_base64, image_token
    if not _AGENT_ACTIVE_GRID:
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail="grid_unavailable")
    clusters = list(_AGENT_ACTIVE_CLUSTERS or [])
    summary = _agent_grid_label_counts(grid=_AGENT_ACTIVE_GRID, clusters=clusters, label=label)
    agent_cells = []
    for cell in summary:
        if not isinstance(cell, dict):
            continue
        agent_cells.append(
            {
                "grid_cell": cell.get("grid_cell"),
                "total": cell.get("total"),
                "counts": cell.get("counts"),
            }
        )
    return {
        "label": _agent_fuzzy_align_label(label, _AGENT_ACTIVE_LABELMAP or []) if label else None,
        "cells": summary,
        "total_cells": len(summary),
        "__agent_view__": {
            "label": _agent_fuzzy_align_label(label, _AGENT_ACTIVE_LABELMAP or []) if label else None,
            "cells": agent_cells,
            "total_cells": len(summary),
        },
    }


@_register_agent_tool("get_labelmap")
def _agent_tool_get_labelmap(
    dataset_id: Optional[str] = None,
    classifier_id: Optional[str] = None,
) -> Dict[str, Any]:
    classes, glossary = _agent_load_labelmap_meta(dataset_id)
    head: Optional[Dict[str, Any]] = None
    if classifier_id:
        classifier_path = _resolve_agent_clip_classifier_path(classifier_id)
        if classifier_path is not None:
            head = _load_clip_head_from_classifier(classifier_path)
    elif isinstance(active_classifier_head, dict):
        head = active_classifier_head
    background = _agent_background_classes_from_head(head)
    return {
        "classes": classes,
        "background_classes": background,
        "glossary": glossary,
        "rules": {
            "reject_background": True,
            "require_labelmap": True,
        },
    }


@_register_agent_tool("list_candidates")
def _agent_tool_list_candidates(
    image_base64: Optional[str] = None,
    image_token: Optional[str] = None,
    label: Optional[str] = None,
    source: Optional[str] = None,
    min_score: Optional[float] = None,
    include_scoreless: Optional[bool] = True,
    sort_by: Optional[str] = "score_desc",
    max_items: Optional[int] = None,
) -> Dict[str, Any]:
    del image_base64, image_token
    active = list(_AGENT_ACTIVE_CLUSTERS or _AGENT_ACTIVE_DETECTIONS or [])
    if not active:
        return {"candidates": [], "total": 0, "returned": 0, "filters": {}}
    label_filter = str(label).strip() if label else None
    aligned_label = _agent_fuzzy_align_label(label_filter, _AGENT_ACTIVE_LABELMAP or [])
    if aligned_label:
        label_filter = aligned_label
    source_filter = str(source).strip() if source else None
    source_terms = {term for term in (source_filter or "").split(",") if term.strip()}
    source_terms = {term.strip().lower() for term in source_terms}
    try:
        min_score_val = float(min_score) if min_score is not None else None
    except (TypeError, ValueError):
        min_score_val = None
    include_scoreless = True if include_scoreless is None else bool(include_scoreless)
    sort_key = (sort_by or "score_desc").strip().lower()

    def score_value(det: Dict[str, Any]) -> Optional[float]:
        raw = det.get("score")
        if raw is None:
            return None
        try:
            return float(raw)
        except (TypeError, ValueError):
            return None

    filtered: List[Dict[str, Any]] = []
    for idx, det in enumerate(active):
        if not isinstance(det, dict):
            continue
        det_label = str(det.get("label") or det.get("class_name") or "").strip()
        if label_filter and det_label and det_label != label_filter:
            continue
        det_score = score_value(det)
        if det_score is None:
            if not include_scoreless:
                continue
        elif min_score_val is not None and det_score < min_score_val:
            continue
        if source_terms:
            det_sources: Set[str] = set()
            for key in ("source", "score_source"):
                val = det.get(key)
                if val:
                    det_sources.add(str(val).strip().lower())
            source_list = det.get("source_list")
            if isinstance(source_list, (list, tuple)):
                for item in source_list:
                    if item:
                        det_sources.add(str(item).strip().lower())
            if not det_sources.intersection(source_terms):
                continue
        entry = {
            "cluster_id": int(det.get("cluster_id")) if det.get("cluster_id") is not None else int(idx + 1),
            "handle": _agent_cluster_handle(det),
            "label": det_label,
            "class_id": det.get("class_id"),
            "score": det.get("score"),
            "score_source": det.get("score_source") or det.get("source") or "unknown",
            "source": det.get("source"),
            "source_list": det.get("source_list"),
            "candidate_count": len(det.get("candidate_ids") or []),
            "grid_cell": det.get("grid_cell"),
            "classifier_best": det.get("classifier_best"),
            "classifier_prob": det.get("classifier_prob"),
            "classifier_accept": det.get("classifier_accept"),
        }
        filtered.append(entry)

    if sort_key in {"score", "score_desc", "confidence"}:
        filtered.sort(key=lambda d: (score_value(d) if score_value(d) is not None else -1.0), reverse=True)
        sort_key = "score_desc"
    elif sort_key in {"score_asc", "confidence_asc"}:
        filtered.sort(key=lambda d: (score_value(d) if score_value(d) is not None else 1e9))
        sort_key = "score_asc"
    elif sort_key == "label":
        filtered.sort(key=lambda d: str(d.get("label") or ""))
    elif sort_key == "source":
        filtered.sort(key=lambda d: str(d.get("source") or ""))
    else:
        sort_key = "none"
    try:
        max_items_val = int(max_items) if max_items is not None else 0
    except (TypeError, ValueError):
        max_items_val = 0
    trimmed = filtered[:max_items_val] if max_items_val > 0 else filtered
    agent_candidates = []
    for item in trimmed:
        if not isinstance(item, dict):
            continue
        agent_candidates.append(
            {
                "handle": item.get("handle"),
                "label": item.get("label"),
                "grid_cell": item.get("grid_cell"),
                "score": item.get("score"),
                "score_source": item.get("score_source"),
                "source": item.get("source"),
                "source_list": item.get("source_list"),
                "verified": bool(item.get("classifier_accept")),
            }
        )
    return {
        "candidates": trimmed,
        "total": len(filtered),
        "returned": len(trimmed),
        "filters": {
            "label": label_filter,
            "source": source_filter,
            "min_score": min_score_val,
            "include_scoreless": include_scoreless,
            "sort_by": sort_key,
            "max_items": max_items_val,
        },
        "__agent_view__": {
            "candidates": agent_candidates,
            "total": len(filtered),
            "returned": len(trimmed),
            "filters": {
                "label": label_filter,
                "source": source_filter,
                "min_score": min_score_val,
                "include_scoreless": include_scoreless,
                "sort_by": sort_key,
                "max_items": max_items_val,
            },
        },
    }


@_register_agent_tool("submit_annotations")
def _agent_tool_submit_annotations(
    image_base64: Optional[str] = None,
    image_token: Optional[str] = None,
    annotations: Optional[List[Dict[str, Any]]] = None,
    cluster_ids: Optional[List[int]] = None,
    handles: Optional[List[str]] = None,
    include_all: Optional[bool] = None,
    dataset_id: Optional[str] = None,
    classifier_id: Optional[str] = None,
    window_bbox_2d: Optional[Sequence[float]] = None,
    iou: Optional[float] = None,
    cross_iou: Optional[float] = None,
    max_det: Optional[int] = None,
) -> Dict[str, Any]:
    global _AGENT_LAST_SUBMIT_DETECTIONS
    pil_img, _, _ = _agent_resolve_image(image_base64, image_token)
    img_w, img_h = pil_img.size
    labelmap = _agent_load_labelmap(dataset_id)
    head: Optional[Dict[str, Any]] = None
    if classifier_id:
        classifier_path = _resolve_agent_clip_classifier_path(classifier_id)
        if classifier_path is not None:
            head = _load_clip_head_from_classifier(classifier_path)
    elif isinstance(active_classifier_head, dict):
        head = active_classifier_head
    background = _agent_background_classes_from_head(head)
    normalized_annotations: List[Dict[str, Any]] = []
    cluster_id_list: List[int] = []
    label_overrides: Dict[int, str] = {}
    if include_all:
        cluster_id_list = [int(c.get("cluster_id")) for c in (_AGENT_ACTIVE_CLUSTERS or []) if c.get("cluster_id") is not None]
    if handles:
        cluster_id_list.extend(_agent_cluster_ids_from_handles(handles))
    if cluster_ids:
        cluster_id_list.extend([int(cid) for cid in cluster_ids])
    for ann in annotations or []:
        if isinstance(ann, (int, float)):
            cluster_id_list.append(int(ann))
            continue
        if isinstance(ann, dict) and ann.get("cluster_id") is not None:
            cid_val = int(ann.get("cluster_id"))
            cluster_id_list.append(cid_val)
            override_label = ann.get("label")
            if override_label:
                label_overrides[cid_val] = str(override_label)
            continue
        if window_bbox_2d is not None and isinstance(ann, dict) and ann.get("window_bbox_2d") is None:
            ann = {**ann, "window_bbox_2d": list(window_bbox_2d)}
        normalized_annotations.append(ann)
    if cluster_id_list:
        seen = set()
        for cid in cluster_id_list:
            if cid in seen:
                continue
            seen.add(cid)
            cluster = _AGENT_ACTIVE_CLUSTER_INDEX.get(int(cid))
            if not cluster:
                continue
            label = label_overrides.get(cid) or cluster.get("label")
            normalized_annotations.append(
                {
                    "label": label,
                    "bbox_xyxy_px": cluster.get("bbox_xyxy_px"),
                    "bbox_2d": cluster.get("bbox_2d"),
                    "bbox_yolo": cluster.get("bbox_yolo"),
                    "score": cluster.get("score"),
                    "score_source": cluster.get("score_source") or cluster.get("source"),
                    "class_id": cluster.get("class_id"),
                    "source": cluster.get("source") or "agent",
                    "source_list": cluster.get("source_list"),
                    "origin": cluster.get("origin"),
                }
            )
    cleaned, rejected = _agent_sanitize_detection_items(
        normalized_annotations,
        pil_img=pil_img,
        classifier_head=head,
        img_w=img_w,
        img_h=img_h,
        labelmap=labelmap,
        background=background,
    )
    merged = _agent_merge_detections(
        cleaned,
        iou_thr=float(iou or 0.5),
        max_det=max_det,
        cross_iou=float(cross_iou) if cross_iou is not None else None,
    )
    _AGENT_LAST_SUBMIT_DETECTIONS = list(merged)
    submitted_handles = _agent_handles_from_cluster_ids(cluster_id_list)
    agent_view = {
        "submitted_handles": sorted(set(submitted_handles)),
        "count": len(merged),
    }
    return {
        "detections": merged,
        "rejected": rejected,
        "count": len(merged),
        "__agent_view__": agent_view,
    }


def _agent_apply_ensemble_filter(
    detections: List[Dict[str, Any]],
    *,
    dataset_id: Optional[str],
    image_name: Optional[str],
    classifier_id: Optional[str],
    job_id: Optional[str],
    trace_writer: Optional[Callable[[Dict[str, Any]], None]] = None,
    trace_full_writer: Optional[Callable[[Dict[str, Any]], None]] = None,
    trace_readable: Optional[Callable[[str], None]] = None,
    warnings: Optional[List[str]] = None,
) -> List[Dict[str, Any]]:
    if not job_id:
        return detections
    if not dataset_id or not image_name:
        if warnings is not None:
            warnings.append("ensemble_filter_missing_dataset_or_image")
        return detections
    job_dir = CALIBRATION_ROOT / job_id
    model_path = job_dir / "ensemble_mlp.pt"
    meta_path = job_dir / "ensemble_mlp.meta.json"
    if not model_path.exists() or not meta_path.exists():
        if warnings is not None:
            warnings.append("ensemble_filter_model_missing")
        return detections
    if not detections:
        return detections
    classifier_path = None
    if classifier_id:
        classifier_path = _resolve_agent_clip_classifier_path(classifier_id)
    if classifier_path is None:
        if warnings is not None:
            warnings.append("ensemble_filter_classifier_missing")
        return detections

    tmp_dir = UPLOAD_ROOT / "tmp_ensemble"
    tmp_dir.mkdir(parents=True, exist_ok=True)
    stamp = uuid.uuid4().hex[:8]
    jsonl_path = tmp_dir / f"ensemble_{stamp}.jsonl"
    features_path = tmp_dir / f"ensemble_{stamp}.npz"
    scored_path = tmp_dir / f"ensemble_{stamp}.scored.jsonl"
    try:
        with jsonl_path.open("w", encoding="utf-8") as handle:
            handle.write(
                json.dumps(
                    {"image": image_name, "detections": detections},
                    ensure_ascii=True,
                )
                + "\n"
            )
        root_dir = Path(__file__).resolve().parent
        build_cmd = [
            sys.executable,
            str(root_dir / "tools" / "build_ensemble_features.py"),
            "--input",
            str(jsonl_path),
            "--dataset",
            dataset_id,
            "--output",
            str(features_path),
            "--support-iou",
            "0.5",
            "--min-crop-size",
            "4",
            "--device",
            "cuda",
            "--classifier-id",
            str(classifier_path),
        ]
        subprocess.run(build_cmd, check=True)
        score_cmd = [
            sys.executable,
            str(root_dir / "tools" / "score_ensemble_candidates.py"),
            "--model",
            str(model_path),
            "--meta",
            str(meta_path),
            "--data",
            str(features_path),
            "--output",
            str(scored_path),
        ]
        subprocess.run(score_cmd, check=True)
        accepted: List[Dict[str, Any]] = []
        for line in scored_path.read_text().splitlines():
            if not line.strip():
                continue
            try:
                entry = json.loads(line)
            except Exception:
                continue
            if entry.get("ensemble_accept"):
                accepted.append(entry)
        if trace_writer:
            trace_writer(
                {
                    "type": "ensemble_filter",
                    "accepted": len(accepted),
                    "total": len(detections),
                    "job_id": job_id,
                }
            )
        if trace_full_writer:
            trace_full_writer(
                {
                    "type": "ensemble_filter",
                    "accepted": len(accepted),
                    "total": len(detections),
                    "job_id": job_id,
                }
            )
        if trace_readable:
            trace_readable(
                f"ensemble filter: accepted={len(accepted)} total={len(detections)} job={job_id}"
            )
        return accepted
    except Exception as exc:  # noqa: BLE001
        if warnings is not None:
            warnings.append(f"ensemble_filter_failed:{exc}")
        return detections


@_register_agent_tool("log_observation")
def _agent_tool_log_observation(
    text: Optional[str] = None,
    bbox_2d: Optional[Sequence[float]] = None,
    window_bbox_2d: Optional[Sequence[float]] = None,
    grid_cell: Optional[str] = None,
    label_hint: Optional[str] = None,
) -> Dict[str, Any]:
    cleaned = _agent_clean_observation_text(text, max_len=160)
    if not cleaned:
        return {"ok": False, "error": "observation_missing"}
    payload: Dict[str, Any] = {
        "ok": True,
        "observation": cleaned,
    }
    if label_hint:
        payload["label_hint"] = str(label_hint)
    if grid_cell:
        payload["grid_cell"] = str(grid_cell)
    _ = bbox_2d
    _ = window_bbox_2d
    return payload


@_register_agent_tool("log_status")
def _agent_tool_log_status(text: Optional[str] = None) -> Dict[str, Any]:
    cleaned = _agent_clean_observation_text(text, max_len=160)
    if not cleaned:
        return {"ok": False, "error": "status_missing"}
    return {"ok": True, "status": cleaned}


def _agent_tool_specs_text(grid_enabled: bool = False) -> str:
    tool_names = [
        "get_tile_context",
        "get_tile_context_chunk",
        "get_global_context",
        "get_global_context_chunk",
        "view_cell_raw",
        "view_cell_overlay",
        "view_full_overlay",
        "list_candidates",
        "grid_label_counts",
        "look_and_inspect",
        "zoom_and_detect",
        "sam3_text",
        "sam3_similarity",
        "qwen_infer",
        "classify_crop",
        "image_zoom_in_tool",
        "think_missed_objects",
        "submit_annotations",
        "log_observation",
        "log_status",
    ]
    tools = _agent_tool_specs_facade(grid_enabled=bool(grid_enabled), tool_names=tool_names)
    return json.dumps(tools, ensure_ascii=False, indent=2)



def _agent_tool_specs(grid_enabled: bool = False) -> List[Dict[str, Any]]:
    """Return the tool specs as a list for Hermes-style tool templates."""
    try:
        return json.loads(_agent_tool_specs_text(grid_enabled=grid_enabled))
    except Exception:
        return []


def _agent_tool_specs_facade(
    *,
    grid_enabled: bool,
    tool_names: Sequence[str],
) -> List[Dict[str, Any]]:
    grid_cell = {
        "type": "string",
        "description": "Grid cell reference (e.g., C2). Columns are letters, rows are numbers; top-left is A1.",
    }
    required_grid = ["grid_cell"] if grid_enabled else []
    handle_list = {
        "type": "array",
        "description": "Handles returned by list_candidates/get_tile_context (e.g., LV12).",
        "items": {"type": "string"},
    }
    handle = {"type": "string", "description": "Handle returned by list_candidates/get_tile_context (e.g., LV12)."}
    label = {"type": "string", "description": "Canonical labelmap class."}
    prompt = {"type": "string", "description": "Single-term prompt/synonym for SAM3."}
    intent = {"type": "string", "description": "Short note on what you are searching for."}
    tool_map = {
        "get_tile_context": {
            "name": "get_tile_context",
            "description": "Return tile context (cluster handles, counts, captions, tool usage).",
            "parameters": {"type": "object", "properties": {"grid_cell": grid_cell}, "required": required_grid},
        },
        "get_tile_context_chunk": {
            "name": "get_tile_context_chunk",
            "description": "Fetch a chunk of a large tile context payload by handle + chunk_index.",
            "parameters": {
                "type": "object",
                "properties": {"context_handle": {"type": "string"}, "chunk_index": {"type": "number"}},
                "required": ["context_handle", "chunk_index"],
            },
        },
        "get_global_context": {
            "name": "get_global_context",
            "description": "Return global context (tile summaries, counts, captions).",
            "parameters": {"type": "object", "properties": {}, "required": []},
        },
        "get_global_context_chunk": {
            "name": "get_global_context_chunk",
            "description": "Fetch a chunk of a large global context payload by handle + chunk_index.",
            "parameters": {
                "type": "object",
                "properties": {"context_handle": {"type": "string"}, "chunk_index": {"type": "number"}},
                "required": ["context_handle", "chunk_index"],
            },
        },
        "view_cell_raw": {
            "name": "view_cell_raw",
            "description": "Return the raw image crop for a grid cell (no detection dots).",
            "parameters": {"type": "object", "properties": {"grid_cell": grid_cell}, "required": required_grid},
        },
        "view_cell_overlay": {
            "name": "view_cell_overlay",
            "description": "Return the overlay crop for a grid cell (colored dots + handles).",
            "parameters": {"type": "object", "properties": {"grid_cell": grid_cell}, "required": required_grid},
        },
        "view_full_overlay": {
            "name": "view_full_overlay",
            "description": "Return the full image with grid + detection overlay and tool usage summary.",
            "parameters": {"type": "object", "properties": {}, "required": []},
        },
        "list_candidates": {
            "name": "list_candidates",
            "description": "List current cluster candidates (handles, labels, scores, grid cells).",
            "parameters": {
                "type": "object",
                "properties": {
                    "label": {"type": "string"},
                    "min_score": {"type": "number"},
                    "include_scoreless": {"type": "boolean"},
                    "sort_by": {"type": "string", "enum": ["score_desc", "score_asc", "label", "source", "none"]},
                    "max_items": {"type": "number"},
                },
                "required": [],
            },
        },
        "grid_label_counts": {
            "name": "grid_label_counts",
            "description": "Return per-grid-cell counts (no IDs).",
            "parameters": {"type": "object", "properties": {"label": {"type": "string"}}, "required": []},
        },
        "look_and_inspect": {
            "name": "look_and_inspect",
            "description": "Inspect a grid cell visually and propose candidate objects.",
            "parameters": {
                "type": "object",
                "properties": {"grid_cell": grid_cell, "intent": intent, "max_objects": {"type": "number"}},
                "required": required_grid,
            },
        },
        "zoom_and_detect": {
            "name": "zoom_and_detect",
            "description": "Run detector on a grid cell (zoomed crop).",
            "parameters": {
                "type": "object",
                "properties": {
                    "grid_cell": grid_cell,
                    "intent": intent,
                    "confirm_label": label,
                    "confirm_topk": {"type": "number"},
                },
                "required": required_grid,
            },
        },
        "sam3_text": {
            "name": "sam3_text",
            "description": "Run SAM3 text prompt for a single term; assign canonical label.",
            "parameters": {
                "type": "object",
                "properties": {"grid_cell": grid_cell, "label": label, "prompt": prompt},
                "required": (required_grid + ["label"]) if required_grid else ["label"],
            },
        },
        "sam3_similarity": {
            "name": "sam3_similarity",
            "description": "Run SAM3 similarity from exemplar handles; assign canonical label.",
            "parameters": {
                "type": "object",
                "properties": {"grid_cell": grid_cell, "label": label, "exemplar_handles": handle_list},
                "required": (required_grid + ["label", "exemplar_handles"]) if required_grid else ["label", "exemplar_handles"],
            },
        },
        "qwen_infer": {
            "name": "qwen_infer",
            "description": "Ask Qwen-VL to propose new boxes for a list of labels.",
            "parameters": {
                "type": "object",
                "properties": {
                    "grid_cell": grid_cell,
                    "items": {"type": "array", "items": {"type": "string"}},
                    "prompt_type": {"type": "string", "enum": ["bbox", "point"]},
                    "intent": intent,
                },
                "required": required_grid,
            },
        },
        "classify_crop": {
            "name": "classify_crop",
            "description": "Classify a cluster crop to verify its label.",
            "parameters": {
                "type": "object",
                "properties": {"handle": handle, "label_hint": label, "topk": {"type": "number"}},
                "required": ["handle"],
            },
        },
        "image_zoom_in_tool": {
            "name": "image_zoom_in_tool",
            "description": "Return a zoomed image for a grid cell or handle.",
            "parameters": {
                "type": "object",
                "properties": {"grid_cell": grid_cell, "handle": handle, "intent": intent},
                "required": [],
            },
        },
        "think_missed_objects": {
            "name": "think_missed_objects",
            "description": "Analyze overlay + captions to suggest missing labels/tiles.",
            "parameters": {"type": "object", "properties": {"notes": {"type": "string"}}, "required": []},
        },
        "submit_annotations": {
            "name": "submit_annotations",
            "description": "Submit final annotations using handles or include_all.",
            "parameters": {
                "type": "object",
                "properties": {"handles": handle_list, "include_all": {"type": "boolean"}},
                "required": [],
            },
        },
        "log_observation": {
            "name": "log_observation",
            "description": "Log a single-line observation about what you see (max 160 chars).",
            "parameters": {"type": "object", "properties": {"text": {"type": "string"}}, "required": ["text"]},
        },
        "log_status": {
            "name": "log_status",
            "description": "Log a short status update (max 160 chars).",
            "parameters": {"type": "object", "properties": {"text": {"type": "string"}}, "required": ["text"]},
        },
    }
    specs: List[Dict[str, Any]] = []
    for name in tool_names:
        spec = tool_map.get(name)
        if not spec:
            continue
        specs.append({"type": "function", "function": spec})
    return specs


def _agent_tool_prompt_qwen(grid_enabled: bool = False) -> str:
    tools = _agent_tool_specs(grid_enabled=grid_enabled)
    tool_names: List[str] = []
    tool_descs: List[str] = []
    for tool in tools:
        fn = tool.get("function") or {}
        name = str(fn.get("name") or "").strip()
        if not name:
            continue
        tool_names.append(name)
        desc = str(fn.get("description") or "").strip()
        params = fn.get("parameters") or {}
        params_text = json.dumps(params, ensure_ascii=False)
        tool_descs.append(f"### {name}\n{name}: {desc} Input parameters: {params_text}")
    names_text = ", ".join(tool_names)
    descs_text = "\n\n".join(tool_descs)
    return (
        "# Tools\n\n"
        "## You have access to the following tools:\n\n"
        f"{descs_text}\n\n"
        "## When you need to call a tool, please insert the following command in your reply:\n\n"
        f"✿FUNCTION✿: The tool to use, should be one of [{names_text}]\n"
        "✿ARGS✿: The input of the tool\n"
        "✿RESULT✿: Tool results\n"
        "✿RETURN✿: Reply based on tool results. Images need to be rendered as ![](url)\n"
        "Return ONLY the tool call, no other text.\n"
        "If the response already starts after ✿FUNCTION✿:, continue with the tool name and do not repeat the token.\n"
        "Keep args minimal; use only grid_cell, handles, labels, and intent as needed.\n"
    )


def _agent_trace_sanitize_payload(payload: QwenPrepassRequest, image_token: Optional[str]) -> Dict[str, Any]:
    try:
        data = payload.dict()
    except Exception:
        data = {}
    image_base64 = data.get("image_base64")
    if isinstance(image_base64, str) and image_base64:
        data["image_base64"] = f"<base64:{len(image_base64)}>"
    if image_token:
        data["resolved_image_token"] = image_token
    return data


def _agent_trace_sanitize_messages(messages: Sequence[Dict[str, Any]]) -> List[Dict[str, Any]]:
    sanitized: List[Dict[str, Any]] = []
    for msg in messages:
        if not isinstance(msg, dict):
            sanitized.append({"role": "unknown", "content": str(msg)})
            continue
        role = msg.get("role")
        content = msg.get("content")
        fn_call = msg.get("function_call")
        fn_payload = None
        if fn_call:
            if hasattr(fn_call, "model_dump"):
                try:
                    fn_call = fn_call.model_dump()
                except Exception:
                    fn_call = None
            if isinstance(fn_call, dict):
                fn_payload = {
                    "name": fn_call.get("name"),
                    "arguments": fn_call.get("arguments"),
                }
            else:
                fn_payload = {
                    "name": getattr(fn_call, "name", None),
                    "arguments": getattr(fn_call, "arguments", None),
                }
        if isinstance(content, list):
            safe_content: List[Dict[str, Any]] = []
            for item in content:
                if not isinstance(item, dict):
                    continue
                item_type = item.get("type")
                if not item_type:
                    if "text" in item:
                        item_type = "text"
                    elif "image" in item:
                        item_type = "image"
                    elif "audio" in item:
                        item_type = "audio"
                    elif "video" in item:
                        item_type = "video"
                    elif "file" in item:
                        item_type = "file"
                if item_type == "text":
                    safe_content.append({"type": "text", "text": str(item.get("text") or "")})
                elif item_type == "image":
                    image_val = item.get("image")
                    if isinstance(image_val, str):
                        if image_val.startswith("data:image"):
                            safe_content.append({"type": "image", "image": f"<image_base64:{len(image_val)}>"})
                        else:
                            safe_content.append({"type": "image", "image": image_val[:200]})
                    else:
                        safe_content.append({"type": "image", "image": "<image>"})
                elif item_type in {"audio", "video", "file"}:
                    safe_content.append({"type": item_type, "value": "<omitted>"})
                else:
                    safe_content.append({"type": str(item_type or "unknown"), "value": "<omitted>"})
            entry = {"role": role, "content": safe_content}
            if fn_payload:
                entry["function_call"] = fn_payload
            sanitized.append(entry)
        else:
            entry = {"role": role, "content": content}
            if fn_payload:
                entry["function_call"] = fn_payload
            sanitized.append(entry)
    return sanitized


def _agent_trace_full_jsonable(value: Any, *, _depth: int = 0) -> Any:
    if _depth > 6:
        return "<max_depth>"
    if value is None or isinstance(value, (int, float, bool)):
        return value
    if isinstance(value, str):
        if value.startswith("data:image"):
            return f"<image_base64:{len(value)}>"
        if len(value) > 2000 and re.fullmatch(r"[A-Za-z0-9+/=\\s]+", value):
            return f"<base64:{len(value)}>"
        return value
    if isinstance(value, Path):
        return str(value)
    if isinstance(value, (list, tuple, set)):
        return [_agent_trace_full_jsonable(item, _depth=_depth + 1) for item in value]
    if isinstance(value, dict):
        return {
            str(key): _agent_trace_full_jsonable(item, _depth=_depth + 1)
            for key, item in value.items()
        }
    if isinstance(value, (bytes, bytearray)):
        return f"<bytes:{len(value)}>"
    if isinstance(value, Image.Image):
        try:
            return f"<image:{value.size[0]}x{value.size[1]} {value.mode}>"
        except Exception:
            return "<image>"
    if isinstance(value, np.ndarray):
        try:
            if value.size <= 2000:
                return value.tolist()
        except Exception:
            pass
        return {"type": "ndarray", "shape": list(value.shape), "dtype": str(value.dtype)}
    if torch is not None and isinstance(value, torch.Tensor):
        try:
            arr = value.detach().cpu().numpy()
            return _agent_trace_full_jsonable(arr, _depth=_depth + 1)
        except Exception:
            return {"type": "tensor", "shape": list(value.shape), "dtype": str(value.dtype)}
    if hasattr(value, "model_dump"):
        try:
            return _agent_trace_full_jsonable(value.model_dump(), _depth=_depth + 1)
        except Exception:
            return str(value)
    try:
        return json.loads(json.dumps(value))
    except Exception:
        return str(value)


def _agent_full_trace_write(record: Dict[str, Any]) -> None:
    if _AGENT_TRACE_FULL_WRITER is None:
        return
    try:
        _AGENT_TRACE_FULL_WRITER(_agent_trace_full_jsonable(record))
    except Exception:
        return


def _agent_readable_write(line: str) -> None:
    if not line:
        return
    try:
        if _AGENT_TRACE_READABLE_WRITER is not None:
            _AGENT_TRACE_READABLE_WRITER(line)
        if PREPASS_READABLE_TO_CONSOLE:
            logging.getLogger("prepass.readable").info(line)
    except Exception:
        return


def _agent_readable_trim(text: Optional[str], max_len: Optional[int] = None) -> str:
    if not text:
        return ""
    text = " ".join(str(text).split())
    if max_len is None or max_len <= 0 or len(text) <= max_len:
        return text
    return text[: max_len - 3] + "..."


def _agent_readable_banner(title: str, *, width: int = 88, fill: str = "=") -> str:
    cleaned = " ".join(str(title or "").strip().split())
    if not cleaned:
        return fill * width
    text = f" {cleaned} "
    if len(text) >= width:
        return text[:width]
    pad = width - len(text)
    left = pad // 2
    right = pad - left
    return f"{fill * left}{text}{fill * right}"


def _agent_grid_cell_for_detection(
    det: Mapping[str, Any],
    img_w: int,
    img_h: int,
    grid: Optional[Mapping[str, Any]],
) -> Optional[str]:
    if not grid:
        return None
    cell_hint = det.get("grid_cell") if isinstance(det, Mapping) else None
    if cell_hint:
        return str(cell_hint)
    center = _agent_detection_center_px(dict(det), img_w, img_h)
    if not center:
        return None
    cx, cy = center
    bbox_2d = _xyxy_to_qwen_bbox(img_w, img_h, cx, cy, cx, cy)
    return _agent_grid_cell_for_window_bbox(grid, bbox_2d)


def _agent_detection_summary_lines(
    detections: Sequence[Dict[str, Any]],
    *,
    grid: Optional[Mapping[str, Any]] = None,
    img_w: int,
    img_h: int,
    warnings: Optional[Sequence[str]] = None,
    max_cells: int = 10,
) -> List[str]:
    total = len(detections)
    label_counts: Dict[str, int] = {}
    cell_counts: Dict[str, Dict[str, int]] = {}
    for det in detections:
        label = str(det.get("label") or det.get("class_name") or "").strip()
        if not label:
            continue
        label_counts[label] = label_counts.get(label, 0) + 1
        if grid:
            cell = _agent_grid_cell_for_detection(det, img_w, img_h, grid)
            if cell:
                cell_counts.setdefault(label, {})
                cell_counts[label][cell] = cell_counts[label].get(cell, 0) + 1
    lines: List[str] = []
    labels_total = len(label_counts)
    grid_state = "on" if grid else "off"
    warnings_text = ""
    if warnings:
        warnings_text = f" warnings={len(warnings)}"
    lines.append(f"summary: total={total} labels={labels_total} grid={grid_state}{warnings_text}")
    ordered = sorted(label_counts.items(), key=lambda item: (-item[1], item[0]))
    for label, count in ordered:
        cell_text = ""
        cells = cell_counts.get(label) if grid else None
        if cells:
            cell_items = sorted(cells.items(), key=lambda item: (-item[1], item[0]))
            parts = [f"{cell}x{cnt}" for cell, cnt in cell_items[:max_cells]]
            if len(cell_items) > max_cells:
                parts.append(f"+{len(cell_items) - max_cells} more")
            cell_text = f" cells={','.join(parts)}"
        lines.append(f"label {label}: {count}{cell_text}")
    if warnings:
        warn_list = ", ".join(str(w) for w in warnings)
        lines.append(f"warnings: {warn_list}")
    return lines


def _agent_readable_detection_line(
    det: Mapping[str, Any],
    *,
    grid: Optional[Mapping[str, Any]] = None,
    img_w: int,
    img_h: int,
) -> str:
    label = str(det.get("label") or det.get("class_name") or "unknown")
    bbox = None
    if isinstance(det.get("bbox_xyxy_px"), (list, tuple)) and len(det.get("bbox_xyxy_px")) >= 4:
        bbox = det.get("bbox_xyxy_px")
    elif isinstance(det.get("bbox_2d"), (list, tuple)) and len(det.get("bbox_2d")) >= 4:
        bbox = det.get("bbox_2d")
    bbox_text = _agent_readable_format_bbox(bbox) if bbox else "[]"
    score = det.get("score")
    if score is None:
        score_text = "score=n/a"
    else:
        try:
            score_text = f"score={float(score):.3f}"
        except (TypeError, ValueError):
            score_text = "score=n/a"
    source = str(det.get("source") or det.get("score_source") or "").strip()
    source_text = f" source={source}" if source else ""
    cluster_id = det.get("cluster_id")
    id_text = f"id={cluster_id} " if cluster_id is not None else ""
    cell_text = ""
    if grid:
        cell = _agent_grid_cell_for_detection(det, img_w, img_h, grid)
        if cell:
            cell_text = f" cell={cell}"
    return f"{id_text}label={label} bbox={bbox_text} {score_text}{source_text}{cell_text}"


def _agent_clean_observation_text(text: Optional[str], max_len: int = 160) -> str:
    if not text:
        return ""
    cleaned = " ".join(str(text).replace("\n", " ").replace("\t", " ").split())
    for prefix in ("observation:", "observations:", "note:", "notes:"):
        if cleaned.lower().startswith(prefix):
            cleaned = cleaned[len(prefix):].strip()
            break
    if len(cleaned) > max_len:
        cleaned = cleaned[: max_len - 3].rstrip() + "..."
    return cleaned


def _agent_readable_format_bbox(bbox: Any) -> str:
    if not isinstance(bbox, (list, tuple)) or len(bbox) < 4:
        return ""
    values: List[str] = []
    for val in bbox[:4]:
        try:
            num = float(val)
        except (TypeError, ValueError):
            values.append("?")
            continue
        rounded = round(num)
        if abs(num - rounded) < 0.01:
            values.append(str(int(rounded)))
        else:
            values.append(f"{num:.1f}")
    return "[" + ", ".join(values) + "]"


def _agent_readable_bbox_from_args(args: Optional[Dict[str, Any]]) -> Tuple[str, str]:
    if not isinstance(args, dict):
        return "", ""
    for key in ("window_bbox_2d", "bbox_2d", "bbox_xyxy_px"):
        bbox = args.get(key)
        if isinstance(bbox, (list, tuple)) and len(bbox) >= 4:
            return key, _agent_readable_format_bbox(bbox)
    return "", ""


def _agent_readable_tool_call_summary(tool_name: str, args: Optional[Dict[str, Any]]) -> str:
    tool = str(tool_name or "")
    key, coords = _agent_readable_bbox_from_args(args)
    grid_cell = _agent_readable_trim((args or {}).get("grid_cell"))
    cluster_id = (args or {}).get("cluster_id")
    if tool == "run_detector":
        mode = str((args or {}).get("mode") or "yolo")
        grid_text = f" grid={grid_cell}" if grid_cell else ""
        return f"run_detector mode={mode}{grid_text}"
    if tool == "zoom_and_detect":
        mode = str((args or {}).get("mode") or "yolo")
        intent = _agent_readable_trim((args or {}).get("intent"))
        confirm_label = _agent_readable_trim((args or {}).get("confirm_label"))
        intent_text = f" intent=\"{intent}\"" if intent else ""
        confirm_text = f" confirm={confirm_label}" if confirm_label else ""
        grid_text = f" grid={grid_cell}" if grid_cell else ""
        return f"zoom_and_detect mode={mode}{intent_text}{confirm_text}{grid_text}"
    if tool == "look_and_inspect":
        max_objects = (args or {}).get("max_objects")
        intent = _agent_readable_trim((args or {}).get("intent"))
        extra = f" max_objects={max_objects}" if max_objects else ""
        if intent:
            extra = f"{extra} intent=\"{intent}\""
        grid_text = f" grid={grid_cell}" if grid_cell else ""
        return f"inspect cell{grid_text}{extra}"
    if tool == "image_zoom_in_tool":
        intent = _agent_readable_trim((args or {}).get("intent"))
        intent_text = f" intent=\"{intent}\"" if intent else ""
        grid_text = f" grid={grid_cell}" if grid_cell else ""
        cluster_text = f" cluster={cluster_id}" if cluster_id is not None else ""
        return f"looking closer{grid_text}{cluster_text}{intent_text}"
    if tool in {"view_cell_raw", "view_cell_overlay"}:
        grid_text = f" grid={grid_cell}" if grid_cell else ""
        return f"{tool}{grid_text}"
    if tool == "get_tile_context":
        grid_text = f" grid={grid_cell}" if grid_cell else ""
        return f"get_tile_context{grid_text}"
    if tool == "get_tile_context_chunk":
        handle = _agent_readable_trim((args or {}).get("context_handle"))
        idx = (args or {}).get("chunk_index")
        return f"get_tile_context_chunk handle={handle} idx={idx}"
    if tool == "get_global_context":
        return "get_global_context"
    if tool == "get_global_context_chunk":
        handle = _agent_readable_trim((args or {}).get("context_handle"))
        idx = (args or {}).get("chunk_index")
        return f"get_global_context_chunk handle={handle} idx={idx}"
    if tool == "think_missed_objects":
        return "think_missed_objects"
    if tool == "log_observation":
        observation = _agent_readable_trim((args or {}).get("text"))
        if observation and grid_cell:
            return f"observation {grid_cell} \"{observation}\""
        if observation:
            return f"observation \"{observation}\""
        return "observation"
    if tool == "log_status":
        status = _agent_readable_trim((args or {}).get("text"))
        if status:
            return f"status \"{status}\""
        return "status"
    if tool == "classify_crop":
        label = _agent_readable_trim((args or {}).get("label_hint"))
        label_text = f" label={label}" if label else ""
        cluster_text = f" cluster={cluster_id}" if cluster_id is not None else ""
        return f"classify crop{cluster_text}{label_text}"
    if tool == "sam3_text":
        prompt = _agent_readable_trim((args or {}).get("prompt"))
        label = _agent_readable_trim((args or {}).get("label"))
        details: List[str] = []
        if label:
            details.append(f"label={label}")
        if prompt:
            details.append(f"prompt=\"{prompt}\"")
        grid_text = f" grid={grid_cell}" if grid_cell else ""
        return ("sam3_text " + " ".join(details) if details else "sam3_text") + grid_text
    if tool == "sam3_similarity":
        exemplars = (args or {}).get("exemplar_cluster_ids")
        count = len(exemplars) if isinstance(exemplars, list) else 0
        grid_text = f" grid={grid_cell}" if grid_cell else ""
        return f"sam3_similarity exemplars={count}{grid_text}"
    if tool == "qwen_infer":
        prompt_type = (args or {}).get("prompt_type") or "bbox"
        items = (args or {}).get("items")
        item_list = (args or {}).get("item_list")
        count = len(items) if isinstance(items, list) else 0
        if not count and isinstance(item_list, str) and item_list.strip():
            count = len([item for item in item_list.split(",") if item.strip()])
        prompt = _agent_readable_trim((args or {}).get("prompt"))
        details = f"type={prompt_type} items={count}"
        if prompt:
            details += f" prompt=\"{prompt}\""
        grid_text = f" grid={grid_cell}" if grid_cell else ""
        return f"qwen_infer {details}{grid_text}"
    if tool == "list_candidates":
        label = _agent_readable_trim((args or {}).get("label"))
        source = _agent_readable_trim((args or {}).get("source"))
        min_score = (args or {}).get("min_score")
        max_items = (args or {}).get("max_items")
        details: List[str] = []
        if label:
            details.append(f"label={label}")
        if source:
            details.append(f"source={source}")
        if isinstance(min_score, (int, float)):
            details.append(f"min_score={float(min_score):.2f}")
        if isinstance(max_items, (int, float)):
            details.append(f"max_items={int(max_items)}")
        return "list_candidates " + " ".join(details) if details else "list_candidates"
    if tool == "grid_label_counts":
        label = _agent_readable_trim((args or {}).get("label"))
        return f"grid_label_counts label={label}" if label else "grid_label_counts"
    if tool == "submit_annotations":
        annotations = (args or {}).get("annotations")
        count = len(annotations) if isinstance(annotations, list) else 0
        clusters = (args or {}).get("cluster_ids")
        cluster_count = len(clusters) if isinstance(clusters, list) else 0
        handles = (args or {}).get("handles")
        handle_count = len(handles) if isinstance(handles, list) else 0
        include_all = bool((args or {}).get("include_all"))
        extra = ""
        if include_all:
            extra = " include_all"
        elif handle_count:
            extra = f" handles={handle_count}"
        elif cluster_count:
            extra = f" clusters={cluster_count}"
        return f"submit annotations count={count}{extra}"
    return tool


def _agent_readable_tool_result_summary(tool_name: str, result: Any) -> str:
    tool = str(tool_name or "")
    if not isinstance(result, dict):
        return f"{tool} result"
    if result.get("blocked"):
        reason = result.get("error") or "blocked"
        return f"{tool} blocked ({reason})"
    if result.get("error"):
        err = result.get("error")
        if isinstance(err, dict):
            code = err.get("code") or "error"
            return f"{tool} error ({code})"
        return f"{tool} error ({err})"
    if result.get("skipped"):
        reason = result.get("reason") or "skipped"
        return f"{tool} skipped ({reason})"
    if tool == "classify_crop":
        best = result.get("best") if isinstance(result, dict) else None
        if isinstance(best, dict):
            label = best.get("label")
            prob = best.get("prob")
            if label is not None and prob is not None:
                return f"classify_crop best={label} prob={prob:.3f}"
        return "classify_crop result"
    if tool in {"run_detector", "zoom_and_detect", "sam3_text", "sam3_similarity", "qwen_infer", "look_and_inspect"}:
        cluster_ids = result.get("cluster_ids")
        count = len(cluster_ids) if isinstance(cluster_ids, list) else None
        new_clusters = result.get("new_clusters")
        label_counts = result.get("label_counts") if isinstance(result, dict) else None
        caption = result.get("caption") if isinstance(result, dict) else None
        if count is not None:
            extra = f" new={new_clusters}" if isinstance(new_clusters, int) else ""
            label_text = ""
            if isinstance(label_counts, dict) and label_counts:
                label_parts = [f"{key}={label_counts[key]}" for key in sorted(label_counts.keys())]
                label_text = " labels=" + ",".join(label_parts[:6])
            caption_text = f" caption=\"{_agent_readable_trim(caption)}\"" if caption and tool == "look_and_inspect" else ""
            return f"{tool} returned {count} clusters{extra}{label_text}{caption_text}"
        return f"{tool} result"
    if tool in {"view_cell_raw", "view_cell_overlay"}:
        cell = result.get("grid_cell")
        cell_text = f" {cell}" if cell else ""
        return f"{tool}{cell_text} ready"
    if tool == "view_full_overlay":
        cells = result.get("grid_usage") if isinstance(result, dict) else None
        count = len(cells) if isinstance(cells, list) else 0
        return f"view_full_overlay cells={count}"
    if tool == "get_tile_context":
        total = result.get("cluster_total") if isinstance(result, dict) else None
        grid_cell = result.get("grid_cell") if isinstance(result, dict) else None
        if total is not None:
            return f"get_tile_context {grid_cell} clusters={total}"
        return "get_tile_context result"
    if tool == "get_tile_context_chunk":
        idx = result.get("chunk_index") if isinstance(result, dict) else None
        total = result.get("chunk_total") if isinstance(result, dict) else None
        return f"get_tile_context_chunk {idx}/{total}" if idx is not None else "get_tile_context_chunk result"
    if tool == "get_global_context":
        count = len(result.get("tile_summaries") or []) if isinstance(result, dict) else 0
        return f"get_global_context tiles={count}"
    if tool == "get_global_context_chunk":
        idx = result.get("chunk_index") if isinstance(result, dict) else None
        total = result.get("chunk_total") if isinstance(result, dict) else None
        return f"get_global_context_chunk {idx}/{total}" if idx is not None else "get_global_context_chunk result"
    if tool == "think_missed_objects":
        labels = result.get("missing_labels") if isinstance(result, dict) else None
        tiles = result.get("missing_tiles") if isinstance(result, dict) else None
        label_count = len(labels) if isinstance(labels, list) else 0
        tile_count = len(tiles) if isinstance(tiles, list) else 0
        return f"think_missed_objects labels={label_count} tiles={tile_count}"
    if tool == "grid_label_counts":
        cells = result.get("cells")
        count = len(cells) if isinstance(cells, list) else 0
        return f"grid_label_counts cells={count}"
    if tool == "submit_annotations":
        clusters = result.get("submitted_clusters")
        cluster_count = len(clusters) if isinstance(clusters, list) else 0
        count = result.get("count")
        if cluster_count:
            return f"submit_annotations clusters={cluster_count} count={count}"
    if tool == "log_observation":
        observation = _agent_readable_trim(result.get("observation")) if isinstance(result, dict) else ""
        if observation:
            return f"observation logged \"{observation}\""
        return "observation logged"
    if tool == "zoom_and_detect":
        confirmation = result.get("confirmation") if isinstance(result, dict) else None
        if isinstance(confirmation, dict):
            label = confirmation.get("label")
            match = confirmation.get("label_match")
            if label is not None and match is not None:
                return f"zoom_and_detect confirmation label={label} match={'yes' if match else 'no'}"
    if tool == "image_zoom_in_tool":
        cell = result.get("grid_cell") if isinstance(result, dict) else None
        cluster = result.get("cluster_id") if isinstance(result, dict) else None
        cell_text = f" grid={cell}" if cell else ""
        cluster_text = f" cluster={cluster}" if cluster is not None else ""
        if cell_text or cluster_text:
            return f"image_zoom_in_tool{cell_text}{cluster_text}"
        return "image_zoom_in_tool result"
    for key in ("detections", "candidates", "annotations"):
        items = result.get(key)
        if isinstance(items, list):
            details = _agent_readable_candidates_summary(items)
            suffix = f": {details}" if details else ""
            return f"{tool} returned {len(items)} {key}{suffix}"
    return f"{tool} result"


def _agent_readable_line(
    event_type: str,
    *,
    step_id: Optional[int] = None,
    tool_name: Optional[str] = None,
    args: Optional[Dict[str, Any]] = None,
    result: Any = None,
    output_text: Optional[str] = None,
) -> str:
    prefix = f"step{step_id}: " if step_id is not None else ""
    if event_type == "tool_call" and tool_name:
        return prefix + _agent_readable_tool_call_summary(tool_name, args)
    if event_type == "tool_result" and tool_name:
        return prefix + _agent_readable_tool_result_summary(tool_name, result)
    if event_type == "model_output" and output_text:
        return prefix + f"model output \"{_agent_readable_trim(output_text)}\""
    if event_type == "prepass" and tool_name:
        return prefix + _agent_readable_tool_result_summary(tool_name, result)
    return ""


def _agent_readable_candidates_summary(items: List[Dict[str, Any]], limit: int = 8) -> str:
    if not items:
        return ""
    parts: List[str] = []
    for item in items[:limit]:
        if not isinstance(item, dict):
            continue
        cand_id = item.get("candidate_id")
        label = str(item.get("label") or item.get("class_name") or "unknown")
        bbox = None
        if isinstance(item.get("bbox_xyxy_px"), (list, tuple)) and len(item.get("bbox_xyxy_px")) >= 4:
            bbox = item.get("bbox_xyxy_px")
        elif isinstance(item.get("bbox_2d"), (list, tuple)) and len(item.get("bbox_2d")) >= 4:
            bbox = item.get("bbox_2d")
        bbox_text = _agent_readable_format_bbox(bbox) if bbox else "[]"
        score = item.get("score")
        if score is None:
            score_text = "score=n/a"
        else:
            try:
                score_text = f"score={float(score):.3f}"
            except (TypeError, ValueError):
                score_text = "score=n/a"
        id_text = f"id={cand_id} " if cand_id is not None else ""
        parts.append(f"{id_text}{label} {bbox_text} {score_text}")
    if len(items) > limit:
        parts.append(f"+{len(items) - limit} more")
    return "; ".join(parts)


def _agent_label_counts_summary(detections: Sequence[Dict[str, Any]], limit: int = 8) -> str:
    counts: Dict[str, int] = {}
    for det in detections:
        label = str(det.get("label") or det.get("class_name") or "").strip()
        if not label:
            continue
        counts[label] = counts.get(label, 0) + 1
    if not counts:
        return "none"
    ordered = sorted(counts.items(), key=lambda item: (-item[1], item[0]))
    parts = [f"{label}({count})" for label, count in ordered[:limit]]
    if len(ordered) > limit:
        parts.append(f"+{len(ordered) - limit} more")
    return ", ".join(parts)


def _agent_clean_plan_text(text: str, max_len: int = 4000) -> str:
    if not text:
        return ""
    cleaned = text.strip()
    for prefix in ("PLAN:", "Plan:", "FINAL:", "Final:"):
        if cleaned.startswith(prefix):
            cleaned = cleaned[len(prefix):].strip()
    match = re.search(r"\\d+[\\).:-]", cleaned)
    if match:
        cleaned = cleaned[match.start():].lstrip()
    lines = [line.strip() for line in cleaned.splitlines() if line.strip()]
    numbered = []
    for line in lines:
        if re.match(r"^\\d+[\\).:-]", line):
            numbered.append(line)
    if not numbered:
        step_parts = re.split(r"(?=\\d+[\\).:-]\\s)", cleaned)
        step_parts = [part.strip() for part in step_parts if part.strip()]
        if step_parts and re.match(r"^\\d+[\\).:-]", step_parts[0]):
            numbered = step_parts
    if numbered:
        cleaned = "\n".join(numbered)
    else:
        cleaned = "\n".join(lines)
        if cleaned:
            cleaned = re.sub(r"^(got it|sure|okay|ok|alright)[,\\s]+", "", cleaned, flags=re.IGNORECASE)
            cleaned = f"1. {cleaned}"
    if len(cleaned) > max_len:
        cleaned = cleaned[: max_len - 3].rstrip() + "..."
    return cleaned


def _qwen_agent_message_text(msg: Any) -> str:
    content = getattr(msg, "content", None)
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        texts: List[str] = []
        for item in content:
            try:
                item_type, item_value = item.get_type_and_value()
            except Exception:
                continue
            if item_type == "text" and item_value:
                texts.append(str(item_value))
        return "\n".join(texts)
    return ""


def _agent_content_to_text(content: Any) -> str:
    if content is None:
        return ""
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        parts: List[str] = []
        for item in content:
            if isinstance(item, dict) and "text" in item:
                parts.append(str(item.get("text") or ""))
            elif hasattr(item, "get_type_and_value"):
                try:
                    item_type, item_value = item.get_type_and_value()
                except Exception:
                    continue
                if item_type == "text" and item_value:
                    parts.append(str(item_value))
        return "".join(parts)
    return str(content)


def _agent_stream_text_from_output(output: Sequence[Any]) -> str:
    if not output:
        return ""
    for msg in reversed(output):
        if msg is None:
            continue
        if isinstance(msg, dict):
            if msg.get("role") == "assistant":
                return _agent_content_to_text(msg.get("content"))
            continue
        if getattr(msg, "role", None) == "assistant":
            text = _qwen_agent_message_text(msg)
            if text:
                return text
    return ""


def _agent_stream_tag_open(text: str, start: str, end: str) -> bool:
    if not text:
        return False
    start_idx = text.rfind(start)
    if start_idx < 0:
        return False
    end_idx = text.rfind(end)
    return end_idx < start_idx


def _agent_stream_extract_tool_name(text: str) -> Optional[str]:
    if not text:
        return None
    tail = text
    if "<tool_call>" in text:
        tail = text.split("<tool_call>")[-1]
    match = re.search(r"\"name\"\\s*:\\s*\"([^\"]+)\"", tail)
    if match:
        return match.group(1)
    return None


def _agent_parse_json_relaxed(payload: Any) -> Optional[Any]:
    if payload is None:
        return None
    if isinstance(payload, (dict, list)):
        return payload
    if not isinstance(payload, str):
        try:
            return json.loads(payload)
        except Exception:
            return None



def _agent_run_prepass(
    payload: QwenPrepassRequest,
    *,
    pil_img: Image.Image,
    image_token: str,
    labelmap: List[str],
    glossary: str,
    as_tool_messages: bool = True,
    trace_writer: Optional[Callable[[Dict[str, Any]], None]] = None,
    trace_full_writer: Optional[Callable[[Dict[str, Any]], None]] = None,
    model_id_override: Optional[str] = None,
) -> Tuple[
    List[Any],
    List[Dict[str, Any]],
    List[str],
    List[AgentTraceEvent],
    bool,
    bool,
    Optional[str],
    Optional[str],
    Optional[str],
    Optional[str],
    List[Dict[str, Any]],
]:
    img_w, img_h = pil_img.size
    mode = (payload.prepass_mode or "").strip().lower()
    if not mode or mode in {"none", "off", "false"}:
        return [], [], [], [], False, False, None, None, None, None, []
    prepass_messages: List[Any] = []
    prepass_records: List[Dict[str, Any]] = []
    prepass_detections: List[Dict[str, Any]] = []
    prepass_warnings: List[str] = []
    prepass_trace: List[AgentTraceEvent] = []
    prepass_has_detector = False
    prepass_has_inspect = False
    qwen_failed = False
    step_id = 0
    max_items = int(payload.prepass_inspect_topk) if payload.prepass_inspect_topk is not None else 0
    caption_summary: Optional[str] = None
    caption_text: Optional[str] = None
    caption_entries: List[Dict[str, Any]] = []
    sam3_text_prompts: Dict[str, List[str]] = {}
    sam3_text_term_map: Dict[str, Dict[str, List[str]]] = {}
    sam3_text_summary: Optional[str] = None
    prepass_source_summary: Optional[str] = None
    grid_for_log: Optional[Dict[str, Any]] = None
    if payload.use_detection_overlay is not False:
        grid_for_log = _agent_grid_spec_for_payload(payload, img_w, img_h)

    def add_tool_result(name: str, result: Dict[str, Any], summary: str) -> None:
        nonlocal step_id
        step_id += 1
        prepass_trace.append(
            AgentTraceEvent(
                step_id=step_id,
                phase="tool_call",
                tool_name=name,
                summary="prepass_tool_call",
                timestamp=time.time(),
            )
        )
        if trace_writer:
            trace_writer(
                {
                    "type": "prepass_tool_call",
                    "tool": name,
                    "summary": summary,
                    "ts": time.time(),
                }
            )
        step_id += 1
        compact = _agent_compact_tool_result(result, max_items=max_items)
        if as_tool_messages:
            prepass_messages.append(
                QwenAgentMessage(
                    role="function",
                    name=name,
                    content=[QwenAgentContentItem(text=json.dumps(compact, ensure_ascii=False))],
                )
            )
        else:
            prepass_records.append({"tool": name, "result": compact})
        prepass_trace.append(
            AgentTraceEvent(
                step_id=step_id,
                phase="tool_result",
                tool_name=name,
                summary=summary,
                timestamp=time.time(),
            )
        )
        if trace_writer:
            trace_writer(
                {
                    "type": "prepass_tool_result",
                    "tool": name,
                    "summary": summary,
                    "result": result,
                    "ts": time.time(),
                }
            )
        if _AGENT_TRACE_READABLE_WRITER is not None:
            count = None
            count_key = None
            if isinstance(result, dict):
                for key in ("detections", "candidates", "annotations"):
                    items = result.get(key)
                    if isinstance(items, list):
                        count = len(items)
                        count_key = key
                        break
            pretty_summary = summary.replace("prepass_", "prepass ")
            line = f"{pretty_summary}"
            if count is not None and count_key:
                line = f"{line} -> {count} {count_key}"
            _agent_readable_write(line)
            if isinstance(result, dict) and count_key:
                items = result.get(count_key)
                if isinstance(items, list) and items:
                    for item in items:
                        if isinstance(item, dict):
                            detail = _agent_readable_detection_line(
                                item,
                                grid=grid_for_log,
                                img_w=img_w,
                                img_h=img_h,
                            )
                            _agent_readable_write(f"{pretty_summary} item: {detail}")
        if isinstance(result.get("detections"), list):
            prepass_detections.extend(result.get("detections") or [])

    detector_modes: List[str] = []
    if "ensemble" in mode:
        detector_modes = ["yolo", "rfdetr"]
    else:
        detector_modes = [payload.detector_mode or "yolo"]
    sahi_enabled = "sahi" in mode
    detector_conf = payload.detector_conf
    if detector_conf is None and _AGENT_ACTIVE_DETECTOR_CONF is not None:
        detector_conf = _AGENT_ACTIVE_DETECTOR_CONF
    for det_mode in detector_modes:
        det_args = {
            "image_token": image_token,
            "detector_id": payload.detector_id if det_mode == (payload.detector_mode or "yolo") else None,
            "mode": det_mode,
            "conf": detector_conf,
            "sahi": {"enabled": True} if sahi_enabled else None,
        }
        if trace_full_writer:
            trace_full_writer(
                {
                    "type": "prepass_tool_call",
                    "tool": "run_detector",
                    "args": det_args,
                    "ts": time.time(),
                }
            )
        try:
            det_result = _agent_tool_run_detector(
                image_token=det_args["image_token"],
                detector_id=det_args["detector_id"],
                mode=det_args["mode"],
                conf=det_args["conf"],
                sahi=det_args["sahi"],
                register=False,
            )
        except Exception as exc:  # noqa: BLE001
            prepass_warnings.append(f"prepass_detector_failed:{det_mode}:{exc}")
            continue
        det_result = {**det_result, "detector_mode": det_mode}
        if trace_full_writer:
            trace_full_writer(
                {
                    "type": "prepass_tool_result",
                    "tool": "run_detector",
                    "result": det_result,
                    "ts": time.time(),
                }
            )
        add_tool_result("run_detector", det_result, f"prepass_detector:{det_mode}")
        prepass_has_detector = True

    if "sam3_text" in mode and labelmap:
        labels = [lbl for lbl in labelmap if str(lbl).strip()]
        if labels:
            prompt_budget = 10 if payload.sam3_text_synonym_budget is None else int(payload.sam3_text_synonym_budget)
            synonym_map, term_meta = _agent_generate_sam3_synonyms(
                labels,
                glossary or "",
                max_synonyms=prompt_budget,
            )
            sam3_text_term_map = term_meta
            score_thr = payload.prepass_sam3_text_thr
            if score_thr is None and _AGENT_ACTIVE_SAM3_SCORE_THR is not None:
                score_thr = _AGENT_ACTIVE_SAM3_SCORE_THR
            mask_thr = payload.sam3_mask_threshold
            if mask_thr is None and _AGENT_ACTIVE_SAM3_MASK_THR is not None:
                mask_thr = _AGENT_ACTIVE_SAM3_MASK_THR
            max_results = None
            for label in labels:
                base_terms = term_meta.get(label, {}).get("base_terms", [])
                expanded_terms = term_meta.get(label, {}).get("expanded_terms", [])
                max_prompts = max(2, len(synonym_map.get(label, [])) + 2)
                prompts = _sam3_prompt_variants(label, synonym_map, max_prompts=max_prompts)
                if not prompts:
                    prompts = [str(label).replace("_", " ").strip() or str(label).strip()]
                for prompt in prompts:
                    prompt_origin = "base"
                    if prompt in expanded_terms:
                        prompt_origin = "expanded"
                    elif prompt not in base_terms:
                        prompt_origin = "unknown"
                    sam3_text_prompts.setdefault(label, []).append(prompt)
                    sam3_args = {
                        "image_token": image_token,
                        "prompt": prompt,
                        "label": label,
                        "score_thr": score_thr,
                        "mask_threshold": mask_thr,
                        "max_results": max_results,
                    }
                    if trace_full_writer:
                        trace_full_writer(
                            {
                                "type": "prepass_tool_call",
                                "tool": "sam3_text",
                                "args": sam3_args,
                                "ts": time.time(),
                            }
                        )
                    try:
                        sam3_result = _agent_tool_sam3_text(
                            image_token=sam3_args["image_token"],
                            prompt=sam3_args["prompt"],
                            label=sam3_args["label"],
                            score_thr=sam3_args["score_thr"],
                            mask_threshold=sam3_args["mask_threshold"],
                            max_results=sam3_args["max_results"],
                            register=False,
                        )
                        dets = sam3_result.get("detections") if isinstance(sam3_result, dict) else None
                        if isinstance(dets, list):
                            for det in dets:
                                if not isinstance(det, dict):
                                    continue
                                det["sam3_prompt_term"] = prompt
                                det["sam3_prompt_label"] = label
                                det["sam3_prompt_source"] = prompt_origin
                        if trace_full_writer:
                            trace_full_writer(
                                {
                                    "type": "prepass_tool_result",
                                    "tool": "sam3_text",
                                    "result": sam3_result,
                                    "ts": time.time(),
                                }
                            )
                        summary = f"prepass_sam3_text:{label} prompt={prompt}"
                        add_tool_result("sam3_text", sam3_result, summary)
                    except Exception as exc:  # noqa: BLE001
                        prepass_warnings.append(f"prepass_sam3_text_failed:{label}:{exc}")

    if "similarity" in mode and prepass_detections:
        per_class = int(payload.prepass_similarity_per_class or 2)
        if payload.prepass_similarity_score is not None:
            score_thr = float(payload.prepass_similarity_score)
        elif _AGENT_ACTIVE_SAM3_SCORE_THR is not None:
            score_thr = float(_AGENT_ACTIVE_SAM3_SCORE_THR)
        else:
            score_thr = 0.2
        by_label: Dict[str, List[Dict[str, Any]]] = {}
        for det in prepass_detections:
            label = str(det.get("label") or det.get("class_name") or "unknown")
            by_label.setdefault(label, []).append(det)
        any_similarity = False
        for label, dets in by_label.items():
            dets.sort(key=lambda d: float(d.get("score") or 0.0), reverse=True)
            exemplar_boxes: List[Dict[str, Any]] = []
            for det in dets[:per_class]:
                if float(det.get("score") or 0.0) < score_thr:
                    continue
                bbox_2d = det.get("bbox_2d")
                if isinstance(bbox_2d, (list, tuple)) and len(bbox_2d) >= 4:
                    exemplar_boxes.append({"bbox_2d": list(bbox_2d), "bbox_space": "full"})
            if not exemplar_boxes:
                continue
            any_similarity = True
            try:
                sim_args = {
                    "image_token": image_token,
                    "exemplar_boxes": exemplar_boxes,
                    "label": label,
                    "bbox_labels": [True for _ in exemplar_boxes],
                    "score_thr": payload.prepass_similarity_score,
                }
                if trace_full_writer:
                    trace_full_writer(
                        {
                            "type": "prepass_tool_call",
                            "tool": "sam3_similarity",
                            "args": sim_args,
                            "ts": time.time(),
                        }
                    )
                sim_result = _agent_tool_sam3_similarity(
                    image_token=sim_args["image_token"],
                    exemplar_boxes=sim_args["exemplar_boxes"],
                    label=sim_args["label"],
                    bbox_labels=sim_args["bbox_labels"],
                    score_thr=sim_args["score_thr"],
                    register=False,
                )
                if trace_full_writer:
                    trace_full_writer(
                        {
                            "type": "prepass_tool_result",
                            "tool": "sam3_similarity",
                            "result": sim_result,
                            "ts": time.time(),
                        }
                    )
                add_tool_result("sam3_similarity", sim_result, f"prepass_sam3_similarity:{label}")
            except Exception as exc:  # noqa: BLE001
                prepass_warnings.append(f"prepass_sam3_similarity_failed:{label}:{exc}")
        if not any_similarity:
            prepass_warnings.append("prepass_similarity_no_exemplars")

    if "inspect" in mode and payload.prepass_inspect_quadrants is not False:
        windows = _agent_quadrant_windows_qwen(0.1)
        for window in windows:
            inspect_args = {
                "image_token": image_token,
                "window_bbox_2d": window.get("bbox_2d"),
                "labelmap": labelmap,
                "labelmap_glossary": glossary,
                "max_objects": payload.prepass_inspect_topk,
            }
            if trace_full_writer:
                trace_full_writer(
                    {
                        "type": "prepass_tool_call",
                        "tool": "look_and_inspect",
                        "args": inspect_args,
                        "ts": time.time(),
                    }
                )
            try:
                inspect_result = _agent_tool_look_and_inspect(
                    image_token=inspect_args["image_token"],
                    window_bbox_2d=inspect_args["window_bbox_2d"],
                    labelmap=inspect_args["labelmap"],
                    labelmap_glossary=inspect_args["labelmap_glossary"],
                    max_objects=inspect_args["max_objects"],
                    register=False,
                    include_caption=False,
                )
                if trace_full_writer:
                    trace_full_writer(
                        {
                            "type": "prepass_tool_result",
                            "tool": "look_and_inspect",
                            "result": inspect_result,
                            "ts": time.time(),
                        }
                    )
                add_tool_result("look_and_inspect", inspect_result, f"prepass_inspect:{window.get('name')}")
                prepass_has_inspect = True
            except Exception as exc:  # noqa: BLE001
                prepass_warnings.append(f"prepass_inspect_failed:{window.get('name')}:{exc}")
                qwen_failed = True

    if "qwen" in mode and labelmap:
        try:
            max_results = int(payload.prepass_inspect_topk or 12)
            if max_results > 30:
                max_results = 30
            qwen_args = {
                "image_token": image_token,
                "items": labelmap,
                "prompt_type": "bbox",
                "max_results": max_results,
                "max_new_tokens": 512,
                "extra_context": glossary,
            }
            if trace_full_writer:
                trace_full_writer(
                    {
                        "type": "prepass_tool_call",
                        "tool": "qwen_infer",
                        "args": qwen_args,
                        "ts": time.time(),
                    }
                )
            qwen_result = _agent_tool_qwen_infer(
                image_token=qwen_args["image_token"],
                items=qwen_args["items"],
                prompt_type=qwen_args["prompt_type"],
                max_results=qwen_args["max_results"],
                max_new_tokens=qwen_args["max_new_tokens"],
                extra_context=qwen_args["extra_context"],
                register=False,
            )
            if trace_full_writer:
                trace_full_writer(
                    {
                        "type": "prepass_tool_result",
                        "tool": "qwen_infer",
                        "result": qwen_result,
                        "ts": time.time(),
                    }
                )
            add_tool_result("qwen_infer", qwen_result, "prepass_qwen_infer")
        except Exception as exc:  # noqa: BLE001
            prepass_warnings.append(f"prepass_qwen_infer_failed:{exc}")
            qwen_failed = True

    if payload.prepass_caption:
        det_hint_sources = {"yolo", "rfdetr"}
        det_hint_items = [det for det in prepass_detections if _agent_detection_has_source(det, det_hint_sources)]
        det_hint_items.sort(key=lambda d: float(d.get("score") or 0.0), reverse=True)
        det_hint_summary = _agent_label_counts_summary(det_hint_items, limit=10)
        hint_limit = min(len(det_hint_items), 120)
        caption_hints: List[QwenCaptionHint] = []
        for det in det_hint_items[:hint_limit]:
            label = str(det.get("label") or det.get("class_name") or "").strip()
            bbox = det.get("bbox_xyxy_px") or None
            if not bbox and det.get("bbox_2d"):
                bbox = _qwen_bbox_to_xyxy(
                    pil_img.width,
                    pil_img.height,
                    det.get("bbox_2d"),
                )
            if not label or not bbox or len(bbox) != 4:
                continue
            caption_hints.append(
                QwenCaptionHint(
                    label=label,
                    bbox=[float(v) for v in bbox],
                    confidence=det.get("score"),
                )
            )
        prepass_prompt = (
            "Describe the image region in detail. "
            "Use detector hints as suggestions, but also mention other visible objects. "
            "Do not mention labels, hints, or coordinates."
        )
        if det_hint_summary and det_hint_summary != "none":
            prepass_prompt = f"{prepass_prompt} Detector hints: {det_hint_summary}."
        caption_profile = (payload.prepass_caption_profile or "light").strip().lower()
        if caption_profile not in {"light", "deep"}:
            caption_profile = "light"

        caption_text = ""
        windowed_captions: List[Tuple[int, int, int, str]] = []
        if caption_profile == "light":
            caption_base_model_id = (
                model_id_override or (active_qwen_metadata or {}).get("model_id") or QWEN_MODEL_NAME
            )
            caption_model_id = (payload.prepass_caption_model_id or "").strip() or None
            caption_variant = payload.prepass_caption_variant or "Instruct"
            if not caption_model_id:
                caption_model_id = _resolve_qwen_variant_model_id(caption_base_model_id, caption_variant)
            caption_max_tokens = int(payload.prepass_caption_max_tokens or 128)
            caption_max_tokens = max(32, min(256, caption_max_tokens))
            system_prompt = (
                "You are a captioning assistant. Respond in English with a detailed sentence."
            )
            decode_override = {"do_sample": False}
            try:
                runtime = (
                    _ensure_qwen_ready_for_caption(caption_model_id)
                    if caption_model_id
                    else _ensure_qwen_ready()
                )
                full_prompt = (
                    "Describe the full image in detail. "
                    "Use detector hints as suggestions, but mention other visible objects. "
                    "Do not mention labels, hints, or coordinates."
                )
                if det_hint_summary and det_hint_summary != "none":
                    full_prompt = f"{full_prompt} Detector hints: {det_hint_summary}."
                caption_raw, _, _ = _run_qwen_inference(
                    full_prompt,
                    pil_img,
                    max_new_tokens=caption_max_tokens,
                    system_prompt_override=system_prompt,
                    runtime_override=runtime,
                    decode_override=decode_override,
                )
                caption_text, _ = _extract_caption_from_text(caption_raw, marker=None)
                caption_text = _sanitize_qwen_caption(caption_text)
                overlap = _resolve_qwen_window_overlap(None)
                window_size = _resolve_qwen_window_size(None, pil_img.width, pil_img.height, overlap=overlap)
                force_two = True
                x_positions = _window_positions(pil_img.width, window_size, overlap, force_two=force_two)
                y_positions = _window_positions(pil_img.height, window_size, overlap, force_two=force_two)
                window_prompt = (
                    "Describe this region in 1 detailed sentence. "
                    "Focus only on this region. "
                    "Do not mention labels, hints, or coordinates."
                )
                for y0 in y_positions:
                    for x0 in x_positions:
                        window_img = pil_img.crop((x0, y0, x0 + window_size, y0 + window_size))
                        window_raw, _, _ = _run_qwen_inference(
                            window_prompt,
                            window_img,
                            max_new_tokens=caption_max_tokens,
                            system_prompt_override=system_prompt,
                            runtime_override=runtime,
                            decode_override=decode_override,
                        )
                        window_caption, _ = _extract_caption_from_text(window_raw, marker=None)
                        window_caption = _sanitize_qwen_caption(window_caption)
                        if window_caption:
                            windowed_captions.append((x0, y0, window_size, window_caption))
                            cell = None
                            if grid_for_log:
                                bbox_2d = _xyxy_to_qwen_bbox(
                                    img_w,
                                    img_h,
                                    float(x0),
                                    float(y0),
                                    float(x0 + window_size),
                                    float(y0 + window_size),
                                )
                                cell = _agent_grid_cell_for_window_bbox(grid_for_log, bbox_2d)
                            cell_text = f" {cell}" if cell else ""
                            _agent_readable_write(
                                f"prepass caption window{cell_text} "
                                f"[{x0},{y0},{x0 + window_size},{y0 + window_size}]: {window_caption}"
                            )
                            if trace_writer:
                                trace_writer(
                                    {
                                        "type": "prepass_caption_window",
                                        "window": [int(x0), int(y0), int(window_size)],
                                        "grid_cell": cell,
                                        "caption": window_caption,
                                        "ts": time.time(),
                                    }
                                )
                            if trace_full_writer:
                                trace_full_writer(
                                    {
                                        "type": "prepass_caption_window",
                                        "window": [int(x0), int(y0), int(window_size)],
                                        "grid_cell": cell,
                                        "caption": window_caption,
                                        "ts": time.time(),
                                    }
                                )
            except Exception as exc:  # noqa: BLE001
                prepass_warnings.append(f"prepass_caption_failed:light:{exc}")
                qwen_failed = True
                caption_text = ""
                windowed_captions = []
        if caption_profile != "light":
            caption_variant = payload.prepass_caption_variant or payload.model_variant or "auto"
            caption_model_id = (payload.prepass_caption_model_id or model_id_override or "").strip() or None
            caption_max_tokens = int(payload.prepass_caption_max_tokens or 512)
            caption_payload = QwenCaptionRequest(
                image_token=image_token,
                user_prompt=prepass_prompt,
                label_hints=caption_hints or None,
                image_width=pil_img.width,
                image_height=pil_img.height,
                include_counts=False,
                include_coords=True,
                max_boxes=min(len(caption_hints), 120),
                max_new_tokens=caption_max_tokens,
                model_variant=caption_variant,
                model_id=caption_model_id,
                final_answer_only=True,
                two_stage_refine=True,
                caption_mode="windowed",
                caption_all_windows=True,
                restrict_to_labels=False,
                fast_mode=True,
                multi_model_cache=True,
            )
            caption_base_model_id = (
                caption_model_id
                or model_id_override
                or (active_qwen_metadata or {}).get("model_id")
                or QWEN_MODEL_NAME
            )
            caption_max_tokens = int(caption_payload.max_new_tokens or 512)

            def run_caption_attempt(request: QwenCaptionRequest) -> Tuple[str, List[Tuple[int, int, int, str]]]:
                attempt_windows: List[Tuple[int, int, int, str]] = []

                def _window_hook(x0: int, y0: int, size: int, caption: str) -> None:
                    attempt_windows.append((x0, y0, size, caption))
                    if _AGENT_TRACE_READABLE_WRITER is None or not caption:
                        return
                    cell = None
                    if grid_for_log:
                        bbox_2d = _xyxy_to_qwen_bbox(
                            img_w,
                            img_h,
                            float(x0),
                            float(y0),
                            float(x0 + size),
                            float(y0 + size),
                        )
                        cell = _agent_grid_cell_for_window_bbox(grid_for_log, bbox_2d)
                    cell_text = f" {cell}" if cell else ""
                    _agent_readable_write(
                        f"prepass caption window{cell_text} "
                        f"[{x0},{y0},{x0 + size},{y0 + size}]: {caption}"
                    )
                    if trace_writer:
                        trace_writer(
                            {
                                "type": "prepass_caption_window",
                                "window": [int(x0), int(y0), int(size)],
                                "grid_cell": cell,
                                "caption": caption,
                                "ts": time.time(),
                            }
                        )
                    if trace_full_writer:
                        trace_full_writer(
                            {
                                "type": "prepass_caption_window",
                                "window": [int(x0), int(y0), int(size)],
                                "grid_cell": cell,
                                "caption": caption,
                                "ts": time.time(),
                            }
                        )

                token = _CAPTION_WINDOW_HOOK.set(_window_hook)
                try:
                    if trace_full_writer:
                        trace_full_writer(
                            {
                                "type": "prepass_caption_call",
                                "payload": {
                                    "model_id": request.model_id,
                                    "variant": request.model_variant,
                                    "caption_mode": request.caption_mode,
                                    "caption_all_windows": request.caption_all_windows,
                                    "restrict_to_labels": request.restrict_to_labels,
                                    "max_new_tokens": request.max_new_tokens,
                                    "hint_count": len(request.label_hints or []),
                                },
                                "ts": time.time(),
                            }
                        )
                    response = qwen_caption(request)
                    return response.caption or "", attempt_windows
                finally:
                    _CAPTION_WINDOW_HOOK.reset(token)

            def caption_is_bad(text: str) -> bool:
                return (
                    not text
                    or _caption_is_degenerate(text)
                    or _caption_needs_completion(text)
                    or _caption_has_meta(text)
                )

            try:
                caption_text, windowed_captions = run_caption_attempt(caption_payload)
            except Exception as exc:  # noqa: BLE001
                prepass_warnings.append(f"prepass_caption_failed:primary:{exc}")
                qwen_failed = True
                caption_text = ""
                windowed_captions = []

            if caption_is_bad(caption_text):
                fallback_payload = caption_payload.copy(
                    update={
                        "model_variant": "Instruct",
                        "use_sampling": False,
                        "two_stage_refine": True,
                    }
                )
                try:
                    caption_text, windowed_captions = run_caption_attempt(fallback_payload)
                except Exception as exc:  # noqa: BLE001
                    prepass_warnings.append(f"prepass_caption_failed:fallback:{exc}")
                    qwen_failed = True

            if caption_is_bad(caption_text):
                fallback_prompt = (
                    "Write a detailed, concrete caption describing the visible scene. "
                    "Mention the main objects and layout. Do not mention labels or coordinates."
                )
                fallback_system = (
                    "You are a captioning assistant. Use the image as truth. "
                    "Respond in English only. Return only the caption."
                )
                try:
                    fallback_raw, _, _ = _run_qwen_inference(
                        fallback_prompt,
                        pil_img,
                        max_new_tokens=caption_max_tokens,
                        system_prompt_override=fallback_system,
                        model_id_override=_resolve_qwen_variant_model_id(caption_base_model_id, "Instruct"),
                        decode_override={"do_sample": False},
                    )
                    caption_text, _ = _extract_caption_from_text(fallback_raw, marker=None)
                    caption_text = _sanitize_qwen_caption(caption_text)
                except Exception as exc:  # noqa: BLE001
                    prepass_warnings.append(f"prepass_caption_failed:direct:{exc}")
                    qwen_failed = True

            if caption_is_bad(caption_text):
                caption_text = _run_qwen_caption_cleanup(
                    caption_text or "Describe the image.",
                    pil_img,
                    caption_max_tokens,
                    caption_base_model_id,
                    use_caption_cache=True,
                    model_id_override=_resolve_qwen_variant_model_id(caption_base_model_id, "Instruct"),
                    runtime_override=None,
                    allowed_labels=None,
                    strict=True,
                    minimal_edit=False,
                )

        if _AGENT_TRACE_READABLE_WRITER is not None and caption_text:
            _agent_readable_write(f"prepass caption summary: {caption_text}")

        if windowed_captions:
            for x0, y0, size, caption in windowed_captions:
                bbox_2d = _xyxy_to_qwen_bbox(
                    pil_img.width,
                    pil_img.height,
                    float(x0),
                    float(y0),
                    float(x0 + size),
                    float(y0 + size),
                )
                x_center = x0 + size / 2.0
                y_center = y0 + size / 2.0
                horiz = "left" if x_center < pil_img.width / 3.0 else "right" if x_center > pil_img.width * 2 / 3.0 else "center"
                vert = "top" if y_center < pil_img.height / 3.0 else "bottom" if y_center > pil_img.height * 2 / 3.0 else "middle"
                name = f"{vert}_{horiz}"
                entry = {
                    "window": name,
                    "bbox_2d": list(bbox_2d),
                    "caption": caption,
                }
                caption_entries.append(entry)
                if trace_full_writer:
                    trace_full_writer(
                        {
                            "type": "prepass_caption_result",
                            "window": {"name": name, "bbox_2d": list(bbox_2d)},
                            "caption": caption,
                            "ts": time.time(),
                        }
                    )
                if _AGENT_TRACE_READABLE_WRITER is not None:
                    coords = _agent_readable_format_bbox(bbox_2d)
                    _agent_readable_write(f"prepass caption {name} {coords}: {caption}")

        caption_lines: List[str] = []
        if caption_text:
            caption_lines.append(f"Refined caption (Qwen VLM): {caption_text}")
        if caption_entries:
            caption_lines.append("Windowed captions (Qwen VLM). Use as hints for hidden objects:")
            for entry in caption_entries:
                coords = _agent_readable_format_bbox(entry.get("bbox_2d"))
                name = entry.get("window") or "window"
                caption_lines.append(f"- {name} {coords}: {entry.get('caption')}")
        caption_summary = "\n".join(caption_lines) if caption_lines else "none"
        prepass_messages.append(
            QwenAgentMessage(
                role="user",
                content=[QwenAgentContentItem(text=caption_summary)],
            )
        )
        step_id += 1
        prepass_trace.append(
            AgentTraceEvent(
                step_id=step_id,
                phase="intent",
                summary="prepass_caption",
                windows=caption_entries,
                timestamp=time.time(),
            )
        )
        if trace_writer:
            trace_writer({"type": "prepass_caption", "windows": caption_entries, "ts": time.time()})
        if trace_full_writer:
            trace_full_writer(
                {
                    "type": "prepass_caption",
                    "caption": caption_text,
                    "windows": caption_entries,
                    "ts": time.time(),
                }
            )

    if sam3_text_prompts:
        summary_parts = []
        for label in sorted(sam3_text_prompts.keys()):
            prompts = [p for p in sam3_text_prompts[label] if str(p).strip()]
            unique = []
            seen = set()
            for prompt in prompts:
                key = str(prompt).strip().lower()
                if key in seen:
                    continue
                seen.add(key)
                unique.append(str(prompt).strip())
            if unique:
                summary_parts.append(f"{label}: {', '.join(unique[:8])}")
        if summary_parts:
            sam3_text_summary = "; ".join(summary_parts)
            prepass_messages.append(
                QwenAgentMessage(
                    role="user",
                    content=[
                        QwenAgentContentItem(
                            text=(
                                "Prepass sam3_text prompts used (label -> prompts). "
                                "Use these as hints; you may add new synonyms:\n"
                                f"{sam3_text_summary}"
                            )
                        )
                    ],
                )
            )
            if trace_full_writer:
                trace_full_writer(
                    {
                        "type": "prepass_sam3_text_summary",
                        "summary": sam3_text_summary,
                        "ts": time.time(),
                    }
                )
            if _AGENT_TRACE_READABLE_WRITER is not None:
                _agent_readable_write(f"prepass sam3_text prompts: {sam3_text_summary}")

    if prepass_detections:
        raw_counts = _agent_source_counts(prepass_detections)
        merged, removed = _agent_merge_prepass_detections(prepass_detections, iou_thr=0.85)
        merged_counts = _agent_source_counts(merged)
        prepass_detections = merged
        prepass_source_summary = (
            f"raw_sources({ _agent_format_source_counts(raw_counts) }); "
            f"dedup_iou>=0.85 removed={removed} kept={len(merged)}; "
            f"merged_sources({ _agent_format_source_counts(merged_counts) })"
        )
        if _AGENT_TRACE_READABLE_WRITER is not None:
            _agent_readable_write(f"prepass dedup: {prepass_source_summary}")
        prepass_messages.append(
            QwenAgentMessage(
                role="user",
                content=[
                    QwenAgentContentItem(
                        text=(
                            "Prepass detection sources: "
                            f"{prepass_source_summary}. "
                            "Caption hints use detector sources only (yolo/rfdetr); "
                            "SAM3 detections are supplemental and may overlap."
                        )
                    )
                ],
            )
        )

    if prepass_records and not as_tool_messages:
        prepass_messages.append(
            QwenAgentMessage(
                role="user",
                content=[
                    QwenAgentContentItem(
                        text=(
                            "Prepass tool results are available via list_candidates and tool calls; "
                            "they are not embedded here to keep prompts compact."
                        )
                    )
                ],
            )
        )
    if qwen_failed:
        try:
            _unload_qwen_runtime()
        except Exception:
            pass
    return (
        prepass_messages,
        prepass_detections,
        prepass_warnings,
        prepass_trace,
        prepass_has_detector,
        prepass_has_inspect,
        caption_summary,
        sam3_text_summary,
        prepass_source_summary,
        caption_text,
        caption_entries,
    )


def _agent_attach_provenance(
    detections: Sequence[Dict[str, Any]],
    *,
    source: str,
    source_primary: Optional[str] = None,
    source_prompt: Optional[str] = None,
    source_exemplar_handles: Optional[Sequence[str]] = None,
    source_detector_run_id: Optional[str] = None,
) -> None:
    for det in detections:
        if not isinstance(det, dict):
            continue
        if source_primary:
            det.setdefault("source_primary", source_primary)
        else:
            det.setdefault("source_primary", source)
        if source_prompt:
            det.setdefault("source_prompt", source_prompt)
        if source_exemplar_handles:
            det.setdefault("source_exemplar_handles", list(source_exemplar_handles))
        if source_detector_run_id:
            det.setdefault("source_detector_run_id", source_detector_run_id)
        sources = det.get("source_list")
        if not isinstance(sources, list):
            sources = []
        if source and source not in sources:
            sources.append(source)
        det["source_list"] = sources


def _agent_run_deep_prepass_part_a(
    payload: QwenPrepassRequest,
    *,
    pil_img: Image.Image,
    image_token: str,
    labelmap: List[str],
    glossary: str,
    trace_writer: Optional[Callable[[Dict[str, Any]], None]] = None,
    trace_full_writer: Optional[Callable[[Dict[str, Any]], None]] = None,
    trace_readable: Optional[Callable[[str], None]] = None,
) -> Dict[str, Any]:
    img_w, img_h = pil_img.size
    detections: List[Dict[str, Any]] = []
    warnings: List[str] = []
    sam3_text_prompts: Dict[str, List[str]] = {}
    sam3_text_term_map: Dict[str, Dict[str, List[str]]] = {}

    def _log_step(event: str, payload_obj: Dict[str, Any]) -> None:
        if trace_writer:
            trace_writer({"type": event, **payload_obj, "ts": time.time()})
        if trace_full_writer:
            trace_full_writer({"type": event, **payload_obj, "ts": time.time()})

    sahi_cfg = {
        "enabled": True,
        "slice_size": int(payload.sahi_window_size or 640),
        "overlap": float(payload.sahi_overlap_ratio or 0.2),
    }
    full_cfg = {"enabled": False}

    detector_conf = payload.detector_conf
    detector_iou = payload.detector_iou
    detector_merge_iou = payload.detector_merge_iou
    max_det = None

    for mode in ("yolo", "rfdetr"):
        if mode == "yolo" and payload.enable_yolo is False:
            continue
        if mode == "rfdetr" and payload.enable_rfdetr is False:
            continue
        det_id = payload.yolo_id if mode == "yolo" else payload.rfdetr_id
        if not det_id:
            det_id = payload.detector_id if (payload.detector_mode or "yolo") == mode else None
        for run_name, run_sahi in (("full", full_cfg), ("sahi", sahi_cfg)):
            args = {
                "image_token": image_token,
                "detector_id": det_id,
                "mode": mode,
                "conf": detector_conf,
                "sahi": run_sahi,
                "max_det": max_det,
                "iou": detector_iou,
                "merge_iou": detector_merge_iou,
                "expected_labelmap": labelmap or None,
            }
            if trace_readable:
                if run_name == "sahi":
                    trace_readable(
                        f"deep_prepass detector:{mode} start sahi="
                        f"{sahi_cfg.get('slice_size')}x{sahi_cfg.get('slice_size')} "
                        f"overlap={sahi_cfg.get('overlap')}"
                    )
                else:
                    trace_readable(f"deep_prepass detector:{mode} start full")
            _log_step("deep_prepass_tool_call", {"tool": "run_detector", "args": args})
            try:
                result = _agent_tool_run_detector(
                    image_token=image_token,
                    detector_id=det_id,
                    mode=mode,
                    conf=detector_conf,
                    sahi=run_sahi,
                    max_det=max_det,
                    iou=detector_iou,
                    merge_iou=detector_merge_iou,
                    expected_labelmap=labelmap or None,
                    register=False,
                )
            except Exception as exc:  # noqa: BLE001
                warnings.append(f"deep_prepass_detector_failed:{mode}:{run_name}:{exc}")
                continue
            _log_step("deep_prepass_tool_result", {"tool": "run_detector", "result": result})
            dets = list(result.get("detections") or [])
            if trace_readable:
                trace_readable(f"deep_prepass detector:{mode} {run_name} detections={len(dets)}")
            run_id = det_id
            if run_name:
                run_id = f"{det_id or mode}:{run_name}"
            _agent_attach_provenance(
                dets,
                source=mode,
                source_primary=mode,
                source_detector_run_id=run_id,
            )
            detections.extend(dets)

    if payload.enable_sam3_text is not False and labelmap:
        labels = [lbl for lbl in labelmap if str(lbl).strip()]
        prompt_budget = payload.sam3_text_synonym_budget
        if prompt_budget is not None:
            prompt_budget = int(prompt_budget)
        synonym_map, term_meta = _agent_generate_sam3_synonyms(
            labels,
            glossary or "",
            max_synonyms=prompt_budget,
        )
        sam3_text_term_map = term_meta
        score_thr = payload.prepass_sam3_text_thr
        if score_thr is None and _AGENT_ACTIVE_SAM3_SCORE_THR is not None:
            score_thr = _AGENT_ACTIVE_SAM3_SCORE_THR
        mask_thr = payload.sam3_mask_threshold
        if mask_thr is None and _AGENT_ACTIVE_SAM3_MASK_THR is not None:
            mask_thr = _AGENT_ACTIVE_SAM3_MASK_THR
        windowed = payload.sam3_text_window_extension is not False
        windows: List[Dict[str, Any]] = []
        if windowed:
            windows = _agent_sam3_text_windows(payload, pil_img=pil_img)
        sam3_model, sam3_processor, _ = _ensure_sam3_text_runtime()
        global_state = None
        try:
            global_state = sam3_processor.set_image(pil_img)
        except Exception:
            global_state = None
        if windows:
            for window in windows:
                window_xyxy = None
                if window.get("bbox_xyxy_px"):
                    window_xyxy = window.get("bbox_xyxy_px")
                elif window.get("bbox_2d") is not None:
                    window_xyxy = _normalize_window_xyxy(
                        {"bbox_2d": window.get("bbox_2d")}, img_w, img_h
                    )
                if not window_xyxy:
                    continue
                x1, y1, x2, y2 = window_xyxy
                crop_img = pil_img.crop((x1, y1, x2, y2))
                try:
                    window_state = sam3_processor.set_image(crop_img)
                except Exception:
                    window_state = None
                window["window_xyxy"] = window_xyxy
                window["crop_img"] = crop_img
                window["state"] = window_state
        prompt_plan: List[Tuple[str, str, str]] = []
        for label in labels:
            base_terms = term_meta.get(label, {}).get("base_terms", [])
            expanded_terms = term_meta.get(label, {}).get("expanded_terms", [])
            max_prompts = max(2, len(synonym_map.get(label, [])) + 2)
            prompts = _sam3_prompt_variants(label, synonym_map, max_prompts=max_prompts)
            if not prompts:
                prompts = [str(label).replace("_", " ").strip() or str(label).strip()]
            for prompt in prompts:
                prompt_origin = "base"
                if prompt in expanded_terms:
                    prompt_origin = "expanded"
                elif prompt not in base_terms:
                    prompt_origin = "unknown"
                sam3_text_prompts.setdefault(label, []).append(prompt)
                prompt_plan.append((label, prompt, prompt_origin))

        contexts: List[Dict[str, Any]] = [
            {
                "name": "full",
                "grid_cell": None,
                "window_xyxy": None,
                "window_bbox_2d": None,
                "crop_img": pil_img,
                "state": global_state,
            }
        ]
        for window in windows:
            contexts.append(
                {
                    "name": window.get("name") or window.get("grid_cell") or "window",
                    "grid_cell": window.get("grid_cell"),
                    "window_xyxy": window.get("window_xyxy"),
                    "window_bbox_2d": window.get("bbox_2d"),
                    "crop_img": window.get("crop_img") or pil_img,
                    "state": window.get("state"),
                }
            )

        for ctx in contexts:
            window_name = ctx.get("name") or "window"
            for label, prompt, prompt_origin in prompt_plan:
                args = {
                    "image_token": image_token,
                    "prompt": prompt,
                    "label": label,
                    "score_thr": score_thr,
                    "mask_threshold": mask_thr,
                    "max_results": None,
                }
                if ctx.get("window_bbox_2d") is not None:
                    args["window_bbox_2d"] = ctx.get("window_bbox_2d")
                if ctx.get("grid_cell"):
                    args["grid_cell"] = ctx.get("grid_cell")
                _log_step("deep_prepass_tool_call", {"tool": "sam3_text", "args": args})
                try:
                    dets, assigned_label, _ = _sam3_text_payloads_from_state(
                        full_img=pil_img,
                        crop_img=ctx.get("crop_img") or pil_img,
                        prompt=prompt,
                        label=label,
                        score_thr=score_thr,
                        mask_threshold=mask_thr,
                        max_results=None,
                        window_xyxy=ctx.get("window_xyxy"),
                        processor_override=sam3_processor,
                        state=ctx.get("state"),
                    )
                except Exception as exc:  # noqa: BLE001
                    if ctx.get("window_xyxy"):
                        warnings.append(f"deep_prepass_sam3_text_failed:{label}:{window_name}:{exc}")
                    else:
                        warnings.append(f"deep_prepass_sam3_text_failed:{label}:{exc}")
                    continue
                _log_step("deep_prepass_tool_result", {"tool": "sam3_text", "result": {"detections": dets}})
                for det in dets:
                    if not isinstance(det, dict):
                        continue
                    det["sam3_prompt_term"] = prompt
                    det["sam3_prompt_label"] = assigned_label or label
                    det["sam3_prompt_source"] = prompt_origin
                if trace_readable:
                    if ctx.get("window_xyxy"):
                        trace_readable(
                            f"deep_prepass sam3_text label={label} prompt={prompt} window={window_name} detections={len(dets)}"
                        )
                    else:
                        trace_readable(
                            f"deep_prepass sam3_text label={label} prompt={prompt} detections={len(dets)}"
                        )
                _agent_attach_provenance(
                    dets,
                    source="sam3_text",
                    source_primary="sam3_text",
                    source_prompt=prompt,
                )
                detections.extend(dets)

    return {
        "detections": detections,
        "warnings": warnings,
        "sam3_text_prompts": sam3_text_prompts,
        "sam3_text_term_map": sam3_text_term_map,
        "image_size": {"width": img_w, "height": img_h},
    }


def _agent_run_deep_prepass(
    payload: QwenPrepassRequest,
    *,
    pil_img: Image.Image,
    image_token: str,
    labelmap: List[str],
    glossary: str,
    trace_writer: Optional[Callable[[Dict[str, Any]], None]] = None,
    trace_full_writer: Optional[Callable[[Dict[str, Any]], None]] = None,
    trace_readable: Optional[Callable[[str], None]] = None,
) -> Dict[str, Any]:
    warnings: List[str] = []
    part_a = _agent_run_deep_prepass_part_a(
        payload,
        pil_img=pil_img,
        image_token=image_token,
        labelmap=labelmap,
        glossary=glossary,
        trace_writer=trace_writer,
        trace_full_writer=trace_full_writer,
        trace_readable=trace_readable,
    )
    detections = list(part_a.get("detections") or [])
    warnings.extend(list(part_a.get("warnings") or []))
    if trace_readable:
        trace_readable(f"deep prepass A: detections={len(detections)}")
    cleanup_a = _agent_deep_prepass_cleanup(
        payload,
        detections=detections,
        pil_img=pil_img,
        labelmap=labelmap,
    )
    detections = list(cleanup_a.get("detections") or [])
    removed = int(cleanup_a.get("removed") or 0)
    scoreless_removed = int(cleanup_a.get("scoreless_removed") or 0)
    rejected = int(cleanup_a.get("rejected") or 0)
    if trace_readable:
        trace_readable(
            "deep prepass A cleanup: "
            f"cleaned={len(detections)} removed={removed} "
            f"scoreless_removed={scoreless_removed} rejected={rejected}"
        )
    added_similarity = 0
    if payload.enable_sam3_similarity is not False and detections:
        exemplars_by_label = _agent_select_similarity_exemplars(
            payload,
            detections=detections,
            trace_readable=trace_readable,
        )
        similarity_detections: List[Dict[str, Any]] = []
        if exemplars_by_label:
            global_result = _agent_run_similarity_global(
                payload,
                pil_img=pil_img,
                image_token=image_token,
                exemplars_by_label=exemplars_by_label,
                trace_writer=trace_writer,
                trace_full_writer=trace_full_writer,
                trace_readable=trace_readable,
            )
            similarity_detections.extend(list(global_result.get("detections") or []))
            warnings.extend(list(global_result.get("warnings") or []))
            if payload.similarity_window_extension:
                window_result = _agent_run_similarity_expansion(
                    payload,
                    pil_img=pil_img,
                    image_token=image_token,
                    exemplars_by_label=exemplars_by_label,
                    trace_writer=trace_writer,
                    trace_full_writer=trace_full_writer,
                    trace_readable=trace_readable,
                )
                similarity_detections.extend(list(window_result.get("detections") or []))
                warnings.extend(list(window_result.get("warnings") or []))
        added_similarity = len(similarity_detections)
        if similarity_detections:
            detections = detections + similarity_detections
            cleanup_b = _agent_deep_prepass_cleanup(
                payload,
                detections=detections,
                pil_img=pil_img,
                labelmap=labelmap,
            )
            detections = list(cleanup_b.get("detections") or [])
            removed_b = int(cleanup_b.get("removed") or 0)
            scoreless_removed_b = int(cleanup_b.get("scoreless_removed") or 0)
            rejected_b = int(cleanup_b.get("rejected") or 0)
            if trace_readable:
                trace_readable(
                    "deep prepass B: "
                    f"added_similarity={added_similarity} cleaned={len(detections)} "
                    f"removed={removed_b} scoreless_removed={scoreless_removed_b} rejected={rejected_b}"
                )
    _agent_finalize_provenance(detections)
    return {
        "detections": detections,
        "warnings": warnings,
        "sam3_text_prompts": part_a.get("sam3_text_prompts") or {},
        "sam3_text_term_map": part_a.get("sam3_text_term_map") or {},
        "image_size": part_a.get("image_size") or {},
    }


def _agent_run_deep_prepass_caption(
    payload: QwenPrepassRequest,
    *,
    pil_img: Image.Image,
    image_token: str,
    detections: List[Dict[str, Any]],
    model_id_override: Optional[str],
    glossary: Optional[str] = None,
    grid_for_log: Optional[Dict[str, Any]] = None,
    trace_writer: Optional[Callable[[Dict[str, Any]], None]] = None,
    trace_full_writer: Optional[Callable[[Dict[str, Any]], None]] = None,
    trace_readable: Optional[Callable[[str], None]] = None,
) -> Tuple[str, List[Dict[str, Any]]]:
    if not payload.prepass_caption:
        return "", []
    caption_hints: List[QwenCaptionHint] = []
    hint_items = list(detections or [])
    hint_items.sort(key=lambda det: float(det.get("score") or 0.0), reverse=True)
    for det in hint_items[: min(len(hint_items), 160)]:
        label = str(det.get("label") or det.get("class_name") or "").strip()
        if not label:
            continue
        bbox = det.get("bbox_xyxy_px")
        if not bbox and det.get("bbox_2d"):
            bbox = _qwen_bbox_to_xyxy(pil_img.width, pil_img.height, det.get("bbox_2d") or [])
        if not bbox or len(bbox) < 4:
            continue
        caption_hints.append(
            QwenCaptionHint(
                label=label,
                bbox=[float(v) for v in bbox[:4]],
                confidence=det.get("score"),
            )
        )
    det_hint_summary = _agent_label_counts_summary(hint_items, limit=10)
    prepass_prompt = (
        "Write a detailed, multi-sentence caption. Use detection hints as suggestions, "
        "but mention other visible objects. Preserve specific details you see (counts, actions, "
        "notable attributes). Do not mention labels, hints, or coordinates. "
        "Never output labelmap tags (e.g., light_vehicle); use natural words like car or van. "
        "Avoid any token with underscores."
    )
    if det_hint_summary and det_hint_summary != "none":
        prepass_prompt = f"{prepass_prompt} Detection hints: {det_hint_summary}."

    caption_profile = (payload.prepass_caption_profile or "light").strip().lower()
    if caption_profile not in {"light", "deep"}:
        caption_profile = "light"
    caption_variant = payload.prepass_caption_variant or payload.model_variant or "auto"
    caption_model_id = (payload.prepass_caption_model_id or model_id_override or "").strip() or None
    caption_max_tokens = int(payload.prepass_caption_max_tokens or (512 if caption_profile == "light" else 1024))
    caption_mode = "windowed"
    caption_all_windows = True
    include_coords = False
    caption_payload = QwenCaptionRequest(
        image_token=image_token,
        user_prompt=prepass_prompt,
        label_hints=caption_hints or None,
        image_width=pil_img.width,
        image_height=pil_img.height,
        include_counts=True,
        include_coords=include_coords,
        max_boxes=min(len(caption_hints), 120),
        max_new_tokens=caption_max_tokens,
        model_variant=caption_variant,
        model_id=caption_model_id,
        final_answer_only=True,
        two_stage_refine=caption_mode == "windowed",
        caption_mode=caption_mode,
        caption_all_windows=caption_all_windows,
        restrict_to_labels=False,
        fast_mode=True,
        multi_model_cache=True,
        labelmap_glossary=glossary,
    )

    def _is_cuda_oom(exc: Exception) -> bool:
        msg = str(exc).lower()
        return "out of memory" in msg or ("cuda error" in msg and "memory" in msg)

    windowed_captions: List[Tuple[int, int, int, str]] = []

    def _window_hook(x0: int, y0: int, size: int, caption: str) -> None:
        windowed_captions.append((x0, y0, size, caption))
        if trace_readable:
            cell = None
            if grid_for_log:
                bbox_2d = _xyxy_to_qwen_bbox(
                    pil_img.width,
                    pil_img.height,
                    float(x0),
                    float(y0),
                    float(x0 + size),
                    float(y0 + size),
                )
                cell = _agent_grid_cell_for_window_bbox(grid_for_log, bbox_2d)
            cell_text = f" {cell}" if cell else ""
            trace_readable(
                f"prepass caption window{cell_text} "
                f"[{x0},{y0},{x0 + size},{y0 + size}]: {caption}"
            )
        if trace_writer:
            trace_writer(
                {
                    "type": "prepass_caption_window",
                    "window": [int(x0), int(y0), int(size)],
                    "grid_cell": _agent_grid_cell_for_window_bbox(
                        grid_for_log, _xyxy_to_qwen_bbox(pil_img.width, pil_img.height, float(x0), float(y0), float(x0 + size), float(y0 + size))
                    )
                    if grid_for_log
                    else None,
                    "caption": caption,
                    "ts": time.time(),
                }
            )
        if trace_full_writer:
            trace_full_writer(
                {
                    "type": "prepass_caption_window",
                    "window": [int(x0), int(y0), int(size)],
                    "grid_cell": _agent_grid_cell_for_window_bbox(
                        grid_for_log, _xyxy_to_qwen_bbox(pil_img.width, pil_img.height, float(x0), float(y0), float(x0 + size), float(y0 + size))
                    )
                    if grid_for_log
                    else None,
                    "caption": caption,
                    "ts": time.time(),
                }
            )

    def _run_caption_with_hook(request: QwenCaptionRequest) -> QwenCaptionResponse:
        token = None
        if caption_mode == "windowed":
            token = _CAPTION_WINDOW_HOOK.set(_window_hook)
        try:
            if trace_full_writer:
                trace_full_writer(
                    {
                        "type": "prepass_caption_call",
                        "payload": {
                            "model_id": request.model_id,
                            "variant": request.model_variant,
                            "caption_mode": request.caption_mode,
                            "caption_all_windows": request.caption_all_windows,
                            "restrict_to_labels": request.restrict_to_labels,
                            "max_new_tokens": request.max_new_tokens,
                            "hint_count": len(request.label_hints or []),
                        },
                        "ts": time.time(),
                    }
                )
            return qwen_caption(request)
        finally:
            if token is not None:
                _CAPTION_WINDOW_HOOK.reset(token)

    try:
        response = _run_caption_with_hook(caption_payload)
    except Exception as exc:  # noqa: BLE001
        if _is_cuda_oom(exc):
            try:
                _unload_non_qwen_runtimes()
                response = _run_caption_with_hook(caption_payload)
            except Exception as retry_exc:  # noqa: BLE001
                detail = str(retry_exc)
                raise HTTPException(
                    status_code=HTTP_503_SERVICE_UNAVAILABLE,
                    detail=f"prepass_caption_failed:{detail}",
                ) from retry_exc
        else:
            detail = str(exc)
            raise HTTPException(
                status_code=HTTP_503_SERVICE_UNAVAILABLE,
                detail=f"prepass_caption_failed:{detail}",
            ) from exc
    caption_text = _sanitize_qwen_caption(response.caption or "")
    if not caption_text:
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail="prepass_caption_failed:empty")
    if trace_readable:
        trace_readable(f"prepass caption summary: {caption_text}")
    caption_entries: List[Dict[str, Any]] = []
    if windowed_captions:
        for x0, y0, size, caption in windowed_captions:
            bbox_2d = _xyxy_to_qwen_bbox(
                pil_img.width,
                pil_img.height,
                float(x0),
                float(y0),
                float(x0 + size),
                float(y0 + size),
            )
            window_name = None
            if grid_for_log:
                window_name = _agent_grid_cell_for_window_bbox(grid_for_log, bbox_2d)
            if not window_name:
                window_name = f"window_{int(x0)}_{int(y0)}"
            caption_entries.append(
                {
                    "window": window_name,
                    "bbox_2d": list(bbox_2d),
                    "caption": caption,
                }
            )
            if trace_full_writer:
                trace_full_writer(
                    {
                        "type": "prepass_caption_result",
                        "window": {"name": window_name, "bbox_2d": list(bbox_2d)},
                        "caption": caption,
                        "ts": time.time(),
                    }
                )
            if trace_readable:
                coords = _agent_readable_format_bbox(bbox_2d)
                trace_readable(f"prepass caption {window_name} {coords}: {caption}")
    return caption_text, caption_entries


def _agent_deep_prepass_cleanup(
    payload: QwenPrepassRequest,
    *,
    detections: List[Dict[str, Any]],
    pil_img: Image.Image,
    labelmap: List[str],
) -> Dict[str, Any]:
    img_w, img_h = pil_img.size
    iou_thr = float(payload.iou or PREPASS_CLUSTER_IOU)
    merged, removed = _agent_merge_prepass_detections(detections, iou_thr=iou_thr)
    scoreless_iou = payload.scoreless_iou or 0.0
    if scoreless_iou:
        merged, scoreless_removed = _agent_filter_scoreless_detections(merged, iou_thr=float(scoreless_iou))
    else:
        scoreless_removed = 0
    head: Optional[Dict[str, Any]] = None
    if not payload.prepass_keep_all:
        classifier_id = payload.classifier_id
        if classifier_id:
            classifier_path = _resolve_agent_clip_classifier_path(classifier_id)
            if classifier_path is not None:
                head = _load_clip_head_from_classifier(classifier_path)
        elif isinstance(active_classifier_head, dict):
            head = dict(active_classifier_head)
    background = _agent_background_classes_from_head(head)
    cleaned, rejected = _agent_sanitize_detection_items(
        merged,
        pil_img=pil_img,
        classifier_head=head,
        img_w=img_w,
        img_h=img_h,
        labelmap=labelmap,
        background=background,
    )
    return {
        "detections": cleaned,
        "removed": removed,
        "scoreless_removed": scoreless_removed,
        "rejected": rejected,
    }


def _agent_select_similarity_exemplars(
    payload: QwenPrepassRequest,
    *,
    detections: List[Dict[str, Any]],
    trace_readable: Optional[Callable[[str], None]] = None,
) -> Dict[str, List[Dict[str, Any]]]:
    min_score = float(payload.similarity_min_exemplar_score or 0.6)
    by_label: Dict[str, List[Dict[str, Any]]] = {}
    for det in detections:
        if not isinstance(det, dict):
            continue
        label = str(det.get("label") or det.get("class_name") or "").strip()
        if not label:
            continue
        by_label.setdefault(label, []).append(det)
    selections: Dict[str, List[Dict[str, Any]]] = {}
    for label, dets in by_label.items():
        dets_sorted = sorted(dets, key=lambda d: float(d.get("score") or 0.0), reverse=True)
        high = [d for d in dets_sorted if float(d.get("score") or 0.0) >= min_score]
        chosen: List[Dict[str, Any]] = []
        if high:
            chosen.extend(high[:3])
        if chosen:
            selections[label] = chosen
            if trace_readable:
                handles = []
                for det in chosen:
                    handle = det.get("handle")
                    if handle:
                        handles.append(str(handle))
                handle_text = ", ".join(handles) if handles else "n/a"
                trace_readable(
                    f"deep_prepass similarity exemplars label={label} count={len(chosen)} handles={handle_text}"
                )
    return selections


def _agent_similarity_windows(
    payload: QwenPrepassRequest,
    *,
    pil_img: Image.Image,
) -> List[Dict[str, Any]]:
    img_w, img_h = pil_img.size
    mode = (payload.similarity_window_mode or "grid").strip().lower()
    windows: List[Dict[str, Any]] = []
    if mode == "sahi":
        slice_size = int(payload.similarity_window_size or payload.sahi_window_size or 640)
        overlap = float(payload.similarity_window_overlap or payload.sahi_overlap_ratio or 0.2)
        slices, starts = _slice_image_sahi(pil_img, slice_size, overlap)
        for idx, start in enumerate(starts):
            x1 = float(start[0])
            y1 = float(start[1])
            x2 = min(float(img_w), x1 + slice_size)
            y2 = min(float(img_h), y1 + slice_size)
            windows.append(
                {
                    "name": f"sahi_{idx}",
                    "bbox_xyxy_px": [x1, y1, x2, y2],
                    "bbox_2d": list(_xyxy_to_qwen_bbox(img_w, img_h, x1, y1, x2, y2)),
                }
            )
        return windows
    grid_spec = _agent_grid_spec_for_payload(payload, img_w, img_h)
    for cell in _agent_grid_cells(grid_spec):
        xyxy = _agent_grid_cell_xyxy(grid_spec, cell, overlap_ratio=payload.grid_overlap_ratio or PREPASS_GRID_OVERLAP_RATIO)
        if not xyxy:
            continue
        x1, y1, x2, y2 = xyxy
        windows.append(
            {
                "name": cell,
                "grid_cell": cell,
                "bbox_xyxy_px": [x1, y1, x2, y2],
                "bbox_2d": list(_xyxy_to_qwen_bbox(img_w, img_h, x1, y1, x2, y2)),
            }
        )
    return windows


def _agent_sam3_text_windows(
    payload: QwenPrepassRequest,
    *,
    pil_img: Image.Image,
) -> List[Dict[str, Any]]:
    img_w, img_h = pil_img.size
    mode = (payload.sam3_text_window_mode or "grid").strip().lower()
    windows: List[Dict[str, Any]] = []
    if mode == "sahi":
        slice_size = int(payload.sam3_text_window_size or payload.sahi_window_size or 640)
        overlap = float(payload.sam3_text_window_overlap or payload.sahi_overlap_ratio or 0.2)
        slices, starts = _slice_image_sahi(pil_img, slice_size, overlap)
        for idx, start in enumerate(starts):
            x1 = float(start[0])
            y1 = float(start[1])
            x2 = min(float(img_w), x1 + slice_size)
            y2 = min(float(img_h), y1 + slice_size)
            windows.append(
                {
                    "name": f"sahi_{idx}",
                    "bbox_xyxy_px": [x1, y1, x2, y2],
                    "bbox_2d": list(_xyxy_to_qwen_bbox(img_w, img_h, x1, y1, x2, y2)),
                }
            )
        return windows
    grid_spec = _agent_grid_spec_for_payload(payload, img_w, img_h)
    for cell in _agent_grid_cells(grid_spec):
        xyxy = _agent_grid_cell_xyxy(
            grid_spec,
            cell,
            overlap_ratio=payload.grid_overlap_ratio or PREPASS_GRID_OVERLAP_RATIO,
        )
        if not xyxy:
            continue
        x1, y1, x2, y2 = xyxy
        windows.append(
            {
                "name": cell,
                "grid_cell": cell,
                "bbox_xyxy_px": [x1, y1, x2, y2],
                "bbox_2d": list(_xyxy_to_qwen_bbox(img_w, img_h, x1, y1, x2, y2)),
            }
        )
    return windows


def _agent_exemplars_for_window(
    exemplars: Sequence[Dict[str, Any]],
    *,
    img_w: int,
    img_h: int,
    window_xyxy: Sequence[float],
) -> List[Dict[str, Any]]:
    if not exemplars:
        return []
    if not window_xyxy or len(window_xyxy) < 4:
        return list(exemplars)
    wx1, wy1, wx2, wy2 = window_xyxy
    filtered: List[Dict[str, Any]] = []
    for det in exemplars:
        if not isinstance(det, dict):
            continue
        xyxy = _resolve_agent_bbox_xyxy(det, img_w, img_h)
        if xyxy is None:
            continue
        x1, y1, x2, y2 = xyxy
        cx = (x1 + x2) / 2.0
        cy = (y1 + y2) / 2.0
        if wx1 <= cx <= wx2 and wy1 <= cy <= wy2:
            filtered.append(det)
    return filtered


def _agent_run_similarity_global(
    payload: QwenPrepassRequest,
    *,
    pil_img: Image.Image,
    image_token: str,
    exemplars_by_label: Dict[str, List[Dict[str, Any]]],
    trace_writer: Optional[Callable[[Dict[str, Any]], None]] = None,
    trace_full_writer: Optional[Callable[[Dict[str, Any]], None]] = None,
    trace_readable: Optional[Callable[[str], None]] = None,
) -> Dict[str, Any]:
    detections: List[Dict[str, Any]] = []
    warnings: List[str] = []
    score_thr = payload.prepass_similarity_score
    mask_thr = payload.sam3_mask_threshold

    def _log_step(event: str, payload_obj: Dict[str, Any]) -> None:
        if trace_writer:
            trace_writer({"type": event, **payload_obj, "ts": time.time()})
        if trace_full_writer:
            trace_full_writer({"type": event, **payload_obj, "ts": time.time()})

    for label, exemplars in exemplars_by_label.items():
        exemplar_boxes = []
        exemplar_handles = []
        for det in exemplars:
            bbox = det.get("bbox_2d")
            if isinstance(bbox, (list, tuple)) and len(bbox) >= 4:
                exemplar_boxes.append({"bbox_2d": list(bbox[:4]), "bbox_space": "full"})
            handle = det.get("handle")
            if handle:
                exemplar_handles.append(str(handle))
        if not exemplar_boxes:
            continue
        args = {
            "image_token": image_token,
            "label": label,
            "exemplar_boxes": exemplar_boxes,
            "score_thr": score_thr,
            "mask_threshold": mask_thr,
        }
        _log_step("deep_prepass_tool_call", {"tool": "sam3_similarity", "args": args})
        try:
            result = _agent_tool_sam3_similarity(
                image_token=image_token,
                exemplar_boxes=exemplar_boxes,
                label=label,
                score_thr=score_thr,
                mask_threshold=mask_thr,
                register=False,
            )
        except Exception as exc:  # noqa: BLE001
            warnings.append(f"deep_prepass_similarity_failed:{label}:full:{exc}")
            continue
        _log_step("deep_prepass_tool_result", {"tool": "sam3_similarity", "result": result})
        dets = list(result.get("detections") or [])
        if trace_readable:
            trace_readable(
                f"deep_prepass sam3_similarity label={label} window=full detections={len(dets)}"
            )
        _agent_attach_provenance(
            dets,
            source="sam3_similarity",
            source_primary="sam3_similarity",
            source_exemplar_handles=exemplar_handles or None,
        )
        detections.extend(dets)
    return {"detections": detections, "warnings": warnings}


def _agent_run_similarity_expansion(
    payload: QwenPrepassRequest,
    *,
    pil_img: Image.Image,
    image_token: str,
    exemplars_by_label: Dict[str, List[Dict[str, Any]]],
    trace_writer: Optional[Callable[[Dict[str, Any]], None]] = None,
    trace_full_writer: Optional[Callable[[Dict[str, Any]], None]] = None,
    trace_readable: Optional[Callable[[str], None]] = None,
) -> Dict[str, Any]:
    img_w, img_h = pil_img.size
    detections: List[Dict[str, Any]] = []
    warnings: List[str] = []
    windows = _agent_similarity_windows(payload, pil_img=pil_img)
    score_thr = payload.prepass_similarity_score
    mask_thr = payload.sam3_mask_threshold

    def _log_step(event: str, payload_obj: Dict[str, Any]) -> None:
        if trace_writer:
            trace_writer({"type": event, **payload_obj, "ts": time.time()})
        if trace_full_writer:
            trace_full_writer({"type": event, **payload_obj, "ts": time.time()})

    for label, exemplars in exemplars_by_label.items():
        for window in windows:
            window_xyxy = window.get("bbox_xyxy_px") or []
            window_bbox_2d = window.get("bbox_2d")
            window_exemplars = _agent_exemplars_for_window(
                exemplars, img_w=img_w, img_h=img_h, window_xyxy=window_xyxy
            )
            if not window_exemplars:
                continue
            exemplar_boxes = []
            exemplar_handles = []
            for det in window_exemplars:
                bbox = det.get("bbox_2d")
                if isinstance(bbox, (list, tuple)) and len(bbox) >= 4:
                    exemplar_boxes.append({"bbox_2d": list(bbox[:4]), "bbox_space": "full"})
                handle = det.get("handle")
                if handle:
                    exemplar_handles.append(str(handle))
            if not exemplar_boxes:
                continue
            args = {
                "image_token": image_token,
                "label": label,
                "exemplar_boxes": exemplar_boxes,
                "score_thr": score_thr,
                "mask_threshold": mask_thr,
                "window_bbox_2d": window_bbox_2d,
            }
            if window.get("grid_cell"):
                args["grid_cell"] = window.get("grid_cell")
            _log_step("deep_prepass_tool_call", {"tool": "sam3_similarity", "args": args})
            try:
                result = _agent_tool_sam3_similarity(
                    image_token=image_token,
                    exemplar_boxes=exemplar_boxes,
                    label=label,
                    score_thr=score_thr,
                    mask_threshold=mask_thr,
                    window_bbox_2d=window_bbox_2d,
                    grid_cell=window.get("grid_cell"),
                    register=False,
                )
            except Exception as exc:  # noqa: BLE001
                warnings.append(f"deep_prepass_similarity_failed:{label}:{window.get('name')}:{exc}")
                continue
            _log_step("deep_prepass_tool_result", {"tool": "sam3_similarity", "result": result})
            dets = list(result.get("detections") or [])
            if trace_readable:
                window_name = window.get("name") or window.get("grid_cell") or "window"
                trace_readable(
                    f"deep_prepass sam3_similarity label={label} window={window_name} detections={len(dets)}"
                )
            _agent_attach_provenance(
                dets,
                source="sam3_similarity",
                source_primary="sam3_similarity",
                source_exemplar_handles=exemplar_handles or None,
            )
            detections.extend(dets)
    return {"detections": detections, "warnings": warnings}


def _agent_finalize_provenance(detections: Sequence[Dict[str, Any]]) -> None:
    for det in detections:
        if not isinstance(det, dict):
            continue
        primary = det.get("source_primary")
        if not primary:
            primary = det.get("source") or det.get("score_source") or "unknown"
            det["source_primary"] = primary
        sources = det.get("source_list")
        if not isinstance(sources, list):
            sources = []
        if primary and primary not in sources:
            sources.append(primary)
        det["source_list"] = sources


def _run_prepass_annotation_qwen(
    payload: QwenPrepassRequest,
    *,
    trace_sink: Optional[Any] = None,
    cancel_event: Optional[threading.Event] = None,
) -> QwenPrepassResponse:
    global QWEN_CAPTION_CACHE_LIMIT
    # Prepass-only mode is enforced; no agentic review loop is executed.
    payload = payload.copy(update={"prepass_only": True})
    _require_sam3_for_prepass(bool(payload.enable_sam3_text), bool(payload.enable_sam3_similarity))
    if int(QWEN_CAPTION_CACHE_LIMIT or 0) < 1:
        QWEN_CAPTION_CACHE_LIMIT = 1
    pil_img, _, token = resolve_image_payload(payload.image_base64, payload.image_token, None)
    trace_path: Optional[str] = None
    full_trace_path: Optional[str] = None
    readable_trace_path: Optional[str] = None
    latest_readable_path: Optional[str] = None
    trace_file = QWEN_PREPASS_TRACE_ROOT / f"prepass_{int(time.time())}_{uuid.uuid4().hex[:8]}.jsonl"
    full_trace_file = QWEN_PREPASS_FULL_TRACE_ROOT / f"prepass_full_{int(time.time())}_{uuid.uuid4().hex[:8]}.jsonl"
    readable_trace_file = QWEN_PREPASS_READABLE_TRACE_ROOT / f"prepass_readable_{int(time.time())}_{uuid.uuid4().hex[:8]}.log"
    try:
        trace_file.parent.mkdir(parents=True, exist_ok=True)
        trace_path = str(trace_file)
    except Exception:
        trace_path = None
    try:
        full_trace_file.parent.mkdir(parents=True, exist_ok=True)
        full_trace_path = str(full_trace_file)
    except Exception:
        full_trace_path = None
    try:
        readable_trace_file.parent.mkdir(parents=True, exist_ok=True)
        readable_trace_path = str(readable_trace_file)
    except Exception:
        readable_trace_path = None
    latest_full_path: Optional[str] = None
    try:
        latest_full_file = QWEN_PREPASS_FULL_TRACE_LATEST
        latest_full_file.parent.mkdir(parents=True, exist_ok=True)
        if not latest_full_file.exists():
            latest_full_file.write_text("", encoding="utf-8")
        latest_full_path = str(latest_full_file)
    except Exception:
        latest_full_path = None
    try:
        latest_readable_file = QWEN_PREPASS_READABLE_TRACE_LATEST
        latest_readable_file.parent.mkdir(parents=True, exist_ok=True)
        if not latest_readable_file.exists():
            latest_readable_file.write_text("", encoding="utf-8")
        latest_readable_path = str(latest_readable_file)
    except Exception:
        latest_readable_path = None

    def _trace_write(record: Dict[str, Any]) -> None:
        if not trace_path:
            return
        record["ts"] = record.get("ts") or time.time()
        try:
            with open(trace_path, "a", encoding="utf-8") as handle:
                handle.write(json.dumps(record, ensure_ascii=True) + "\n")
        except Exception:
            pass
    def _trace_write_full(record: Dict[str, Any]) -> None:
        if not full_trace_path and not latest_full_path:
            return
        record["ts"] = record.get("ts") or time.time()
        try:
            payload = _agent_trace_full_jsonable(record)
        except Exception:
            payload = {"error": "trace_encode_failed", "record": str(record)}
        if full_trace_path:
            try:
                with open(full_trace_path, "a", encoding="utf-8") as handle:
                    handle.write(json.dumps(payload, ensure_ascii=False) + "\n")
            except Exception:
                pass
        if latest_full_path and latest_full_path != full_trace_path:
            try:
                with open(latest_full_path, "a", encoding="utf-8") as handle:
                    handle.write(json.dumps(payload, ensure_ascii=False) + "\n")
            except Exception:
                pass
    def _trace_write_readable(line: str) -> None:
        if not line or (not readable_trace_path and not latest_readable_path):
            return
        stamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
        text = f"{stamp} {line}\n"
        if readable_trace_path:
            try:
                with open(readable_trace_path, "a", encoding="utf-8") as handle:
                    handle.write(text)
            except Exception:
                pass
        if latest_readable_path and latest_readable_path != readable_trace_path:
            try:
                with open(latest_readable_path, "a", encoding="utf-8") as handle:
                    handle.write(text)
            except Exception:
                pass
        if PREPASS_READABLE_TO_CONSOLE:
            logging.getLogger("prepass.readable").info(line)
    global _AGENT_ACTIVE_IMAGE_TOKEN, _AGENT_ACTIVE_IMAGE_BASE64, _AGENT_ACTIVE_DATASET_ID
    global _AGENT_ACTIVE_LABELMAP, _AGENT_ACTIVE_GLOSSARY, _AGENT_ACTIVE_INSPECTED_WINDOWS
    global _AGENT_ACTIVE_CLASSIFIER_ID, _AGENT_ACTIVE_TIGHTEN_FP
    global _AGENT_ACTIVE_DETECTOR_CONF, _AGENT_ACTIVE_SAM3_SCORE_THR, _AGENT_ACTIVE_SAM3_MASK_THR
    global _AGENT_ACTIVE_CLASSIFIER_MIN_PROB, _AGENT_ACTIVE_CLASSIFIER_MARGIN
    global _AGENT_ACTIVE_CLASSIFIER_BG_MARGIN, _AGENT_ACTIVE_SCORELESS_IOU
    global _AGENT_ACTIVE_DETECTIONS, _AGENT_ACTIVE_CLUSTERS, _AGENT_ACTIVE_GRID
    global _AGENT_ACTIVE_GRID_IMAGE, _AGENT_ACTIVE_OVERLAY_IMAGE
    global _AGENT_LAST_SUBMIT_DETECTIONS, _AGENT_PENDING_CLASSIFY_IDS
    global _AGENT_TRACE_FULL_WRITER, _AGENT_TRACE_READABLE_WRITER, _AGENT_PREPASS_COMPLETE
    def _agent_unit_float(value: Optional[float], fallback: Optional[float]) -> Optional[float]:
        if value is None:
            return fallback
        try:
            return max(0.0, min(1.0, float(value)))
        except (TypeError, ValueError):
            return fallback

    tighten_fp = bool(payload.tighten_fp if payload.tighten_fp is not None else True)
    detector_conf = _agent_unit_float(
        payload.detector_conf,
        PREPASS_TIGHT_DEFAULT_DETECTOR_CONF,
    )
    sam3_score_thr = _agent_unit_float(
        payload.sam3_score_thr,
        PREPASS_TIGHT_DEFAULT_SAM3_SCORE,
    )
    sam3_mask_thr = _agent_unit_float(
        payload.sam3_mask_threshold,
        PREPASS_TIGHT_DEFAULT_SAM3_MASK,
    )
    classifier_min_prob = _agent_unit_float(
        payload.classifier_min_prob,
        PREPASS_TIGHT_DEFAULT_CLASSIFIER_MIN_PROB,
    )
    classifier_margin = _agent_unit_float(
        payload.classifier_margin,
        PREPASS_TIGHT_DEFAULT_CLASSIFIER_MARGIN,
    )
    classifier_bg_margin = _agent_unit_float(
        payload.classifier_bg_margin,
        PREPASS_TIGHT_DEFAULT_CLASSIFIER_BG_MARGIN,
    )
    scoreless_iou = _agent_unit_float(
        payload.scoreless_iou,
        PREPASS_TIGHT_DEFAULT_SCORELESS_IOU if tighten_fp else 0.0,
    )

    _AGENT_ACTIVE_IMAGE_TOKEN = token
    _AGENT_ACTIVE_IMAGE_BASE64 = payload.image_base64
    _AGENT_ACTIVE_DATASET_ID = payload.dataset_id
    _AGENT_TRACE_FULL_WRITER = _trace_write_full if full_trace_path else None
    _AGENT_TRACE_READABLE_WRITER = _trace_write_readable if readable_trace_path or latest_readable_path else None
    _AGENT_ACTIVE_TIGHTEN_FP = tighten_fp
    _AGENT_ACTIVE_DETECTOR_CONF = detector_conf
    _AGENT_ACTIVE_SAM3_SCORE_THR = sam3_score_thr
    _AGENT_ACTIVE_SAM3_MASK_THR = sam3_mask_thr
    _AGENT_ACTIVE_CLASSIFIER_MIN_PROB = classifier_min_prob if tighten_fp else None
    _AGENT_ACTIVE_CLASSIFIER_MARGIN = classifier_margin if tighten_fp else None
    _AGENT_ACTIVE_CLASSIFIER_BG_MARGIN = classifier_bg_margin if tighten_fp else None
    _AGENT_ACTIVE_SCORELESS_IOU = scoreless_iou if tighten_fp else 0.0
    img_w, img_h = pil_img.size
    labelmap: List[str] = []
    glossary = ""
    if payload.labelmap:
        labelmap = [str(x).strip() for x in payload.labelmap if str(x).strip()]
        if labelmap:
            glossary = _default_agent_glossary_for_labelmap(labelmap)
    if not labelmap:
        labelmap, glossary = _agent_load_labelmap_meta(payload.dataset_id)
    labelmap = labelmap or []
    warnings: List[str] = []
    if not labelmap:
        warnings.append("labelmap_missing")
    if payload.labelmap_glossary is not None:
        glossary = _normalize_labelmap_glossary(payload.labelmap_glossary)
    _AGENT_ACTIVE_LABELMAP = labelmap
    _AGENT_ACTIVE_GLOSSARY = glossary
    _AGENT_ACTIVE_INSPECTED_WINDOWS = set()
    _agent_reset_registries()
    classifier_id_for_run = payload.classifier_id
    if not classifier_id_for_run and not isinstance(active_classifier_head, dict):
        classifier_id_for_run = _agent_default_classifier_for_dataset(payload.dataset_id)
    _AGENT_ACTIVE_CLASSIFIER_ID = classifier_id_for_run
    head: Optional[Dict[str, Any]] = None
    if classifier_id_for_run:
        classifier_path = _resolve_agent_clip_classifier_path(classifier_id_for_run)
        if classifier_path is not None:
            head = _load_clip_head_from_classifier(classifier_path)
    elif isinstance(active_classifier_head, dict):
        head = active_classifier_head
    if isinstance(head, dict):
        head = dict(head)
        if _AGENT_ACTIVE_TIGHTEN_FP:
            min_prob = float(head.get("min_prob") or 0.5)
            margin = float(head.get("margin") or 0.0)
            bg_margin = float(head.get("background_margin") or 0.0)
            if _AGENT_ACTIVE_CLASSIFIER_MIN_PROB is not None:
                min_prob = max(min_prob, _AGENT_ACTIVE_CLASSIFIER_MIN_PROB)
            if _AGENT_ACTIVE_CLASSIFIER_MARGIN is not None:
                margin = max(margin, _AGENT_ACTIVE_CLASSIFIER_MARGIN)
            if _AGENT_ACTIVE_CLASSIFIER_BG_MARGIN is not None:
                bg_margin = max(bg_margin, _AGENT_ACTIVE_CLASSIFIER_BG_MARGIN)
            head["min_prob"] = min_prob
            head["margin"] = margin
            head["background_margin"] = bg_margin
            _trace_write(
                {
                    "type": "precision_profile",
                    "tighten_fp": True,
                    "detector_conf": _AGENT_ACTIVE_DETECTOR_CONF,
                    "sam3_score_thr": _AGENT_ACTIVE_SAM3_SCORE_THR,
                    "sam3_mask_thr": _AGENT_ACTIVE_SAM3_MASK_THR,
                    "classifier_min_prob": head["min_prob"],
                    "classifier_margin": head["margin"],
                    "classifier_bg_margin": head["background_margin"],
                    "scoreless_iou": _AGENT_ACTIVE_SCORELESS_IOU,
                }
            )
            _trace_write_full(
                {
                    "type": "precision_profile",
                    "tighten_fp": True,
                    "detector_conf": _AGENT_ACTIVE_DETECTOR_CONF,
                    "sam3_score_thr": _AGENT_ACTIVE_SAM3_SCORE_THR,
                    "sam3_mask_thr": _AGENT_ACTIVE_SAM3_MASK_THR,
                    "classifier_min_prob": head["min_prob"],
                    "classifier_margin": head["margin"],
                    "classifier_bg_margin": head["background_margin"],
                    "scoreless_iou": _AGENT_ACTIVE_SCORELESS_IOU,
                }
            )
            _trace_write_readable(
                "precision_profile: "
                f"detector_conf={_AGENT_ACTIVE_DETECTOR_CONF or 0:.2f} "
                f"sam3_score_thr={_AGENT_ACTIVE_SAM3_SCORE_THR or 0:.2f} "
                f"sam3_mask_thr={_AGENT_ACTIVE_SAM3_MASK_THR or 0:.2f} "
                f"classifier_min_prob={head['min_prob']:.2f} "
                f"margin={head['margin']:.2f} "
                f"bg_margin={head['background_margin']:.2f} "
                f"scoreless_iou={(_AGENT_ACTIVE_SCORELESS_IOU or 0):.2f}"
            )
    background = _agent_background_classes_from_head(head)
    _trace_write({"type": "start", "payload": _agent_trace_sanitize_payload(payload, token)})
    try:
        _trace_write_full({"type": "start", "payload": payload.dict()})
    except Exception:
        _trace_write_full({"type": "start", "payload": str(payload)})
    readable_model_id = payload.model_id or (active_qwen_metadata or {}).get("model_id") or QWEN_MODEL_NAME
    start_title = f"IMAGE START {payload.image_name or 'unknown'}"
    _trace_write_readable(_agent_readable_banner(start_title, fill="="))
    _trace_write_readable(
        "start: "
        f"dataset_id={payload.dataset_id or 'none'} "
        f"image={payload.image_name or 'unknown'} "
        f"model={readable_model_id} "
        f"variant={payload.model_variant or 'auto'}"
    )
    base_model_id = (active_qwen_metadata or {}).get("model_id") or QWEN_MODEL_NAME
    desired_variant = (payload.model_variant or "auto").strip()
    if payload.model_id:
        model_id_override = payload.model_id
    elif desired_variant in {"Instruct", "Thinking"}:
        model_id_override = _resolve_qwen_variant_model_id(base_model_id, desired_variant)
    else:
        model_id_override = base_model_id
    grid_spec: Optional[Dict[str, Any]] = _agent_grid_spec_for_payload(payload, img_w, img_h)
    _AGENT_ACTIVE_GRID = grid_spec
    trace: List[AgentTraceEvent] = []
    step_id = 0

    def _append_trace(phase: str, summary: str, counts: Optional[Dict[str, int]] = None) -> None:
        nonlocal step_id
        step_id += 1
        event = AgentTraceEvent(
            step_id=step_id,
            phase=phase,
            summary=summary,
            counts=counts,
            timestamp=time.time(),
        )
        trace.append(event)
        if trace_sink:
            trace_sink(event)

    _trace_write_readable(_agent_readable_banner("DEEP PREPASS START", fill="-"))
    deep_prepass = _agent_run_deep_prepass(
        payload,
        pil_img=pil_img,
        image_token=token,
        labelmap=labelmap,
        glossary=glossary,
        trace_writer=_trace_write,
        trace_full_writer=_trace_write_full,
        trace_readable=_trace_write_readable,
    )
    _trace_write_readable(_agent_readable_banner("DEEP PREPASS END", fill="-"))
    deep_detections = list(deep_prepass.get("detections") or [])
    deep_warnings = list(deep_prepass.get("warnings") or [])
    if deep_warnings:
        warnings.extend(deep_warnings)
    if payload.ensemble_enabled and payload.ensemble_job_id:
        deep_detections = _agent_apply_ensemble_filter(
            deep_detections,
            dataset_id=payload.dataset_id,
            image_name=payload.image_name,
            classifier_id=classifier_id_for_run,
            job_id=payload.ensemble_job_id,
            trace_writer=_trace_write,
            trace_full_writer=_trace_write_full,
            trace_readable=_trace_write_readable,
            warnings=warnings,
        )
    _append_trace("merge", "deep_prepass_complete", counts={"detections": len(deep_detections)})

    caption_text, caption_entries = _agent_run_deep_prepass_caption(
        payload,
        pil_img=pil_img,
        image_token=token,
        detections=deep_detections,
        model_id_override=model_id_override,
        glossary=glossary,
        grid_for_log=grid_spec,
        trace_writer=_trace_write,
        trace_full_writer=_trace_write_full,
        trace_readable=_trace_write_readable,
    )
    _AGENT_ACTIVE_OVERALL_CAPTION = caption_text
    _AGENT_ACTIVE_WINDOWED_CAPTIONS = caption_entries

    source_summary = _agent_format_source_counts(_agent_source_counts(deep_detections))
    _trace_write(
        {
            "type": "deep_prepass_summary",
            "detections": len(deep_detections),
            "source_summary": source_summary,
            "warnings": deep_warnings,
        }
    )
    _trace_write_full(
        {
            "type": "deep_prepass_summary",
            "detections": len(deep_detections),
            "source_summary": source_summary,
            "warnings": deep_warnings,
        }
    )

    detections: List[Dict[str, Any]] = []
    if deep_detections:
        _agent_register_detections(
            deep_detections,
            img_w=img_w,
            img_h=img_h,
            grid=grid_spec,
            labelmap=labelmap,
            background=background,
            source_override=None,
        )
        detections = list(_AGENT_ACTIVE_CLUSTERS or [])
    _agent_set_active_clusters(detections)
    _AGENT_PREPASS_COMPLETE = True

    overlay_image: Optional[Image.Image] = None
    overlay_radius: Optional[int] = None
    use_overlay = bool(payload.use_detection_overlay if payload.use_detection_overlay is not None else True)
    if payload.overlay_dot_radius is not None:
        try:
            overlay_radius = max(1, int(payload.overlay_dot_radius))
        except (TypeError, ValueError):
            overlay_radius = None
    grid_image = _agent_render_grid_overlay(pil_img, grid_spec) if grid_spec else pil_img
    _AGENT_ACTIVE_GRID_IMAGE = grid_image
    if use_overlay and labelmap:
        label_colors = _agent_current_label_colors(labelmap)
        label_prefixes = _agent_current_label_prefixes(labelmap)
        overlay_image = _agent_render_detection_overlay(
            grid_image,
            detections,
            label_colors,
            dot_radius=overlay_radius,
            label_prefixes=label_prefixes,
        )
        _AGENT_ACTIVE_LABEL_COLORS = label_colors
        _AGENT_ACTIVE_LABEL_PREFIXES = label_prefixes
        _AGENT_ACTIVE_OVERLAY_DOT_RADIUS = overlay_radius
        _trace_write(
            {
                "type": "overlay",
                "enabled": True,
                "detections": len(detections),
                "dot_radius": overlay_radius,
                "label_colors": label_colors,
                "grid": grid_spec,
            }
        )
        _trace_write_full(
            {
                "type": "overlay",
                "enabled": True,
                "detections": len(detections),
                "dot_radius": overlay_radius,
                "label_colors": label_colors,
                "grid": grid_spec,
            }
        )
        _trace_write_readable(
            f"deep prepass overlay: detections={len(detections)} "
            f"radius={overlay_radius if overlay_radius is not None else 'auto'}"
        )
    else:
        overlay_image = grid_image
        _trace_write(
            {
                "type": "overlay",
                "enabled": True,
                "detections": len(detections),
                "dot_radius": overlay_radius,
                "grid": grid_spec,
            }
        )
        _trace_write_full(
            {
                "type": "overlay",
                "enabled": True,
                "detections": len(detections),
                "dot_radius": overlay_radius,
                "grid": grid_spec,
            }
        )

    if labelmap:
        _AGENT_ACTIVE_LABEL_COLORS = _agent_current_label_colors(labelmap)
        _AGENT_ACTIVE_LABEL_PREFIXES = _agent_current_label_prefixes(labelmap)
    _AGENT_ACTIVE_OVERLAY_DOT_RADIUS = overlay_radius
    _AGENT_ACTIVE_OVERLAY_IMAGE = overlay_image
    _AGENT_ACTIVE_DETECTIONS = deep_detections

    if not payload.prepass_only:
        warnings.append("prepass_only_enforced")

    final_detections = list(_AGENT_ACTIVE_CLUSTERS or [])
    if payload.prepass_finalize:
        submit_result = _agent_tool_submit_annotations(
            image_token=token,
            dataset_id=payload.dataset_id,
            classifier_id=payload.classifier_id,
            include_all=True,
            iou=payload.iou,
            cross_iou=payload.cross_iou,
            max_det=None,
        )
        if isinstance(submit_result, dict) and submit_result.get("detections") is not None:
            final_detections = list(submit_result.get("detections") or [])
            rejected = submit_result.get("rejected")
            if rejected:
                warnings.append(f"prepass_finalize_rejected:{rejected}")
    summary_lines = _agent_detection_summary_lines(
        final_detections,
        grid=grid_spec,
        img_w=img_w,
        img_h=img_h,
        warnings=warnings,
    )
    _trace_write_readable(_agent_readable_banner("IMAGE END SUMMARY", fill="="))
    for line in summary_lines:
        _trace_write_readable(line)
    _trace_write_readable(_agent_readable_banner("END IMAGE", fill="="))
    _AGENT_TRACE_FULL_WRITER = None
    _AGENT_TRACE_READABLE_WRITER = None
    return QwenPrepassResponse(
        detections=final_detections,
        trace=trace,
        warnings=warnings or None,
        caption=None,
        trace_path=trace_path,
        trace_full_path=full_trace_path,
    )


def _run_prepass_annotation(
    payload: QwenPrepassRequest,
    *,
    trace_sink: Optional[Any] = None,
    cancel_event: Optional[threading.Event] = None,
) -> QwenPrepassResponse:
    return _run_prepass_annotation_qwen(payload, trace_sink=trace_sink, cancel_event=cancel_event)


class Sam3TextPromptResponse(BaseModel):
    detections: List[QwenDetection] = Field(default_factory=list)
    warnings: List[str] = Field(default_factory=list)
    image_token: Optional[str] = None
    # Optional masks aligned to detections (packed and base64-encoded to stay compact)
    masks: Optional[List[Dict[str, Any]]] = None


class Sam3TextPromptAutoResponse(BaseModel):
    detections: List[SamPointAutoResponse] = Field(default_factory=list)
    warnings: List[str] = Field(default_factory=list)
    image_token: Optional[str] = None


class QwenPromptSection(BaseModel):
    base_prompt: str
    default_image_type: str = "image"
    default_extra_context: str = ""

    @root_validator(skip_on_failure=True)
    def _validate_qwen_section(cls, values):  # noqa: N805
        template = values.get("base_prompt") or ""
        if "{items}" not in template:
            raise ValueError("base_prompt_missing_items_placeholder")
        if "{image_type}" not in template:
            raise ValueError("base_prompt_missing_image_type_placeholder")
        if "{extra_context}" not in template:
            raise ValueError("base_prompt_missing_extra_context_placeholder")
        return values


class QwenPromptConfig(BaseModel):
    bbox: QwenPromptSection
    point: QwenPromptSection


class QwenRuntimeSettings(BaseModel):
    trust_remote_code: bool = False


class QwenRuntimeSettingsUpdate(BaseModel):
    trust_remote_code: Optional[bool] = None


DEFAULT_QWEN_PROMPT_CONFIG = QwenPromptConfig(
    bbox=QwenPromptSection(
        base_prompt=(
            "Output a JSON formatted list of very tight bounding boxes with coordinates in format (x1,y1,x2,y2) "
            "of detections in this {image_type}. Make a single bounding box for each unique instance of the things we want to detect. "
            "The objects we want to detect are: {items}. {extra_context}"
        ),
        default_image_type="image",
        default_extra_context="Return only JSON, no additional text.",
    ),
    point=QwenPromptSection(
        base_prompt=(
            "Output a JSON formatted list of positive click points with coordinates in format (x,y) for detections in this {image_type}. "
            "Each entry must contain \"point_2d\": [x, y] centered on the object so Segment Anything can turn it into a mask/bbox. "
            "Make one point per object. The objects we want to detect are: {items}. {extra_context}"
        ),
        default_image_type="image",
        default_extra_context="Respond with JSON only.",
    ),
)

qwen_prompt_config = DEFAULT_QWEN_PROMPT_CONFIG.copy(deep=True)


class QwenTrainRequest(BaseModel):
    dataset_id: Optional[str] = None
    run_name: Optional[str] = None
    model_id: Optional[str] = None
    training_mode: Optional[Literal["official_lora", "trl_qlora"]] = None
    system_prompt: Optional[str] = None
    devices: Optional[str] = None
    batch_size: Optional[int] = None
    max_epochs: Optional[int] = None
    lr: Optional[float] = None
    accumulate_grad_batches: Optional[int] = None
    warmup_steps: Optional[int] = None
    num_workers: Optional[int] = None
    lora_rank: Optional[int] = None
    lora_alpha: Optional[int] = None
    lora_dropout: Optional[float] = None
    lora_target_modules: Optional[List[str]] = None
    log_every_n_steps: Optional[int] = None
    min_pixels: Optional[int] = None
    max_pixels: Optional[int] = None
    max_length: Optional[int] = None
    seed: Optional[int] = None
    random_split: Optional[bool] = None
    val_percent: Optional[float] = None
    split_seed: Optional[int] = None
    train_limit: Optional[int] = None
    val_limit: Optional[int] = None

    @root_validator(skip_on_failure=True)
    def _validate_dataset_fields(cls, values):  # noqa: N805
        if not values.get("dataset_id"):
            raise ValueError("dataset_id_required")
        return values


class Sam3TrainRequest(BaseModel):
    dataset_id: str
    run_name: Optional[str] = None
    experiment_log_dir: Optional[str] = None
    train_batch_size: Optional[int] = None
    val_batch_size: Optional[int] = None
    num_train_workers: Optional[int] = None
    num_val_workers: Optional[int] = None
    max_epochs: Optional[int] = None
    resolution: Optional[int] = None
    lr_scale: Optional[float] = None
    gradient_accumulation_steps: Optional[int] = None
    val_epoch_freq: Optional[int] = None
    target_epoch_size: Optional[int] = None
    scheduler_warmup: Optional[int] = None
    scheduler_timescale: Optional[int] = None
    num_gpus: Optional[int] = None
    enable_inst_interactivity: Optional[bool] = None
    balance_classes: Optional[bool] = None
    balance_strategy: Optional[str] = None
    balance_power: Optional[float] = None
    balance_clip: Optional[float] = None
    balance_beta: Optional[float] = None
    balance_gamma: Optional[float] = None
    train_limit: Optional[int] = None
    val_limit: Optional[int] = None
    log_freq: Optional[int] = None
    log_every_batch: Optional[bool] = None
    enable_segmentation_head: Optional[bool] = None
    train_segmentation: Optional[bool] = None
    freeze_language_backbone: Optional[bool] = None
    language_backbone_lr: Optional[float] = None
    prompt_variants: Optional[Dict[str, Any]] = None
    prompt_randomize: Optional[bool] = None
    val_score_thresh: Optional[float] = None
    val_max_dets: Optional[int] = None
    random_split: Optional[bool] = None
    val_percent: Optional[float] = None
    split_seed: Optional[int] = None


class YoloTrainRequest(BaseModel):
    dataset_id: Optional[str] = None
    dataset_root: Optional[str] = None
    run_name: Optional[str] = None
    task: Literal["detect", "segment"] = "detect"
    variant: Optional[str] = None
    from_scratch: Optional[bool] = None
    base_weights: Optional[str] = None
    epochs: Optional[int] = None
    img_size: Optional[int] = None
    batch: Optional[int] = None
    workers: Optional[int] = None
    devices: Optional[List[int]] = None
    seed: Optional[int] = None
    augmentations: Optional[Dict[str, Any]] = None
    accept_tos: Optional[bool] = None

    @root_validator(skip_on_failure=True)
    def _validate_dataset_fields(cls, values):  # noqa: N805
        if not (values.get("dataset_id") or values.get("dataset_root")):
            raise ValueError("dataset_id_or_root_required")
        return values


class RfDetrTrainRequest(BaseModel):
    dataset_id: Optional[str] = None
    dataset_root: Optional[str] = None
    run_name: Optional[str] = None
    task: Literal["detect", "segment"] = "detect"
    variant: Optional[str] = None
    epochs: Optional[int] = None
    batch: Optional[int] = None
    grad_accum: Optional[int] = None
    workers: Optional[int] = None
    devices: Optional[List[int]] = None
    seed: Optional[int] = None
    resolution: Optional[int] = None
    from_scratch: Optional[bool] = None
    pretrain_weights: Optional[str] = None
    use_ema: Optional[bool] = None
    early_stopping: Optional[bool] = None
    early_stopping_patience: Optional[int] = None
    multi_scale: Optional[bool] = None
    expanded_scales: Optional[bool] = None
    augmentations: Optional[Dict[str, Any]] = None
    accept_tos: Optional[bool] = None

    @root_validator(skip_on_failure=True)
    def _validate_dataset_fields(cls, values):  # noqa: N805
        if not (values.get("dataset_id") or values.get("dataset_root")):
            raise ValueError("dataset_id_or_root_required")
        return values


class YoloActiveRequest(BaseModel):
    run_id: str


class RfDetrActiveRequest(BaseModel):
    run_id: str


class YoloRegionRequest(BaseModel):
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    region: List[float]
    conf: Optional[float] = 0.25
    iou: Optional[float] = 0.45
    max_det: Optional[int] = 300
    center_only: Optional[bool] = True
    image_is_cropped: Optional[bool] = False
    full_width: Optional[int] = None
    full_height: Optional[int] = None
    expected_labelmap: Optional[List[str]] = None

    @root_validator(skip_on_failure=True)
    def _validate_region(cls, values):  # noqa: N805
        region = values.get("region")
        if not isinstance(region, list) or len(region) < 4:
            raise ValueError("region_required")
        if not values.get("image_base64") and not values.get("image_token"):
            raise ValueError("image_required")
        return values


class YoloRegionDetection(BaseModel):
    bbox: List[float]
    class_id: int
    class_name: Optional[str] = None
    score: Optional[float] = None


class YoloRegionResponse(BaseModel):
    detections: List[YoloRegionDetection]
    labelmap: Optional[List[str]] = None
    warnings: Optional[List[str]] = None


class YoloFullRequest(BaseModel):
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    conf: Optional[float] = 0.25
    iou: Optional[float] = 0.45
    max_det: Optional[int] = 300
    expected_labelmap: Optional[List[str]] = None

    @root_validator(skip_on_failure=True)
    def _validate_image(cls, values):  # noqa: N805
        if not values.get("image_base64") and not values.get("image_token"):
            raise ValueError("image_required")
        return values


class YoloWindowedRequest(BaseModel):
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    conf: Optional[float] = 0.25
    iou: Optional[float] = 0.45
    max_det: Optional[int] = 300
    expected_labelmap: Optional[List[str]] = None
    slice_size: Optional[int] = 640
    overlap: Optional[float] = 0.2
    merge_iou: Optional[float] = 0.5

    @root_validator(skip_on_failure=True)
    def _validate_image(cls, values):  # noqa: N805
        if not values.get("image_base64") and not values.get("image_token"):
            raise ValueError("image_required")
        return values


class RfDetrRegionRequest(BaseModel):
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    region: List[float]
    conf: Optional[float] = 0.25
    max_det: Optional[int] = 300
    center_only: Optional[bool] = True
    image_is_cropped: Optional[bool] = False
    full_width: Optional[int] = None
    full_height: Optional[int] = None
    expected_labelmap: Optional[List[str]] = None

    @root_validator(skip_on_failure=True)
    def _validate_region(cls, values):  # noqa: N805
        region = values.get("region")
        if not isinstance(region, list) or len(region) < 4:
            raise ValueError("region_required")
        if not values.get("image_base64") and not values.get("image_token"):
            raise ValueError("image_required")
        return values


class RfDetrRegionDetection(BaseModel):
    bbox: List[float]
    class_id: int
    class_name: Optional[str] = None
    score: Optional[float] = None


class RfDetrRegionResponse(BaseModel):
    detections: List[RfDetrRegionDetection]
    labelmap: Optional[List[str]] = None
    warnings: Optional[List[str]] = None


class RfDetrFullRequest(BaseModel):
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    conf: Optional[float] = 0.25
    max_det: Optional[int] = 300
    expected_labelmap: Optional[List[str]] = None

    @root_validator(skip_on_failure=True)
    def _validate_image(cls, values):  # noqa: N805
        if not values.get("image_base64") and not values.get("image_token"):
            raise ValueError("image_required")
        return values


class RfDetrWindowedRequest(BaseModel):
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    conf: Optional[float] = 0.25
    max_det: Optional[int] = 300
    expected_labelmap: Optional[List[str]] = None
    slice_size: Optional[int] = 640
    overlap: Optional[float] = 0.2
    merge_iou: Optional[float] = 0.5

    @root_validator(skip_on_failure=True)
    def _validate_image(cls, values):  # noqa: N805
        if not values.get("image_base64") and not values.get("image_token"):
            raise ValueError("image_required")
        return values


def _apply_expected_labelmap_warnings(expected: Optional[List[str]], labelmap: List[str], warnings: List[str]) -> None:
    if expected and not labelmap:
        warnings.append("labelmap_missing")
    elif expected and labelmap and expected != labelmap:
        warnings.append("labelmap_mismatch")


def _normalize_labelmap_entries(values: Sequence[str]) -> List[str]:
    normalized: List[str] = []
    for raw in values or []:
        text = str(raw or "").strip()
        if not text:
            normalized.append("")
            continue
        norm = _normalize_class_name_for_match(text)
        normalized.append(norm or text.lower())
    return normalized


def _labelmaps_match(expected: Sequence[str], actual: Sequence[str]) -> bool:
    if not expected or not actual:
        return True
    exp_norm = _normalize_labelmap_entries(expected)
    act_norm = _normalize_labelmap_entries(actual)
    if len(exp_norm) != len(act_norm):
        return False
    for exp, act in zip(exp_norm, act_norm):
        if exp != act:
            return False
    return True


def _raise_on_labelmap_mismatch(
    *,
    expected: Optional[Sequence[str]],
    actual: Optional[Sequence[str]],
    context: str,
) -> None:
    if not expected or not actual:
        return
    if not _labelmaps_match(expected, actual):
            raise HTTPException(
                status_code=HTTP_412_PRECONDITION_FAILED,
                detail=f"detector_labelmap_mismatch:{context}",
            )


def _clamp_conf_value(conf: float, warnings: List[str]) -> float:
    if conf < 0 or conf > 1:
        warnings.append("conf_clamped")
        return min(1.0, max(0.0, conf))
    return conf


def _clamp_iou_value(iou: float, warnings: List[str]) -> float:
    if iou < 0 or iou > 1:
        warnings.append("iou_clamped")
        return min(1.0, max(0.0, iou))
    return iou


def _clamp_max_det_value(max_det: int, warnings: List[str]) -> int:
    if max_det < 1:
        warnings.append("max_det_clamped")
        return 1
    if max_det > 5000:
        warnings.append("max_det_clamped")
        return 5000
    return max_det


def _clamp_slice_params(
    slice_size: int,
    overlap: float,
    merge_iou: float,
    img_w: int,
    img_h: int,
    warnings: List[str],
) -> Tuple[int, float, float]:
    max_dim = max(img_w, img_h, 1)
    if slice_size < 64:
        slice_size = 64
        warnings.append("slice_size_clamped")
    if slice_size > max_dim:
        slice_size = max_dim
        warnings.append("slice_size_clamped")
    if overlap < 0 or overlap >= 0.95:
        overlap = min(0.9, max(0.0, overlap))
        warnings.append("overlap_clamped")
    if merge_iou < 0 or merge_iou > 1:
        merge_iou = min(1.0, max(0.0, merge_iou))
        warnings.append("merge_iou_clamped")
    return slice_size, overlap, merge_iou


def _iou_xywh(box_a: List[float], box_b: List[float]) -> float:
    ax, ay, aw, ah = box_a
    bx, by, bw, bh = box_b
    if aw <= 0 or ah <= 0 or bw <= 0 or bh <= 0:
        return 0.0
    ax2 = ax + aw
    ay2 = ay + ah
    bx2 = bx + bw
    by2 = by + bh
    inter_x1 = max(ax, bx)
    inter_y1 = max(ay, by)
    inter_x2 = min(ax2, bx2)
    inter_y2 = min(ay2, by2)
    inter_w = max(0.0, inter_x2 - inter_x1)
    inter_h = max(0.0, inter_y2 - inter_y1)
    inter = inter_w * inter_h
    union = (aw * ah) + (bw * bh) - inter
    if union <= 0:
        return 0.0
    return inter / union


def _nms_indices(boxes: List[List[float]], scores: List[float], iou_thr: float) -> List[int]:
    order = sorted(range(len(boxes)), key=lambda idx: scores[idx], reverse=True)
    keep: List[int] = []
    while order:
        current = order.pop(0)
        keep.append(current)
        remaining = []
        for idx in order:
            if _iou_xywh(boxes[current], boxes[idx]) <= iou_thr:
                remaining.append(idx)
        order = remaining
    return keep


def _merge_detections_nms(
    detections: List[Dict[str, Any]],
    iou_thr: float,
    max_det: Optional[int],
) -> List[Dict[str, Any]]:
    if not detections:
        return []
    if iou_thr <= 0:
        merged = detections
    else:
        by_class: Dict[int, List[int]] = {}
        for idx, det in enumerate(detections):
            class_id = int(det.get("class_id", -1))
            by_class.setdefault(class_id, []).append(idx)
        keep_idx: List[int] = []
        for idxs in by_class.values():
            boxes = [detections[i]["bbox"] for i in idxs]
            scores = [float(detections[i].get("score") or 0.0) for i in idxs]
            for keep in _nms_indices(boxes, scores, iou_thr):
                keep_idx.append(idxs[keep])
        merged = [detections[i] for i in keep_idx]
    merged.sort(key=lambda det: float(det.get("score") or 0.0), reverse=True)
    if max_det:
        return merged[:max_det]
    return merged


def _slice_image_sahi(pil_img: Image.Image, slice_size: int, overlap: float) -> Tuple[List[np.ndarray], List[Tuple[int, int]]]:
    try:
        from sahi.slicing import slice_image  # type: ignore
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=f"sahi_unavailable:{exc}") from exc
    array = np.array(pil_img)
    result = slice_image(
        image=array,
        slice_height=slice_size,
        slice_width=slice_size,
        overlap_height_ratio=overlap,
        overlap_width_ratio=overlap,
    )
    slices = getattr(result, "images", None)
    starts = getattr(result, "starting_pixels", None)
    if slices is None or starts is None:
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail="sahi_slice_failed")
    return slices, starts

class Sam3ModelActivateRequest(BaseModel):
    checkpoint_path: Optional[str] = None
    label: Optional[str] = None
    enable_segmentation: Optional[bool] = None


class QwenModelActivateRequest(BaseModel):
    model_id: str


class ActiveModelRequest(BaseModel):
    classifier_path: Optional[str] = None
    labelmap_path: Optional[str] = None
    clip_model: Optional[str] = None
    logit_adjustment_inference: Optional[bool] = None


class ActiveModelResponse(BaseModel):
    clip_model: Optional[str]
    encoder_type: Optional[str] = None
    encoder_model: Optional[str] = None
    classifier_path: Optional[str]
    labelmap_path: Optional[str]
    clip_ready: bool
    labelmap_entries: List[str] = []
    logit_adjustment_inference: Optional[bool] = None


class SegmentationBuildRequest(BaseModel):
    source_dataset_id: str = Field(..., description="Existing bbox dataset id (Qwen or SAM3)")
    output_name: Optional[str] = Field(None, description="Optional output dataset name")
    sam_variant: Literal["sam1", "sam3"] = Field("sam3", description="Generator to use for masks")
    output_format: Literal["yolo-seg"] = Field("yolo-seg", description="Target mask encoding (polygons)")
    mask_threshold: float = Field(0.5, ge=0.0, le=1.0, description="Mask probability threshold")
    score_threshold: float = Field(0.0, ge=0.0, le=1.0, description="Box confidence threshold")
    simplify_epsilon: float = Field(30.0, ge=0.0, description="Polygon simplification epsilon (px)")
    min_size: float = Field(0.0, ge=0.0, description="Minimum mask area (px^2)")
    max_results: int = Field(1, ge=1, description="Max detections per box prompt")


@dataclass
class ClipTrainingJob:
    job_id: str
    status: str = "queued"
    progress: float = 0.0
    message: str = "Queued"
    logs: List[Dict[str, Any]] = field(default_factory=list)
    metrics: List[Dict[str, Any]] = field(default_factory=list)
    artifacts: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    created_at: float = field(default_factory=time.time)
    updated_at: float = field(default_factory=time.time)
    temp_dir: Optional[str] = None
    images_dir: Optional[str] = None
    labels_dir: Optional[str] = None
    labelmap_path: Optional[str] = None
    cancel_event: threading.Event = field(default_factory=threading.Event)


@dataclass
class ClipDatasetUploadJob:
    job_id: str
    root_dir: Path
    images_dir: Path
    labels_dir: Path
    created_at: float = field(default_factory=time.time)
    image_count: int = 0
    label_count: int = 0
    completed: bool = False


@dataclass
class QwenDatasetUploadJob:
    job_id: str
    root_dir: Path
    train_dir: Path
    val_dir: Path
    train_annotations: Path
    val_annotations: Path
    created_at: float = field(default_factory=time.time)
    run_name: Optional[str] = None
    train_count: int = 0
    val_count: int = 0
    completed: bool = False


TRAINING_JOBS: Dict[str, ClipTrainingJob] = {}
TRAINING_JOBS_LOCK = threading.Lock()

QWEN_JOB_ROOT = Path(os.environ.get("QWEN_TRAINING_ROOT", "./uploads/qwen_runs"))
QWEN_JOB_ROOT.mkdir(parents=True, exist_ok=True)
QWEN_DATASET_ROOT = QWEN_JOB_ROOT / "datasets"
QWEN_DATASET_ROOT.mkdir(parents=True, exist_ok=True)
SAM3_JOB_ROOT = Path(os.environ.get("SAM3_TRAINING_ROOT", "./uploads/sam3_runs"))
SAM3_JOB_ROOT.mkdir(parents=True, exist_ok=True)
SAM3_DATASET_ROOT = SAM3_JOB_ROOT / "datasets"
SAM3_DATASET_ROOT.mkdir(parents=True, exist_ok=True)
SAM3_DATASET_META_NAME = "sam3_dataset.json"
YOLO_JOB_ROOT = Path(os.environ.get("YOLO_TRAINING_ROOT", "./uploads/yolo_runs"))
YOLO_JOB_ROOT.mkdir(parents=True, exist_ok=True)
YOLO_MODEL_ROOT = Path(os.environ.get("YOLO_MODEL_ROOT", "./uploads/yolo_models"))
YOLO_MODEL_ROOT.mkdir(parents=True, exist_ok=True)
YOLO_ACTIVE_PATH = YOLO_MODEL_ROOT / "active.json"
YOLO_DATASET_CACHE_ROOT = YOLO_JOB_ROOT / "datasets"
YOLO_DATASET_CACHE_ROOT.mkdir(parents=True, exist_ok=True)
YOLO_RUN_META_NAME = "run.json"
YOLO_KEEP_FILES = {
    "best.pt",
    "results.csv",
    "args.yaml",
    "data.yaml",
    "metrics.json",
    "metrics_series.json",
    "labelmap.txt",
    YOLO_RUN_META_NAME,
}

RFDETR_JOB_ROOT = Path(os.environ.get("RFDETR_TRAINING_ROOT", "./uploads/rfdetr_runs"))
RFDETR_JOB_ROOT.mkdir(parents=True, exist_ok=True)
RFDETR_MODEL_ROOT = Path(os.environ.get("RFDETR_MODEL_ROOT", "./uploads/rfdetr_models"))
RFDETR_MODEL_ROOT.mkdir(parents=True, exist_ok=True)
RFDETR_ACTIVE_PATH = RFDETR_MODEL_ROOT / "active.json"
DETECTOR_PREFS_ROOT = Path(os.environ.get("DETECTOR_PREFS_ROOT", "./uploads/detectors"))
DETECTOR_PREFS_ROOT.mkdir(parents=True, exist_ok=True)
DETECTOR_DEFAULT_PATH = DETECTOR_PREFS_ROOT / "default.json"
RFDETR_RUN_META_NAME = "run.json"
RFDETR_KEEP_FILES = {
    "checkpoint_best_regular.pth",
    "checkpoint_best_ema.pth",
    "checkpoint_best_total.pth",
    "checkpoint_best_optimized.pt",
    "results.json",
    "metrics_series.json",
    "metrics_plot.png",
    "log.txt",
    "labelmap.txt",
    RFDETR_RUN_META_NAME,
}

GLOSSARY_LIBRARY_ROOT = Path(os.environ.get("GLOSSARY_ROOT", "./uploads/glossaries"))
GLOSSARY_LIBRARY_ROOT.mkdir(parents=True, exist_ok=True)
GLOSSARY_LIBRARY_PATH = GLOSSARY_LIBRARY_ROOT / "glossaries.json"
GLOSSARY_LIBRARY_LOCK = threading.Lock()

YOLO_INFER_LOCK = threading.RLock()
yolo_infer_model: Any = None
yolo_infer_path: Optional[str] = None
yolo_infer_labelmap: List[str] = []
yolo_infer_task: Optional[str] = None
RFDETR_INFER_LOCK = threading.RLock()
rfdetr_infer_model: Any = None
rfdetr_infer_path: Optional[str] = None
rfdetr_infer_labelmap: List[str] = []
rfdetr_infer_task: Optional[str] = None
rfdetr_infer_variant: Optional[str] = None
YOLO_VARIANTS = [
    {"id": "yolov8n", "label": "YOLOv8 Nano", "task": "detect"},
    {"id": "yolov8s", "label": "YOLOv8 Small", "task": "detect"},
    {"id": "yolov8m", "label": "YOLOv8 Medium", "task": "detect"},
    {"id": "yolov8l", "label": "YOLOv8 Large", "task": "detect"},
    {"id": "yolov8x", "label": "YOLOv8 XLarge", "task": "detect"},
    {"id": "yolov8n-seg", "label": "YOLOv8 Nano (seg)", "task": "segment"},
    {"id": "yolov8s-seg", "label": "YOLOv8 Small (seg)", "task": "segment"},
    {"id": "yolov8m-seg", "label": "YOLOv8 Medium (seg)", "task": "segment"},
    {"id": "yolov8l-seg", "label": "YOLOv8 Large (seg)", "task": "segment"},
    {"id": "yolov8x-seg", "label": "YOLOv8 XLarge (seg)", "task": "segment"},
    {"id": "yolov8n-p2", "label": "YOLOv8 Nano (P2)", "task": "detect"},
    {"id": "yolov8s-p2", "label": "YOLOv8 Small (P2)", "task": "detect"},
    {"id": "yolov8m-p2", "label": "YOLOv8 Medium (P2)", "task": "detect"},
    {"id": "yolov8l-p2", "label": "YOLOv8 Large (P2)", "task": "detect"},
    {"id": "yolov8x-p2", "label": "YOLOv8 XLarge (P2)", "task": "detect"},
]
RFDETR_VARIANTS = [
    {"id": "rfdetr-nano", "label": "RF-DETR Nano", "task": "detect"},
    {"id": "rfdetr-small", "label": "RF-DETR Small", "task": "detect"},
    {"id": "rfdetr-medium", "label": "RF-DETR Medium", "task": "detect"},
    {"id": "rfdetr-base", "label": "RF-DETR Base", "task": "detect"},
    {"id": "rfdetr-large", "label": "RF-DETR Large", "task": "detect"},
    {"id": "rfdetr-seg-preview", "label": "RF-DETR Seg Preview", "task": "segment"},
]
DATASET_REGISTRY_ROOT = Path(os.environ.get("DATASET_ROOT", "./uploads/datasets"))
DATASET_REGISTRY_ROOT.mkdir(parents=True, exist_ok=True)
DATASET_META_NAME = "dataset.json"
PROMPT_HELPER_JOB_ROOT = Path(os.environ.get("SAM3_PROMPT_HELPER_ROOT", "./uploads/prompt_helper_jobs"))
PROMPT_HELPER_JOB_ROOT.mkdir(parents=True, exist_ok=True)
SEG_BUILDER_ROOT = Path(os.environ.get("SEGMENTATION_ROOT", "./uploads/seg_runs"))
SEG_BUILDER_ROOT.mkdir(parents=True, exist_ok=True)
SAM3_REPO_ROOT = Path(__file__).resolve().parent.resolve()
SAM3_VENDOR_ROOT = SAM3_REPO_ROOT / "sam3"
SAM3_PACKAGE_ROOT = SAM3_VENDOR_ROOT / "sam3"
SAM3_CONFIG_TEMPLATE = SAM3_REPO_ROOT / "sam3_local" / "local_yolo_ft.yaml"
SAM3_GENERATED_CONFIG_DIR = SAM3_PACKAGE_ROOT / "train/configs/generated"
SAM3_GENERATED_CONFIG_DIR.mkdir(parents=True, exist_ok=True)
SAM3_BPE_PATH = SAM3_VENDOR_ROOT / "assets" / "bpe_simple_vocab_16e6.txt.gz"
SAM3_MAX_LOG_LINES = 500
SAM3_MAX_METRIC_POINTS = 2000
SAM3_STORAGE_SCOPES = {"all", "checkpoints", "logs", "tensorboard", "dumps"}
YOLO_MAX_LOG_LINES = 300


def _prepass_recipe_dir(recipe_id: str, *, create: bool = False) -> Path:
    safe = _sanitize_yolo_run_id(recipe_id)
    path = PREPASS_RECIPE_ROOT / safe
    if create:
        path.mkdir(parents=True, exist_ok=True)
    return path


def _prepass_recipe_meta_path(recipe_id: str) -> Path:
    return _prepass_recipe_dir(recipe_id) / PREPASS_RECIPE_META


def _prepass_recipe_assets_dir(recipe_id: str, *, create: bool = False) -> Path:
    path = _prepass_recipe_dir(recipe_id, create=create) / PREPASS_RECIPE_ASSETS
    if create:
        path.mkdir(parents=True, exist_ok=True)
    return path


def _sha256_path(path: Path) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()


def _copy_tree_filtered(src: Path, dest: Path, *, keep_files: Optional[set[str]] = None) -> List[Dict[str, Any]]:
    copied: List[Dict[str, Any]] = []
    if not src.exists():
        return copied
    dest.mkdir(parents=True, exist_ok=True)
    for item in src.iterdir():
        if item.is_dir():
            sub_dest = dest / item.name
            copied.extend(_copy_tree_filtered(item, sub_dest, keep_files=keep_files))
            continue
        if keep_files is not None and item.name not in keep_files:
            continue
        target = dest / item.name
        shutil.copy2(item, target)
        copied.append(
            {
                "path": str(target.relative_to(dest.parent)),
                "size": target.stat().st_size,
                "sha256": _sha256_path(target),
            }
        )
    return copied


def _unique_prepass_recipe_name(name: str) -> Tuple[str, Optional[str]]:
    cleaned = (name or "").strip() or "Imported recipe"
    existing = {str(entry.get("name") or "").strip() for entry in _list_prepass_recipes()}
    if cleaned not in existing:
        return cleaned, None
    base = cleaned
    idx = 2
    while f"{base} ({idx})" in existing:
        idx += 1
    return f"{base} ({idx})", base


def _validate_prepass_recipe_manifest(manifest: Dict[str, Any], extract_dir: Path) -> None:
    assets = manifest.get("assets") or {}
    copied = assets.get("copied") or []
    extract_root = extract_dir.resolve()
    if not isinstance(copied, list):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="prepass_recipe_manifest_invalid")
    for entry in copied:
        if not isinstance(entry, dict):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="prepass_recipe_manifest_invalid")
        rel = entry.get("path")
        if not isinstance(rel, str) or not rel:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="prepass_recipe_manifest_invalid")
        target = (extract_root / rel).resolve()
        if not _path_is_within_root(target, extract_root):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="prepass_recipe_manifest_invalid_path")
        if not target.exists():
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="prepass_recipe_manifest_missing_asset")
        if entry.get("sha256") and _sha256_path(target) != entry.get("sha256"):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="prepass_recipe_manifest_hash_mismatch")


def _write_prepass_recipe_meta(recipe_dir: Path, payload: Dict[str, Any]) -> None:
    meta_path = recipe_dir / PREPASS_RECIPE_META
    meta_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")


def _load_prepass_recipe_meta(recipe_dir: Path) -> Dict[str, Any]:
    meta_path = recipe_dir / PREPASS_RECIPE_META
    if not meta_path.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="prepass_recipe_not_found")
    return json.loads(meta_path.read_text())


def _list_prepass_recipes() -> List[Dict[str, Any]]:
    recipes: List[Dict[str, Any]] = []
    for entry in PREPASS_RECIPE_ROOT.iterdir():
        if not entry.is_dir():
            continue
        meta_path = entry / PREPASS_RECIPE_META
        if not meta_path.exists():
            continue
        try:
            meta = json.loads(meta_path.read_text())
        except Exception:
            continue
        recipes.append(
            {
                "id": meta.get("id") or entry.name,
                "name": meta.get("name") or entry.name,
                "description": meta.get("description") or "",
                "created_at": meta.get("created_at"),
                "updated_at": meta.get("updated_at"),
            }
        )
    recipes.sort(key=lambda r: r.get("updated_at") or r.get("created_at") or 0, reverse=True)
    return recipes


@dataclass
class QwenTrainingJob:
    job_id: str
    status: str = "queued"
    progress: float = 0.0
    message: str = "Queued"
    config: Dict[str, Any] = field(default_factory=dict)
    logs: List[Dict[str, Any]] = field(default_factory=list)
    metrics: List[Dict[str, Any]] = field(default_factory=list)
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    created_at: float = field(default_factory=time.time)
    updated_at: float = field(default_factory=time.time)
    cancel_event: threading.Event = field(default_factory=threading.Event)


@dataclass
class Sam3TrainingJob:
    job_id: str
    status: str = "queued"
    progress: float = 0.0
    message: str = "Queued"
    config: Dict[str, Any] = field(default_factory=dict)
    logs: List[Dict[str, Any]] = field(default_factory=list)
    metrics: List[Dict[str, Any]] = field(default_factory=list)
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    created_at: float = field(default_factory=time.time)
    updated_at: float = field(default_factory=time.time)
    cancel_event: threading.Event = field(default_factory=threading.Event)
    process: Optional[subprocess.Popen] = None
    log_seq: int = 0


@dataclass
class YoloTrainingJob:
    job_id: str
    status: str = "queued"
    progress: float = 0.0
    message: str = "Queued"
    config: Dict[str, Any] = field(default_factory=dict)
    logs: List[Dict[str, Any]] = field(default_factory=list)
    metrics: List[Dict[str, Any]] = field(default_factory=list)
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    created_at: float = field(default_factory=time.time)
    updated_at: float = field(default_factory=time.time)
    cancel_event: threading.Event = field(default_factory=threading.Event)


@dataclass
class RfDetrTrainingJob:
    job_id: str
    status: str = "queued"
    progress: float = 0.0
    message: str = "Queued"
    config: Dict[str, Any] = field(default_factory=dict)
    logs: List[Dict[str, Any]] = field(default_factory=list)
    metrics: List[Dict[str, Any]] = field(default_factory=list)
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    created_at: float = field(default_factory=time.time)
    updated_at: float = field(default_factory=time.time)
    cancel_event: threading.Event = field(default_factory=threading.Event)


@dataclass
class SegmentationBuildJob:
    job_id: str
    status: str = "queued"
    progress: float = 0.0
    message: str = "Queued"
    config: Dict[str, Any] = field(default_factory=dict)
    logs: List[Dict[str, Any]] = field(default_factory=list)
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    created_at: float = field(default_factory=time.time)
    updated_at: float = field(default_factory=time.time)
    cancel_event: threading.Event = field(default_factory=threading.Event)


@dataclass
class PromptHelperJob:
    job_id: str
    status: str = "queued"
    message: str = "Queued"
    progress: float = 0.0
    request: Dict[str, Any] = field(default_factory=dict)
    result: Optional[Dict[str, Any]] = None
    logs: List[Dict[str, Any]] = field(default_factory=list)
    total_steps: int = 0
    completed_steps: int = 0
    error: Optional[str] = None
    created_at: float = field(default_factory=time.time)
    updated_at: float = field(default_factory=time.time)


@dataclass
class AgentMiningJob:
    job_id: str
    status: str = "queued"
    message: str = "Queued"
    progress: float = 0.0
    request: Dict[str, Any] = field(default_factory=dict)
    result: Optional[Dict[str, Any]] = None
    logs: List[Dict[str, Any]] = field(default_factory=list)
    error: Optional[str] = None
    created_at: float = field(default_factory=time.time)
    updated_at: float = field(default_factory=time.time)
    cancel_event: threading.Event = field(default_factory=threading.Event)


@dataclass
class CalibrationJob:
    job_id: str
    status: str = "queued"
    message: str = "Queued"
    phase: str = "queued"
    progress: float = 0.0
    processed: int = 0
    total: int = 0
    request: Dict[str, Any] = field(default_factory=dict)
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    created_at: float = field(default_factory=time.time)
    updated_at: float = field(default_factory=time.time)
    cancel_event: threading.Event = field(default_factory=threading.Event)




QWEN_TRAINING_JOBS: Dict[str, QwenTrainingJob] = {}
QWEN_TRAINING_JOBS_LOCK = threading.Lock()
SAM3_TRAINING_JOBS: Dict[str, Sam3TrainingJob] = {}
SAM3_TRAINING_JOBS_LOCK = threading.Lock()
YOLO_TRAINING_JOBS: Dict[str, YoloTrainingJob] = {}
YOLO_TRAINING_JOBS_LOCK = threading.Lock()
RFDETR_TRAINING_JOBS: Dict[str, RfDetrTrainingJob] = {}
RFDETR_TRAINING_JOBS_LOCK = threading.Lock()
SEGMENTATION_BUILD_JOBS: Dict[str, SegmentationBuildJob] = {}
SEGMENTATION_BUILD_JOBS_LOCK = threading.Lock()
PROMPT_HELPER_JOBS: Dict[str, PromptHelperJob] = {}
PROMPT_HELPER_JOBS_LOCK = threading.Lock()
AGENT_MINING_JOBS: Dict[str, AgentMiningJob] = {}
AGENT_MINING_JOBS_LOCK = threading.Lock()
CALIBRATION_JOBS: Dict[str, CalibrationJob] = {}
CALIBRATION_JOBS_LOCK = threading.Lock()
UPLOAD_ROOT = Path("uploads")
UPLOAD_ROOT.mkdir(exist_ok=True)
PREPASS_RECIPE_ROOT = UPLOAD_ROOT / "prepass_recipes"
PREPASS_RECIPE_ROOT.mkdir(parents=True, exist_ok=True)
PREPASS_RECIPE_META = "recipe.json"
PREPASS_RECIPE_ASSETS = "assets"
PREPASS_RECIPE_SCHEMA_VERSION = 1
PREPASS_RECIPE_TMP_ROOT = UPLOAD_ROOT / "tmp_prepass_recipes"
PREPASS_RECIPE_TMP_ROOT.mkdir(parents=True, exist_ok=True)
PREPASS_RECIPE_EXPORT_ROOT = Path(
    os.environ.get("PREPASS_RECIPE_EXPORT_ROOT", str(UPLOAD_ROOT / "prepass_recipe_exports"))
)
PREPASS_RECIPE_EXPORT_ROOT.mkdir(parents=True, exist_ok=True)
PROMPT_HELPER_PRESET_ROOT = UPLOAD_ROOT / "prompt_helper_presets"
PROMPT_HELPER_PRESET_ROOT.mkdir(parents=True, exist_ok=True)


class PrepassRecipeRequest(BaseModel):
    recipe_id: Optional[str] = None
    name: str
    description: Optional[str] = None
    config: Dict[str, Any]
    glossary: Optional[str] = None


class PrepassRecipeResponse(BaseModel):
    id: str
    name: str
    description: Optional[str] = None
    created_at: float
    updated_at: float
    config: Dict[str, Any]
    glossary: Optional[str] = None
    schema_version: int = PREPASS_RECIPE_SCHEMA_VERSION
    renamed_from: Optional[str] = None
    notice: Optional[str] = None
CLIP_DATASET_UPLOAD_ROOT = UPLOAD_ROOT / "clip_dataset_uploads"
CLIP_DATASET_UPLOAD_ROOT.mkdir(parents=True, exist_ok=True)
DATASET_UPLOAD_ROOT = UPLOAD_ROOT / "dataset_uploads"
DATASET_UPLOAD_ROOT.mkdir(parents=True, exist_ok=True)
CLIP_NEGATIVE_REPLAY_ROOT = UPLOAD_ROOT / "clip_negative_replay"
CLIP_NEGATIVE_REPLAY_ROOT.mkdir(parents=True, exist_ok=True)
QWEN_PREPASS_TRACE_ROOT = UPLOAD_ROOT / "qwen_prepass_traces"
QWEN_PREPASS_TRACE_ROOT.mkdir(parents=True, exist_ok=True)
QWEN_PREPASS_FULL_TRACE_ROOT = UPLOAD_ROOT / "qwen_prepass_traces_full"
QWEN_PREPASS_FULL_TRACE_ROOT.mkdir(parents=True, exist_ok=True)
QWEN_PREPASS_FULL_TRACE_LATEST = QWEN_PREPASS_FULL_TRACE_ROOT / "latest.jsonl"
LOG_ROOT = Path("logs")
LOG_ROOT.mkdir(parents=True, exist_ok=True)
QWEN_PREPASS_READABLE_TRACE_ROOT = LOG_ROOT / "prepass_readable"
QWEN_PREPASS_READABLE_TRACE_ROOT.mkdir(parents=True, exist_ok=True)
QWEN_PREPASS_READABLE_TRACE_LATEST = QWEN_PREPASS_READABLE_TRACE_ROOT / "latest.log"
CALIBRATION_ROOT = UPLOAD_ROOT / "calibration_jobs"
CALIBRATION_CACHE_ROOT = UPLOAD_ROOT / "calibration_cache"
CALIBRATION_FEATURES_VERSION = 3
CALIBRATION_ROOT.mkdir(parents=True, exist_ok=True)


def _prune_job_registry(registry: Dict[str, Any], lock: threading.Lock, ttl_hours: Optional[int] = None) -> None:
    if ttl_hours is None:
        ttl_hours = JOB_REGISTRY_TTL_HOURS
    ttl_seconds = max(0, ttl_hours) * 3600
    if ttl_seconds == 0:
        return
    now = time.time()
    terminal = {"completed", "failed", "cancelled"}
    with lock:
        to_delete: List[str] = []
        for job_id, job in list(registry.items()):
            status = getattr(job, "status", "")
            updated = getattr(job, "updated_at", getattr(job, "created_at", now))
            if status in {"running", "queued", "cancelling"}:
                continue
            if status and status not in terminal:
                continue
            try:
                if now - float(updated) > ttl_seconds:
                    to_delete.append(job_id)
            except Exception:
                continue
        for job_id in to_delete:
            registry.pop(job_id, None)


def _purge_staging_dirs(
    root: Path,
    *,
    ttl_hours: Optional[int] = None,
    active_roots: Optional[set[str]] = None,
    prefix: Optional[str] = None,
) -> Dict[str, int]:
    """Delete old staging directories that are not active. Returns stats."""
    stats = {"deleted": 0, "bytes": 0}
    if ttl_hours is None:
        ttl_hours = STAGING_TTL_HOURS
    if not root.exists() or ttl_hours <= 0:
        return stats
    cutoff = time.time() - ttl_hours * 3600
    active_roots = active_roots or set()
    for entry in root.iterdir():
        try:
            if not entry.is_dir():
                continue
            if prefix and not entry.name.startswith(prefix):
                continue
            if str(entry.resolve()) in active_roots:
                continue
            mtime = entry.stat().st_mtime
            if mtime > cutoff:
                continue
            stats["bytes"] += _purge_directory(entry)
            stats["deleted"] += 1
        except Exception:
            continue
    return stats
JOB_REGISTRY_TTL_HOURS = _env_int("JOB_REGISTRY_TTL_HOURS", 72)
STAGING_TTL_HOURS = _env_int("STAGING_TTL_HOURS", 24)
AGENT_MINING_ROOT = UPLOAD_ROOT / "agent_mining"
AGENT_MINING_ROOT.mkdir(parents=True, exist_ok=True)
AGENT_MINING_JOB_ROOT = AGENT_MINING_ROOT / "jobs"
AGENT_MINING_JOB_ROOT.mkdir(parents=True, exist_ok=True)
AGENT_MINING_CACHE_ROOT = AGENT_MINING_ROOT / "cache"
AGENT_MINING_CACHE_ROOT.mkdir(parents=True, exist_ok=True)
AGENT_MINING_META_ROOT = AGENT_MINING_ROOT / "meta"
AGENT_MINING_META_ROOT.mkdir(parents=True, exist_ok=True)
AGENT_MINING_DET_CACHE_ROOT = AGENT_MINING_ROOT / "detections"
AGENT_MINING_DET_CACHE_ROOT.mkdir(parents=True, exist_ok=True)
AGENT_MINING_RECIPES_ROOT = AGENT_MINING_ROOT / "recipes"
AGENT_MINING_RECIPES_ROOT.mkdir(parents=True, exist_ok=True)
AGENT_MINING_CASCADES_ROOT = AGENT_MINING_ROOT / "cascades"
AGENT_MINING_CASCADES_ROOT.mkdir(parents=True, exist_ok=True)


def _purge_dataset_artifacts(dataset_id: str) -> None:
    """Remove per-dataset agent/prompt-helper artifacts."""
    safe_dataset = _normalise_relative_path(dataset_id)
    for derived_root in (
        AGENT_MINING_META_ROOT / safe_dataset,
        AGENT_MINING_DET_CACHE_ROOT / safe_dataset,
    ):
        try:
            shutil.rmtree(derived_root, ignore_errors=True)
        except Exception:
            pass
    try:
        for preset_path in PROMPT_HELPER_PRESET_ROOT.glob("*.json"):
            try:
                with preset_path.open("r", encoding="utf-8") as handle:
                    preset_data = json.load(handle)
                if preset_data.get("dataset_id") in {dataset_id, safe_dataset}:
                    preset_path.unlink(missing_ok=True)
            except Exception:
                continue
    except Exception:
        pass
CLIP_DATASET_JOBS: Dict[str, ClipDatasetUploadJob] = {}
CLIP_DATASET_JOBS_LOCK = threading.Lock()
QWEN_DATASET_JOBS: Dict[str, QwenDatasetUploadJob] = {}
QWEN_DATASET_JOBS_LOCK = threading.Lock()

MAX_JOB_LOGS = 250
MAX_QWEN_METRIC_POINTS: Optional[int] = None


def _job_log(job: ClipTrainingJob, message: str) -> None:
    entry = {"timestamp": time.time(), "message": message}
    job.logs.append(entry)
    if len(job.logs) > MAX_JOB_LOGS:
        job.logs[:] = job.logs[-MAX_JOB_LOGS:]
    job.updated_at = time.time()
    try:
        logger.info("[clip-train %s] %s", job.job_id[:8], message)
    except Exception:  # noqa: BLE001 - logging failures should never break workflow
        pass


def _clip_job_append_metric(job: ClipTrainingJob, metric: Dict[str, Any]) -> None:
    if not metric:
        return
    job.metrics.append(metric)
    if len(job.metrics) > 2000:
        job.metrics[:] = job.metrics[-2000:]
    job.updated_at = time.time()


def _job_update(job: ClipTrainingJob, *, status: Optional[str] = None, message: Optional[str] = None,
                progress: Optional[float] = None, error: Optional[str] = None,
                artifacts: Optional[Dict[str, Any]] = None) -> None:
    if status is not None:
        job.status = status
    if message is not None:
        if message != job.message:
            job.message = message
            _job_log(job, message)
        else:
            job.message = message
    if progress is not None:
        job.progress = max(0.0, min(1.0, progress))
    if error is not None:
        job.error = error
    if artifacts is not None:
        job.artifacts = artifacts


def _qwen_job_log(job: QwenTrainingJob, message: str) -> None:
    entry = {"timestamp": time.time(), "message": message}
    job.logs.append(entry)
    if len(job.logs) > MAX_JOB_LOGS:
        job.logs[:] = job.logs[-MAX_JOB_LOGS:]
    job.updated_at = time.time()
    try:
        logger.info("[qwen-train %s] %s", job.job_id[:8], message)
    except Exception:
        pass


def _qwen_job_update(
    job: QwenTrainingJob,
    *,
    status: Optional[str] = None,
    message: Optional[str] = None,
    progress: Optional[float] = None,
    error: Optional[str] = None,
    result: Optional[Dict[str, Any]] = None,
    log_message: bool = True,
) -> None:
    if status is not None:
        job.status = status
    if message is not None:
        if message != job.message:
            job.message = message
            if log_message:
                _qwen_job_log(job, message)
        else:
            job.message = message
    if progress is not None:
        job.progress = max(0.0, min(1.0, progress))
    if error is not None:
        job.error = error
    if result is not None:
        job.result = result
    job.updated_at = time.time()


def _serialize_job(job: ClipTrainingJob) -> Dict[str, Any]:
    return {
        "job_id": job.job_id,
        "status": job.status,
        "progress": job.progress,
        "message": job.message,
        "logs": job.logs,
        "metrics": job.metrics,
        "artifacts": job.artifacts,
        "error": job.error,
        "created_at": job.created_at,
        "updated_at": job.updated_at,
    }


def _serialize_qwen_job(job: QwenTrainingJob) -> Dict[str, Any]:
    return {
        "job_id": job.job_id,
        "status": job.status,
        "progress": job.progress,
        "message": job.message,
        "logs": job.logs,
        "config": job.config,
        "metrics": job.metrics,
        "result": job.result,
        "error": job.error,
        "created_at": job.created_at,
        "updated_at": job.updated_at,
    }


def _sam3_job_log(job: Sam3TrainingJob, message: str) -> None:
    job.log_seq += 1
    entry = {"timestamp": time.time(), "message": message, "seq": job.log_seq}
    job.logs.append(entry)
    if len(job.logs) > SAM3_MAX_LOG_LINES:
        job.logs[:] = job.logs[-SAM3_MAX_LOG_LINES:]
    job.updated_at = time.time()
    try:
        logger.info("[sam3-train %s] %s", job.job_id[:8], message)
    except Exception:
        pass


def _sam3_job_append_metric(job: Sam3TrainingJob, metric: Dict[str, Any]) -> None:
    if not metric:
        return
    job.metrics.append(metric)
    if SAM3_MAX_METRIC_POINTS and len(job.metrics) > SAM3_MAX_METRIC_POINTS:
        job.metrics[:] = job.metrics[-SAM3_MAX_METRIC_POINTS :]
    job.updated_at = time.time()


def _sam3_job_update(
    job: Sam3TrainingJob,
    *,
    status: Optional[str] = None,
    message: Optional[str] = None,
    progress: Optional[float] = None,
    error: Optional[str] = None,
    result: Optional[Dict[str, Any]] = None,
    log_message: bool = True,
) -> None:
    if status is not None:
        job.status = status
    if message is not None:
        if message != job.message:
            job.message = message
            if log_message:
                _sam3_job_log(job, message)
        else:
            job.message = message
    if progress is not None:
        job.progress = max(0.0, min(1.0, progress))
    if error is not None:
        job.error = error
    if result is not None:
        job.result = result
    job.updated_at = time.time()


def _serialize_sam3_job(job: Sam3TrainingJob) -> Dict[str, Any]:
    return {
        "job_id": job.job_id,
        "status": job.status,
        "progress": job.progress,
        "message": job.message,
        "logs": job.logs,
        "metrics": job.metrics,
        "result": job.result,
        "error": job.error,
        "created_at": job.created_at,
        "updated_at": job.updated_at,
    }


def _serialize_yolo_job(job: YoloTrainingJob) -> Dict[str, Any]:
    return {
        "job_id": job.job_id,
        "status": job.status,
        "progress": job.progress,
        "message": job.message,
        "config": job.config,
        "logs": job.logs,
        "metrics": job.metrics,
        "result": job.result,
        "error": job.error,
        "created_at": job.created_at,
        "updated_at": job.updated_at,
    }


def _yolo_job_update(
    job: YoloTrainingJob,
    *,
    status: Optional[str] = None,
    message: Optional[str] = None,
    progress: Optional[float] = None,
    error: Optional[str] = None,
    result: Optional[Dict[str, Any]] = None,
) -> None:
    if status is not None:
        job.status = status
    if message is not None:
        job.message = message
    if progress is not None:
        job.progress = max(0.0, min(1.0, progress))
    if error is not None:
        job.error = error
    if result is not None:
        job.result = result
    job.updated_at = time.time()


def _yolo_job_log(job: YoloTrainingJob, message: str) -> None:
    entry = {"timestamp": time.time(), "message": message}
    job.logs.append(entry)
    if len(job.logs) > YOLO_MAX_LOG_LINES:
        job.logs[:] = job.logs[-YOLO_MAX_LOG_LINES:]
    job.updated_at = time.time()
    try:
        logger.info("[yolo-train %s] %s", job.job_id[:8], message)
    except Exception:
        pass


def _yolo_job_append_metric(job: YoloTrainingJob, metric: Dict[str, Any]) -> None:
    if not metric:
        return
    job.metrics.append(metric)
    if len(job.metrics) > 2000:
        job.metrics[:] = job.metrics[-2000:]
    job.updated_at = time.time()


def _serialize_rfdetr_job(job: RfDetrTrainingJob) -> Dict[str, Any]:
    return {
        "job_id": job.job_id,
        "status": job.status,
        "progress": job.progress,
        "message": job.message,
        "config": job.config,
        "logs": job.logs,
        "metrics": job.metrics,
        "result": job.result,
        "error": job.error,
        "created_at": job.created_at,
        "updated_at": job.updated_at,
    }


def _rfdetr_job_update(
    job: RfDetrTrainingJob,
    *,
    status: Optional[str] = None,
    message: Optional[str] = None,
    progress: Optional[float] = None,
    error: Optional[str] = None,
    result: Optional[Dict[str, Any]] = None,
) -> None:
    if status is not None:
        job.status = status
    if message is not None:
        job.message = message
    if progress is not None:
        job.progress = max(0.0, min(1.0, progress))
    if error is not None:
        job.error = error
    if result is not None:
        job.result = result
    job.updated_at = time.time()


def _rfdetr_job_log(job: RfDetrTrainingJob, message: str) -> None:
    entry = {"timestamp": time.time(), "message": message}
    job.logs.append(entry)
    if len(job.logs) > MAX_JOB_LOGS:
        job.logs[:] = job.logs[-MAX_JOB_LOGS:]
    job.updated_at = time.time()
    try:
        logger.info("[rfdetr-train %s] %s", job.job_id[:8], message)
    except Exception:
        pass


def _rfdetr_job_append_metric(job: RfDetrTrainingJob, metric: Dict[str, Any]) -> None:
    if not metric:
        return
    job.metrics.append(metric)
    if len(job.metrics) > 2000:
        job.metrics[:] = job.metrics[-2000:]
    job.updated_at = time.time()


def _seg_job_log(job: SegmentationBuildJob, message: str) -> None:
    entry = {"timestamp": time.time(), "message": message}
    job.logs.append(entry)
    if len(job.logs) > MAX_JOB_LOGS:
        job.logs[:] = job.logs[-MAX_JOB_LOGS:]
    job.updated_at = time.time()
    try:
        logger.info("[seg-build %s] %s", job.job_id[:8], message)
    except Exception:
        pass


def _seg_job_update(
    job: SegmentationBuildJob,
    *,
    status: Optional[str] = None,
    message: Optional[str] = None,
    progress: Optional[float] = None,
    error: Optional[str] = None,
    result: Optional[Dict[str, Any]] = None,
    log_message: bool = True,
) -> None:
    if status is not None:
        job.status = status
    if message is not None:
        if message != job.message:
            job.message = message
            if log_message:
                _seg_job_log(job, message)
        else:
            job.message = message
    if progress is not None:
        job.progress = max(0.0, min(1.0, progress))
    if error is not None:
        job.error = error
    if result is not None:
        job.result = result
    job.updated_at = time.time()


def _serialize_seg_job(job: SegmentationBuildJob) -> Dict[str, Any]:
    return {
        "job_id": job.job_id,
        "status": job.status,
        "progress": job.progress,
        "message": job.message,
        "logs": job.logs,
        "config": job.config,
        "result": job.result,
        "error": job.error,
        "created_at": job.created_at,
        "updated_at": job.updated_at,
    }


def _log_qwen_get_request(endpoint: str, jobs: Sequence[QwenTrainingJob]) -> None:
    try:
        if not jobs:
            logger.info("[qwen-train] GET %s -> 0 jobs", endpoint)
            return
        for job in jobs:
            config = job.config or {}
            tracked_fields = {
                "accelerator": config.get("accelerator"),
                "devices": config.get("devices"),
                "batch_size": config.get("batch_size"),
                "accumulate_grad_batches": config.get("accumulate_grad_batches"),
            }
            logger.info(
                "[qwen-train %s] GET %s -> status=%s message=%s config=%s",
                job.job_id[:8],
                endpoint,
                job.status,
                job.message,
                json.dumps(tracked_fields, ensure_ascii=False),
            )
    except Exception:  # noqa: BLE001
        logger.exception("Failed to log Qwen GET request for %s", endpoint)


def _coerce_metric_value(value: Any) -> Any:
    if value is None:
        return None
    if isinstance(value, (str, int, float, bool)):
        return value
    if isinstance(value, dict):
        return {str(key): _coerce_metric_value(val) for key, val in value.items()}
    if isinstance(value, (list, tuple)):
        return [_coerce_metric_value(item) for item in value]
    try:
        return float(value)
    except (TypeError, ValueError):
        return str(value)


def _qwen_job_append_metric(job: QwenTrainingJob, metric: Dict[str, Any]) -> None:
    if not metric:
        return
    sanitized = {str(key): _coerce_metric_value(val) for key, val in metric.items()}
    job.metrics.append(sanitized)
    limit = MAX_QWEN_METRIC_POINTS
    if isinstance(limit, int) and limit > 0 and len(job.metrics) > limit:
        job.metrics[:] = job.metrics[-limit:]
    job.updated_at = time.time()


def _summarize_qwen_metric(metric: Dict[str, Any]) -> str:
    phase = (metric.get("phase") or "").lower()
    epoch = metric.get("epoch")
    total_epochs = metric.get("total_epochs")
    parts: List[str] = []
    if isinstance(epoch, (int, float)):
        if isinstance(total_epochs, (int, float)) and total_epochs:
            parts.append(f"Epoch {int(epoch)}/{int(total_epochs)}")
        else:
            parts.append(f"Epoch {int(epoch)}")
    if phase == "train":
        batch = metric.get("batch")
        batches_per_epoch = metric.get("batches_per_epoch")
        if isinstance(batch, (int, float)) and isinstance(batches_per_epoch, (int, float)) and batches_per_epoch:
            parts.append(f"Batch {int(batch)}/{int(batches_per_epoch)}")
        train_loss = metric.get("train_loss")
        if isinstance(train_loss, (int, float)):
            parts.append(f"Loss {float(train_loss):.4f}")
    elif phase == "val":
        value = metric.get("value")
        metric_name = metric.get("metric") or "validation"
        if isinstance(value, (int, float)):
            parts.append(f"{metric_name} {float(value):.4f}")
    if not parts:
        return "Training in progress ..."
    return " • ".join(parts)


def _write_qwen_metadata(meta_path: Path, metadata: Dict[str, Any]) -> None:
    try:
        with meta_path.open("w", encoding="utf-8") as handle:
            json.dump(metadata, handle, ensure_ascii=False, indent=2)
    except Exception as exc:  # noqa: BLE001
        logger.warning("Failed to write Qwen metadata for %s: %s", meta_path.parent, exc)


def _load_json_metadata(path: Path) -> Optional[Dict[str, Any]]:
    if not path.exists():
        return None
    try:
        with path.open("r", encoding="utf-8") as handle:
            data = json.load(handle)
            if isinstance(data, dict):
                return data
    except Exception as exc:  # noqa: BLE001
        logger.warning("Failed to read metadata file %s: %s", path, exc)
    return None


def _ensure_qwen_dataset_signature(dataset_dir: Path, metadata: Dict[str, Any]) -> Tuple[Dict[str, Any], str]:
    signature = metadata.get("signature")
    if signature:
        return metadata, str(signature)
    signature = _compute_dir_signature(dataset_dir)
    metadata["signature"] = signature
    _persist_qwen_dataset_metadata(dataset_dir, metadata)
    return metadata, signature


def _find_qwen_dataset_by_signature(signature: str) -> Optional[Path]:
    if not signature:
        return None
    for path in QWEN_DATASET_ROOT.iterdir():
        if not path.is_dir():
            continue
        meta = _load_qwen_dataset_metadata(path)
        if not meta:
            continue
        _, sig = _ensure_qwen_dataset_signature(path, meta)
        if sig == signature:
            return path
    return None


def _load_registry_dataset_metadata(dataset_dir: Path) -> Optional[Dict[str, Any]]:
    return _load_json_metadata(dataset_dir / DATASET_META_NAME)


def _persist_dataset_metadata(dataset_dir: Path, metadata: Dict[str, Any]) -> None:
    meta_path = dataset_dir / DATASET_META_NAME
    try:
        with meta_path.open("w", encoding="utf-8") as handle:
            json.dump(metadata, handle, ensure_ascii=False, indent=2)
    except Exception as exc:  # noqa: BLE001
        logger.warning("Failed to write dataset metadata for %s: %s", dataset_dir, exc)


def _coerce_dataset_metadata(dataset_dir: Path, raw_meta: Optional[Dict[str, Any]], source: str) -> Dict[str, Any]:
    meta = dict(raw_meta or {})
    updated = False
    if "id" not in meta:
        meta["id"] = dataset_dir.name
        updated = True
    if "label" not in meta:
        meta["label"] = meta["id"]
        updated = True
    dataset_type = meta.get("type") or meta.get("dataset_type") or "bbox"
    meta["type"] = dataset_type
    if "classes" not in meta:
        meta["classes"] = []
        updated = True
    if "context" not in meta and meta.get("dataset_context"):
        meta["context"] = meta.get("dataset_context") or ""
        updated = True
    if "created_at" not in meta:
        meta["created_at"] = dataset_dir.stat().st_mtime
        updated = True
    if "source" not in meta:
        meta["source"] = source
        updated = True
    signature = meta.get("signature")
    if not signature:
        signature = _compute_dir_signature(dataset_dir)
        meta["signature"] = signature
        updated = True
    if source == "registry" and updated:
        _persist_dataset_metadata(dataset_dir, meta)
    return meta


def _list_all_datasets(prefer_registry: bool = True) -> List[Dict[str, Any]]:
    """Collect datasets across registry, SAM3, and Qwen roots."""
    entries: List[Dict[str, Any]] = []
    seen: Dict[str, Tuple[int, str]] = {}
    sources = [
        ("registry", DATASET_REGISTRY_ROOT, _load_registry_dataset_metadata),
        ("sam3", SAM3_DATASET_ROOT, _load_sam3_dataset_metadata),
        ("qwen", QWEN_DATASET_ROOT, _load_qwen_dataset_metadata),
    ]
    for source, root, loader in sources:
        if not root.exists():
            continue
        for path in root.iterdir():
            if not path.is_dir():
                continue
            raw_meta = loader(path)
            if not raw_meta and source == "registry":
                raw_meta = _load_sam3_dataset_metadata(path) or _load_qwen_dataset_metadata(path)
            if not raw_meta:
                continue
            meta = _coerce_dataset_metadata(path, raw_meta, source)
            sam3_meta = _load_sam3_dataset_metadata(path) if source != "sam3" else meta
            coco_train = None
            coco_val = None
            coco_ready = False
            if sam3_meta:
                coco_train = sam3_meta.get("coco_train_json")
                coco_val = sam3_meta.get("coco_val_json")
                coco_ready = bool(coco_train and coco_val)
            labelmap_path = path / "labelmap.txt"
            train_images = path / "train" / "images"
            train_labels = path / "train" / "labels"
            root_images = path / "images"
            root_labels = path / "labels"
            yolo_images_dir: Optional[str] = None
            yolo_labels_dir: Optional[str] = None
            yolo_layout: Optional[str] = None
            if labelmap_path.exists():
                if train_images.exists() and train_labels.exists():
                    yolo_images_dir = str(train_images)
                    yolo_labels_dir = str(train_labels)
                    yolo_layout = "split"
                elif root_images.exists() and root_labels.exists():
                    yolo_images_dir = str(root_images)
                    yolo_labels_dir = str(root_labels)
                    yolo_layout = "flat"
            yolo_ready = bool(labelmap_path.exists() and yolo_images_dir and yolo_labels_dir)
            qwen_meta = _load_qwen_dataset_metadata(path)
            qwen_ready = bool(
                qwen_meta
                and (path / "train" / "annotations.jsonl").exists()
                and (path / "val" / "annotations.jsonl").exists()
            )
            if not yolo_ready:
                try:
                    coco_exists = (path / "train" / "_annotations.coco.json").exists() or (path / "val" / "_annotations.coco.json").exists()
                    if not coco_exists and qwen_ready:
                        _convert_qwen_dataset_to_coco(path)
                        coco_exists = True
                    if coco_exists:
                        _convert_coco_dataset_to_yolo(path)
                        labelmap_path = path / "labelmap.txt"
                        if labelmap_path.exists():
                            if train_images.exists() and train_labels.exists():
                                yolo_images_dir = str(train_images)
                                yolo_labels_dir = str(train_labels)
                                yolo_layout = "split"
                            elif root_images.exists() and root_labels.exists():
                                yolo_images_dir = str(root_images)
                                yolo_labels_dir = str(root_labels)
                                yolo_layout = "flat"
                            yolo_ready = bool(labelmap_path.exists() and yolo_images_dir and yolo_labels_dir)
                            if yolo_ready and not meta.get("classes"):
                                try:
                                    with labelmap_path.open("r", encoding="utf-8") as handle:
                                        meta["classes"] = [line.strip() for line in handle if line.strip()]
                                except Exception:
                                    pass
                except Exception as exc:  # noqa: BLE001
                    logger.warning("Failed to auto-convert COCO to YOLO for %s: %s", path, exc)
            dataset_format = "unknown"
            dataset_type = meta.get("type", "bbox")
            if yolo_ready:
                dataset_format = "yolo"
            elif qwen_ready:
                dataset_format = "qwen"
            yolo_seg_ready = False
            if yolo_ready and yolo_labels_dir:
                yolo_seg_ready = _yolo_labels_have_polygons(Path(yolo_labels_dir))
                if yolo_seg_ready and dataset_type != "seg":
                    dataset_type = "seg"
            signature = meta.get("signature") or ""
            caption_count, caption_dir_present = _count_caption_labels(path)
            image_total = meta.get("image_count")
            if not image_total:
                image_total = _count_dataset_images(path)
            caption_percent = None
            if image_total and image_total > 0:
                caption_percent = (caption_count / image_total) * 100.0
            key = signature or meta["id"]
            entry = {
                "id": meta.get("id") or path.name,
                "label": meta.get("label") or path.name,
                "dataset_root": str(path),
                "created_at": meta.get("created_at") or path.stat().st_mtime,
                "image_count": meta.get("image_count"),
                "train_count": meta.get("train_count"),
                "val_count": meta.get("val_count"),
                "classes": meta.get("classes", []),
                "context": meta.get("context", "") or meta.get("dataset_context", ""),
                "signature": signature,
                "source": meta.get("source") or source,
                "type": dataset_type,
                "coco_ready": coco_ready,
                "coco_seg_ready": bool(coco_ready and dataset_type == "seg"),
                "coco_train_json": coco_train,
                "coco_val_json": coco_val,
                "format": dataset_format,
                "yolo_ready": yolo_ready,
                "yolo_seg_ready": yolo_seg_ready,
                "yolo_images_dir": yolo_images_dir,
                "yolo_labels_dir": yolo_labels_dir,
                "yolo_labelmap_path": str(labelmap_path) if labelmap_path.exists() else None,
                "yolo_layout": yolo_layout,
                "qwen_ready": qwen_ready,
                "qwen_train_count": qwen_meta.get("train_count") if qwen_meta else None,
                "qwen_val_count": qwen_meta.get("val_count") if qwen_meta else None,
                "caption_count": caption_count,
                "caption_dir": caption_dir_present,
                "caption_percent": caption_percent,
                "caption_total": image_total,
            }
            existing = seen.get(key)
            if existing is not None:
                existing_idx, existing_origin = existing
                if prefer_registry:
                    if existing_origin == "registry":
                        continue
                    if source == "registry":
                        entries[existing_idx] = entry
                        seen[key] = (existing_idx, source)
                        continue
                continue
            seen[key] = (len(entries), source)
            entries.append(entry)
    entries.sort(key=lambda item: item.get("created_at") or 0, reverse=True)
    return entries


def _load_sam3_dataset_metadata(dataset_dir: Path) -> Optional[Dict[str, Any]]:
    meta_path = dataset_dir / SAM3_DATASET_META_NAME
    data = _load_json_metadata(meta_path)
    if not data:
        return None
    # Backfill defaults for older datasets.
    updated = False
    if "id" not in data:
        data["id"] = dataset_dir.name
        updated = True
    if "type" not in data:
        data["type"] = "bbox"
        updated = True
    if updated:
        _persist_sam3_dataset_metadata(dataset_dir, data)
    return data


def _persist_sam3_dataset_metadata(dataset_dir: Path, metadata: Dict[str, Any]) -> None:
    meta_path = dataset_dir / SAM3_DATASET_META_NAME
    try:
        with meta_path.open("w", encoding="utf-8") as handle:
            json.dump(metadata, handle, ensure_ascii=False, indent=2)
    except Exception as exc:  # noqa: BLE001
        logger.warning("Failed to write SAM3 dataset metadata for %s: %s", dataset_dir, exc)


def _dir_size_bytes(path: Path) -> int:
    if not path.exists():
        return 0
    total = 0
    for root, _, files in os.walk(path):
        for name in files:
            try:
                total += (Path(root) / name).stat().st_size
            except Exception:
                continue
    return total


def _active_run_paths_for_variant(variant: str) -> set[Path]:
    paths: set[Path] = set()
    with SAM3_TRAINING_JOBS_LOCK:
        jobs = list(SAM3_TRAINING_JOBS.values())
    for job in jobs:
        if job.status not in {"running", "queued", "cancelling"}:
            continue
        exp_dir = None
        try:
            exp_dir = job.config.get("paths", {}).get("experiment_log_dir")
        except Exception:
            exp_dir = None
        if exp_dir:
            try:
                paths.add(Path(exp_dir).resolve())
            except Exception:
                continue
    return paths


def _describe_run_dir(run_dir: Path, variant: str, active_paths: set[Path]) -> Dict[str, Any]:
    checkpoints_dir = run_dir / "checkpoints"
    logs_dir = run_dir / "logs"
    tensorboard_dir = run_dir / "tensorboard"
    dumps_dir = run_dir / "dumps"
    marker_path = run_dir / ".promoted"
    promoted = False
    promoted_at: Optional[float] = None
    if marker_path.exists():
        promoted = True
        try:
            meta = json.loads(marker_path.read_text())
            promoted_at = meta.get("timestamp")
        except Exception:
            promoted_at = None
    checkpoints: List[Dict[str, Any]] = []
    if checkpoints_dir.exists():
        for ckpt in sorted(checkpoints_dir.iterdir(), key=lambda p: p.stat().st_mtime if p.exists() else 0, reverse=True):
            if ckpt.is_file():
                try:
                    stat = ckpt.stat()
                    checkpoints.append(
                        {
                            "file": ckpt.name,
                            "path": str(ckpt),
                            "size_bytes": stat.st_size,
                            "updated_at": stat.st_mtime,
                        }
                    )
                except Exception:
                    continue
    try:
        dir_stat = run_dir.stat()
        created_at = dir_stat.st_ctime
        updated_at = dir_stat.st_mtime
    except Exception:
        created_at = time.time()
        updated_at = created_at
    entry = {
        "id": run_dir.name,
        "variant": variant,
        "path": str(run_dir),
        "created_at": created_at,
        "updated_at": updated_at,
        "size_bytes": _dir_size_bytes(run_dir),
        "checkpoints_size_bytes": _dir_size_bytes(checkpoints_dir),
        "logs_size_bytes": _dir_size_bytes(logs_dir),
        "tensorboard_size_bytes": _dir_size_bytes(tensorboard_dir),
        "dumps_size_bytes": _dir_size_bytes(dumps_dir),
        "checkpoints": checkpoints,
        "active": run_dir.resolve() in active_paths,
        "promoted": promoted,
        "promoted_at": promoted_at,
    }
    return entry


def _list_sam3_runs(variant: str) -> List[Dict[str, Any]]:
    root = SAM3_JOB_ROOT
    if not root.exists():
        return []
    active_paths = _active_run_paths_for_variant(variant)
    runs: List[Dict[str, Any]] = []
    for child in sorted(root.iterdir(), key=lambda p: p.stat().st_mtime if p.exists() else 0, reverse=True):
        if not child.is_dir():
            continue
        # Skip dataset folder and other non-run directories under the root
        if variant == "sam3" and child.resolve() == SAM3_DATASET_ROOT.resolve():
            continue
        if child.name.lower() == "datasets":
            continue
        try:
            runs.append(_describe_run_dir(child, variant, active_paths))
        except Exception:
            continue
    return runs


def _run_dir_for_request(run_id: str, variant: str) -> Path:
    root = SAM3_JOB_ROOT
    candidate = (root / run_id).resolve()
    if not str(candidate).startswith(str(root.resolve())):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="invalid_run_id")
    if not candidate.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="sam3_run_not_found")
    return candidate


def _delete_run_scope(run_dir: Path, scope: str) -> Tuple[List[str], int]:
    targets: List[Path] = []
    if scope == "all":
        targets.append(run_dir)
    else:
        mapping = {
            "checkpoints": run_dir / "checkpoints",
            "logs": run_dir / "logs",
            "tensorboard": run_dir / "tensorboard",
            "dumps": run_dir / "dumps",
        }
        target = mapping.get(scope)
        if target:
            targets.append(target)
    deleted: List[str] = []
    freed = 0
    for target in targets:
        if not target.exists():
            continue
        freed += _dir_size_bytes(target)
        try:
            shutil.rmtree(target)
        except Exception:
            continue
        deleted.append(str(target))
    return deleted, freed


def _sanitize_yolo_run_id(raw: str) -> str:
    cleaned = re.sub(r"[^a-zA-Z0-9._-]+", "-", (raw or "").strip()).strip("-_.")
    if cleaned:
        return cleaned
    return uuid.uuid4().hex[:12]


def _yolo_run_dir(run_id: str, *, create: bool = False) -> Path:
    safe_id = _sanitize_yolo_run_id(run_id)
    candidate = (YOLO_JOB_ROOT / safe_id).resolve()
    if not str(candidate).startswith(str(YOLO_JOB_ROOT.resolve())):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="invalid_run_id")
    if create:
        candidate.mkdir(parents=True, exist_ok=True)
    return candidate


def _yolo_load_run_meta(run_dir: Path) -> Dict[str, Any]:
    meta_path = run_dir / YOLO_RUN_META_NAME
    if not meta_path.exists():
        return {}
    try:
        return json.loads(meta_path.read_text())
    except Exception:
        return {}


def _yolo_write_run_meta(run_dir: Path, meta: Dict[str, Any]) -> None:
    run_dir.mkdir(parents=True, exist_ok=True)
    payload = dict(meta or {})
    now = time.time()
    payload.setdefault("created_at", now)
    payload["updated_at"] = now
    (run_dir / YOLO_RUN_META_NAME).write_text(json.dumps(payload, indent=2, sort_keys=True))


def _yolo_prune_run_dir(run_dir: Path, keep_files: Optional[set[str]] = None) -> Dict[str, Any]:
    kept: List[str] = []
    deleted: List[str] = []
    freed = 0
    if not run_dir.exists():
        return {"kept": kept, "deleted": deleted, "freed_bytes": freed}
    keep = set(keep_files or YOLO_KEEP_FILES)
    for child in run_dir.iterdir():
        if child.is_file() and child.suffix == ".yaml":
            keep.add(child.name)
    weights_dir = run_dir / "weights"
    if weights_dir.exists():
        best_path = weights_dir / "best.pt"
        target_best = run_dir / "best.pt"
        if best_path.exists() and not target_best.exists():
            try:
                shutil.copy2(best_path, target_best)
            except Exception:
                pass
    for child in list(run_dir.iterdir()):
        if child.name in keep:
            kept.append(child.name)
            continue
        try:
            if child.is_dir():
                freed += _dir_size_bytes(child)
                shutil.rmtree(child)
            else:
                freed += child.stat().st_size
                child.unlink()
            deleted.append(child.name)
        except Exception:
            continue
    return {"kept": kept, "deleted": deleted, "freed_bytes": freed}


def _sanitize_rfdetr_run_id(raw: str) -> str:
    return _sanitize_yolo_run_id(raw)


def _rfdetr_run_dir(run_id: str, *, create: bool = False) -> Path:
    safe_id = _sanitize_rfdetr_run_id(run_id)
    candidate = (RFDETR_JOB_ROOT / safe_id).resolve()
    if not str(candidate).startswith(str(RFDETR_JOB_ROOT.resolve())):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="invalid_run_id")
    if create:
        candidate.mkdir(parents=True, exist_ok=True)
    return candidate


def _rfdetr_load_run_meta(run_dir: Path) -> Dict[str, Any]:
    meta_path = run_dir / RFDETR_RUN_META_NAME
    if not meta_path.exists():
        return {}
    try:
        return json.loads(meta_path.read_text())
    except Exception:
        return {}


def _rfdetr_write_run_meta(run_dir: Path, meta: Dict[str, Any]) -> None:
    run_dir.mkdir(parents=True, exist_ok=True)
    payload = dict(meta or {})
    now = time.time()
    payload.setdefault("created_at", now)
    payload["updated_at"] = now
    (run_dir / RFDETR_RUN_META_NAME).write_text(json.dumps(payload, indent=2, sort_keys=True))


def _rfdetr_prune_run_dir(run_dir: Path, keep_files: Optional[set[str]] = None) -> Dict[str, Any]:
    kept: List[str] = []
    deleted: List[str] = []
    freed = 0
    if not run_dir.exists():
        return {"kept": kept, "deleted": deleted, "freed_bytes": freed}
    keep = set(keep_files or RFDETR_KEEP_FILES)
    for child in list(run_dir.iterdir()):
        if child.name in keep:
            kept.append(child.name)
            continue
        try:
            if child.is_dir():
                freed += _dir_size_bytes(child)
                shutil.rmtree(child)
            else:
                freed += child.stat().st_size
                child.unlink()
            deleted.append(child.name)
        except Exception:
            continue
    return {"kept": kept, "deleted": deleted, "freed_bytes": freed}


def _collect_yolo_artifacts(run_dir: Path) -> Dict[str, bool]:
    return {
        "best_pt": (run_dir / "best.pt").exists(),
        "metrics_json": (run_dir / "metrics.json").exists(),
        "metrics_series": (run_dir / "metrics_series.json").exists(),
        "results_csv": (run_dir / "results.csv").exists(),
        "args_yaml": (run_dir / "args.yaml").exists(),
        "labelmap": (run_dir / "labelmap.txt").exists(),
        "run_meta": (run_dir / YOLO_RUN_META_NAME).exists(),
    }


def _collect_rfdetr_artifacts(run_dir: Path) -> Dict[str, bool]:
    return {
        "best_regular": (run_dir / "checkpoint_best_regular.pth").exists(),
        "best_ema": (run_dir / "checkpoint_best_ema.pth").exists(),
        "best_total": (run_dir / "checkpoint_best_total.pth").exists(),
        "best_optimized": (run_dir / "checkpoint_best_optimized.pt").exists(),
        "results_json": (run_dir / "results.json").exists(),
        "metrics_series": (run_dir / "metrics_series.json").exists(),
        "log_txt": (run_dir / "log.txt").exists(),
        "labelmap": (run_dir / "labelmap.txt").exists(),
        "run_meta": (run_dir / RFDETR_RUN_META_NAME).exists(),
    }


def _read_labelmap_lines(path: Path) -> List[str]:
    if not path.exists():
        return []
    try:
        lines = [line.strip() for line in path.read_text().splitlines()]
        return [line for line in lines if line]
    except Exception:
        return []


def _flatten_metrics(obj: Any, prefix: str = "", out: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    if out is None:
        out = {}
    if isinstance(obj, dict):
        for key, value in obj.items():
            next_prefix = f"{prefix}/{key}" if prefix else str(key)
            _flatten_metrics(value, next_prefix, out)
    else:
        out[prefix] = obj
    return out


def _lookup_metric(flat: Dict[str, Any], keys: List[str]) -> Optional[float]:
    if not flat:
        return None
    lowered = {str(k).lower(): v for k, v in flat.items()}
    for key in keys:
        if key in flat:
            value = flat[key]
        else:
            value = lowered.get(key.lower())
        if value is None:
            continue
        try:
            return float(value)
        except Exception:
            continue
    return None


def _read_csv_last_row(path: Path) -> Optional[Dict[str, str]]:
    if not path.exists():
        return None
    try:
        with path.open(newline="", encoding="utf-8") as handle:
            reader = csv.DictReader(handle)
            last_row = None
            for row in reader:
                last_row = row
            return last_row
    except Exception:
        return None


def _yolo_metrics_summary(run_dir: Path) -> Dict[str, float]:
    summary: Dict[str, float] = {}
    metrics_path = run_dir / "metrics.json"
    if metrics_path.exists():
        try:
            data = json.loads(metrics_path.read_text())
            flat = _flatten_metrics(data)
            summary["map50_95"] = _lookup_metric(flat, ["metrics/mAP50-95(B)", "metrics/mAP50-95", "map50-95"])
            summary["map50"] = _lookup_metric(flat, ["metrics/mAP50(B)", "metrics/mAP50", "map50"])
            summary["precision"] = _lookup_metric(flat, ["metrics/precision(B)", "metrics/precision", "precision"])
            summary["recall"] = _lookup_metric(flat, ["metrics/recall(B)", "metrics/recall", "recall"])
        except Exception:
            pass
    if any(value is not None for value in summary.values()):
        return {k: v for k, v in summary.items() if v is not None}
    csv_path = run_dir / "results.csv"
    last_row = _read_csv_last_row(csv_path)
    if not last_row:
        return {}
    def _csv_value(name_variants: List[str]) -> Optional[float]:
        for key in name_variants:
            if key in last_row:
                try:
                    return float(last_row[key])
                except Exception:
                    return None
            for col, val in last_row.items():
                if col.strip().lower() == key.strip().lower():
                    try:
                        return float(val)
                    except Exception:
                        return None
        return None
    return {
        "map50_95": _csv_value(["metrics/mAP50-95(B)", "metrics/mAP50-95"]),
        "map50": _csv_value(["metrics/mAP50(B)", "metrics/mAP50"]),
        "precision": _csv_value(["metrics/precision(B)", "metrics/precision"]),
        "recall": _csv_value(["metrics/recall(B)", "metrics/recall"]),
    }


def _rfdetr_metrics_summary(run_dir: Path) -> Dict[str, float]:
    metrics_path = run_dir / "results.json"
    if not metrics_path.exists():
        return {}
    try:
        data = json.loads(metrics_path.read_text())
    except Exception:
        return {}
    flat = _flatten_metrics(data)
    return {
        "map": _lookup_metric(flat, ["coco/bbox_mAP", "bbox_mAP", "metrics/bbox_mAP", "map"]),
        "map50": _lookup_metric(flat, ["coco/bbox_mAP50", "bbox_mAP50", "metrics/bbox_mAP50", "map50"]),
        "map75": _lookup_metric(flat, ["coco/bbox_mAP75", "bbox_mAP75", "metrics/bbox_mAP75", "map75"]),
    }


def _clean_metric_summary(summary: Dict[str, Optional[float]]) -> Dict[str, float]:
    return {key: float(value) for key, value in summary.items() if value is not None}


def _list_yolo_runs() -> List[Dict[str, Any]]:
    runs: List[Dict[str, Any]] = []
    active = _load_yolo_active()
    active_id = active.get("run_id") if isinstance(active, dict) else None
    for entry in YOLO_JOB_ROOT.iterdir():
        if not entry.is_dir():
            continue
        if entry == YOLO_DATASET_CACHE_ROOT:
            continue
        meta_path = entry / YOLO_RUN_META_NAME
        if not meta_path.exists():
            continue
        meta = _yolo_load_run_meta(entry)
        run_id = meta.get("job_id") or entry.name
        config = meta.get("config") or {}
        dataset = config.get("dataset") or {}
        run_name = config.get("run_name") or dataset.get("label") or dataset.get("id") or run_id
        created_at = meta.get("created_at")
        if not created_at:
            try:
                created_at = entry.stat().st_mtime
            except Exception:
                created_at = None
        runs.append(
            {
                "run_id": run_id,
                "run_name": run_name,
                "status": meta.get("status"),
                "message": meta.get("message"),
                "created_at": created_at,
                "updated_at": meta.get("updated_at"),
                "dataset_id": dataset.get("id") or dataset.get("dataset_id"),
                "dataset_label": dataset.get("label"),
                "artifacts": _collect_yolo_artifacts(entry),
                "is_active": bool(active_id and run_id == active_id),
            }
        )
    runs.sort(key=lambda item: item.get("created_at") or 0, reverse=True)
    return runs


def _list_rfdetr_runs() -> List[Dict[str, Any]]:
    runs: List[Dict[str, Any]] = []
    active = _load_rfdetr_active()
    active_id = active.get("run_id") if isinstance(active, dict) else None
    for entry in RFDETR_JOB_ROOT.iterdir():
        if not entry.is_dir():
            continue
        meta_path = entry / RFDETR_RUN_META_NAME
        if not meta_path.exists():
            continue
        meta = _rfdetr_load_run_meta(entry)
        run_id = meta.get("job_id") or entry.name
        config = meta.get("config") or {}
        dataset = config.get("dataset") or {}
        run_name = config.get("run_name") or dataset.get("label") or dataset.get("id") or run_id
        created_at = meta.get("created_at")
        if not created_at:
            try:
                created_at = entry.stat().st_mtime
            except Exception:
                created_at = None
        runs.append(
            {
                "run_id": run_id,
                "run_name": run_name,
                "status": meta.get("status"),
                "message": meta.get("message"),
                "created_at": created_at,
                "updated_at": meta.get("updated_at"),
                "dataset_id": dataset.get("id") or dataset.get("dataset_id"),
                "dataset_label": dataset.get("label"),
                "artifacts": _collect_rfdetr_artifacts(entry),
                "is_active": bool(active_id and run_id == active_id),
            }
        )
    runs.sort(key=lambda item: item.get("created_at") or 0, reverse=True)
    return runs


def _load_yolo_active() -> Dict[str, Any]:
    if not YOLO_ACTIVE_PATH.exists():
        return {}
    try:
        return json.loads(YOLO_ACTIVE_PATH.read_text())
    except Exception:
        return {}


def _save_yolo_active(payload: Dict[str, Any]) -> Dict[str, Any]:
    YOLO_ACTIVE_PATH.parent.mkdir(parents=True, exist_ok=True)
    data = dict(payload or {})
    data["updated_at"] = time.time()
    if "created_at" not in data:
        data["created_at"] = data["updated_at"]
    YOLO_ACTIVE_PATH.write_text(json.dumps(data, indent=2, sort_keys=True))
    return data


def _load_rfdetr_active() -> Dict[str, Any]:
    if not RFDETR_ACTIVE_PATH.exists():
        return {}
    try:
        return json.loads(RFDETR_ACTIVE_PATH.read_text())
    except Exception:
        return {}


def _save_rfdetr_active(payload: Dict[str, Any]) -> Dict[str, Any]:
    RFDETR_ACTIVE_PATH.parent.mkdir(parents=True, exist_ok=True)
    data = dict(payload or {})
    data["updated_at"] = time.time()
    if "created_at" not in data:
        data["created_at"] = data["updated_at"]
    RFDETR_ACTIVE_PATH.write_text(json.dumps(data, indent=2, sort_keys=True))
    return data


def _load_detector_default() -> Dict[str, Any]:
    if not DETECTOR_DEFAULT_PATH.exists():
        return {"mode": "rfdetr"}
    try:
        payload = json.loads(DETECTOR_DEFAULT_PATH.read_text())
        if isinstance(payload, dict):
            mode = str(payload.get("mode") or "").strip().lower()
            if mode in {"yolo", "rfdetr"}:
                return payload
    except Exception:
        pass
    return {"mode": "rfdetr"}


def _save_detector_default(payload: Dict[str, Any]) -> Dict[str, Any]:
    DETECTOR_DEFAULT_PATH.parent.mkdir(parents=True, exist_ok=True)
    data = dict(payload or {})
    mode = str(data.get("mode") or "").strip().lower()
    if mode not in {"yolo", "rfdetr"}:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="detector_mode_invalid")
    data["mode"] = mode
    data["updated_at"] = time.time()
    if "created_at" not in data:
        data["created_at"] = data["updated_at"]
    DETECTOR_DEFAULT_PATH.write_text(json.dumps(data, indent=2, sort_keys=True))
    return data


def _detect_yolo_layout(dataset_root: Path) -> Dict[str, Any]:
    labelmap_path = dataset_root / "labelmap.txt"
    train_images = dataset_root / "train" / "images"
    train_labels = dataset_root / "train" / "labels"
    root_images = dataset_root / "images"
    root_labels = dataset_root / "labels"
    yolo_images_dir: Optional[str] = None
    yolo_labels_dir: Optional[str] = None
    yolo_layout: Optional[str] = None
    if labelmap_path.exists():
        if train_images.exists() and train_labels.exists():
            yolo_images_dir = str(train_images)
            yolo_labels_dir = str(train_labels)
            yolo_layout = "split"
        elif root_images.exists() and root_labels.exists():
            yolo_images_dir = str(root_images)
            yolo_labels_dir = str(root_labels)
            yolo_layout = "flat"
    yolo_ready = bool(labelmap_path.exists() and yolo_images_dir and yolo_labels_dir)
    return {
        "yolo_ready": yolo_ready,
        "yolo_images_dir": yolo_images_dir,
        "yolo_labels_dir": yolo_labels_dir,
        "yolo_labelmap_path": str(labelmap_path) if labelmap_path.exists() else None,
        "yolo_layout": yolo_layout,
    }


def _yolo_labels_have_polygons(
    labels_dir: Optional[Path],
    *,
    max_files: int = 200,
    max_lines: int = 2000,
) -> bool:
    if not labels_dir or not labels_dir.exists():
        return False
    checked_files = 0
    checked_lines = 0
    for label_path in labels_dir.rglob("*.txt"):
        checked_files += 1
        if checked_files > max_files:
            break
        try:
            with label_path.open("r", encoding="utf-8", errors="ignore") as handle:
                for line in handle:
                    if checked_lines >= max_lines:
                        return False
                    stripped = line.strip()
                    if not stripped:
                        continue
                    checked_lines += 1
                    parts = stripped.split()
                    # class + 4 bbox coords is 5 tokens; polygons add more.
                    if len(parts) > 5:
                        return True
        except Exception:
            continue
    return False


def _resolve_dataset_entry(dataset_id: str) -> Optional[Dict[str, Any]]:
    cleaned = (dataset_id or "").strip()
    if not cleaned:
        return None
    for entry in _list_all_datasets():
        if cleaned in (entry.get("id"), entry.get("signature")):
            return entry
    return None


def _resolve_yolo_training_dataset(payload: YoloTrainRequest) -> Dict[str, Any]:
    task = (payload.task or "detect").lower().strip()
    dataset_id = (payload.dataset_id or "").strip()
    dataset_root: Optional[Path] = None
    entry: Optional[Dict[str, Any]] = None
    if dataset_id:
        entry = _resolve_dataset_entry(dataset_id)
        if entry and entry.get("dataset_root"):
            dataset_root = Path(entry["dataset_root"]).resolve()
        else:
            dataset_root = _resolve_sam3_or_qwen_dataset(dataset_id)
    elif payload.dataset_root:
        dataset_root = Path(payload.dataset_root).expanduser().resolve()
    if not dataset_root or not dataset_root.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="dataset_not_found")
    dataset_signature = None
    if entry and entry.get("signature"):
        dataset_signature = str(entry.get("signature"))
    if not dataset_signature:
        dataset_signature = _compute_dir_signature(dataset_root)
    safe_name = _sanitize_yolo_run_id(entry.get("id") if entry else dataset_root.name)
    cache_key = _stable_hash([safe_name, dataset_signature, task])[:12]
    cache_root = (YOLO_DATASET_CACHE_ROOT / f"{safe_name}_{cache_key}").resolve()
    cache_layout = _detect_yolo_layout(cache_root) if cache_root.exists() else None
    layout = _detect_yolo_layout(dataset_root)
    if cache_layout and cache_layout.get("yolo_ready"):
        prepared_root = cache_root
        yolo_ready = True
        yolo_images_dir = cache_layout.get("yolo_images_dir")
        yolo_labels_dir = cache_layout.get("yolo_labels_dir")
        yolo_labelmap_path = cache_layout.get("yolo_labelmap_path")
        yolo_layout = cache_layout.get("yolo_layout")
        source = "cache"
    else:
        prepared_root = dataset_root
        yolo_ready = bool(layout.get("yolo_ready"))
        yolo_images_dir = layout.get("yolo_images_dir")
        yolo_labels_dir = layout.get("yolo_labels_dir")
        yolo_labelmap_path = layout.get("yolo_labelmap_path")
        yolo_layout = layout.get("yolo_layout")
        source = "registry" if entry else "custom"
    yolo_seg_ready = False
    if yolo_ready and yolo_labels_dir:
        yolo_seg_ready = _yolo_labels_have_polygons(Path(yolo_labels_dir))
    return {
        "dataset_id": entry.get("id") if entry else dataset_root.name,
        "dataset_root": str(dataset_root),
        "prepared_root": str(prepared_root),
        "signature": dataset_signature,
        "task": task,
        "yolo_ready": yolo_ready,
        "yolo_seg_ready": yolo_seg_ready,
        "yolo_images_dir": yolo_images_dir,
        "yolo_labels_dir": yolo_labels_dir,
        "yolo_labelmap_path": yolo_labelmap_path,
        "yolo_layout": yolo_layout,
        "cache_root": str(cache_root),
        "cache_key": cache_key,
        "source": source,
    }


def _resolve_rfdetr_training_dataset(payload: RfDetrTrainRequest) -> Dict[str, Any]:
    task = (payload.task or "detect").lower().strip()
    dataset_id = (payload.dataset_id or "").strip()
    dataset_root: Optional[Path] = None
    entry: Optional[Dict[str, Any]] = None
    if dataset_id:
        entry = _resolve_dataset_entry(dataset_id)
        if entry and entry.get("dataset_root"):
            dataset_root = Path(entry["dataset_root"]).resolve()
        else:
            dataset_root = _resolve_sam3_or_qwen_dataset(dataset_id)
    elif payload.dataset_root:
        dataset_root = Path(payload.dataset_root).expanduser().resolve()
    if not dataset_root or not dataset_root.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="dataset_not_found")
    meta = _load_sam3_dataset_metadata(dataset_root) or {}
    coco_train = meta.get("coco_train_json")
    coco_val = meta.get("coco_val_json")
    coco_ready = bool(coco_train and coco_val)
    dataset_type = (entry.get("type") if entry else None) or meta.get("type", "bbox")
    yolo_layout = _detect_yolo_layout(dataset_root)
    yolo_seg_ready = False
    if yolo_layout.get("yolo_ready") and yolo_layout.get("yolo_labels_dir"):
        yolo_seg_ready = _yolo_labels_have_polygons(Path(yolo_layout["yolo_labels_dir"]))
        if yolo_seg_ready and dataset_type != "seg":
            dataset_type = "seg"
    if task == "segment" and dataset_type != "seg":
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="rfdetr_seg_requires_polygons")
    if not coco_ready:
        if entry and entry.get("yolo_ready"):
            meta = _convert_yolo_dataset_to_coco(dataset_root)
        elif entry and entry.get("qwen_ready"):
            meta = _convert_qwen_dataset_to_coco(dataset_root)
        else:
            # Attempt to infer from on-disk layout.
            layout = _detect_yolo_layout(dataset_root)
            if layout.get("yolo_ready"):
                meta = _convert_yolo_dataset_to_coco(dataset_root)
            elif _load_qwen_dataset_metadata(dataset_root):
                meta = _convert_qwen_dataset_to_coco(dataset_root)
            else:
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="dataset_not_ready")
        coco_train = meta.get("coco_train_json")
        coco_val = meta.get("coco_val_json")
        coco_ready = bool(coco_train and coco_val)
        dataset_type = meta.get("type", dataset_type)
    if not coco_ready:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="dataset_not_ready")
    if coco_train:
        _ensure_coco_supercategory(Path(coco_train))
    if coco_val:
        _ensure_coco_supercategory(Path(coco_val))
    dataset_label = entry.get("label") if entry else meta.get("label") or dataset_root.name
    return {
        "dataset_id": entry.get("id") if entry else dataset_root.name,
        "dataset_root": str(dataset_root),
        "dataset_label": dataset_label,
        "task": task,
        "coco_train_json": coco_train,
        "coco_val_json": coco_val,
        "type": dataset_type,
    }


def _yolo_resolve_split_paths(dataset_root: Path, layout: Optional[str]) -> Tuple[str, str]:
    if layout == "split":
        train_images = dataset_root / "train" / "images"
        val_images = dataset_root / "val" / "images"
        train_rel = str(train_images.relative_to(dataset_root))
        if val_images.exists():
            val_rel = str(val_images.relative_to(dataset_root))
        else:
            val_rel = train_rel
        return train_rel, val_rel
    images = dataset_root / "images"
    train_rel = str(images.relative_to(dataset_root))
    val_images = dataset_root / "val" / "images"
    if val_images.exists():
        val_rel = str(val_images.relative_to(dataset_root))
    else:
        val_rel = train_rel
    return train_rel, val_rel


def _yolo_load_labelmap(labelmap_path: Path) -> List[str]:
    try:
        return [line.strip() for line in labelmap_path.read_text().splitlines() if line.strip()]
    except Exception:
        return []


def _validate_yolo_label_ids(labels_dir: Path, label_count: int) -> None:
    if label_count <= 0:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="labelmap_empty")
    if not labels_dir.exists():
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="labels_dir_missing")
    for label_path in labels_dir.rglob("*.txt"):
        rel_name = None
        try:
            rel_name = str(label_path.relative_to(labels_dir))
        except Exception:
            rel_name = str(label_path.name)
        try:
            with label_path.open("r", encoding="utf-8", errors="ignore") as handle:
                for line_no, line in enumerate(handle, 1):
                    stripped = line.strip()
                    if not stripped:
                        continue
                    parts = stripped.split()
                    if not parts:
                        continue
                    try:
                        raw = float(parts[0])
                    except Exception:
                        raise HTTPException(
                            status_code=HTTP_400_BAD_REQUEST,
                            detail=f"labelmap_class_id_invalid:{rel_name}:{line_no}",
                        )
                    idx = int(raw)
                    if abs(raw - idx) > 1e-6:
                        raise HTTPException(
                            status_code=HTTP_400_BAD_REQUEST,
                            detail=f"labelmap_class_id_non_int:{rel_name}:{line_no}",
                        )
                    if idx < 0 or idx >= label_count:
                        raise HTTPException(
                            status_code=HTTP_400_BAD_REQUEST,
                            detail=f"labelmap_class_id_out_of_range:{idx}/{label_count}:{rel_name}:{line_no}",
                        )
        except HTTPException:
            raise
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(
                status_code=HTTP_400_BAD_REQUEST,
                detail=f"labelmap_label_read_failed:{rel_name}:{exc}",
            ) from exc


def _rfdetr_load_labelmap(dataset_root: Path, coco_train_json: Optional[str] = None) -> List[str]:
    labelmap_path = dataset_root / "labelmap.txt"
    if labelmap_path.exists():
        return _yolo_load_labelmap(labelmap_path)
    coco_path = Path(coco_train_json) if coco_train_json else None
    if coco_path and coco_path.exists():
        try:
            data = json.loads(coco_path.read_text())
            categories = data.get("categories", [])
            categories = [c for c in categories if isinstance(c, dict) and "id" in c and "name" in c]
            categories.sort(key=lambda c: int(c.get("id", 0)))
            return [str(c["name"]) for c in categories]
        except Exception:
            return []
    return []


def _rfdetr_variant_info(task: str, variant: Optional[str]) -> Dict[str, Any]:
    task_norm = (task or "detect").lower().strip()
    variant_norm = (variant or "").strip().lower()
    if task_norm == "segment":
        variant_norm = "rfdetr-seg-preview"
    if not variant_norm:
        variant_norm = "rfdetr-medium" if task_norm == "detect" else "rfdetr-seg-preview"
    variant_map = {entry["id"]: entry for entry in RFDETR_VARIANTS}
    info = variant_map.get(variant_norm)
    if not info:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="rfdetr_variant_unknown")
    if task_norm == "segment" and info.get("task") != "segment":
        variant_norm = "rfdetr-seg-preview"
        info = variant_map.get(variant_norm)
    return info or {}


def _rfdetr_best_checkpoint(run_dir: Path) -> Optional[str]:
    for name in ("checkpoint_best_total.pth", "checkpoint_best_ema.pth", "checkpoint_best_regular.pth"):
        path = run_dir / name
        if path.exists():
            return str(path)
    return None


def _rfdetr_parse_log_series(log_path: Path) -> List[Dict[str, Any]]:
    if not log_path.exists():
        return []
    series: List[Dict[str, Any]] = []
    for line in log_path.read_text().splitlines():
        if not line.strip():
            continue
        try:
            series.append(json.loads(line))
        except Exception:
            continue
    return series


def _rfdetr_sanitize_metric(metric: Dict[str, Any]) -> Dict[str, Any]:
    def _coerce(obj: Any) -> Any:
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, np.generic):
            try:
                return obj.item()
            except Exception:
                return float(obj)
        if isinstance(obj, Path):
            return str(obj)
        if isinstance(obj, set):
            return list(obj)
        return obj

    try:
        return json.loads(json.dumps(metric, default=_coerce))
    except Exception:
        return {}


def _rfdetr_normalize_aug_policy(raw: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    if not raw or not isinstance(raw, dict):
        return None
    def _clamp(value: Any, default: float = 0.0, maximum: float = 1.0) -> float:
        try:
            num = float(value)
        except Exception:
            num = default
        return max(0.0, min(maximum, num))

    policy = {
        "hsv_h": _clamp(raw.get("hsv_h"), 0.0, 0.5),
        "hsv_s": _clamp(raw.get("hsv_s"), 0.0, 1.0),
        "hsv_v": _clamp(raw.get("hsv_v"), 0.0, 1.0),
        "blur_prob": _clamp(raw.get("blur_prob"), 0.0, 1.0),
        "gray_prob": _clamp(raw.get("gray_prob"), 0.0, 1.0),
    }
    kernel = raw.get("blur_kernel")
    try:
        kernel = int(kernel)
    except Exception:
        kernel = 0
    if kernel and kernel % 2 == 0:
        kernel += 1
    policy["blur_kernel"] = max(0, kernel)
    if not any(
        [
            policy["hsv_h"],
            policy["hsv_s"],
            policy["hsv_v"],
            policy["blur_prob"],
            policy["gray_prob"],
        ]
    ):
        return None
    return policy


def _rfdetr_install_augmentations(policy: Optional[Dict[str, Any]]) -> Optional[Tuple[Any, Any]]:
    if not policy:
        return None
    try:
        import random as _random
        import torchvision.transforms as tvt
        import rfdetr.datasets.coco as coco_mod
    except Exception:
        return None

    class _ImageOnlyTransform:
        def __init__(self, transform, p: float = 1.0) -> None:
            self.transform = transform
            self.p = float(p)

        def __call__(self, img, target):
            if self.p < 1.0 and _random.random() > self.p:
                return img, target
            return self.transform(img), target

    aug_transforms = []
    hsv_h = float(policy.get("hsv_h") or 0.0)
    hsv_s = float(policy.get("hsv_s") or 0.0)
    hsv_v = float(policy.get("hsv_v") or 0.0)
    if hsv_h > 0 or hsv_s > 0 or hsv_v > 0:
        color_jitter = tvt.ColorJitter(
            brightness=hsv_v,
            contrast=hsv_v,
            saturation=hsv_s,
            hue=min(0.5, hsv_h),
        )
        aug_transforms.append(_ImageOnlyTransform(color_jitter, p=1.0))
    gray_prob = float(policy.get("gray_prob") or 0.0)
    if gray_prob > 0:
        aug_transforms.append(_ImageOnlyTransform(tvt.RandomGrayscale(p=1.0), p=gray_prob))
    blur_prob = float(policy.get("blur_prob") or 0.0)
    blur_kernel = int(policy.get("blur_kernel") or 0)
    if blur_prob > 0 and blur_kernel >= 3:
        blur_kernel = blur_kernel if blur_kernel % 2 == 1 else blur_kernel + 1
        blur = tvt.GaussianBlur(kernel_size=blur_kernel, sigma=(0.1, 2.0))
        aug_transforms.append(_ImageOnlyTransform(blur, p=blur_prob))

    if not aug_transforms:
        return None

    original_make = coco_mod.make_coco_transforms
    original_make_square = coco_mod.make_coco_transforms_square_div_64

    def _wrap_make(make_func):
        def _wrapped(*args, **kwargs):
            base = make_func(*args, **kwargs)
            try:
                if hasattr(base, "transforms") and isinstance(base.transforms, list):
                    insert_idx = max(0, len(base.transforms) - 1)
                    base.transforms[insert_idx:insert_idx] = list(aug_transforms)
            except Exception:
                pass
            return base
        return _wrapped

    coco_mod.make_coco_transforms = _wrap_make(original_make)
    coco_mod.make_coco_transforms_square_div_64 = _wrap_make(original_make_square)
    return (original_make, original_make_square)


def _rfdetr_restore_augmentations(restore: Optional[Tuple[Any, Any]]) -> None:
    if not restore:
        return
    try:
        import rfdetr.datasets.coco as coco_mod
    except Exception:
        return
    try:
        coco_mod.make_coco_transforms = restore[0]
        coco_mod.make_coco_transforms_square_div_64 = restore[1]
    except Exception:
        pass


def _rfdetr_latest_checkpoint_epoch(run_dir: Path) -> Optional[int]:
    try:
        best = None
        for path in run_dir.glob("checkpoint*.pth"):
            name = path.name
            if not name.startswith("checkpoint") or not name.endswith(".pth"):
                continue
            token = name[len("checkpoint") : -len(".pth")]
            if not token.isdigit():
                continue
            value = int(token)
            if best is None or value > best:
                best = value
        return best
    except Exception:
        return None


def _rfdetr_monitor_training(job: RfDetrTrainingJob, run_dir: Path, total_epochs: int, stop_event: threading.Event) -> None:
    log_path = run_dir / "log.txt"
    last_pos = 0
    pending = ""
    last_epoch: Optional[int] = None
    while not stop_event.is_set():
        if job.cancel_event.is_set() or job.status not in {"running", "queued"}:
            break
        new_metrics: List[Dict[str, Any]] = []
        if log_path.exists():
            try:
                with log_path.open("r", encoding="utf-8", errors="ignore") as handle:
                    handle.seek(last_pos)
                    chunk = handle.read()
                    last_pos = handle.tell()
            except Exception:
                chunk = ""
            if chunk:
                chunk = pending + chunk
                pending = ""
                if not chunk.endswith("\n"):
                    last_newline = chunk.rfind("\n")
                    if last_newline == -1:
                        pending = chunk
                        chunk = ""
                    else:
                        pending = chunk[last_newline + 1 :]
                        chunk = chunk[: last_newline + 1]
                for line in chunk.splitlines():
                    if not line.strip():
                        continue
                    try:
                        metric = json.loads(line)
                    except Exception:
                        continue
                    metric = _rfdetr_sanitize_metric(metric)
                    if metric:
                        new_metrics.append(metric)
        if new_metrics:
            for metric in new_metrics:
                _rfdetr_job_append_metric(job, metric)
            latest = new_metrics[-1]
            epoch = latest.get("epoch")
            if isinstance(epoch, (int, float)):
                try:
                    epoch_idx = int(epoch)
                except Exception:
                    epoch_idx = None
                if epoch_idx is not None and epoch_idx != last_epoch:
                    last_epoch = epoch_idx
                    if total_epochs > 0:
                        progress = max(0.0, min(0.99, epoch_idx / total_epochs))
                        _rfdetr_job_update(job, progress=progress, message=f"Epoch {epoch_idx}/{total_epochs}")
        else:
            checkpoint_epoch = _rfdetr_latest_checkpoint_epoch(run_dir)
            if checkpoint_epoch is not None and checkpoint_epoch != last_epoch:
                last_epoch = checkpoint_epoch
                if total_epochs > 0:
                    progress = max(0.0, min(0.99, checkpoint_epoch / total_epochs))
                    _rfdetr_job_update(job, progress=progress, message=f"Epoch {checkpoint_epoch}/{total_epochs}")
        stop_event.wait(15.0)


def _yolo_write_data_yaml(run_dir: Path, dataset_root: Path, layout: Optional[str], labelmap_path: Optional[str]) -> Path:
    train_rel, val_rel = _yolo_resolve_split_paths(dataset_root, layout)
    names = []
    if labelmap_path:
        names = _yolo_load_labelmap(Path(labelmap_path))
    data = {
        "path": str(dataset_root),
        "train": train_rel,
        "val": val_rel,
        "names": names,
    }
    data_path = run_dir / "data.yaml"
    data_path.write_text(yaml.safe_dump(data, sort_keys=False))
    if labelmap_path:
        try:
            shutil.copy2(labelmap_path, run_dir / "labelmap.txt")
        except Exception:
            pass
    return data_path


def _yolo_device_arg(devices: Optional[List[int]]) -> Optional[str]:
    if not devices:
        return None
    cleaned = [str(int(d)) for d in devices if isinstance(d, (int, str)) and str(d).strip().isdigit()]
    return ",".join(cleaned) if cleaned else None


def _yolo_p2_scale(model_id: str) -> Optional[str]:
    match = re.match(r"^yolov8([nsmlx])-p2$", model_id)
    if match:
        return match.group(1)
    return None


def _yolo_resolve_model_source(
    variant: Optional[str],
    task: str,
    from_scratch: bool,
    base_weights: Optional[str],
) -> Tuple[str, str]:
    model_id = (variant or "yolov8n").strip()
    if _yolo_p2_scale(model_id):
        return "cfg", "yolov8-p2.yaml"
    if base_weights:
        return "custom", base_weights
    if from_scratch:
        suffix = "-seg" if task == "segment" and "seg" not in model_id else ""
        return "cfg", f"{model_id}{suffix}.yaml"
    if task == "segment" and "seg" not in model_id:
        model_id = f"{model_id}-seg"
    return "weights", f"{model_id}.pt"


def _ensure_yolo_inference_runtime() -> Tuple[Any, List[str], Optional[str]]:
    global yolo_infer_model, yolo_infer_path, yolo_infer_labelmap, yolo_infer_task
    active = _load_yolo_active()
    if not isinstance(active, dict):
        raise HTTPException(status_code=HTTP_412_PRECONDITION_FAILED, detail="yolo_active_missing")
    best_path = active.get("best_path")
    if not best_path:
        raise HTTPException(status_code=HTTP_412_PRECONDITION_FAILED, detail="yolo_active_missing")
    labelmap_path = active.get("labelmap_path")
    task = active.get("task")
    with YOLO_INFER_LOCK:
        if yolo_infer_model is not None and yolo_infer_path == best_path:
            return yolo_infer_model, yolo_infer_labelmap, yolo_infer_task
        try:
            from ultralytics import YOLO  # type: ignore
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=f"yolo_unavailable:{exc}") from exc
        model = YOLO(best_path)
        labelmap = _yolo_load_labelmap(Path(labelmap_path)) if labelmap_path else []
        resolved_task = task or getattr(model, "task", None)
        yolo_infer_model = model
        yolo_infer_path = best_path
        yolo_infer_labelmap = labelmap
        yolo_infer_task = resolved_task
        return model, labelmap, resolved_task


def _ensure_rfdetr_inference_runtime() -> Tuple[Any, List[str], Optional[str]]:
    global rfdetr_infer_model, rfdetr_infer_path, rfdetr_infer_labelmap, rfdetr_infer_task, rfdetr_infer_variant
    active = _load_rfdetr_active()
    if not isinstance(active, dict):
        raise HTTPException(status_code=HTTP_412_PRECONDITION_FAILED, detail="rfdetr_active_missing")
    best_path = active.get("best_path")
    if not best_path:
        raise HTTPException(status_code=HTTP_412_PRECONDITION_FAILED, detail="rfdetr_active_missing")
    labelmap_path = active.get("labelmap_path")
    task = active.get("task") or "detect"
    variant = active.get("variant")
    with RFDETR_INFER_LOCK:
        if rfdetr_infer_model is not None and rfdetr_infer_path == best_path:
            return rfdetr_infer_model, rfdetr_infer_labelmap, rfdetr_infer_task
        try:
            from rfdetr import (
                RFDETRBase,
                RFDETRLarge,
                RFDETRNano,
                RFDETRSmall,
                RFDETRMedium,
                RFDETRSegPreview,
            )
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=f"rfdetr_unavailable:{exc}") from exc
        variant_info = _rfdetr_variant_info(task, variant)
        variant_id = variant_info.get("id")
        model_cls_map = {
            "rfdetr-nano": RFDETRNano,
            "rfdetr-small": RFDETRSmall,
            "rfdetr-medium": RFDETRMedium,
            "rfdetr-base": RFDETRBase,
            "rfdetr-large": RFDETRLarge,
            "rfdetr-seg-preview": RFDETRSegPreview,
        }
        model_cls = model_cls_map.get(variant_id)
        if not model_cls:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="rfdetr_variant_unknown")
        model_kwargs: Dict[str, Any] = {
            "pretrain_weights": best_path,
            "device": "cuda" if torch.cuda.is_available() else "cpu",
        }
        if variant_id == "rfdetr-seg-preview" or task == "segment":
            model_kwargs["segmentation_head"] = True
        model = model_cls(**model_kwargs)
        labelmap = _yolo_load_labelmap(Path(labelmap_path)) if labelmap_path else []
        if labelmap:
            try:
                model.model.class_names = labelmap
            except Exception:
                pass
        rfdetr_infer_model = model
        rfdetr_infer_path = best_path
        rfdetr_infer_labelmap = labelmap
        rfdetr_infer_task = task
        rfdetr_infer_variant = variant_id
        return model, labelmap, task


def _yolo_build_aug_args(aug: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    if not aug:
        return {}
    payload = dict(aug)
    mapping = {
        "flip_lr": "fliplr",
        "flip_ud": "flipud",
        "hsv_h": "hsv_h",
        "hsv_s": "hsv_s",
        "hsv_v": "hsv_v",
        "mosaic": "mosaic",
        "mixup": "mixup",
        "copy_paste": "copy_paste",
        "scale": "scale",
        "translate": "translate",
        "degrees": "degrees",
        "shear": "shear",
        "perspective": "perspective",
        "erasing": "erasing",
    }
    aug_args: Dict[str, Any] = {}
    for key, dest in mapping.items():
        if key in payload:
            aug_args[dest] = payload[key]
    return {k: v for k, v in aug_args.items() if v is not None}


def _yolo_parse_results_csv(results_path: Path) -> List[Dict[str, Any]]:
    if not results_path.exists():
        return []
    rows: List[Dict[str, Any]] = []
    try:
        with results_path.open("r", encoding="utf-8", newline="") as handle:
            reader = csv.DictReader(handle)
            for idx, raw in enumerate(reader):
                if not raw:
                    continue
                parsed: Dict[str, Any] = {}
                for key, value in raw.items():
                    if key is None or value is None:
                        continue
                    name = str(key).strip()
                    if not name:
                        continue
                    text = str(value).strip()
                    if text == "":
                        continue
                    try:
                        num = float(text)
                    except ValueError:
                        continue
                    if name == "epoch":
                        parsed["epoch"] = int(num) if float(num).is_integer() else num
                    else:
                        parsed[name] = num
                if "epoch" not in parsed:
                    parsed["epoch"] = idx + 1
                if parsed:
                    rows.append(parsed)
    except Exception:  # noqa: BLE001
        return []
    return rows


def _yolo_monitor_training(job: YoloTrainingJob, run_dir: Path, total_epochs: int, stop_event: threading.Event) -> None:
    results_path = run_dir / "train" / "results.csv"
    last_len = 0
    while not stop_event.is_set():
        if job.cancel_event.is_set() or job.status not in {"running", "queued"}:
            break
        series = _yolo_parse_results_csv(results_path)
        if series and len(series) > last_len:
            new_entries = series[last_len:]
            for metric in new_entries:
                _yolo_job_append_metric(job, metric)
            last_len = len(series)
            latest = series[-1]
            epoch = latest.get("epoch")
            if isinstance(epoch, (int, float)):
                try:
                    epoch_idx = int(epoch)
                except Exception:
                    epoch_idx = None
                if epoch_idx is not None and total_epochs > 0:
                    progress = max(0.0, min(0.99, epoch_idx / total_epochs))
                    _yolo_job_update(job, progress=progress, message=f"Epoch {epoch_idx}/{total_epochs}")
        stop_event.wait(12.0)


def _strip_checkpoint_optimizer(ckpt_path: Path) -> Tuple[bool, int, int]:
    """Remove optimizer/scheduler state from a torch checkpoint to shrink size."""
    before = ckpt_path.stat().st_size if ckpt_path.exists() else 0
    if not ckpt_path.exists() or before == 0:
        return False, before, before
    try:
        payload = torch.load(ckpt_path, map_location="cpu")
        removed = False
        for key in ["optimizer", "optimizers", "lr_schedulers", "schedulers", "trainer"]:
            if key in payload:
                payload.pop(key, None)
                removed = True
        if not removed:
            return False, before, before
        tmp_path = ckpt_path.with_suffix(ckpt_path.suffix + ".tmp")
        torch.save(payload, tmp_path)
        tmp_size = tmp_path.stat().st_size
        tmp_path.replace(ckpt_path)
        return True, before, tmp_size
    except Exception:
        return False, before, before


def _promote_run(run_id: str, variant: str) -> Dict[str, Any]:
    run_dir = _run_dir_for_request(run_id, variant)
    active_paths = _active_run_paths_for_variant(variant)
    if run_dir.resolve() in active_paths:
        raise HTTPException(status_code=HTTP_409_CONFLICT, detail="sam3_run_active")
    ckpt_dir = run_dir / "checkpoints"
    if not ckpt_dir.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="sam3_checkpoint_dir_missing")
    ckpts = [p for p in ckpt_dir.iterdir() if p.is_file() and p.suffix in {".ckpt", ".pth", ".pt"}]
    if not ckpts:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="sam3_checkpoints_missing")
    # choose keep candidate: prefer last.ckpt else newest
    keep = None
    for p in ckpts:
        if p.name == "last.ckpt":
            keep = p
            break
    if keep is None:
        keep = max(ckpts, key=lambda p: p.stat().st_mtime if p.exists() else 0)
    deleted = []
    freed = 0
    for p in ckpts:
        if p == keep:
            continue
        try:
            size = p.stat().st_size
        except Exception:
            size = 0
        try:
            p.unlink()
            deleted.append(str(p))
            freed += size
        except Exception:
            continue
    stripped, before, after = _strip_checkpoint_optimizer(keep)
    freed += max(0, before - after)
    marker = run_dir / ".promoted"
    try:
        marker.write_text(json.dumps({"timestamp": time.time(), "keep": str(keep)}), encoding="utf-8")
    except Exception:
        pass
    return {
        "kept": str(keep),
        "kept_size_bytes": keep.stat().st_size if keep.exists() else 0,
        "stripped_optimizer": stripped,
        "deleted": deleted,
        "freed_bytes": freed,
        "run_path": str(run_dir),
        "promoted": True,
        "promoted_at": time.time(),
    }


def _resolve_dataset_legacy(dataset_id: str) -> Path:
    cleaned = (dataset_id or "").strip().replace("\\", "/")
    safe = re.sub(r"[^A-Za-z0-9._/-]", "_", cleaned)
    candidate_qwen = (QWEN_DATASET_ROOT / safe).resolve()
    if str(candidate_qwen).startswith(str(QWEN_DATASET_ROOT.resolve())) and candidate_qwen.exists():
        return candidate_qwen
    candidate_sam3 = (SAM3_DATASET_ROOT / safe).resolve()
    if str(candidate_sam3).startswith(str(SAM3_DATASET_ROOT.resolve())) and candidate_sam3.exists():
        return candidate_sam3
    candidate_registry = (DATASET_REGISTRY_ROOT / safe).resolve()
    if (
        str(candidate_registry).startswith(str(DATASET_REGISTRY_ROOT.resolve()))
        and candidate_registry.exists()
    ):
        return candidate_registry
    raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="sam3_dataset_not_found")


def _resolve_sam3_or_qwen_dataset(dataset_id: str) -> Path:
    cleaned = (dataset_id or "").strip()
    for entry in _list_all_datasets():
        if cleaned in (entry.get("id"), entry.get("signature")):
            path = Path(entry["dataset_root"]).resolve()
            if path.exists():
                return path
    # Fallback to legacy per-root resolution
    return _resolve_dataset_legacy(dataset_id)


def _stable_hash(entries: Sequence[str]) -> str:
    digest = hashlib.sha256()
    for item in entries:
        digest.update(item.encode("utf-8"))
    return digest.hexdigest()


def _decode_image_base64(
    image_base64: str,
    *,
    max_bytes: Optional[int] = BASE64_IMAGE_MAX_BYTES,
    max_dim: Optional[int] = BASE64_IMAGE_MAX_DIM,
    allow_downscale: bool = True,
) -> Tuple[Image.Image, np.ndarray]:
    """Decode base64 image with size/dimension guards and optional downscale."""
    if not image_base64:
        raise HTTPException(status_code=HTTP_428_PRECONDITION_REQUIRED, detail="image_payload_missing")
    raw = image_base64
    if raw.startswith("data:") and "," in raw:
        raw = raw.split(",", 1)[1]
    if max_bytes:
        est_bytes = (len(raw) * 3) // 4
        if est_bytes > max_bytes * 2:
            raise HTTPException(status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="image_base64_too_large")
    try:
        data = base64.b64decode(raw)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"invalid_base64:{exc}") from exc
    if max_bytes and len(data) > max_bytes:
        raise HTTPException(status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="image_bytes_too_large")
    try:
        pil_img = Image.open(BytesIO(data)).convert("RGB")
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"invalid_image:{exc}") from exc
    if max_dim:
        width, height = pil_img.size
        if width > max_dim or height > max_dim:
            if not allow_downscale:
                raise HTTPException(status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="image_too_large_dim")
            try:
                resample = getattr(Image, "Resampling", Image).LANCZOS  # Pillow 10 compat
            except Exception:
                resample = Image.LANCZOS
            pil_img = pil_img.copy()
            pil_img.thumbnail((max_dim, max_dim), resample)
    np_img = np.array(pil_img)
    return pil_img, np_img


def _compute_dir_signature(root: Path, *, allowed_exts: Optional[set[str]] = None) -> str:
    """Return a stable signature for all files under ``root``."""
    entries: List[str] = []
    if not root.exists():
        return ""
    for path in sorted(root.rglob("*")):
        if not path.is_file():
            continue
        if allowed_exts is not None and path.suffix.lower() not in allowed_exts:
            continue
        try:
            stat = path.stat()
        except OSError:
            continue
        rel = path.relative_to(root)
        entries.append(f"{rel}:{stat.st_mtime_ns}:{stat.st_size}")
    return _stable_hash(entries)


def _path_is_within_root(path: Path, root: Path) -> bool:
    try:
        path.relative_to(root)
        return True
    except Exception:
        return False


def _agent_mining_meta_dir(dataset_id: str) -> Path:
    cleaned = (dataset_id or "").strip().replace("\\", "/").strip("/")
    safe = re.sub(r"[^A-Za-z0-9._/-]", "_", cleaned)
    meta_dir = (AGENT_MINING_META_ROOT / safe).resolve()
    if not _path_is_within_root(meta_dir, AGENT_MINING_META_ROOT.resolve()):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_mining_dataset_invalid")
    meta_dir.mkdir(parents=True, exist_ok=True)
    return meta_dir


def _agent_mining_cache_dir(dataset_id: str) -> Path:
    cleaned = (dataset_id or "").strip().replace("\\", "/").strip("/")
    safe = re.sub(r"[^A-Za-z0-9._/-]", "_", cleaned)
    cache_dir = (AGENT_MINING_DET_CACHE_ROOT / safe).resolve()
    if not _path_is_within_root(cache_dir, AGENT_MINING_DET_CACHE_ROOT.resolve()):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_mining_dataset_invalid")
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir


def _normalize_agent_recipe_steps(steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    normalized: List[Dict[str, Any]] = []
    extra_keys = {
        "similarity_score",
        "seed_prompt",
        "fps",
        "gain",
        "source",
        "precision",
        "recall",
        "coverage",
        "duplicates",
    }
    for step in steps:
        prompt = step.get("prompt")
        threshold = step.get("threshold")
        has_exemplar = step.get("exemplar") is not None
        if (prompt is None and not has_exemplar) or threshold is None:
            continue
        try:
            thr_val = float(threshold)
        except Exception:
            continue
        if math.isnan(thr_val) or thr_val < 0.0 or thr_val > 1.0:
            continue
        entry = {
            "prompt": "" if prompt is None else str(prompt),
            "threshold": thr_val,
            "type": step.get("type"),
            "exemplar": dict(step["exemplar"]) if isinstance(step.get("exemplar"), dict) else step.get("exemplar"),
        }
        sim_raw = step.get("similarity_score")
        if sim_raw is not None:
            try:
                sim_val = float(sim_raw)
            except Exception:
                sim_val = None
            if sim_val is not None and 0.0 <= sim_val <= 1.0:
                entry["similarity_score"] = sim_val
        for key in extra_keys:
            if key in entry:
                continue
            if key in step:
                entry[key] = step[key]
        normalized.append(entry)
    return normalized


def _parse_agent_recipe_schema_version(recipe_obj: Dict[str, Any]) -> Optional[int]:
    raw = recipe_obj.get("schema_version")
    if raw is None:
        return None
    try:
        return int(raw)
    except Exception:
        return None


def _classify_agent_recipe_mode(recipe_obj: Dict[str, Any]) -> Literal["sam3_steps", "sam3_greedy", "legacy_steps"]:
    """
    Classify an agent recipe into one of:
    - sam3_steps: explicit multi-step recipe (schema_version=2 or mode=sam3_steps)
    - sam3_greedy: prompt-bank + crop-bank / head recipe (legacy "greedy" semantics)
    - legacy_steps: older prompt-recipe style "steps" recipes (threshold per prompt/exemplar)
    """
    schema_version = _parse_agent_recipe_schema_version(recipe_obj)
    mode_raw = recipe_obj.get("mode")
    mode = mode_raw.strip() if isinstance(mode_raw, str) else None
    if mode == "sam3_steps" or schema_version == 2:
        return "sam3_steps"
    if mode == "sam3_greedy":
        return "sam3_greedy"
    # Back-compat: older greedy recipes may omit mode, but include prompt/crop bank fields.
    if isinstance(recipe_obj.get("text_prompts"), list) or isinstance(recipe_obj.get("positives"), list):
        return "sam3_greedy"
    return "legacy_steps"


def _normalize_agent_recipe_execution_plan(recipe_obj: Dict[str, Any]) -> Dict[str, Any]:
    """
    Normalize an agent recipe into a lightweight execution plan.

    This is intentionally shallow and does not change behavior by itself; it exists so future
    inference/mining code paths can share a single normalization step.
    """
    mode = _classify_agent_recipe_mode(recipe_obj)
    schema_version = _parse_agent_recipe_schema_version(recipe_obj)
    return {"mode": mode, "schema_version": schema_version, "recipe": recipe_obj}


def _validate_agent_recipe_structure(recipe_obj: Dict[str, Any]) -> None:
    """Lightweight schema guard to avoid accepting malformed recipes."""
    if not isinstance(recipe_obj, dict):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
    mode = _classify_agent_recipe_mode(recipe_obj)

    if mode == "sam3_steps":
        # Schema v2 (step-based) recipe: explicit steps. May optionally embed crop banks / clip head.
        steps = recipe_obj.get("steps")
        if not isinstance(steps, list) or not steps:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        for step in steps:
            if not isinstance(step, dict):
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
            if "enabled" in step and not isinstance(step.get("enabled"), bool):
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
            prompt = step.get("prompt")
            prompts = step.get("prompts")
            if prompt is not None and not isinstance(prompt, str):
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
            if prompts is not None:
                if not isinstance(prompts, list) or any(not isinstance(p, str) for p in prompts):
                    raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
            has_any_prompt = bool((isinstance(prompt, str) and prompt.strip()) or (isinstance(prompts, list) and any(str(p).strip() for p in prompts)))
            if not has_any_prompt:
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")

            # Optional numeric fields (kept permissive; more detailed validation happens in execution code).
            for key in (
                "seed_threshold",
                "expand_threshold",
                "mask_threshold",
                "seed_dedupe_iou",
                "step_dedupe_iou",
                "dedupe_iou",
            ):
                if key not in step or step.get(key) is None:
                    continue
                try:
                    v = float(step.get(key))
                except Exception:
                    raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
                if math.isnan(v) or v < 0.0 or v > 1.0:
                    raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
            for key in ("max_visual_seeds", "expand_max_results", "max_results"):
                if key not in step or step.get(key) is None:
                    continue
                try:
                    iv = int(step.get(key))
                except Exception:
                    raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
                if iv < 0:
                    raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")

            for clip_key in ("clip_seed", "clip_final"):
                if clip_key not in step or step.get(clip_key) is None:
                    continue
                if not isinstance(step.get(clip_key), dict):
                    raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        positives = recipe_obj.get("positives")
        negatives = recipe_obj.get("negatives")
        if positives is not None and not isinstance(positives, list):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        if negatives is not None and not isinstance(negatives, list):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        clip_head = recipe_obj.get("clip_head")
        if clip_head is not None and not isinstance(clip_head, dict):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        params = recipe_obj.get("params")
        if params is not None and not isinstance(params, dict):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        return

    # New greedy recipe format: prompt bank + positive/negative crop banks.
    if mode == "sam3_greedy":
        text_prompts = recipe_obj.get("text_prompts")
        positives = recipe_obj.get("positives")
        negatives = recipe_obj.get("negatives")
        steps = recipe_obj.get("steps")
        if text_prompts is not None and not isinstance(text_prompts, list):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        if positives is not None and not isinstance(positives, list):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        if negatives is not None and not isinstance(negatives, list):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        if steps is not None and not isinstance(steps, list):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        if isinstance(text_prompts, list):
            for p in text_prompts:
                if not isinstance(p, str):
                    raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        for crop_list in (positives, negatives):
            if not isinstance(crop_list, list):
                continue
            for ex in crop_list:
                if not isinstance(ex, dict):
                    raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        if not (text_prompts or positives or steps):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        return
    steps = recipe_obj.get("steps")
    if not isinstance(steps, list) or not steps:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
    for step in steps:
        if not isinstance(step, dict):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        if "threshold" not in step:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        try:
            thr_val = float(step.get("threshold"))
        except Exception:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        if math.isnan(thr_val) or thr_val < 0.0 or thr_val > 1.0:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        if "prompt" not in step and "exemplar" not in step:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        if "exemplar" in step and step["exemplar"] is not None and not isinstance(step["exemplar"], dict):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        sim_val = step.get("similarity_score")
        if sim_val is not None:
            try:
                sim_val_f = float(sim_val)
            except Exception:
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
            if math.isnan(sim_val_f) or sim_val_f < 0.0 or sim_val_f > 1.0:
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
    negatives = recipe_obj.get("negatives")
    if negatives is not None and not isinstance(negatives, list):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
    if isinstance(negatives, list):
        for neg in negatives:
            if not isinstance(neg, dict):
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")


def _compute_labelmap_hash(categories: List[Dict[str, Any]]) -> Tuple[str, List[str]]:
    names: List[Tuple[int, str]] = []
    for idx, cat in enumerate(categories):
        try:
            cid = int(cat.get("id", idx))
        except Exception:
            cid = idx
        names.append((cid, str(cat.get("name", f"class_{cid}"))))
    names.sort(key=lambda c: c[0])
    labels = [name for _, name in names]
    try:
        digest = hashlib.sha256("|".join(labels).encode("utf-8")).hexdigest()[:12]
    except Exception:
        digest = "unknown"
    return digest, labels


def _compute_dataset_signature(dataset_id: str, dataset_root: Path, images: Dict[int, Dict[str, Any]], categories: List[Dict[str, Any]]) -> str:
    """
    Create a location-agnostic signature for portability:
    - dataset_id
    - counts of images/categories
    - hashes of category names (sorted)
    - hashes of image file names (sorted)
    """
    try:
        cat_names = [str(c.get("name", f"class_{idx}")) for idx, c in enumerate(categories)]
        cat_hash = hashlib.sha256("|".join(sorted(cat_names)).encode("utf-8")).hexdigest()[:12]
        file_names = [Path(info.get("file_name") or "").name for info in images.values() if info.get("file_name")]
        file_hash = hashlib.sha256("|".join(sorted(file_names)).encode("utf-8")).hexdigest()[:12]
        payload = f"{dataset_id}|{len(images)}|{len(categories)}|{cat_hash}|{file_hash}"
        return hashlib.sha256(payload.encode("utf-8")).hexdigest()[:12]
    except Exception:
        return "unknown"


def _save_exemplar_crop(
    *,
    exemplar: Dict[str, Any],
    images: Dict[int, Dict[str, Any]],
    crop_dir: Path,
    step_idx: int,
    crop_name: Optional[str] = None,
) -> Optional[Dict[str, Any]]:
    """Persist a single exemplar crop to disk and return enriched metadata."""
    img_id = exemplar.get("image_id")
    if img_id is None:
        return None
    info = images.get(int(img_id))
    if not info:
        return None
    bbox = exemplar.get("bbox")
    if not bbox or len(bbox) < 4:
        return None
    try:
        x, y, w, h = map(float, bbox[:4])
    except Exception:
        return None
    try:
        img_path = info.get("path")
        if not img_path:
            return None
        with Image.open(img_path) as pil_img:
            pil_img = pil_img.convert("RGB")
            width, height = pil_img.width, pil_img.height
            x0 = max(0, x)
            y0 = max(0, y)
            x1 = min(width, x + w)
            y1 = min(height, y + h)
            crop = pil_img.crop((x0, y0, x1, y1))
            crop_dir.mkdir(parents=True, exist_ok=True)
            filename = crop_name or f"step_{step_idx:02d}_exemplar.png"
            crop_path = crop_dir / filename
            crop.save(crop_path, format="PNG")
    except Exception:
        return None
    bbox_norm = None
    try:
        bbox_norm = [x / width, y / height, w / width, h / height]
    except Exception:
        bbox_norm = None
    enriched = {
        **exemplar,
        "bbox": [x, y, w, h],
        "bbox_xyxy": [x0, y0, x1, y1],
        "bbox_norm": bbox_norm,
        "image_size": [width, height],
        "crop_path": str(Path("crops") / crop_path.name),
        "crop_size": [crop.width, crop.height],
    }
    return enriched


def _export_hard_negative_replay(
    *,
    dataset_id: str,
    class_id: int,
    class_name: str,
    entries: Sequence[Dict[str, Any]],
    max_crops: int,
    log_fn: Optional[Callable[[str], None]] = None,
) -> Optional[Dict[str, Any]]:
    if max_crops <= 0 or not entries:
        return None
    safe_name = re.sub(r"[^a-zA-Z0-9_-]+", "_", str(class_name or "").strip()).strip("_")
    if not safe_name:
        safe_name = f"class_{int(class_id)}"
    stamp = time.strftime("%Y%m%d_%H%M%S", time.localtime())
    run_dir = (CLIP_NEGATIVE_REPLAY_ROOT / str(dataset_id) / f"{int(class_id):03d}_{safe_name}" / stamp).resolve()
    if not _path_is_within_root(run_dir, CLIP_NEGATIVE_REPLAY_ROOT):
        return None
    crops_dir = run_dir / "crops"
    crops_dir.mkdir(parents=True, exist_ok=True)

    def _score(entry: Dict[str, Any]) -> float:
        try:
            return float(entry.get("score") or 0.0)
        except Exception:
            return 0.0

    entries_sorted = sorted([e for e in entries if isinstance(e, dict)], key=_score, reverse=True)
    seen_keys: set[Tuple[int, Tuple[float, float, float, float]]] = set()
    saved: List[Dict[str, Any]] = []
    for entry in entries_sorted:
        if len(saved) >= int(max_crops):
            break
        try:
            img_id = int(entry.get("image_id"))
        except Exception:
            continue
        bbox = entry.get("bbox_xyxy")
        if not isinstance(bbox, (list, tuple)) or len(bbox) < 4:
            continue
        try:
            bbox_key = (float(bbox[0]), float(bbox[1]), float(bbox[2]), float(bbox[3]))
        except Exception:
            continue
        dedupe_key = (int(img_id), bbox_key)
        if dedupe_key in seen_keys:
            continue
        seen_keys.add(dedupe_key)
        image_path = entry.get("image_path")
        if not isinstance(image_path, str) or not image_path:
            continue
        try:
            with Image.open(image_path) as img:
                pil_img = img.convert("RGB")
        except Exception:
            continue
        x1, y1, x2, y2 = bbox_key
        if x2 <= x1 or y2 <= y1:
            continue
        try:
            crop = pil_img.crop((x1, y1, x2, y2))
        except Exception:
            continue
        filename = f"hn_{len(saved):05d}.png"
        crop_path = crops_dir / filename
        try:
            crop.save(crop_path, format="PNG")
        except Exception:
            continue
        saved.append(
            {
                "image_id": int(img_id),
                "image_path": str(image_path),
                "bbox_xyxy": [float(x1), float(y1), float(x2), float(y2)],
                "score": float(entry.get("score") or 0.0),
                "clip_prob": entry.get("clip_prob"),
                "clip_bg_prob": entry.get("clip_bg_prob"),
                "clip_margin": entry.get("clip_margin"),
                "prompt": entry.get("prompt"),
                "crop_path": str(Path("crops") / crop_path.name),
            }
        )

    if not saved:
        return None

    manifest = {
        "dataset_id": str(dataset_id),
        "class_id": int(class_id),
        "class_name": str(class_name or f"class_{class_id}"),
        "created_at": float(time.time()),
        "count": int(len(saved)),
        "entries": saved,
    }
    try:
        with (run_dir / "manifest.json").open("w", encoding="utf-8") as fp:
            json.dump(manifest, fp, indent=2)
    except Exception:
        return None
    if log_fn:
        try:
            log_fn(f"[steps] Hard-negative export: saved {len(saved)}/{len(entries_sorted)} crops to {run_dir}")
        except Exception:
            pass
    return {
        "enabled": True,
        "count": int(len(saved)),
        "max_crops": int(max_crops),
        "root": str(run_dir),
        "manifest": str(run_dir / "manifest.json"),
    }


def _persist_agent_recipe(
    dataset_id: Optional[str],
    class_id: Optional[int],
    class_name: Optional[str],
    label: str,
    recipe: Dict[str, Any],
    *,
    crop_overrides: Optional[Dict[str, bytes]] = None,
    clip_head_overrides: Optional[Dict[str, bytes]] = None,
    meta_overrides: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    if not isinstance(recipe, dict) or not recipe:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_empty")
    # Accept either a raw recipe body, or a wrapper containing {"recipe": {...}} (e.g., imported payload).
    recipe_body: Dict[str, Any] = recipe
    if (
        isinstance(recipe.get("recipe"), dict)
        and not any(k in recipe for k in ("steps", "text_prompts", "positives", "mode"))
    ):
        recipe_body = recipe.get("recipe") or {}
    if not isinstance(recipe_body, dict) or not recipe_body:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_empty")
    _validate_agent_recipe_structure(recipe_body)
    cleaned_label = label.strip() or "agent_recipe"
    recipe_id = f"ar_{uuid.uuid4().hex[:8]}"
    images: Dict[int, Dict[str, Any]] = {}
    categories: List[Dict[str, Any]] = []
    dataset_signature: Optional[str] = None
    labelmap_hash: Optional[str] = None
    labelmap_entries: Optional[List[str]] = None
    dataset_root: Optional[Path] = None
    dataset_id_clean = (dataset_id or "").strip()
    try:
        if dataset_id_clean:
            dataset_root = _resolve_sam3_or_qwen_dataset(dataset_id_clean)
            coco, _, images = _load_coco_index(dataset_root)
            categories = coco.get("categories") or []
            dataset_signature = _compute_dataset_signature(dataset_id_clean, dataset_root, images, categories)
            labelmap_hash, labelmap_entries = _compute_labelmap_hash(categories)
            if class_id is not None:
                try:
                    cid = int(class_id)
                except Exception:
                    cid = None
                if cid is not None:
                    found = any(int(cat.get("id", idx)) == cid for idx, cat in enumerate(categories))
                    if not found and not crop_overrides:
                        raise HTTPException(status_code=HTTP_412_PRECONDITION_FAILED, detail="agent_recipe_class_missing")
    except HTTPException:
        if not crop_overrides and not meta_overrides:
            raise
    except Exception:
        # Allow portability when importing with embedded crops; we'll fall back to meta overrides.
        pass
    if not dataset_signature and meta_overrides:
        dataset_signature = meta_overrides.get("dataset_signature")
    if not labelmap_hash and meta_overrides:
        labelmap_hash = meta_overrides.get("labelmap_hash")
        labelmap_entries = meta_overrides.get("labelmap")
    if not labelmap_entries:
        raise HTTPException(status_code=HTTP_412_PRECONDITION_FAILED, detail="agent_recipe_labelmap_missing")
    recipe_mode = _classify_agent_recipe_mode(recipe_body)
    if recipe_mode == "sam3_steps":
        raw_steps = recipe_body.get("steps")
        if raw_steps is None:
            raw_steps = []
        if not isinstance(raw_steps, list):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        steps_raw = [dict(s) for s in raw_steps if isinstance(s, dict)]
    else:
        steps_raw = _normalize_agent_recipe_steps(recipe_body.get("steps") or [])
    text_prompts_raw = recipe_body.get("text_prompts")
    positives_raw = recipe_body.get("positives")
    negatives_raw = recipe_body.get("negatives")
    if text_prompts_raw is None:
        text_prompts_raw = recipe.get("text_prompts")
    if positives_raw is None:
        positives_raw = recipe.get("positives")
    if negatives_raw is None:
        negatives_raw = recipe.get("negatives")
    text_prompts: List[str] = []
    if isinstance(text_prompts_raw, list):
        text_prompts = _sanitize_prompts([str(p) for p in text_prompts_raw if str(p).strip()])
    positives_list: List[Dict[str, Any]] = [p for p in (positives_raw or []) if isinstance(p, dict)] if isinstance(positives_raw, list) else []
    negatives_list: List[Dict[str, Any]] = [n for n in (negatives_raw or []) if isinstance(n, dict)] if isinstance(negatives_raw, list) else []
    is_greedy = bool(recipe_body.get("mode") == "sam3_greedy" or text_prompts or positives_list)
    if not (steps_raw or text_prompts or positives_list):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_empty")
    recipe_dir = AGENT_MINING_RECIPES_ROOT / recipe_id
    cleanup_recipe_dir = True
    try:
        crops_dir = recipe_dir / "crops"
        crops_dir.mkdir(parents=True, exist_ok=True)
        # Optional portable CLIP head artifacts (embedded into the recipe package).
        clip_head_cfg_raw: Optional[Dict[str, Any]] = None
        if isinstance(recipe_body.get("clip_head"), dict):
            clip_head_cfg_raw = recipe_body.get("clip_head")
        elif isinstance(recipe.get("clip_head"), dict):
            clip_head_cfg_raw = recipe.get("clip_head")

        clip_head_classifier_path: Optional[str] = None
        for src in (recipe_body, recipe):
            if isinstance(src, dict) and isinstance(src.get("_clip_head_classifier_path"), str):
                clip_head_classifier_path = str(src.get("_clip_head_classifier_path"))
                break

        clip_head_written = False
        clip_dir = recipe_dir / "clip_head"
        head_npz_bytes = None
        head_meta_bytes = None
        if clip_head_overrides:
            head_npz_bytes = clip_head_overrides.get("clip_head/head.npz") or clip_head_overrides.get("head.npz")
            head_meta_bytes = clip_head_overrides.get("clip_head/meta.json") or clip_head_overrides.get("meta.json")
        if head_npz_bytes:
            try:
                clip_dir.mkdir(parents=True, exist_ok=True)
                (clip_dir / "head.npz").write_bytes(head_npz_bytes)
                clip_head_written = True
            except Exception as exc:  # noqa: BLE001
                raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"agent_recipe_clip_head_write_failed:{exc}") from exc
        if head_meta_bytes:
            try:
                clip_dir.mkdir(parents=True, exist_ok=True)
                (clip_dir / "meta.json").write_bytes(head_meta_bytes)
                clip_head_written = True
            except Exception as exc:  # noqa: BLE001
                raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"agent_recipe_clip_head_meta_write_failed:{exc}") from exc
        if not clip_head_written and clip_head_classifier_path:
            resolved_classifier = _resolve_agent_clip_classifier_path(clip_head_classifier_path)
            if resolved_classifier is not None:
                head = _load_clip_head_from_classifier(resolved_classifier)
                if head is not None:
                    min_prob = 0.5
                    margin = 0.0
                    if clip_head_cfg_raw:
                        try:
                            if clip_head_cfg_raw.get("min_prob") is not None:
                                min_prob = float(clip_head_cfg_raw.get("min_prob"))
                            if clip_head_cfg_raw.get("margin") is not None:
                                margin = float(clip_head_cfg_raw.get("margin"))
                        except Exception:
                            min_prob = 0.5
                            margin = 0.0
                    _save_clip_head_artifacts(recipe_dir=recipe_dir, head=head, min_prob=min_prob, margin=margin)
                    clip_head_written = True

        clip_head_cfg_clean: Optional[Dict[str, Any]] = None
        if clip_head_written:
            loaded = _load_clip_head_artifacts(recipe_dir=recipe_dir, fallback_meta=clip_head_cfg_raw)
            if loaded is not None:
                min_prob = 0.5
                margin = 0.0
                if loaded.get("min_prob") is not None:
                    try:
                        min_prob = float(loaded.get("min_prob"))
                    except Exception:
                        min_prob = 0.5
                if loaded.get("margin") is not None:
                    try:
                        margin = float(loaded.get("margin"))
                    except Exception:
                        margin = 0.0
                clip_head_cfg_clean = {
                    "artifact": "clip_head/head.npz",
                    "clip_model": loaded.get("clip_model"),
                    "proba_mode": loaded.get("proba_mode"),
                    "classes": loaded.get("classes") if isinstance(loaded.get("classes"), list) else [],
                    "min_prob": float(max(0.0, min(1.0, min_prob))),
                    "margin": float(max(0.0, min(1.0, margin))),
                }
                if clip_head_cfg_raw:
                    if clip_head_cfg_raw.get("auto_tuned") is not None:
                        clip_head_cfg_clean["auto_tuned"] = bool(clip_head_cfg_raw.get("auto_tuned"))
                    if clip_head_cfg_raw.get("target_precision") is not None:
                        try:
                            clip_head_cfg_clean["target_precision"] = float(clip_head_cfg_raw.get("target_precision"))
                        except Exception:
                            pass
            try:
                total = 0
                if clip_dir.exists():
                    for f in clip_dir.iterdir():
                        if f.is_file():
                            total += f.stat().st_size
                if total > AGENT_RECIPE_MAX_CLIP_HEAD_BYTES:
                    raise HTTPException(
                        status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE,
                        detail="agent_recipe_clip_head_too_large",
                    )
            except HTTPException:
                raise
            except Exception:
                pass
        def _safe_crop_filename(preferred: Optional[str], prefix: str, idx: int) -> str:
            try:
                name = Path(str(preferred)).name if preferred else ""
            except Exception:
                name = ""
            if not name:
                name = f"{prefix}_{idx:03d}.png"
            if not name.lower().endswith(".png"):
                name = f"{name}.png"
            name = re.sub(r"[^A-Za-z0-9._-]", "_", name)
            base, ext = os.path.splitext(name)
            if not ext:
                ext = ".png"
            candidate = f"{base}{ext}"
            counter = 1
            while (crops_dir / candidate).exists():
                counter += 1
                candidate = f"{base}_{counter}{ext}"
            return candidate

        def _materialize_crop_entry(entry: Dict[str, Any], *, prefix: str, idx: int, fallback_step_idx: int) -> Optional[Dict[str, Any]]:
            """Materialize a crop into crops_dir and return a portable entry dict."""
            entry_copy = dict(entry)
            entry_copy.pop("crop_base64", None)
            crop_key = entry_copy.get("crop_path") or entry_copy.get("path")
            crop_bytes = None
            if crop_overrides and crop_key:
                crop_bytes = crop_overrides.get(str(crop_key))
                if crop_bytes is None:
                    try:
                        crop_bytes = crop_overrides.get(str(Path("crops") / Path(str(crop_key)).name))
                    except Exception:
                        crop_bytes = None
            filename = _safe_crop_filename(str(crop_key) if crop_key else None, prefix, idx)
            if crop_bytes is not None:
                crop_path = crops_dir / filename
                try:
                    with crop_path.open("wb") as fp:
                        fp.write(crop_bytes)
                    entry_copy["crop_path"] = str(Path("crops") / crop_path.name)
                    entry_copy.pop("path", None)
                    entry_copy.pop("embed_id", None)
                    entry_copy.pop("crop_base64", None)
                    return entry_copy
                except Exception:
                    # Fall back to a portable dict without guarantees if write fails.
                    entry_copy.pop("path", None)
                    entry_copy.pop("embed_id", None)
                    entry_copy.pop("crop_base64", None)
                    return entry_copy
            enriched = None
            if images:
                enriched = _save_exemplar_crop(
                    exemplar=entry_copy,
                    images=images,
                    crop_dir=crops_dir,
                    step_idx=fallback_step_idx,
                    crop_name=filename,
                )
            if enriched is None:
                entry_copy.pop("path", None)
                entry_copy.pop("embed_id", None)
                entry_copy.pop("crop_base64", None)
                # Ensure crop_path, if present, is made portable.
                if crop_key:
                    try:
                        entry_copy["crop_path"] = str(Path("crops") / Path(str(crop_key)).name)
                    except Exception:
                        pass
                return entry_copy
            enriched.pop("path", None)
            enriched.pop("embed_id", None)
            enriched.pop("crop_base64", None)
            return enriched

        portable_steps: List[Dict[str, Any]] = []
        portable_positives: List[Dict[str, Any]] = []
        portable_negatives: List[Dict[str, Any]] = []
        for idx, step in enumerate(steps_raw, start=1):
            entry = dict(step)
            ex = step.get("exemplar")
            if ex:
                enriched = None
                # Prefer provided crops if present (e.g., imported package), else derive from dataset.
                crop_key = None
                if isinstance(ex, dict):
                    crop_key = ex.get("crop_path")
                    crop_bytes = None
                    if crop_overrides and crop_key:
                        crop_bytes = crop_overrides.get(crop_key)
                        if crop_bytes is None:
                            try:
                                alt_key = str(Path("crops") / Path(crop_key).name)
                                crop_bytes = crop_overrides.get(alt_key)
                            except Exception:
                                crop_bytes = None
                    if crop_bytes is not None:
                        crop_path = crops_dir / Path(crop_key).name
                        try:
                            with crop_path.open("wb") as fp:
                                fp.write(crop_bytes)
                            if crop_path.exists():
                                pass
                            enriched = {
                                **ex,
                                "crop_path": str(Path("crops") / crop_path.name),
                            }
                        except Exception:
                            enriched = dict(ex)
                if enriched is None and images and isinstance(ex, dict):
                    enriched = _save_exemplar_crop(exemplar=ex, images=images, crop_dir=crops_dir, step_idx=idx)
                if enriched is None and isinstance(ex, dict):
                    enriched = dict(ex)
                entry["exemplar"] = enriched
            portable_steps.append(entry)

        # Greedy-mode crop banks.
        if is_greedy and positives_list:
            for p_idx, pos in enumerate(positives_list, start=1):
                enriched_pos = _materialize_crop_entry(pos, prefix="pos", idx=p_idx, fallback_step_idx=2000 + p_idx)
                if enriched_pos:
                    portable_positives.append(enriched_pos)
        for n_idx, neg in enumerate(negatives_list, start=1):
            enriched_neg = _materialize_crop_entry(neg, prefix="neg", idx=n_idx, fallback_step_idx=3000 + n_idx)
            if enriched_neg:
                portable_negatives.append(enriched_neg)
        def _normalize_crop_path(path_str: Optional[str]) -> Optional[str]:
            if not path_str:
                return None
            try:
                return str(Path("crops") / Path(path_str).name)
            except Exception:
                return None

        # Normalize crop paths in steps and negatives for portability.
        for entry in portable_steps:
            ex = entry.get("exemplar")
            if isinstance(ex, dict) and ex.get("crop_path"):
                normalized = _normalize_crop_path(ex.get("crop_path"))
                if normalized:
                    ex["crop_path"] = normalized
                ex.pop("path", None)
                ex.pop("crop_base64", None)
        for entry in portable_positives:
            if isinstance(entry, dict) and entry.get("crop_path"):
                normalized = _normalize_crop_path(entry.get("crop_path"))
                if normalized:
                    entry["crop_path"] = normalized
                entry.pop("path", None)
                entry.pop("crop_base64", None)
        for neg in portable_negatives:
            if isinstance(neg, dict) and neg.get("crop_path"):
                normalized = _normalize_crop_path(neg.get("crop_path"))
                if normalized:
                    neg["crop_path"] = normalized
                neg.pop("path", None)
                neg.pop("crop_base64", None)

        # Enforce crop count/byte limits after all crops are materialized.
        def _assert_crop_limits() -> None:
            if not crops_dir.exists():
                return
            count = 0
            total = 0
            try:
                for cf in crops_dir.glob("*.png"):
                    count += 1
                    try:
                        total += cf.stat().st_size
                    except Exception:
                        continue
                if count > AGENT_RECIPE_MAX_CROPS or total > AGENT_RECIPE_MAX_CROP_BYTES:
                    raise HTTPException(status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="agent_recipe_crops_too_large")
            except HTTPException as exc:
                raise exc

        _assert_crop_limits()

        params_src = recipe_body.get("params")
        if not isinstance(params_src, dict):
            params_src = recipe.get("params") if isinstance(recipe.get("params"), dict) else None
        params = params_src or {
            "mask_threshold": recipe_body.get("mask_threshold", recipe.get("mask_threshold")),
            "min_size": recipe_body.get("min_size", recipe.get("min_size")),
            "simplify_epsilon": recipe_body.get("simplify_epsilon", recipe.get("simplify_epsilon")),
            "max_results": recipe_body.get("max_results", recipe.get("max_results")),
            "similarity_score": recipe_body.get("similarity_score", recipe.get("similarity_score")),
            "seed_threshold": recipe_body.get("seed_threshold", recipe.get("seed_threshold")),
            "expand_threshold": recipe_body.get("expand_threshold", recipe.get("expand_threshold")),
            "max_visual_seeds": recipe_body.get("max_visual_seeds", recipe.get("max_visual_seeds")),
            "seed_dedupe_iou": recipe_body.get("seed_dedupe_iou", recipe.get("seed_dedupe_iou")),
            "dedupe_iou": recipe_body.get("dedupe_iou", recipe.get("dedupe_iou")),
            "use_clip_fp_guard": recipe_body.get("use_clip_fp_guard", recipe.get("use_clip_fp_guard")),
            "use_negative_exemplars": recipe_body.get("use_negative_exemplars", recipe.get("use_negative_exemplars")),
            "negative_strength": recipe_body.get("negative_strength", recipe.get("negative_strength")),
            "clip_head_background_guard": recipe_body.get("clip_head_background_guard", recipe.get("clip_head_background_guard")),
            "clip_head_background_margin": recipe_body.get("clip_head_background_margin", recipe.get("clip_head_background_margin")),
            "clip_head_background_apply": recipe_body.get("clip_head_background_apply", recipe.get("clip_head_background_apply")),
            "clip_head_background_penalty": recipe_body.get("clip_head_background_penalty", recipe.get("clip_head_background_penalty")),
        }
        if isinstance(params, dict) and "clip_head_background_guard" not in params:
            params["clip_head_background_guard"] = recipe_body.get("clip_head_background_guard", recipe.get("clip_head_background_guard"))
        if isinstance(params, dict) and "clip_head_background_margin" not in params:
            params["clip_head_background_margin"] = recipe_body.get("clip_head_background_margin", recipe.get("clip_head_background_margin"))
        if isinstance(params, dict) and "clip_head_background_apply" not in params:
            params["clip_head_background_apply"] = recipe_body.get("clip_head_background_apply", recipe.get("clip_head_background_apply"))
        if isinstance(params, dict) and "clip_head_background_penalty" not in params:
            params["clip_head_background_penalty"] = recipe_body.get("clip_head_background_penalty", recipe.get("clip_head_background_penalty"))
        thresholds = sorted({float(s.get("threshold", 0.0)) for s in portable_steps if s.get("threshold") is not None})
        if thresholds:
            params["thresholds"] = thresholds
        schema_version_out: Optional[int] = None
        try:
            schema_version_out = int(recipe_body.get("schema_version")) if recipe_body.get("schema_version") is not None else None
        except Exception:
            schema_version_out = None
        mode_out: Optional[str] = recipe_body.get("mode") if isinstance(recipe_body.get("mode"), str) else None
        if recipe_mode == "sam3_steps":
            schema_version_out = 2
            mode_out = "sam3_steps"
        elif is_greedy:
            mode_out = mode_out or "sam3_greedy"
        optimizer_raw: Optional[Dict[str, Any]] = None
        for src in (recipe_body, recipe):
            if isinstance(src, dict) and isinstance(src.get("optimizer"), dict):
                optimizer_raw = src.get("optimizer")  # type: ignore[assignment]
                break
        optimizer_clean: Optional[Dict[str, Any]] = dict(optimizer_raw) if isinstance(optimizer_raw, dict) else None
        payload = {
            "id": recipe_id,
            "dataset_id": dataset_id,
            "dataset_signature": dataset_signature,
            "labelmap_hash": labelmap_hash,
            "labelmap": labelmap_entries,
            "class_id": class_id,
            "class_name": class_name,
            "label": cleaned_label,
            "created_at": time.time(),
            "params": params,
            "recipe": {
                "schema_version": schema_version_out,
                "mode": mode_out,
                "text_prompts": text_prompts if text_prompts else None,
                "positives": portable_positives if portable_positives else None,
                "steps": portable_steps,
                "negatives": portable_negatives,
                "clip_head": clip_head_cfg_clean,
                "optimizer": optimizer_clean,
                "summary": recipe_body.get("summary") or recipe.get("summary"),
            },
        }
        path = (AGENT_MINING_RECIPES_ROOT / f"{recipe_id}.json").resolve()
        path.parent.mkdir(parents=True, exist_ok=True)
        if not _path_is_within_root(path, AGENT_MINING_RECIPES_ROOT.resolve()):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_path_invalid")
        try:
            with path.open("w", encoding="utf-8") as fp:
                json.dump(payload, fp, ensure_ascii=False, indent=2)
            # Persist a portable zip alongside the JSON for download.
            zip_path = (AGENT_MINING_RECIPES_ROOT / f"{recipe_id}.zip").resolve()
            with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                zf.writestr("recipe.json", json.dumps(payload, ensure_ascii=False, indent=2))
                if crops_dir.exists():
                    for crop_file in crops_dir.glob("*.png"):
                        try:
                            zf.write(crop_file, arcname=f"crops/{crop_file.name}")
                        except Exception:
                            continue
                clip_dir = recipe_dir / "clip_head"
                if clip_dir.exists():
                    for artifact in clip_dir.iterdir():
                        try:
                            if not artifact.is_file():
                                continue
                            if artifact.name not in {"head.npz", "meta.json"}:
                                continue
                            zf.write(artifact, arcname=f"clip_head/{artifact.name}")
                        except Exception:
                            continue
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"agent_recipe_save_failed:{exc}") from exc
        payload["_path"] = str(path)
        payload["_zip"] = str((AGENT_MINING_RECIPES_ROOT / f"{recipe_id}.zip").resolve())
        cleanup_recipe_dir = False
        return payload
    finally:
        if cleanup_recipe_dir:
            try:
                shutil.rmtree(recipe_dir, ignore_errors=True)
            except Exception:
                pass


def _load_agent_recipe(recipe_id: str) -> Dict[str, Any]:
    path = (AGENT_MINING_RECIPES_ROOT / f"{recipe_id}.json").resolve()
    if not _path_is_within_root(path, AGENT_MINING_RECIPES_ROOT.resolve()) or not path.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="agent_recipe_not_found")
    try:
        with path.open("r", encoding="utf-8") as fp:
            data = json.load(fp)
        if not isinstance(data, dict):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        if not isinstance(data.get("recipe"), dict):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        _validate_agent_recipe_structure(data.get("recipe") or {})
        data["_path"] = str(path)
        zip_path = (AGENT_MINING_RECIPES_ROOT / f"{recipe_id}.zip").resolve()
        if zip_path.exists():
            data["_zip"] = str(zip_path)
        # Inline a small number of crop previews if present on disk (kept small so
        # /agent_mining/apply_image payloads don't explode when the UI forwards recipes).
        recipe_block = data.get("recipe") or {}
        recipe_dir = (AGENT_MINING_RECIPES_ROOT / recipe_id).resolve()
        if not _path_is_within_root(recipe_dir, AGENT_MINING_RECIPES_ROOT.resolve()):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_path_invalid")
        crop_dir = (recipe_dir / "crops").resolve()
        if not _path_is_within_root(crop_dir, recipe_dir):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_path_invalid")
        max_inline = 8
        inlined = 0

        def _inline_crop(entry: Dict[str, Any]) -> None:
            nonlocal inlined
            if inlined >= max_inline:
                return
            crop_path = entry.get("crop_path")
            if not crop_path or not isinstance(crop_path, str):
                return
            try:
                crop_name = Path(crop_path).name
            except Exception:
                crop_name = ""
            abs_path = (crop_dir / crop_name).resolve() if crop_name else None
            if abs_path and _path_is_within_root(abs_path, crop_dir) and abs_path.exists() and abs_path.is_file():
                try:
                    with abs_path.open("rb") as cfp:
                        b64 = base64.b64encode(cfp.read()).decode("ascii")
                    entry["crop_base64"] = f"data:image/png;base64,{b64}"
                    entry["crop_path"] = str(Path("crops") / crop_name)
                    inlined += 1
                except Exception:
                    return
            else:
                # Normalize to relative path in case it was absolute originally.
                try:
                    entry["crop_path"] = f"crops/{Path(crop_path).name}"
                except Exception:
                    return

        steps = recipe_block.get("steps") or []
        if isinstance(steps, list):
            for step in steps:
                ex = step.get("exemplar") if isinstance(step, dict) else None
                if isinstance(ex, dict):
                    _inline_crop(ex)
        positives = recipe_block.get("positives") or []
        if isinstance(positives, list):
            for ex in positives:
                if isinstance(ex, dict):
                    _inline_crop(ex)
        negatives = recipe_block.get("negatives") or []
        if isinstance(negatives, list):
            for ex in negatives:
                if isinstance(ex, dict):
                    _inline_crop(ex)
        return data
    except HTTPException:
        raise
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"agent_recipe_load_failed:{exc}") from exc


def _load_agent_recipe_json_only(recipe_id: str) -> Dict[str, Any]:
    """Load an agent recipe payload without inlining crop_base64 blobs (suitable for inference/export)."""
    path = (AGENT_MINING_RECIPES_ROOT / f"{recipe_id}.json").resolve()
    if not _path_is_within_root(path, AGENT_MINING_RECIPES_ROOT.resolve()) or not path.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="agent_recipe_not_found")
    try:
        with path.open("r", encoding="utf-8") as fp:
            data = json.load(fp)
        if not isinstance(data, dict):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_invalid_schema")
        data["_path"] = str(path)
        zip_path = (AGENT_MINING_RECIPES_ROOT / f"{recipe_id}.zip").resolve()
        if zip_path.exists():
            data["_zip"] = str(zip_path)
        return data
    except HTTPException:
        raise
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"agent_recipe_load_failed:{exc}") from exc


def _delete_agent_recipe(recipe_id: str) -> None:
    json_path = (AGENT_MINING_RECIPES_ROOT / f"{recipe_id}.json").resolve()
    zip_path = (AGENT_MINING_RECIPES_ROOT / f"{recipe_id}.zip").resolve()
    recipe_dir = (AGENT_MINING_RECIPES_ROOT / recipe_id).resolve()
    if not _path_is_within_root(json_path, AGENT_MINING_RECIPES_ROOT.resolve()):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_path_invalid")
    removed_any = False
    for path in (json_path, zip_path):
        if path.exists():
            try:
                path.unlink()
                removed_any = True
            except Exception:
                pass
    if recipe_dir.exists() and recipe_dir.is_dir():
        try:
            shutil.rmtree(recipe_dir)
            removed_any = True
        except Exception:
            pass
    if not removed_any:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="agent_recipe_not_found")


def _list_agent_recipes(dataset_id: Optional[str] = None) -> List[Dict[str, Any]]:
    recipes: List[Dict[str, Any]] = []
    for path in AGENT_MINING_RECIPES_ROOT.glob("*.json"):
        try:
            with path.open("r", encoding="utf-8") as fp:
                data = json.load(fp)
            if dataset_id and data.get("dataset_id") != dataset_id:
                continue
            data["_path"] = str(path)
            zip_path = (AGENT_MINING_RECIPES_ROOT / f"{data.get('id','')}.zip").resolve()
            if zip_path.exists():
                data["_zip"] = str(zip_path)
            recipes.append(data)
        except Exception:
            continue
    recipes.sort(key=lambda r: r.get("created_at", 0), reverse=True)
    return recipes


def _ensure_recipe_zip(recipe: Dict[str, Any]) -> Path:
    recipe_id = recipe.get("id")
    if not recipe_id:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_missing_id")
    zip_path = (AGENT_MINING_RECIPES_ROOT / f"{recipe_id}.zip").resolve()
    if zip_path.exists():
        return zip_path
    recipe_dir = AGENT_MINING_RECIPES_ROOT / recipe_id
    crops_dir = recipe_dir / "crops"
    clip_head_dir = recipe_dir / "clip_head"
    try:
        # Never embed crop_base64 blobs inside the portable zip JSON; the PNGs are included separately.
        def _strip_unportable_fields(obj: Any) -> None:
            if isinstance(obj, dict):
                # UI-only / internal fields should not ship in portable zips.
                for k in list(obj.keys()):
                    if isinstance(k, str) and k.startswith("_"):
                        obj.pop(k, None)
                obj.pop("crop_base64", None)
                for v in obj.values():
                    _strip_unportable_fields(v)
            elif isinstance(obj, list):
                for v in obj:
                    _strip_unportable_fields(v)

        clean_recipe = json.loads(json.dumps(recipe))
        _strip_unportable_fields(clean_recipe)
        with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
            zf.writestr("recipe.json", json.dumps(clean_recipe, ensure_ascii=False, indent=2))
            if crops_dir.exists():
                for crop_file in crops_dir.glob("*.png"):
                    try:
                        zf.write(crop_file, arcname=f"crops/{crop_file.name}")
                    except Exception:
                        continue
            if clip_head_dir.exists():
                for artifact in clip_head_dir.iterdir():
                    try:
                        if not artifact.is_file():
                            continue
                        if artifact.name not in {"head.npz", "meta.json"}:
                            continue
                        zf.write(artifact, arcname=f"clip_head/{artifact.name}")
                    except Exception:
                        continue
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"agent_recipe_export_failed:{exc}") from exc
    return zip_path


def _validate_agent_cascade_structure(cascade_obj: Dict[str, Any]) -> None:
    if not isinstance(cascade_obj, dict):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_invalid_schema")
    steps = cascade_obj.get("steps")
    if not isinstance(steps, list) or not steps:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_invalid_schema")
    for step in steps:
        if not isinstance(step, dict):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_invalid_schema")
        recipe_id = step.get("recipe_id")
        if not isinstance(recipe_id, str) or not recipe_id.strip():
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_invalid_schema")
    dedupe = cascade_obj.get("dedupe")
    if dedupe is not None and not isinstance(dedupe, dict):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_invalid_schema")


def _persist_agent_cascade(
    label: str,
    cascade: Dict[str, Any],
) -> Dict[str, Any]:
    if not isinstance(cascade, dict):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_invalid_schema")
    cleaned_label = str(label or "").strip() or "recipe_cascade"
    steps_raw = cascade.get("steps")
    if not isinstance(steps_raw, list) or not steps_raw:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="steps_required")
    dedupe_raw = cascade.get("dedupe") if isinstance(cascade.get("dedupe"), dict) else {}

    steps: List[Dict[str, Any]] = []
    recipe_ids: List[str] = []
    seen = set()
    for raw in steps_raw:
        if not isinstance(raw, dict):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_invalid_schema")
        recipe_id = raw.get("recipe_id")
        if not recipe_id and isinstance(raw.get("recipe"), dict):
            recipe_id = raw["recipe"].get("id")
        if not isinstance(recipe_id, str) or not recipe_id.strip():
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_invalid_schema")
        recipe_id = recipe_id.strip()
        # Ensure referenced recipe exists.
        _load_agent_recipe_json_only(recipe_id)
        if recipe_id not in seen:
            seen.add(recipe_id)
            recipe_ids.append(recipe_id)
        steps.append(
            {
                "enabled": bool(raw.get("enabled", True)),
                "recipe_id": recipe_id,
                "override_class_id": raw.get("override_class_id"),
                "override_class_name": raw.get("override_class_name"),
                "dedupe_group": raw.get("dedupe_group"),
                "participate_cross_class_dedupe": bool(raw.get("participate_cross_class_dedupe", True)),
                "clip_head_min_prob_override": raw.get("clip_head_min_prob_override"),
                "clip_head_margin_override": raw.get("clip_head_margin_override"),
                "extra_clip_classifier_path": raw.get("extra_clip_classifier_path"),
                "extra_clip_min_prob": raw.get("extra_clip_min_prob"),
                "extra_clip_margin": raw.get("extra_clip_margin"),
            }
        )
    if not any(s.get("enabled", True) for s in steps):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="steps_required")

    cascade_id = f"ac_{uuid.uuid4().hex[:8]}"
    payload = {
        "id": cascade_id,
        "label": cleaned_label,
        "created_at": time.time(),
        "version": 1,
        "steps": steps,
        "dedupe": dedupe_raw,
        "recipe_ids": recipe_ids,
    }
    json_path = (AGENT_MINING_CASCADES_ROOT / f"{cascade_id}.json").resolve()
    if not _path_is_within_root(json_path, AGENT_MINING_CASCADES_ROOT.resolve()):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_path_invalid")
    try:
        with json_path.open("w", encoding="utf-8") as fp:
            json.dump(payload, fp, ensure_ascii=False, indent=2)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"agent_cascade_save_failed:{exc}") from exc
    payload["_path"] = str(json_path)
    return payload


def _load_agent_cascade(cascade_id: str) -> Dict[str, Any]:
    json_path = (AGENT_MINING_CASCADES_ROOT / f"{cascade_id}.json").resolve()
    if not _path_is_within_root(json_path, AGENT_MINING_CASCADES_ROOT.resolve()) or not json_path.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="agent_cascade_not_found")
    try:
        with json_path.open("r", encoding="utf-8") as fp:
            data = json.load(fp)
        if not isinstance(data, dict):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_invalid_schema")
        _validate_agent_cascade_structure(data)
        data["_path"] = str(json_path)
        zip_path = (AGENT_MINING_CASCADES_ROOT / f"{cascade_id}.zip").resolve()
        if zip_path.exists():
            data["_zip"] = str(zip_path)
        return data
    except HTTPException:
        raise
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"agent_cascade_load_failed:{exc}") from exc


def _delete_agent_cascade(cascade_id: str) -> None:
    json_path = (AGENT_MINING_CASCADES_ROOT / f"{cascade_id}.json").resolve()
    zip_path = (AGENT_MINING_CASCADES_ROOT / f"{cascade_id}.zip").resolve()
    if not _path_is_within_root(json_path, AGENT_MINING_CASCADES_ROOT.resolve()):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_path_invalid")
    removed_any = False
    for path in (json_path, zip_path):
        if path.exists():
            try:
                path.unlink()
                removed_any = True
            except Exception:
                pass
    if not removed_any:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="agent_cascade_not_found")


def _list_agent_cascades() -> List[Dict[str, Any]]:
    cascades: List[Dict[str, Any]] = []
    for path in AGENT_MINING_CASCADES_ROOT.glob("*.json"):
        try:
            with path.open("r", encoding="utf-8") as fp:
                data = json.load(fp)
            if not isinstance(data, dict):
                continue
            if not data.get("id"):
                continue
            _validate_agent_cascade_structure(data)
            data["_path"] = str(path)
            zip_path = (AGENT_MINING_CASCADES_ROOT / f"{data.get('id','')}.zip").resolve()
            if zip_path.exists():
                data["_zip"] = str(zip_path)
            cascades.append(data)
        except Exception:
            continue
    cascades.sort(key=lambda c: c.get("created_at", 0), reverse=True)
    return cascades


def _ensure_cascade_zip(cascade: Dict[str, Any]) -> Path:
    cascade_id = cascade.get("id")
    if not cascade_id:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_missing_id")
    zip_path = (AGENT_MINING_CASCADES_ROOT / f"{cascade_id}.zip").resolve()
    if zip_path.exists():
        return zip_path
    if not _path_is_within_root(zip_path, AGENT_MINING_CASCADES_ROOT.resolve()):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_path_invalid")

    def _strip_internal(obj: Any) -> Any:
        if isinstance(obj, dict):
            return {k: _strip_internal(v) for k, v in obj.items() if not (isinstance(k, str) and k.startswith("_"))}
        if isinstance(obj, list):
            return [_strip_internal(v) for v in obj]
        return obj

    clean_cascade = _strip_internal(json.loads(json.dumps(cascade)))
    recipe_ids: List[str] = []
    classifier_refs: List[str] = []
    classifiers_root = (UPLOAD_ROOT / "classifiers").resolve()
    for step in clean_cascade.get("steps") or []:
        if not isinstance(step, dict):
            continue
        if isinstance(step.get("recipe_id"), str):
            recipe_ids.append(step["recipe_id"])
        if isinstance(step.get("extra_clip_classifier_path"), str) and step.get("extra_clip_classifier_path").strip():
            raw_path = str(step.get("extra_clip_classifier_path")).strip()
            # Cascade zips should carry classifier paths as rel_path under uploads/classifiers for portability.
            try:
                resolved = _resolve_agent_clip_classifier_path(raw_path)
            except Exception:
                resolved = None
            if resolved and _path_is_within_root(resolved.resolve(), classifiers_root):
                try:
                    rel = str(resolved.resolve().relative_to(classifiers_root))
                except Exception:
                    rel = raw_path
                step["extra_clip_classifier_path"] = rel
                classifier_refs.append(rel)
            else:
                classifier_refs.append(raw_path)
    dedupe = clean_cascade.get("dedupe") if isinstance(clean_cascade.get("dedupe"), dict) else {}
    if isinstance(dedupe.get("clip_head_recipe_id"), str) and dedupe.get("clip_head_recipe_id"):
        recipe_ids.append(str(dedupe.get("clip_head_recipe_id")))
    # Preserve order, drop duplicates.
    seen = set()
    recipe_ids_unique: List[str] = []
    for rid in recipe_ids:
        if rid in seen:
            continue
        seen.add(rid)
        recipe_ids_unique.append(rid)

    seen_cls = set()
    classifier_refs_unique: List[str] = []
    for ref in classifier_refs:
        if ref in seen_cls:
            continue
        seen_cls.add(ref)
        classifier_refs_unique.append(ref)

    try:
        with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
            zf.writestr("cascade.json", json.dumps(clean_cascade, ensure_ascii=False, indent=2))
            for rid in recipe_ids_unique:
                recipe = _load_agent_recipe_json_only(rid)
                recipe_zip = _ensure_recipe_zip(recipe)
                try:
                    zf.write(recipe_zip, arcname=f"recipes/{rid}.zip")
                except Exception:
                    continue
            for rel in classifier_refs_unique:
                try:
                    resolved = _resolve_agent_clip_classifier_path(rel)
                except Exception:
                    resolved = None
                if not resolved:
                    continue
                try:
                    rel_norm = str(resolved.resolve().relative_to(classifiers_root))
                except Exception:
                    rel_norm = str(Path(str(rel)).as_posix())
                try:
                    zf.write(resolved, arcname=f"classifiers/{Path(rel_norm).as_posix()}")
                except Exception:
                    continue
                try:
                    meta_path = Path(os.path.splitext(str(resolved))[0] + ".meta.pkl").resolve()
                except Exception:
                    meta_path = None
                if meta_path and meta_path.exists() and meta_path.is_file() and _path_is_within_root(meta_path, classifiers_root):
                    try:
                        rel_meta = str(meta_path.relative_to(classifiers_root))
                    except Exception:
                        rel_meta = f"{Path(rel_norm).as_posix()}.meta.pkl"
                    try:
                        zf.write(meta_path, arcname=f"classifiers/{Path(rel_meta).as_posix()}")
                    except Exception:
                        pass
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"agent_cascade_export_failed:{exc}") from exc
    return zip_path


def _import_agent_recipe_zip_bytes(zip_bytes: bytes) -> Tuple[Optional[str], Dict[str, Any]]:
    """Import a portable agent recipe zip from bytes; returns (source_recipe_id, persisted_recipe)."""
    if not zip_bytes:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_import_zip_only")
    bio = BytesIO(zip_bytes)
    if not zipfile.is_zipfile(bio):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_import_zip_only")
    data: Dict[str, Any] = {}
    crops: Dict[str, bytes] = {}
    clip_head_files: Dict[str, bytes] = {}
    with zipfile.ZipFile(bio) as zf:
        names = zf.namelist()
        json_name = None
        for name in names:
            if name.lower().endswith(".json"):
                json_name = name
                break
        if not json_name:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_import_no_json")
        json_info = zf.getinfo(json_name)
        if json_info.file_size > AGENT_RECIPE_MAX_JSON_BYTES:
            raise HTTPException(status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="agent_recipe_import_json_too_large")
        json_path = Path(json_name)
        if json_path.is_absolute() or ".." in json_path.parts:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_import_invalid_path")
        with zf.open(json_name) as jf:
            data = json.load(jf)

        total_bytes = 0
        crop_count = 0
        clip_head_bytes = 0
        for name in names:
            info = zf.getinfo(name)
            arc_path = Path(name)
            if arc_path.is_dir():
                continue
            if arc_path.is_absolute() or ".." in arc_path.parts:
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_import_invalid_path")
            if len(arc_path.parts) < 2 or arc_path.parts[0] != "crops":
                if len(arc_path.parts) == 2 and arc_path.parts[0] == "clip_head" and arc_path.name in {"head.npz", "meta.json"}:
                    clip_head_bytes += info.file_size
                    if clip_head_bytes > AGENT_RECIPE_MAX_CLIP_HEAD_BYTES:
                        raise HTTPException(status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="agent_recipe_import_clip_head_too_large")
                    clip_head_files[f"clip_head/{arc_path.name}"] = zf.read(name)
                continue
            if arc_path.suffix.lower() != ".png":
                continue
            crop_count += 1
            total_bytes += info.file_size
            if crop_count > AGENT_RECIPE_MAX_CROPS or total_bytes > AGENT_RECIPE_MAX_CROP_BYTES:
                raise HTTPException(status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="agent_recipe_import_crops_too_large")
            crops[f"crops/{arc_path.name}"] = zf.read(name)

    if not isinstance(data, dict) or not data:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_import_no_json")
    dataset_id = data.get("dataset_id") or (data.get("recipe") or {}).get("dataset_id") or ""
    label = data.get("label") or (data.get("recipe") or {}).get("label") or "imported_recipe"
    class_id = data.get("class_id")
    class_name = data.get("class_name")
    meta_overrides = {
        "dataset_signature": data.get("dataset_signature"),
        "labelmap_hash": data.get("labelmap_hash"),
        "labelmap": data.get("labelmap"),
    }
    persisted = _persist_agent_recipe(
        dataset_id,
        class_id,
        class_name,
        label,
        data,
        crop_overrides=crops,
        clip_head_overrides=clip_head_files,
        meta_overrides=meta_overrides,
    )
    source_id = data.get("id") if isinstance(data.get("id"), str) else None
    return source_id, persisted


def _bbox_to_xyxy_pixels(
    bbox: Sequence[float],
    img_w: int,
    img_h: int,
) -> Optional[Tuple[float, float, float, float]]:
    if not bbox or len(bbox) < 4:
        return None
    try:
        bbox4 = list(map(float, bbox[:4]))
    except Exception:
        return None
    if all(0.0 <= v <= 1.0 for v in bbox4):  # YOLO normalized (cx, cy, w, h)
        try:
            left, top, right, bottom = yolo_to_corners(bbox4, img_w, img_h)
            return float(left), float(top), float(right), float(bottom)
        except Exception:
            return None
    # Pixel xywh
    try:
        x, y, w, h = bbox4
    except Exception:
        return None
    x1 = float(x)
    y1 = float(y)
    x2 = float(x + w)
    y2 = float(y + h)
    return x1, y1, x2, y2


def _iou_xyxy(
    box_a: Tuple[float, float, float, float],
    box_b: Tuple[float, float, float, float],
) -> float:
    ax1, ay1, ax2, ay2 = box_a
    bx1, by1, bx2, by2 = box_b
    ix1, iy1 = max(ax1, bx1), max(ay1, by1)
    ix2, iy2 = min(ax2, bx2), min(ay2, by2)
    iw, ih = max(0.0, ix2 - ix1), max(0.0, iy2 - iy1)
    inter = iw * ih
    if inter <= 0:
        return 0.0
    area_a = max(0.0, ax2 - ax1) * max(0.0, ay2 - ay1)
    area_b = max(0.0, bx2 - bx1) * max(0.0, by2 - by1)
    denom = area_a + area_b - inter
    return inter / denom if denom > 0 else 0.0


def _dedupe_qwen_detections_iou(
    dets: Sequence[QwenDetection],
    *,
    img_w: int,
    img_h: int,
    iou_thresh: float,
) -> List[QwenDetection]:
    """Simple NMS-style dedupe across a list of detections, keeping highest-score boxes."""
    iou_thresh = float(max(0.0, min(1.0, iou_thresh)))
    if not dets:
        return []
    boxes: List[Tuple[float, float, float, float]] = []
    scored: List[Tuple[float, int]] = []
    for idx, det in enumerate(dets):
        bbox = det.bbox or []
        box_xyxy = _bbox_to_xyxy_pixels(bbox, img_w, img_h)
        if box_xyxy is None:
            continue
        boxes.append(box_xyxy)
        score = det.score if det.score is not None else 0.0
        scored.append((float(score), idx))
    if not scored:
        return list(dets)
    scored.sort(key=lambda x: x[0], reverse=True)
    kept: List[QwenDetection] = []
    kept_boxes: List[Tuple[float, float, float, float]] = []
    for _, det_idx in scored:
        try:
            det = dets[det_idx]
        except Exception:
            continue
        bbox = det.bbox or []
        box_xyxy = _bbox_to_xyxy_pixels(bbox, img_w, img_h)
        if box_xyxy is None:
            continue
        if any(_iou_xyxy(box_xyxy, kb) > iou_thresh for kb in kept_boxes):
            continue
        kept.append(det)
        kept_boxes.append(box_xyxy)
    return kept


def _ensure_agent_clip_backbone_for_device(
    device_str: str,
    *,
    model_name_override: Optional[str] = None,
) -> Tuple[Optional[Any], Optional[Any], threading.Lock, str]:
    """
    Return (model, preprocess, lock, normalized_device_str) for encoding crops on a specific device.
    Reuses the global CLIP model for cuda:0/cuda when available; otherwise loads a per-device backbone.
    """
    model_name = str(model_name_override).strip() if model_name_override is not None and str(model_name_override).strip() else None
    if not model_name:
        model_name = clip_model_name or DEFAULT_CLIP_MODEL
    normalized_device = str(device_str or "").strip() or str(device)
    global_device = str(device)
    global_model_name = clip_model_name or DEFAULT_CLIP_MODEL
    # Reuse the global backbone when possible to avoid duplicate VRAM usage.
    if (
        clip_model is not None
        and clip_preprocess is not None
        and global_model_name == model_name
        and (
            normalized_device == global_device
            or (global_device in {"cuda", "cuda:0"} and normalized_device in {"cuda", "cuda:0"})
        )
    ):
        return clip_model, clip_preprocess, clip_lock, global_device
    key = (model_name, normalized_device)
    with _agent_clip_backbones_lock:
        cached = _agent_clip_backbones.get(key)
        cached_lock = _agent_clip_locks.get(key)
        if cached is not None and cached_lock is not None:
            return cached[0], cached[1], cached_lock, normalized_device
        # Initialize lock early so concurrent callers for the same key serialize load.
        if cached_lock is None:
            cached_lock = threading.Lock()
            _agent_clip_locks[key] = cached_lock
    # Load outside the global cache lock (but guarded by the per-key lock).
    with cached_lock:
        with _agent_clip_backbones_lock:
            cached = _agent_clip_backbones.get(key)
            if cached is not None:
                return cached[0], cached[1], cached_lock, normalized_device
        try:
            if clip is None:
                return None, None, cached_lock, normalized_device
            model, preprocess = clip.load(model_name, device=normalized_device)
        except Exception as exc:  # noqa: BLE001
            logger.warning("Failed to load CLIP backbone %s on %s: %s", model_name, normalized_device, exc)
            return None, None, cached_lock, normalized_device
        with _agent_clip_backbones_lock:
            _agent_clip_backbones[key] = (model, preprocess)
        return model, preprocess, cached_lock, normalized_device


def _format_hf_model_error(exc: Exception) -> str:
    msg = str(exc)
    lowered = msg.lower()
    if any(token in lowered for token in ("401", "403", "unauthorized", "forbidden", "gated")):
        return (
            "Access denied. If this model is gated, run `huggingface-cli login` or set HF_TOKEN "
            "and accept the model license on Hugging Face."
        )
    if "not found" in lowered or "404" in lowered:
        return "Model not found. Check the model name and spelling."
    if "connection" in lowered or "offline" in lowered or "timeout" in lowered:
        return "Network error while fetching the model. Check connectivity or pre-download the weights."
    return msg


def _load_dinov3_backbone(
    model_name: str,
    target_device: str,
    *,
    raise_on_error: bool = False,
) -> Tuple[Optional[Any], Optional[Any]]:
    global dinov3_cuda_disabled
    requested_device = str(target_device or "").strip() or "cpu"
    effective_device = _dinov3_resolve_device(requested_device)
    if effective_device != requested_device:
        logger.warning("DINOv3 CUDA disabled; using %s instead of %s.", effective_device, requested_device)
    try:
        from transformers import AutoImageProcessor, AutoModel
    except Exception as exc:  # noqa: BLE001
        msg = f"DINOv3 requires transformers: {exc}"
        logger.warning("%s", msg)
        if raise_on_error:
            raise RuntimeError(msg) from exc
        return None, None
    try:
        processor = AutoImageProcessor.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        model.eval()
        model.to(effective_device)
        return model, processor
    except Exception as exc:  # noqa: BLE001
        raw_msg = str(exc)
        msg = _format_hf_model_error(exc)
        logger.warning("Failed to load DINOv3 backbone %s on %s: %s", model_name, effective_device, msg)
        if str(effective_device).startswith("cuda"):
            lowered = raw_msg.lower()
            if "cuda error" in lowered or "device-side assert" in lowered:
                dinov3_cuda_disabled = True
                logger.warning("Disabling DINOv3 CUDA usage after error on %s.", effective_device)
        if raise_on_error:
            raise RuntimeError(msg) from exc
        return None, None


def _ensure_agent_dinov3_backbone_for_device(
    device_str: str,
    *,
    model_name_override: Optional[str] = None,
) -> Tuple[Optional[Any], Optional[Any], threading.Lock, str]:
    model_name = str(model_name_override).strip() if model_name_override is not None and str(model_name_override).strip() else None
    if not model_name:
        model_name = dinov3_model_name
    if not model_name:
        return None, None, dinov3_lock, str(device_str or device)
    normalized_device = str(device_str or "").strip() or str(device)
    normalized_device = _dinov3_resolve_device(normalized_device)
    global_device = str(dinov3_model_device or device)
    global_device = _dinov3_resolve_device(global_device)
    if (
        dinov3_model is not None
        and dinov3_processor is not None
        and dinov3_model_name == model_name
        and (
            normalized_device == global_device
            or (global_device in {"cuda", "cuda:0"} and normalized_device in {"cuda", "cuda:0"})
        )
    ):
        return dinov3_model, dinov3_processor, dinov3_lock, global_device
    key = (model_name, normalized_device)
    with _agent_dinov3_backbones_lock:
        cached = _agent_dinov3_backbones.get(key)
        cached_lock = _agent_dinov3_locks.get(key)
        if cached is not None and cached_lock is not None:
            return cached[0], cached[1], cached_lock, normalized_device
        if cached_lock is None:
            cached_lock = threading.Lock()
            _agent_dinov3_locks[key] = cached_lock
    with cached_lock:
        with _agent_dinov3_backbones_lock:
            cached = _agent_dinov3_backbones.get(key)
            if cached is not None:
                return cached[0], cached[1], cached_lock, normalized_device
        model, processor = _load_dinov3_backbone(model_name, normalized_device)
        if model is None or processor is None:
            return None, None, cached_lock, normalized_device
        with _agent_dinov3_backbones_lock:
            _agent_dinov3_backbones[key] = (model, processor)
        return model, processor, cached_lock, normalized_device


def _clip_encode_pil_batch(
    crops: Sequence[Image.Image],
    *,
    device_override: Optional[str] = None,
    clip_model_override: Optional[str] = None,
    normalize: bool = True,
    batch_size_override: Optional[int] = None,
) -> Optional[np.ndarray]:
    """Encode a batch of PIL crops with raw CLIP, returning (N, D) float32 normalized embeddings."""
    if not crops:
        return None
    preferred = str(device_override or "").strip() if device_override is not None else ""
    # Attempt device chain:
    # 1) requested override (if any)
    # 2) global CLIP device (usually cuda:0) when different
    # 3) CPU fallback
    attempt_devices: List[str] = []
    if preferred:
        attempt_devices.append(preferred)
    global_device = str(device)
    if global_device and global_device not in attempt_devices:
        attempt_devices.append(global_device)
    if "cpu" not in attempt_devices:
        attempt_devices.append("cpu")

    batch_size = 64
    if batch_size_override is not None:
        try:
            batch_size = int(batch_size_override)
        except (TypeError, ValueError):
            batch_size = 64
    else:
        try:
            raw = os.environ.get("AGENT_CLIP_BATCH_SIZE")
            if raw is not None and str(raw).strip():
                batch_size = max(1, int(raw))
        except Exception:
            batch_size = 64
    batch_size = max(1, min(int(batch_size), 512))

    last_error: Optional[BaseException] = None
    for dev in attempt_devices:
        model, preprocess, lock, target_device = _ensure_agent_clip_backbone_for_device(
            str(dev),
            model_name_override=clip_model_override,
        )
        if model is None or preprocess is None:
            continue
        try:
            chunks: List[torch.Tensor] = []
            with lock:
                try:
                    target_dtype = next(model.parameters()).dtype
                except Exception:
                    target_dtype = torch.float32
                for start in range(0, len(crops), batch_size):
                    batch = crops[start : start + batch_size]
                    inp = torch.stack([preprocess(c) for c in batch], dim=0).to(target_device)
                    if inp.dtype != target_dtype:
                        inp = inp.to(dtype=target_dtype)
                    with torch.no_grad():
                        feats = model.encode_image(inp)
                    chunks.append(feats.to(dtype=torch.float32, device="cpu"))
            if not chunks:
                continue
            feats_all = torch.cat(chunks, dim=0)
            if normalize:
                feats_all = feats_all / (feats_all.norm(dim=-1, keepdim=True) + 1e-8)
            return feats_all.cpu().numpy()
        except torch.cuda.OutOfMemoryError as exc:
            last_error = exc
            if torch.cuda.is_available() and str(target_device).startswith("cuda"):
                try:
                    torch.cuda.empty_cache()
                except Exception:
                    pass
            continue
        except Exception as exc:  # noqa: BLE001
            last_error = exc
            continue

    if last_error is not None:
        logger.debug("CLIP batch encode failed (fallbacks exhausted): %s", last_error)
    return None


def _dinov3_encode_pil_batch(
    crops: Sequence[Image.Image],
    *,
    device_override: Optional[str] = None,
    model_name_override: Optional[str] = None,
    normalize: bool = True,
    batch_size_override: Optional[int] = None,
) -> Optional[np.ndarray]:
    if not crops:
        return None
    preferred = str(device_override or "").strip() if device_override is not None else ""
    attempt_devices: List[str] = []
    if preferred:
        attempt_devices.append(preferred)
    global_device = str(device)
    if global_device and global_device not in attempt_devices:
        attempt_devices.append(global_device)
    if "cpu" not in attempt_devices:
        attempt_devices.append("cpu")
    if dinov3_cuda_disabled:
        attempt_devices = [dev for dev in attempt_devices if not str(dev).startswith("cuda")]
        if "cpu" not in attempt_devices:
            attempt_devices.append("cpu")

    batch_size = 32
    if batch_size_override is not None:
        try:
            batch_size = int(batch_size_override)
        except (TypeError, ValueError):
            batch_size = 32
    else:
        try:
            raw = os.environ.get("AGENT_DINOV3_BATCH_SIZE")
            if raw is not None and str(raw).strip():
                batch_size = max(1, int(raw))
        except Exception:
            batch_size = 32
    batch_size = max(1, min(int(batch_size), 256))

    last_error: Optional[BaseException] = None
    for dev in attempt_devices:
        model, processor, lock, target_device = _ensure_agent_dinov3_backbone_for_device(
            str(dev),
            model_name_override=model_name_override,
        )
        if model is None or processor is None:
            continue
        try:
            chunks: List[torch.Tensor] = []
            with lock:
                for start in range(0, len(crops), batch_size):
                    batch = crops[start : start + batch_size]
                    inputs = processor(images=list(batch), return_tensors="pt")
                    inputs = {k: v.to(target_device) for k, v in inputs.items()}
                    with torch.no_grad():
                        outputs = model(**inputs)
                    feats = getattr(outputs, "pooler_output", None)
                    if feats is None:
                        feats = getattr(outputs, "last_hidden_state", None)
                        if feats is None:
                            raise RuntimeError("dinov3_missing_outputs")
                        feats = feats[:, 0, :]
                    chunks.append(feats.to(dtype=torch.float32, device="cpu"))
            if not chunks:
                continue
            feats_all = torch.cat(chunks, dim=0)
            if normalize:
                feats_all = feats_all / (feats_all.norm(dim=-1, keepdim=True) + 1e-8)
            return feats_all.cpu().numpy()
        except torch.cuda.OutOfMemoryError as exc:
            last_error = exc
            if torch.cuda.is_available() and str(target_device).startswith("cuda"):
                try:
                    torch.cuda.empty_cache()
                except Exception:
                    pass
            continue
        except Exception as exc:  # noqa: BLE001
            last_error = exc
            continue

    if last_error is not None:
        logger.debug("DINOv3 batch encode failed (fallbacks exhausted): %s", last_error)
    return None


def _encode_pil_batch(
    crops: Sequence[Image.Image],
    *,
    encoder_type: str,
    encoder_model: Optional[str],
    device_override: Optional[str] = None,
    normalize: bool = True,
    batch_size_override: Optional[int] = None,
) -> Optional[np.ndarray]:
    encoder_type_norm = str(encoder_type or "clip").strip().lower()
    if encoder_type_norm == "dinov3":
        return _dinov3_encode_pil_batch(
            crops,
            device_override=device_override,
            model_name_override=encoder_model,
            normalize=normalize,
            batch_size_override=batch_size_override,
        )
    return _clip_encode_pil_batch(
        crops,
        device_override=device_override,
        clip_model_override=encoder_model,
        normalize=normalize,
        batch_size_override=batch_size_override,
    )


def _apply_embedding_transform(
    feats: Optional[np.ndarray],
    *,
    center_values: Optional[Sequence[float]] = None,
    std_values: Optional[Sequence[float]] = None,
) -> Optional[np.ndarray]:
    if feats is None:
        return None
    try:
        arr = np.asarray(feats, dtype=np.float32)
    except Exception:
        return feats
    if arr.ndim != 2 or arr.shape[0] == 0:
        return arr
    if center_values is not None:
        try:
            center = np.asarray(center_values, dtype=np.float32)
            if center.ndim == 1 and center.shape[0] == arr.shape[1]:
                arr = arr - center.reshape(1, -1)
        except Exception:
            pass
    if std_values is not None:
        try:
            scale = np.asarray(std_values, dtype=np.float32)
            if scale.ndim == 1 and scale.shape[0] == arr.shape[1]:
                safe = np.maximum(scale, 1e-6)
                arr = arr / safe.reshape(1, -1)
        except Exception:
            pass
    return arr


def _encode_pil_batch_for_head(
    crops: Sequence[Image.Image],
    *,
    head: Dict[str, Any],
    device_override: Optional[str] = None,
    batch_size_override: Optional[int] = None,
) -> Optional[np.ndarray]:
    encoder_type = head.get("encoder_type") if isinstance(head.get("encoder_type"), str) else "clip"
    encoder_model = head.get("encoder_model")
    if not isinstance(encoder_model, str) or not encoder_model.strip():
        encoder_model = head.get("clip_model")
    normalize = _resolve_head_normalize_embeddings(head, default=True)
    feats = _encode_pil_batch(
        crops,
        encoder_type=encoder_type,
        encoder_model=str(encoder_model) if encoder_model is not None else None,
        device_override=device_override,
        normalize=normalize,
        batch_size_override=batch_size_override,
    )
    if feats is None:
        return None
    center_vals = head.get("embedding_center_values")
    std_vals = head.get("embedding_std_values")
    return _apply_embedding_transform(feats, center_values=center_vals, std_values=std_vals)


def _encode_pil_batch_for_active(
    crops: Sequence[Image.Image],
    *,
    device_override: Optional[str] = None,
    batch_size_override: Optional[int] = None,
) -> Optional[np.ndarray]:
    normalize = bool(active_head_normalize_embeddings)
    feats = _encode_pil_batch(
        crops,
        encoder_type=active_encoder_type,
        encoder_model=active_encoder_model,
        device_override=device_override,
        normalize=normalize,
        batch_size_override=batch_size_override,
    )
    if feats is None:
        return None
    center_vals = None
    std_vals = None
    if active_classifier_head:
        center_vals = active_classifier_head.get("embedding_center_values")
        std_vals = active_classifier_head.get("embedding_std_values")
    elif active_classifier_meta:
        center_vals = active_classifier_meta.get("embedding_center_values")
        std_vals = active_classifier_meta.get("embedding_std_values")
    return _apply_embedding_transform(feats, center_values=center_vals, std_values=std_vals)


def _clip_encode_text_batch(
    texts: Sequence[str],
    *,
    device_override: Optional[str] = None,
    clip_model_override: Optional[str] = None,
) -> Optional[np.ndarray]:
    """Encode a batch of text prompts with raw CLIP, returning (N, D) float32 normalized embeddings."""
    if not texts:
        return None
    prompt_list = [str(t).strip() for t in texts if str(t).strip()]
    if not prompt_list:
        return None
    preferred = str(device_override or "").strip() if device_override is not None else ""
    attempt_devices: List[str] = []
    if preferred:
        attempt_devices.append(preferred)
    global_device = str(device)
    if global_device and global_device not in attempt_devices:
        attempt_devices.append(global_device)
    if "cpu" not in attempt_devices:
        attempt_devices.append("cpu")

    batch_size = 64
    try:
        raw = os.environ.get("AGENT_CLIP_BATCH_SIZE")
        if raw is not None and str(raw).strip():
            batch_size = max(1, int(raw))
    except Exception:
        batch_size = 64
    batch_size = max(1, min(int(batch_size), 512))

    last_error: Optional[BaseException] = None
    for dev in attempt_devices:
        model, _preprocess, lock, target_device = _ensure_agent_clip_backbone_for_device(
            str(dev),
            model_name_override=clip_model_override,
        )
        if model is None or clip is None:
            continue
        try:
            chunks: List[torch.Tensor] = []
            with lock:
                for start in range(0, len(prompt_list), batch_size):
                    batch = prompt_list[start : start + batch_size]
                    tokens = clip.tokenize(batch, truncate=True).to(target_device)
                    with torch.no_grad():
                        feats = model.encode_text(tokens)
                    chunks.append(feats.to(dtype=torch.float32, device="cpu"))
            if not chunks:
                continue
            feats_all = torch.cat(chunks, dim=0)
            feats_all = feats_all / (feats_all.norm(dim=-1, keepdim=True) + 1e-8)
            return feats_all.cpu().numpy()
        except torch.cuda.OutOfMemoryError as exc:
            last_error = exc
            if torch.cuda.is_available() and str(target_device).startswith("cuda"):
                try:
                    torch.cuda.empty_cache()
                except Exception:
                    pass
            continue
        except Exception as exc:  # noqa: BLE001
            last_error = exc
            continue

    if last_error is not None:
        logger.debug("CLIP text encode failed (fallbacks exhausted): %s", last_error)
    return None


def _resolve_agent_clip_classifier_path(path_str: Optional[str]) -> Optional[Path]:
    if not path_str:
        return None
    allowed_root = (UPLOAD_ROOT / "classifiers").resolve()
    raw = Path(str(path_str))
    candidate = Path(os.path.abspath(str(path_str))).resolve()
    if not _path_is_within_root(candidate, allowed_root):
        # Also accept paths relative to the classifiers root (what /clip/classifiers returns as rel_path).
        try:
            candidate_alt = (allowed_root / raw).resolve()
        except Exception:
            candidate_alt = None
        if candidate_alt is None or not _path_is_within_root(candidate_alt, allowed_root):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_clip_classifier_path_not_allowed")
        candidate = candidate_alt
    if candidate.suffix.lower() not in CLASSIFIER_ALLOWED_EXTS:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_clip_classifier_ext_not_allowed")
    if not candidate.exists() or not candidate.is_file():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="agent_clip_classifier_not_found")
    return candidate


def _load_clip_head_from_classifier(classifier_path: Path) -> Optional[Dict[str, Any]]:
    """
    Load a classifier head and return a portable dict containing:
    - classes: list[str]
    - coef: np.ndarray float32 (K,D) or (1,D) for binary
    - intercept: np.ndarray float32 (K,) or (1,)
    - clip_model: str|None
    - encoder_type: str
    - encoder_model: str|None
    - embedding_dim: int
    - proba_mode: 'binary' | 'softmax' | 'ovr'
    """
    try:
        clf_obj = joblib.load(str(classifier_path))
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"agent_clip_classifier_load_failed:{exc}") from exc

    clip_model_used = None
    encoder_type_used = None
    encoder_model_used = None
    meta_found = False
    solver_used = None
    normalize_embeddings: Optional[bool] = None
    embedding_center_values: Optional[Sequence[float]] = None
    embedding_std_values: Optional[Sequence[float]] = None
    calibration_temperature: Optional[float] = None
    logit_adjustment: Optional[Sequence[float]] = None
    logit_adjustment_inference: bool = False
    arcface_enabled: bool = False
    arcface_margin: Optional[float] = None
    arcface_scale: Optional[float] = None
    meta_path = os.path.splitext(str(classifier_path))[0] + ".meta.pkl"
    if os.path.exists(meta_path):
        try:
            meta_obj = joblib.load(meta_path)
            if isinstance(meta_obj, dict):
                meta_found = True
                clip_model_used = meta_obj.get("clip_model")
                encoder_type_used = meta_obj.get("encoder_type")
                encoder_model_used = meta_obj.get("encoder_model")
                solver_used = meta_obj.get("solver")
                if meta_obj.get("mlp_normalize_embeddings") is not None:
                    normalize_embeddings = bool(meta_obj.get("mlp_normalize_embeddings"))
                if meta_obj.get("embedding_center_values") is not None:
                    embedding_center_values = meta_obj.get("embedding_center_values")
                if meta_obj.get("embedding_std_values") is not None:
                    embedding_std_values = meta_obj.get("embedding_std_values")
                if meta_obj.get("calibration_temperature") is not None:
                    try:
                        calibration_temperature = float(meta_obj.get("calibration_temperature"))
                    except Exception:
                        calibration_temperature = None
                if meta_obj.get("logit_adjustment") is not None:
                    logit_adjustment = meta_obj.get("logit_adjustment")
                if meta_obj.get("logit_adjustment_inference") is not None:
                    try:
                        logit_adjustment_inference = bool(meta_obj.get("logit_adjustment_inference"))
                    except Exception:
                        logit_adjustment_inference = False
                if meta_obj.get("arcface_enabled") is not None:
                    arcface_enabled = bool(meta_obj.get("arcface_enabled"))
                if meta_obj.get("arcface_margin") is not None:
                    try:
                        arcface_margin = float(meta_obj.get("arcface_margin"))
                    except Exception:
                        arcface_margin = None
                if meta_obj.get("arcface_scale") is not None:
                    try:
                        arcface_scale = float(meta_obj.get("arcface_scale"))
                    except Exception:
                        arcface_scale = None
        except Exception:
            clip_model_used = None
            encoder_type_used = None
            encoder_model_used = None
            solver_used = None
            meta_found = False
    if not solver_used:
        try:
            raw_solver = getattr(clf_obj, "solver", None)
            if raw_solver is not None and str(raw_solver).strip():
                solver_used = str(raw_solver).strip()
        except Exception:
            solver_used = None

    if isinstance(clf_obj, dict) and str(clf_obj.get("classifier_type") or clf_obj.get("head_type") or "").lower() == "mlp":
        classes = [str(c) for c in list(clf_obj.get("classes") or [])]
        layers_raw = clf_obj.get("layers")
        if not classes or not isinstance(layers_raw, list) or not layers_raw:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_clip_classifier_invalid")
        if not meta_found:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_clip_classifier_meta_required")
        layers: List[Dict[str, Any]] = []
        embedding_dim = int(clf_obj.get("embedding_dim") or 0)
        total_layers = len(layers_raw)
        for idx, layer in enumerate(layers_raw):
            try:
                weight = np.asarray(layer.get("weight"), dtype=np.float32)
                bias = np.asarray(layer.get("bias"), dtype=np.float32).reshape(-1)
            except Exception as exc:  # noqa: BLE001
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"agent_clip_classifier_invalid:{exc}") from exc
            if weight.ndim != 2 or bias.ndim != 1 or weight.shape[0] != bias.shape[0]:
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_clip_classifier_invalid_shape")
            if embedding_dim <= 0:
                embedding_dim = int(weight.shape[1])
            activation = str(layer.get("activation") or "").strip().lower()
            if not activation:
                activation = "linear" if idx == total_layers - 1 else "relu"
            if activation not in {"relu", "linear", "none", "identity"}:
                activation = "relu" if idx < total_layers - 1 else "linear"
            layer_entry: Dict[str, Any] = {
                "weight": weight,
                "bias": bias,
                "activation": activation,
            }
            if layer.get("layer_norm_weight") is not None:
                layer_entry["layer_norm_weight"] = np.asarray(layer.get("layer_norm_weight"), dtype=np.float32)
                if layer.get("layer_norm_bias") is not None:
                    layer_entry["layer_norm_bias"] = np.asarray(layer.get("layer_norm_bias"), dtype=np.float32)
                if layer.get("layer_norm_eps") is not None:
                    try:
                        layer_entry["layer_norm_eps"] = float(layer.get("layer_norm_eps"))
                    except Exception:
                        pass
            layers.append(layer_entry)
        if embedding_dim <= 0:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_clip_classifier_invalid_shape")
        bg_indices = _clip_head_background_indices(classes)
        bg_classes = [classes[idx] for idx in bg_indices] if bg_indices else []
        if not isinstance(encoder_type_used, str) or not encoder_type_used.strip():
            encoder_type_used = "clip"
        if not isinstance(encoder_model_used, str) or not encoder_model_used.strip():
            encoder_model_used = clip_model_used
        normalize_flag = normalize_embeddings if normalize_embeddings is not None else _resolve_head_normalize_embeddings(clf_obj, default=True)
        return {
            "classes": classes,
            "background_indices": bg_indices,
            "background_classes": bg_classes,
            "layers": layers,
            "clip_model": str(clip_model_used) if clip_model_used else None,
            "encoder_type": str(encoder_type_used),
            "encoder_model": str(encoder_model_used) if encoder_model_used else None,
            "embedding_dim": int(embedding_dim),
            "proba_mode": "softmax",
            "classifier_type": "mlp",
            "normalize_embeddings": bool(normalize_flag),
            "embedding_center_values": embedding_center_values,
            "embedding_std_values": embedding_std_values,
            "temperature": calibration_temperature,
            "logit_adjustment": logit_adjustment,
            "logit_adjustment_inference": bool(logit_adjustment_inference),
            "arcface": bool(arcface_enabled),
            "arcface_margin": arcface_margin,
            "arcface_scale": arcface_scale,
        }

    classes_raw = getattr(clf_obj, "classes_", None)
    coef_raw = getattr(clf_obj, "coef_", None)
    intercept_raw = getattr(clf_obj, "intercept_", None)
    if classes_raw is None or coef_raw is None or intercept_raw is None:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_clip_classifier_invalid")
    try:
        classes = [str(c) for c in list(classes_raw)]
        coef = np.asarray(coef_raw, dtype=np.float32)
        intercept = np.asarray(intercept_raw, dtype=np.float32).reshape(-1)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"agent_clip_classifier_invalid:{exc}") from exc
    if coef.ndim != 2 or intercept.ndim != 1:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_clip_classifier_invalid_shape")
    if coef.shape[0] != intercept.shape[0]:
        # sklearn binary case often stores coef as (1,D) but classes has length 2; intercept is (1,).
        if not (coef.shape[0] == 1 and intercept.shape[0] == 1 and len(classes) == 2):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_clip_classifier_invalid_shape")

    # Back-compat: older classifier uploads may not have the .meta.pkl sidecar (which records clip_model).
    # Infer clip backbone from embedding dim when possible, otherwise fall back to the active global CLIP model.
    embedding_dim = 0
    try:
        embedding_dim = int(coef.shape[1]) if hasattr(coef, "shape") else 0
    except Exception:
        embedding_dim = 0
    if not meta_found:
        if embedding_dim and int(embedding_dim) not in {512, 768}:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_clip_classifier_meta_required")
        inferred = _infer_clip_model_from_embedding_dim(embedding_dim, active_name=clip_model_name or DEFAULT_CLIP_MODEL)
        if inferred:
            clip_model_used = inferred
    if clip_model_used:
        try:
            expected_dim = {"ViT-B/32": 512, "ViT-B/16": 512, "ViT-L/14": 768}.get(str(clip_model_used))
            if expected_dim and embedding_dim and int(expected_dim) != int(embedding_dim):
                inferred = _infer_clip_model_from_embedding_dim(embedding_dim, active_name=clip_model_name or DEFAULT_CLIP_MODEL)
                if inferred and inferred != clip_model_used:
                    logger.warning(
                        "CLIP head %s embedding dim %s mismatches clip_model=%s; using inferred %s instead.",
                        classifier_path.name,
                        embedding_dim,
                        clip_model_used,
                        inferred,
                    )
                    clip_model_used = inferred
                else:
                    logger.warning(
                        "CLIP head %s embedding dim %s mismatches clip_model=%s (expected %s); head probabilities may be unavailable.",
                        classifier_path.name,
                        embedding_dim,
                        clip_model_used,
                        expected_dim,
                    )
        except Exception:
            pass
    if not isinstance(encoder_type_used, str) or not encoder_type_used.strip():
        encoder_type_used = "clip"
    if not isinstance(encoder_model_used, str) or not encoder_model_used.strip():
        encoder_model_used = clip_model_used
    multi_class_used = None
    try:
        raw_multi = getattr(clf_obj, "multi_class", None)
        if raw_multi is not None and str(raw_multi).strip():
            multi_class_used = str(raw_multi).strip()
    except Exception:
        multi_class_used = None

    n_classes = len(classes)
    proba_mode: str
    if n_classes == 2 and coef.shape[0] == 1:
        proba_mode = "binary"
    elif (solver_used and str(solver_used).strip().lower() == "liblinear") or (
        multi_class_used and str(multi_class_used).strip().lower() == "ovr"
    ):
        proba_mode = "ovr"
    else:
        proba_mode = "softmax"

    bg_indices = _clip_head_background_indices(classes)
    bg_classes = [classes[idx] for idx in bg_indices] if bg_indices else []
    if normalize_embeddings is None:
        normalize_embeddings = True

    return {
        "classes": classes,
        "background_indices": bg_indices,
        "background_classes": bg_classes,
        "coef": coef,
        "intercept": intercept,
        "clip_model": str(clip_model_used) if clip_model_used else None,
        "encoder_type": str(encoder_type_used),
        "encoder_model": str(encoder_model_used) if encoder_model_used else None,
        "embedding_dim": int(embedding_dim),
        "proba_mode": proba_mode,
        "classifier_type": "logreg",
        "normalize_embeddings": bool(normalize_embeddings),
        "embedding_center_values": embedding_center_values,
        "embedding_std_values": embedding_std_values,
        "temperature": calibration_temperature,
        "logit_adjustment": logit_adjustment,
        "logit_adjustment_inference": bool(logit_adjustment_inference),
    }


try:
    if active_classifier_path and active_classifier_head is None and os.path.isfile(active_classifier_path):
        active_classifier_head = _load_clip_head_from_classifier(Path(active_classifier_path))
except Exception:
    active_classifier_head = None


def _clip_head_predict_proba(feats: np.ndarray, head: Dict[str, Any]) -> Optional[np.ndarray]:
    """Compute predict_proba(feats) for an exported classifier head (logreg or MLP)."""
    if feats is None:
        return None
    classes = head.get("classes") or []
    if not isinstance(classes, list) or not classes:
        return None
    try:
        X = np.asarray(feats, dtype=np.float32)
    except Exception:
        return None
    if X.ndim != 2:
        return None
    temperature = head.get("temperature")
    try:
        temperature_val = float(temperature) if temperature is not None else None
    except Exception:
        temperature_val = None
    if temperature_val is not None and temperature_val <= 0:
        temperature_val = None
    if isinstance(head.get("classifier_type"), str) and head.get("classifier_type") == "mlp" or head.get("layers"):
        layers = head.get("layers")
        if not isinstance(layers, list) or not layers:
            return None
        arcface_enabled = bool(head.get("arcface"))
        try:
            arcface_scale = float(head.get("arcface_scale") or 1.0)
        except Exception:
            arcface_scale = 1.0
        out = X
        total_layers = len(layers)
        last_weight = None
        for idx, layer in enumerate(layers):
            try:
                W = np.asarray(layer.get("weight"), dtype=np.float32)
                b = np.asarray(layer.get("bias"), dtype=np.float32).reshape(-1)
            except Exception:
                return None
            if W.ndim != 2 or b.ndim != 1 or W.shape[0] != b.shape[0] or out.shape[1] != W.shape[1]:
                return None
            is_last = idx == total_layers - 1
            if arcface_enabled and is_last:
                last_weight = W
                break
            out = out @ W.T + b.reshape(1, -1)
            ln_weight = layer.get("layer_norm_weight")
            ln_bias = layer.get("layer_norm_bias")
            if ln_weight is not None:
                try:
                    gamma = np.asarray(ln_weight, dtype=np.float32).reshape(1, -1)
                    beta = np.asarray(ln_bias, dtype=np.float32).reshape(1, -1) if ln_bias is not None else 0.0
                    eps = float(layer.get("layer_norm_eps") or 1e-5)
                    mean = np.mean(out, axis=1, keepdims=True)
                    var = np.mean((out - mean) ** 2, axis=1, keepdims=True)
                    out = (out - mean) / np.sqrt(var + eps)
                    out = out * gamma + beta
                except Exception:
                    pass
            activation = str(layer.get("activation") or "").strip().lower()
            if not activation:
                activation = "linear" if is_last else "relu"
            if activation == "relu":
                out = np.maximum(out, 0.0)
            elif activation == "gelu":
                out = 0.5 * out * (1.0 + np.tanh(np.sqrt(2.0 / np.pi) * (out + 0.044715 * (out ** 3))))
            elif activation in {"linear", "none", "identity"}:
                pass
            else:
                out = np.maximum(out, 0.0)
        if arcface_enabled:
            if last_weight is None:
                return None
            if out.shape[1] != last_weight.shape[1]:
                return None
            feats = out
            feat_norm = feats / (np.linalg.norm(feats, axis=1, keepdims=True) + 1e-8)
            weight_norm = last_weight / (np.linalg.norm(last_weight, axis=1, keepdims=True) + 1e-8)
            out = feat_norm @ weight_norm.T
            out = out * float(arcface_scale)
        if out.shape[1] != len(classes):
            return None
        logit_adj = None if arcface_enabled else head.get("logit_adjustment")
        if not arcface_enabled and head.get("logit_adjustment_inference") and logit_adj is not None:
            try:
                adj = np.asarray(logit_adj, dtype=np.float32).reshape(1, -1)
                if adj.shape[1] == out.shape[1]:
                    out = out + adj
            except Exception:
                pass
        if temperature_val is not None and temperature_val != 1.0:
            out = out / temperature_val
        max_logit = np.max(out, axis=1, keepdims=True)
        exp_logits = np.exp(out - max_logit)
        denom = exp_logits.sum(axis=1, keepdims=True) + 1e-8
        return (exp_logits / denom).astype(np.float32)

    coef = head.get("coef")
    intercept = head.get("intercept")
    proba_mode = head.get("proba_mode")
    try:
        W = np.asarray(coef, dtype=np.float32)
        b = np.asarray(intercept, dtype=np.float32).reshape(-1)
    except Exception:
        return None
    if W.ndim != 2 or b.ndim != 1:
        return None
    if W.shape[1] != X.shape[1]:
        return None
    if proba_mode == "binary":
        if len(classes) != 2 or W.shape[0] != 1 or b.shape[0] != 1:
            return None
        logits = (X @ W[0].reshape(-1, 1)).reshape(-1) + float(b[0])
        logit_adj = head.get("logit_adjustment")
        if head.get("logit_adjustment_inference") and logit_adj is not None:
            try:
                adj = np.asarray(logit_adj, dtype=np.float32).reshape(-1)
                if adj.shape[0] == 2:
                    logits = logits + (adj[1] - adj[0])
            except Exception:
                pass
        if temperature_val is not None and temperature_val != 1.0:
            logits = logits / temperature_val
        probs_1 = 1.0 / (1.0 + np.exp(-logits))
        probs_0 = 1.0 - probs_1
        return np.stack([probs_0, probs_1], axis=1).astype(np.float32)

    logits = X @ W.T
    if b.shape[0] == logits.shape[1]:
        logits = logits + b.reshape(1, -1)
    else:
        return None
    logit_adj = head.get("logit_adjustment")
    if head.get("logit_adjustment_inference") and logit_adj is not None:
        try:
            adj = np.asarray(logit_adj, dtype=np.float32).reshape(1, -1)
            if adj.shape[1] == logits.shape[1]:
                logits = logits + adj
        except Exception:
            pass
    if temperature_val is not None and temperature_val != 1.0:
        logits = logits / temperature_val

    if proba_mode == "ovr":
        probs = 1.0 / (1.0 + np.exp(-logits))
        denom = probs.sum(axis=1, keepdims=True) + 1e-8
        return (probs / denom).astype(np.float32)

    # softmax
    max_logit = np.max(logits, axis=1, keepdims=True)
    exp_logits = np.exp(logits - max_logit)
    denom = exp_logits.sum(axis=1, keepdims=True) + 1e-8
    return (exp_logits / denom).astype(np.float32)


def _clip_head_keep_mask(
    proba: np.ndarray,
    *,
    target_index: int,
    min_prob: float,
    margin: float,
    background_indices: Optional[Sequence[int]] = None,
    background_guard: bool = False,
    background_margin: float = 0.0,
) -> Optional[np.ndarray]:
    """Return boolean keep mask for rows in proba."""
    try:
        probs = np.asarray(proba, dtype=np.float32)
    except Exception:
        return None
    if probs.ndim != 2 or probs.shape[0] == 0:
        return None
    if target_index < 0 or target_index >= probs.shape[1]:
        return None
    try:
        min_prob_f = float(min_prob)
    except Exception:
        min_prob_f = 0.0
    try:
        margin_f = float(margin)
    except Exception:
        margin_f = 0.0

    p_target = probs[:, target_index]
    keep = p_target >= min_prob_f

    try:
        bg_margin_f = float(background_margin)
    except Exception:
        bg_margin_f = 0.0

    if background_guard:
        bg_indices: List[int] = []
        if background_indices:
            for idx in background_indices:
                if isinstance(idx, int) and 0 <= idx < probs.shape[1] and idx != target_index:
                    bg_indices.append(idx)
        if bg_indices:
            p_bg = np.max(probs[:, bg_indices], axis=1)
            keep &= p_target >= (p_bg + max(0.0, bg_margin_f))

    # "Margin" is optional: a value of 0 disables the margin check (i.e., do not require the target
    # class to be the argmax). When enabled, require p(target) >= p(best_other) + margin.
    if margin_f > 0.0:
        if probs.shape[1] > 1:
            masked = probs.copy()
            masked[:, target_index] = -1.0
            p_other = np.max(masked, axis=1)
        else:
            p_other = np.zeros_like(p_target)
        keep &= p_target >= (p_other + margin_f)
    return keep


def _save_clip_head_artifacts(
    *,
    recipe_dir: Path,
    head: Dict[str, Any],
    min_prob: float,
    margin: float,
) -> None:
    """Persist a portable CLIP head artifact into a recipe package directory."""
    clip_dir = (recipe_dir / "clip_head").resolve()
    if not _path_is_within_root(clip_dir, recipe_dir.resolve()):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_clip_head_path_invalid")
    clip_dir.mkdir(parents=True, exist_ok=True)

    npz_path = clip_dir / "head.npz"
    meta_path = clip_dir / "meta.json"
    classifier_type = str(head.get("classifier_type") or "").strip().lower()
    if not classifier_type and head.get("layers"):
        classifier_type = "mlp"
    if classifier_type == "mlp":
        layers = head.get("layers")
        if not isinstance(layers, list) or not layers:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_clip_head_invalid")
        arrays: Dict[str, np.ndarray] = {}
        layers_meta: List[Dict[str, str]] = []
        total_layers = len(layers)
        normalize_flag = _resolve_head_normalize_embeddings(head, default=True)
        for idx, layer in enumerate(layers):
            try:
                weight = np.asarray(layer.get("weight"), dtype=np.float32)
                bias = np.asarray(layer.get("bias"), dtype=np.float32).reshape(-1)
            except Exception as exc:  # noqa: BLE001
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"agent_recipe_clip_head_invalid:{exc}") from exc
            if weight.ndim != 2 or bias.ndim != 1 or weight.shape[0] != bias.shape[0]:
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_clip_head_invalid")
            weight_key = f"layer{idx}_weight"
            bias_key = f"layer{idx}_bias"
            arrays[weight_key] = weight
            arrays[bias_key] = bias
            activation = str(layer.get("activation") or "").strip().lower()
            if not activation:
                activation = "linear" if idx == total_layers - 1 else "relu"
            if activation not in {"relu", "linear", "none", "identity"}:
                activation = "relu" if idx < total_layers - 1 else "linear"
            layers_meta.append({
                "weight": weight_key,
                "bias": bias_key,
                "activation": activation,
            })
        try:
            np.savez_compressed(str(npz_path), **arrays)
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"agent_recipe_clip_head_write_failed:{exc}") from exc
        classes = head.get("classes") if isinstance(head.get("classes"), list) else []
        encoder_type = head.get("encoder_type") if isinstance(head.get("encoder_type"), str) else "clip"
        encoder_model = head.get("encoder_model")
        if not isinstance(encoder_model, str) or not encoder_model.strip():
            encoder_model = head.get("clip_model")
        meta = {
            "clip_model": head.get("clip_model"),
            "encoder_type": encoder_type,
            "encoder_model": encoder_model,
            "proba_mode": "softmax",
            "classifier_type": "mlp",
            "classes": [str(c) for c in classes],
            "layers": layers_meta,
            "normalize_embeddings": bool(normalize_flag),
            "embedding_center_values": head.get("embedding_center_values").tolist() if isinstance(head.get("embedding_center_values"), np.ndarray) else head.get("embedding_center_values"),
            "embedding_std_values": head.get("embedding_std_values").tolist() if isinstance(head.get("embedding_std_values"), np.ndarray) else head.get("embedding_std_values"),
            "calibration_temperature": head.get("temperature") if head.get("temperature") is not None else head.get("calibration_temperature"),
            "logit_adjustment": head.get("logit_adjustment"),
            "logit_adjustment_inference": bool(head.get("logit_adjustment_inference")),
            "arcface": bool(head.get("arcface")),
            "arcface_margin": head.get("arcface_margin"),
            "arcface_scale": head.get("arcface_scale"),
            "min_prob": float(min_prob),
            "margin": float(margin),
        }
    else:
        try:
            coef = np.asarray(head.get("coef"), dtype=np.float32)
            intercept = np.asarray(head.get("intercept"), dtype=np.float32).reshape(-1)
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"agent_recipe_clip_head_invalid:{exc}") from exc
        if coef.ndim != 2 or intercept.ndim != 1:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_clip_head_invalid")

        try:
            np.savez_compressed(str(npz_path), coef=coef, intercept=intercept)
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"agent_recipe_clip_head_write_failed:{exc}") from exc

        classes = head.get("classes") if isinstance(head.get("classes"), list) else []
        encoder_type = head.get("encoder_type") if isinstance(head.get("encoder_type"), str) else "clip"
        encoder_model = head.get("encoder_model")
        if not isinstance(encoder_model, str) or not encoder_model.strip():
            encoder_model = head.get("clip_model")
        normalize_flag = _resolve_head_normalize_embeddings(head, default=True)
        meta = {
            "clip_model": head.get("clip_model"),
            "encoder_type": encoder_type,
            "encoder_model": encoder_model,
            "proba_mode": head.get("proba_mode"),
            "classifier_type": "logreg",
            "classes": [str(c) for c in classes],
            "normalize_embeddings": bool(normalize_flag),
            "embedding_center_values": head.get("embedding_center_values").tolist() if isinstance(head.get("embedding_center_values"), np.ndarray) else head.get("embedding_center_values"),
            "embedding_std_values": head.get("embedding_std_values").tolist() if isinstance(head.get("embedding_std_values"), np.ndarray) else head.get("embedding_std_values"),
            "calibration_temperature": head.get("temperature") if head.get("temperature") is not None else head.get("calibration_temperature"),
            "logit_adjustment": head.get("logit_adjustment"),
            "logit_adjustment_inference": bool(head.get("logit_adjustment_inference")),
            "min_prob": float(min_prob),
            "margin": float(margin),
        }
    try:
        with meta_path.open("w", encoding="utf-8") as fp:
            json.dump(meta, fp, ensure_ascii=False, indent=2)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"agent_recipe_clip_head_meta_write_failed:{exc}") from exc

    try:
        total = npz_path.stat().st_size + meta_path.stat().st_size
    except Exception:
        total = 0
    if total and total > AGENT_RECIPE_MAX_CLIP_HEAD_BYTES:
        raise HTTPException(status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="agent_recipe_clip_head_too_large")


def _load_clip_head_artifacts(
    *,
    recipe_dir: Path,
    fallback_meta: Optional[Dict[str, Any]] = None,
) -> Optional[Dict[str, Any]]:
    """Load a portable CLIP head artifact from a recipe package directory."""
    clip_dir = (recipe_dir / "clip_head").resolve()
    npz_path = (clip_dir / "head.npz").resolve()
    meta_path = (clip_dir / "meta.json").resolve()
    if not _path_is_within_root(npz_path, clip_dir) or not _path_is_within_root(meta_path, clip_dir):
        return None
    if not npz_path.exists() or not npz_path.is_file():
        return None
    meta: Dict[str, Any] = {}
    if meta_path.exists() and meta_path.is_file():
        try:
            with meta_path.open("r", encoding="utf-8") as fp:
                loaded = json.load(fp)
            if isinstance(loaded, dict):
                meta = loaded
        except Exception:
            meta = {}
    if not meta and isinstance(fallback_meta, dict):
        meta = fallback_meta
    classes_raw = meta.get("classes") if isinstance(meta.get("classes"), list) else []
    classes = [str(c) for c in classes_raw]
    proba_mode = meta.get("proba_mode")
    min_prob = meta.get("min_prob")
    margin = meta.get("margin")
    classifier_type = str(meta.get("classifier_type") or "").strip().lower()
    layers_meta = meta.get("layers") if isinstance(meta.get("layers"), list) else None
    normalize_flag = meta.get("normalize_embeddings")
    center_vals = meta.get("embedding_center_values")
    std_vals = meta.get("embedding_std_values")
    temperature_val = meta.get("calibration_temperature")
    logit_adjustment = meta.get("logit_adjustment")
    logit_adjustment_inference = meta.get("logit_adjustment_inference")
    arcface_flag = meta.get("arcface")
    if arcface_flag is None:
        arcface_flag = meta.get("arcface_enabled")
    arcface_margin = meta.get("arcface_margin")
    arcface_scale = meta.get("arcface_scale")

    min_prob_val: Optional[float] = None
    margin_val: Optional[float] = None
    try:
        if min_prob is not None:
            min_prob_val = float(min_prob)
    except Exception:
        min_prob_val = None
    try:
        if margin is not None:
            margin_val = float(margin)
    except Exception:
        margin_val = None

    clip_model_val: Optional[str] = None
    try:
        raw = meta.get("clip_model")
        if isinstance(raw, str) and raw.strip():
            clip_model_val = raw.strip()
    except Exception:
        clip_model_val = None

    encoder_type = meta.get("encoder_type") if isinstance(meta.get("encoder_type"), str) else "clip"
    encoder_model = meta.get("encoder_model")
    if not isinstance(encoder_model, str) or not encoder_model.strip():
        encoder_model = clip_model_val

    if classifier_type == "mlp" or layers_meta:
        try:
            with np.load(str(npz_path)) as data:
                layers: List[Dict[str, Any]] = []
                total_layers = len(layers_meta or [])
                for idx, layer in enumerate(layers_meta or []):
                    weight_key = layer.get("weight")
                    bias_key = layer.get("bias")
                    if not weight_key or not bias_key:
                        return None
                    weight = np.asarray(data.get(weight_key), dtype=np.float32)
                    bias = np.asarray(data.get(bias_key), dtype=np.float32).reshape(-1)
                    if weight.ndim != 2 or bias.ndim != 1 or weight.shape[0] != bias.shape[0]:
                        return None
                    activation = str(layer.get("activation") or "").strip().lower()
                    if not activation:
                        activation = "linear" if idx == total_layers - 1 else "relu"
                    if activation not in {"relu", "linear", "none", "identity"}:
                        activation = "relu" if idx < total_layers - 1 else "linear"
                    layers.append({
                        "weight": weight,
                        "bias": bias,
                        "activation": activation,
                    })
        except Exception:
            return None
        bg_indices = _clip_head_background_indices(classes)
        bg_classes = [classes[idx] for idx in bg_indices] if bg_indices else []
        embedding_dim = 0
        try:
            embedding_dim = int(layers[0].get("weight").shape[1]) if layers else 0
        except Exception:
            embedding_dim = 0
        return {
            "classes": classes,
            "background_indices": bg_indices,
            "background_classes": bg_classes,
            "layers": layers,
            "clip_model": clip_model_val,
            "encoder_type": encoder_type,
            "encoder_model": encoder_model,
            "embedding_dim": embedding_dim,
            "proba_mode": "softmax",
            "classifier_type": "mlp",
            "normalize_embeddings": bool(normalize_flag) if normalize_flag is not None else True,
            "embedding_center_values": center_vals,
            "embedding_std_values": std_vals,
            "temperature": temperature_val,
            "logit_adjustment": logit_adjustment,
            "logit_adjustment_inference": bool(logit_adjustment_inference),
            "arcface": bool(arcface_flag),
            "arcface_margin": arcface_margin,
            "arcface_scale": arcface_scale,
            "min_prob": min_prob_val,
            "margin": margin_val,
        }

    try:
        with np.load(str(npz_path)) as data:
            coef = np.asarray(data["coef"], dtype=np.float32)
            intercept = np.asarray(data["intercept"], dtype=np.float32).reshape(-1)
    except Exception:
        return None

    if not isinstance(proba_mode, str) or not proba_mode:
        if coef.shape[0] == 1 and len(classes) == 2:
            proba_mode = "binary"
        else:
            proba_mode = "softmax"

    if not clip_model_val and coef.ndim == 2:
        emb_dim = 0
        try:
            emb_dim = int(coef.shape[1])
        except Exception:
            emb_dim = 0
        clip_model_val = _infer_clip_model_from_embedding_dim(emb_dim, active_name=clip_model_name or DEFAULT_CLIP_MODEL)

    bg_indices = _clip_head_background_indices(classes)
    bg_classes = [classes[idx] for idx in bg_indices] if bg_indices else []

    return {
        "classes": classes,
        "background_indices": bg_indices,
        "background_classes": bg_classes,
        "coef": coef,
        "intercept": intercept,
        "clip_model": clip_model_val,
        "encoder_type": encoder_type,
        "encoder_model": encoder_model,
        "proba_mode": proba_mode,
        "classifier_type": "logreg",
        "normalize_embeddings": bool(normalize_flag) if normalize_flag is not None else True,
        "embedding_center_values": center_vals,
        "embedding_std_values": std_vals,
        "temperature": temperature_val,
        "logit_adjustment": logit_adjustment,
        "logit_adjustment_inference": bool(logit_adjustment_inference),
        "min_prob": min_prob_val,
        "margin": margin_val,
    }


def _normalize_class_name_for_match(name: Optional[str]) -> str:
    if not name:
        return ""
    try:
        s = str(name).strip().lower()
    except Exception:
        return ""
    # Treat underscores/hyphens/spaces as equivalent and ignore punctuation.
    return re.sub(r"[^a-z0-9]+", "", s)


def _is_background_class_name(name: Optional[str]) -> bool:
    try:
        label = str(name or "").strip().lower()
    except Exception:
        return False
    return label.startswith("__bg_")


def _clip_head_background_indices(classes: Sequence[str]) -> List[int]:
    return [idx for idx, label in enumerate(classes) if _is_background_class_name(label)]


def _resolve_clip_head_background_settings(payload: "AgentMiningRequest") -> Tuple[bool, bool, float, str]:
    try:
        guard = bool(getattr(payload, "clip_head_background_guard", False))
    except Exception:
        guard = False
    try:
        apply_raw = str(getattr(payload, "clip_head_background_apply", "final") or "final").strip().lower()
    except Exception:
        apply_raw = "final"
    apply_mode = apply_raw if apply_raw in {"seed", "final", "both"} else "final"
    try:
        margin_val = float(getattr(payload, "clip_head_background_margin", 0.0) or 0.0)
    except Exception:
        margin_val = 0.0
    margin_val = max(0.0, min(1.0, margin_val))
    guard_seed = bool(guard and apply_mode in {"seed", "both"})
    guard_final = bool(guard and apply_mode in {"final", "both"})
    return guard_seed, guard_final, float(margin_val), apply_mode


def _find_clip_head_target_index(classes: Sequence[str], class_name: Optional[str]) -> Optional[int]:
    target = _normalize_class_name_for_match(class_name)
    if not target:
        return None
    for idx, c in enumerate(classes):
        if _normalize_class_name_for_match(c) == target:
            return int(idx)
    return None


def _clip_auto_predict_label(
    feats_np: np.ndarray,
    *,
    background_guard: bool = False,
) -> Tuple[str, Optional[float], Optional[str]]:
    """Return (label, probability, error) for auto-classification using the active CLIP head."""
    if clf is None:
        return "unknown", None, "clip_unavailable"
    head = active_classifier_head if isinstance(active_classifier_head, dict) else None
    classes_raw = getattr(clf, "classes_", None)
    if head is not None:
        classes = [str(c) for c in list(head.get("classes") or [])]
    elif classes_raw is None and isinstance(clf, dict):
        classes = [str(c) for c in list(clf.get("classes") or [])]
    else:
        classes = [str(c) for c in list(classes_raw)] if classes_raw is not None else []
    proba_arr: Optional[np.ndarray] = None
    if head is not None:
        proba_arr = _clip_head_predict_proba(feats_np, head)
    elif hasattr(clf, "predict_proba"):
        try:
            proba = clf.predict_proba(feats_np)
            proba_arr = np.asarray(proba, dtype=np.float32)
        except Exception:
            proba_arr = None
    elif isinstance(clf, dict):
        proba_arr = _clip_head_predict_proba(feats_np, clf)
    if proba_arr is not None and proba_arr.ndim == 2 and proba_arr.shape[0] >= 1 and proba_arr.shape[1] == len(classes):
        row = proba_arr[0]
        bg_indices = _clip_head_background_indices(classes)
        if bg_indices:
            non_bg_indices = [idx for idx in range(len(classes)) if idx not in bg_indices]
            if not non_bg_indices:
                return "unknown", float(np.max(row)) if row.size else None, "clip_background"
            best_non_bg = non_bg_indices[int(np.argmax(row[non_bg_indices]))]
            p_non_bg = float(row[best_non_bg])
            if background_guard:
                p_bg = float(np.max(row[bg_indices])) if bg_indices else -1.0
                if p_bg >= p_non_bg:
                    return "unknown", p_bg, "clip_background"
            return str(classes[best_non_bg]), p_non_bg, None
        best_idx = int(np.argmax(row))
        return str(classes[best_idx]), float(row[best_idx]), None
    try:
        pred_cls = clf.predict(feats_np)[0]
    except Exception as exc:  # noqa: BLE001
        return "unknown", None, f"classifier_error:{exc}"
    label = str(pred_cls)
    if _is_background_class_name(label):
        return "unknown", None, "clip_background"
    return label, None, None


def _clip_auto_predict_details(
    feats_np: np.ndarray,
    *,
    background_guard: bool = False,
) -> Dict[str, Optional[object]]:
    if clf is None:
        return {
            "label": "unknown",
            "proba": None,
            "second_label": None,
            "second_proba": None,
            "margin": None,
            "error": "clip_unavailable",
        }
    head = active_classifier_head if isinstance(active_classifier_head, dict) else None
    classes_raw = getattr(clf, "classes_", None)
    if head is not None:
        classes = [str(c) for c in list(head.get("classes") or [])]
    elif classes_raw is None and isinstance(clf, dict):
        classes = [str(c) for c in list(clf.get("classes") or [])]
    else:
        classes = [str(c) for c in list(classes_raw)] if classes_raw is not None else []
    proba_arr: Optional[np.ndarray] = None
    if head is not None:
        proba_arr = _clip_head_predict_proba(feats_np, head)
    elif hasattr(clf, "predict_proba"):
        try:
            proba = clf.predict_proba(feats_np)
            proba_arr = np.asarray(proba, dtype=np.float32)
        except Exception:
            proba_arr = None
    elif isinstance(clf, dict):
        proba_arr = _clip_head_predict_proba(feats_np, clf)
    if proba_arr is not None and proba_arr.ndim == 2 and proba_arr.shape[0] >= 1 and proba_arr.shape[1] == len(classes):
        row = proba_arr[0]
        bg_indices = _clip_head_background_indices(classes)

        def _best_two(indices: Sequence[int]) -> Tuple[Optional[int], Optional[float], Optional[int], Optional[float]]:
            if not indices:
                return None, None, None, None
            ordered = sorted(indices, key=lambda i: float(row[i]), reverse=True)
            best_idx = ordered[0]
            second_idx = ordered[1] if len(ordered) > 1 else None
            best_val = float(row[best_idx])
            second_val = float(row[second_idx]) if second_idx is not None else None
            return best_idx, best_val, second_idx, second_val

        if bg_indices:
            non_bg = [idx for idx in range(len(classes)) if idx not in bg_indices]
            best_idx, best_val, second_idx, second_val = _best_two(non_bg)
            if best_idx is None:
                return {
                    "label": "unknown",
                    "proba": float(np.max(row)) if row.size else None,
                    "second_label": None,
                    "second_proba": None,
                    "margin": None,
                    "error": "clip_background",
                }
            if background_guard:
                p_bg = float(np.max(row[bg_indices])) if bg_indices else -1.0
                if p_bg >= best_val:
                    return {
                        "label": "unknown",
                        "proba": p_bg,
                        "second_label": str(classes[best_idx]),
                        "second_proba": best_val,
                        "margin": float(p_bg - best_val),
                        "error": "clip_background",
                    }
            margin = float(best_val - second_val) if second_val is not None else None
            return {
                "label": str(classes[best_idx]),
                "proba": best_val,
                "second_label": str(classes[second_idx]) if second_idx is not None else None,
                "second_proba": second_val,
                "margin": margin,
                "error": None,
            }

        best_idx, best_val, second_idx, second_val = _best_two(range(len(classes)))
        margin = float(best_val - second_val) if second_val is not None and best_val is not None else None
        return {
            "label": str(classes[best_idx]) if best_idx is not None else "unknown",
            "proba": best_val,
            "second_label": str(classes[second_idx]) if second_idx is not None else None,
            "second_proba": second_val,
            "margin": margin,
            "error": None,
        }

    try:
        pred_cls = clf.predict(feats_np)[0]
    except Exception as exc:  # noqa: BLE001
        return {
            "label": "unknown",
            "proba": None,
            "second_label": None,
            "second_proba": None,
            "margin": None,
            "error": f"classifier_error:{exc}",
        }
    label = str(pred_cls)
    if _is_background_class_name(label):
        return {
            "label": "unknown",
            "proba": None,
            "second_label": None,
            "second_proba": None,
            "margin": None,
            "error": "clip_background",
        }
    return {
        "label": label,
        "proba": None,
        "second_label": None,
        "second_proba": None,
        "margin": None,
        "error": None,
    }


def _score_detections_with_clip_head(
    dets: Sequence[QwenDetection],
    *,
    pil_img: Image.Image,
    clip_head: Dict[str, Any],
    score_mode: Literal["clip_head_prob", "clip_head_margin"],
) -> Optional[Dict[int, float]]:
    """
    Compute CLIP-head-based scores for a list of detections.

    Returns a dict mapping id(det)->score, and also populates det.clip_head_prob/margin where available.
    """
    if not dets:
        return {}
    if not isinstance(clip_head, dict):
        return None
    classes = clip_head.get("classes") if isinstance(clip_head.get("classes"), list) else []
    if not classes:
        return None
    crops: List[Image.Image] = []
    det_refs: List[QwenDetection] = []
    for det in dets:
        bbox_xyxy = _bbox_to_xyxy_pixels(det.bbox or [], pil_img.width, pil_img.height)
        if bbox_xyxy is None:
            continue
        x1, y1, x2, y2 = bbox_xyxy
        if x2 <= x1 or y2 <= y1:
            continue
        try:
            crops.append(pil_img.crop((x1, y1, x2, y2)))
        except Exception:
            continue
        det_refs.append(det)
    if not det_refs:
        return {}

    feats = _encode_pil_batch_for_head(crops, head=clip_head)
    if feats is None or not isinstance(feats, np.ndarray) or feats.size == 0:
        return None
    proba = _clip_head_predict_proba(feats, clip_head)
    if proba is None:
        return None

    bg_indices = _clip_head_background_indices(classes)
    scores: Dict[int, float] = {}
    for det, row in zip(det_refs, proba):
        label = det.class_name or det.qwen_label
        t_idx = _find_clip_head_target_index(classes, label)
        if t_idx is None or t_idx < 0 or t_idx >= row.shape[0]:
            continue
        try:
            p_t = float(row[int(t_idx)])
        except Exception:
            continue
        p_other = 0.0
        try:
            if row.shape[0] > 1:
                other = np.asarray(row, dtype=np.float32).copy()
                other[int(t_idx)] = -1.0
                p_other = float(np.max(other))
        except Exception:
            p_other = 0.0
        p_bg: Optional[float] = None
        if bg_indices:
            try:
                p_bg = float(np.max(row[bg_indices]))
            except Exception:
                p_bg = None
        det.clip_head_prob = p_t
        det.clip_head_margin = float(p_t - p_other)
        if p_bg is not None:
            det.clip_head_bg_prob = float(p_bg)
            det.clip_head_bg_margin = float(p_t - p_bg)
        score = p_t if score_mode == "clip_head_prob" else float(p_t - p_other)
        scores[id(det)] = float(score)
    return scores


def _select_diverse_indices(
    feats: np.ndarray,
    *,
    k: int,
    scores: Optional[np.ndarray] = None,
) -> List[int]:
    """
    Pick up to k diverse indices from feats using a greedy k-center heuristic.
    Distance is cosine distance (1 - cosine_sim). If scores provided, start from max score.
    """
    if feats is None:
        return []
    try:
        feats = np.asarray(feats, dtype=np.float32)
    except Exception:
        return []
    if feats.ndim != 2 or feats.shape[0] == 0:
        return []
    k = int(max(1, k))
    n = feats.shape[0]
    if k >= n:
        return list(range(n))
    # Normalize (should already be normalized).
    norms = np.linalg.norm(feats, axis=1, keepdims=True) + 1e-8
    vecs = feats / norms
    if scores is not None:
        try:
            start_idx = int(np.argmax(scores))
        except Exception:
            start_idx = 0
    else:
        start_idx = 0
    selected = [start_idx]
    dists = 1.0 - (vecs @ vecs[start_idx].reshape(-1, 1)).reshape(-1)
    while len(selected) < k:
        next_idx = int(np.argmax(dists))
        if next_idx in selected:
            break
        selected.append(next_idx)
        d_new = 1.0 - (vecs @ vecs[next_idx].reshape(-1, 1)).reshape(-1)
        dists = np.minimum(dists, d_new)
    return selected


def _apply_agent_recipe_to_image(
    recipe: Dict[str, Any],
    *,
    image: Dict[str, Any],
    dataset_id: str,
    images: Dict[int, Dict[str, Any]],
    mask_threshold: float,
    min_size: int,
    simplify_epsilon: float,
    max_results: int,
    class_id: Optional[int],
    class_name: Optional[str],
    clip_head_min_prob_override: Optional[float] = None,
    clip_head_margin_override: Optional[float] = None,
    extra_clip_classifier_path: Optional[str] = None,
    extra_clip_min_prob: Optional[float] = None,
    extra_clip_margin: Optional[float] = None,
    warnings: Optional[List[str]] = None,
) -> List[QwenDetection]:
    """
    Apply a portable Agent Mining recipe to a single image.

    Supported formats:
    - schema v2 ("sam3_steps"): run each step (text seeds -> optional CLIP seed gate -> expand -> de-dupe),
      then apply final CLIP filtering (embedded CLIP head thresholds, optionally tightened by overrides),
      merge all steps, and de-dupe.
    - legacy ("sam3_greedy"/"legacy_steps"): crop-bank-based flow kept for backward compatibility.

    Notes:
    - `clip_head_min_prob_override` / `clip_head_margin_override` are extra per-run filters applied on top
      of the baked-in recipe thresholds (cumulative via max(...)).
    - `extra_clip_*` applies an additional CLIP head after the recipe's own filtering (also cumulative).
    """
    recipe_body = recipe.get("recipe") if "steps" not in recipe and isinstance(recipe.get("recipe"), dict) else recipe
    img_path = image.get("path")
    if not img_path:
        return []
    try:
        with Image.open(img_path) as pil_img_ctx:
            pil_img = pil_img_ctx.convert("RGB")
    except Exception:
        return []
    recipe_id = recipe.get("id")
    params_combined: Dict[str, Any] = {}
    for src in (recipe.get("params"), recipe_body.get("params")):
        if isinstance(src, dict):
            params_combined.update(src)
    recipe_mode: Literal["sam3_steps", "sam3_greedy", "legacy_steps"] = (
        _classify_agent_recipe_mode(recipe_body) if isinstance(recipe_body, dict) else "sam3_greedy"
    )
    # New-format fields (fallback to legacy step-based recipes for portability/back-compat).
    text_prompts_raw = recipe_body.get("text_prompts")
    positives_raw = recipe_body.get("positives")
    negatives_raw = recipe_body.get("negatives")
    legacy_steps: List[Dict[str, Any]] = []
    steps_v2: List[Dict[str, Any]] = []
    if recipe_mode == "sam3_steps":
        raw_steps = recipe_body.get("steps")
        if isinstance(raw_steps, list):
            steps_v2 = [s for s in raw_steps if isinstance(s, dict)]
    else:
        legacy_steps = _normalize_agent_recipe_steps(recipe_body.get("steps") or [])
        if not isinstance(text_prompts_raw, list) or not text_prompts_raw:
            text_prompts_raw = [
                s.get("prompt")
                for s in legacy_steps
                if (s.get("type") or "text") == "text" and s.get("prompt")
            ]
        if not isinstance(positives_raw, list) or not positives_raw:
            positives_raw = [s.get("exemplar") for s in legacy_steps if s.get("exemplar")]
    if not isinstance(negatives_raw, list):
        negatives_raw = []

    text_prompts = _sanitize_prompts([str(p) for p in (text_prompts_raw or []) if str(p).strip()])
    if not text_prompts and class_name:
        text_prompts = _sanitize_prompts([str(class_name)])

    # Optional embedded CLIP head (trained logistic regression) for filtering.
    recipe_target_class_name = str(recipe.get("class_name") or class_name or "").strip() if isinstance(recipe, dict) else str(class_name or "").strip()
    clip_head_cfg: Optional[Dict[str, Any]] = None
    if isinstance(recipe_body, dict) and isinstance(recipe_body.get("clip_head"), dict):
        clip_head_cfg = recipe_body.get("clip_head")
    elif isinstance(recipe, dict) and isinstance(recipe.get("clip_head"), dict):
        clip_head_cfg = recipe.get("clip_head")
    clip_head_min_prob = 0.5
    clip_head_margin = 0.0
    cfg_sets_min_prob = False
    cfg_sets_margin = False
    if clip_head_cfg:
        try:
            if clip_head_cfg.get("min_prob") is not None:
                clip_head_min_prob = float(clip_head_cfg.get("min_prob"))
                cfg_sets_min_prob = True
            if clip_head_cfg.get("margin") is not None:
                clip_head_margin = float(clip_head_cfg.get("margin"))
                cfg_sets_margin = True
        except Exception:
            clip_head_min_prob = 0.5
            clip_head_margin = 0.0
    clip_head: Optional[Dict[str, Any]] = None
    clip_head_target_index: Optional[int] = None
    clip_head_bg_indices: List[int] = []
    clip_head_missing_class = False
    if recipe_id:
        recipe_root = (AGENT_MINING_RECIPES_ROOT / str(recipe_id)).resolve()
        if _path_is_within_root(recipe_root, AGENT_MINING_RECIPES_ROOT.resolve()):
            clip_head = _load_clip_head_artifacts(recipe_dir=recipe_root, fallback_meta=clip_head_cfg)
    if clip_head:
        # If the recipe JSON doesn't specify these, fall back to embedded meta.json defaults.
        if not cfg_sets_min_prob and clip_head.get("min_prob") is not None:
            try:
                clip_head_min_prob = float(clip_head.get("min_prob"))
            except Exception:
                pass
        if not cfg_sets_margin and clip_head.get("margin") is not None:
            try:
                clip_head_margin = float(clip_head.get("margin"))
            except Exception:
                pass
        classes_list = clip_head.get("classes") if isinstance(clip_head.get("classes"), list) else []
        head_encoder_type_raw = clip_head.get("encoder_type") if isinstance(clip_head, dict) else None
        head_encoder_type = str(head_encoder_type_raw or "clip").lower().strip()
        prefilter_allowed = head_encoder_type == "clip"
        clip_head_bg_indices = _clip_head_background_indices(classes_list)
        clip_head_target_index = _find_clip_head_target_index(classes_list, recipe_target_class_name)
        if clip_head_target_index is None and classes_list and recipe_target_class_name:
            clip_head_missing_class = True
        if recipe_target_class_name and classes_list:
            logger.info(
                "CLIP head class mapping: target=%s index=%s classes=%s",
                recipe_target_class_name,
                clip_head_target_index,
                classes_list,
            )

    def _recipe_param(key: str) -> Any:
        if key in params_combined:
            return params_combined.get(key)
        if isinstance(recipe_body, dict):
            return recipe_body.get(key)
        return None

    def _bool_param(key: str, default: bool) -> bool:
        raw = _recipe_param(key)
        if raw is None:
            return default
        if isinstance(raw, bool):
            return raw
        if isinstance(raw, (int, float)):
            return bool(raw)
        if isinstance(raw, str):
            val = raw.strip().lower()
            if val in {"1", "true", "yes", "y", "on"}:
                return True
            if val in {"0", "false", "no", "n", "off"}:
                return False
        return default

    def _float_param(key: str, default: float) -> float:
        raw = _recipe_param(key)
        if raw is None:
            return default
        try:
            return float(raw)
        except Exception:
            return default

    def _int_param(key: str, default: int) -> int:
        raw = _recipe_param(key)
        if raw is None:
            return default
        try:
            return int(raw)
        except Exception:
            return default

    def _str_param(key: str, default: str) -> str:
        raw = _recipe_param(key)
        if raw is None:
            return default
        try:
            val = str(raw).strip()
        except Exception:
            return default
        return val if val else default

    clip_head_background_guard = _bool_param("clip_head_background_guard", True)
    clip_head_background_margin = _float_param("clip_head_background_margin", 0.0)
    clip_head_background_apply = _str_param("clip_head_background_apply", "final").lower()
    if clip_head_background_apply not in {"seed", "final", "both"}:
        clip_head_background_apply = "final"
    clip_head_background_margin = max(0.0, min(1.0, float(clip_head_background_margin)))
    bg_guard_seed = bool(clip_head_background_guard and clip_head_background_apply in {"seed", "both"})
    bg_guard_final = bool(clip_head_background_guard and clip_head_background_apply in {"final", "both"})

    # Config knobs (recipe params override request params; request params are passed via function args).
    use_clip_guard = _bool_param("use_clip_fp_guard", True)
    use_negatives = _bool_param("use_negative_exemplars", True)
    neg_strength = _float_param("negative_strength", 0.5)
    similarity_floor = _float_param("similarity_score", 0.25)
    seed_threshold = _float_param("seed_threshold", 0.05)
    expand_threshold = _float_param("expand_threshold", 0.3)
    max_visual_seeds = _int_param("max_visual_seeds", 25)
    seed_dedupe_iou = _float_param("seed_dedupe_iou", 0.9)
    out_dedupe_iou = _float_param("dedupe_iou", 0.5)
    seed_threshold = max(0.0, min(1.0, seed_threshold))
    expand_threshold = max(0.0, min(1.0, expand_threshold))
    similarity_floor = max(0.0, min(1.0, similarity_floor))
    max_visual_seeds = max(0, min(500, max_visual_seeds))

    def _add_warning(code: str) -> None:
        if warnings is None:
            return
        if code not in warnings:
            warnings.append(code)

    if clip_head_missing_class:
        _add_warning("clip_head_class_missing")

    def _crop_embed_key(prefix: str, crop_ref: Optional[str]) -> Optional[str]:
        if not crop_ref:
            return None
        try:
            crop_name = Path(str(crop_ref)).name
        except Exception:
            return None
        if not crop_name:
            return None
        return f"{prefix}:{crop_name}"

    # Build embeddings for exemplars/negatives if present.
    def _resolve_recipe_crop_path(recipe_id_val: Optional[str], crop_path_raw: Optional[str]) -> Optional[str]:
        """Resolve a crop path inside the recipe package; ignore absolute/external paths."""
        if not recipe_id_val or not crop_path_raw:
            return None
        try:
            crop_name = Path(crop_path_raw).name
        except Exception:
            return None
        recipe_root = (AGENT_MINING_RECIPES_ROOT / recipe_id_val).resolve()
        if not _path_is_within_root(recipe_root, AGENT_MINING_RECIPES_ROOT.resolve()):
            return None
        crop_root = (recipe_root / "crops").resolve()
        if not _path_is_within_root(crop_root, recipe_root):
            return None
        candidate = (crop_root / crop_name).resolve()
        try:
            if candidate.exists() and candidate.is_file() and _path_is_within_root(candidate, crop_root):
                return str(candidate)
        except Exception:
            return None
        return None

    exemplar_entries: List[Dict[str, Any]] = []
    for ex_raw in positives_raw or []:
        if not isinstance(ex_raw, dict):
            continue
        ex = dict(ex_raw)
        resolved = _resolve_recipe_crop_path(recipe_id, ex.get("crop_path") or ex.get("path"))
        if resolved:
            ex["path"] = resolved
        if ex.get("image_id") is None:
            ex["image_id"] = 0
        ex["bbox"] = [0.5, 0.5, 1.0, 1.0]
        ex_key = _crop_embed_key("crop", ex.get("crop_path") or ex.get("path"))
        if ex_key:
            ex["embed_id"] = ex_key
        if not ex.get("path"):
            _add_warning("missing_crop_exemplar")
        exemplar_entries.append(ex)
    exemplar_embeddings: Dict[str, np.ndarray] = {}
    exemplar_warnings: List[str] = []
    if exemplar_entries:
        exemplar_embeddings, exemplar_warnings = _clip_embed_regions(exemplar_entries, images, max_regions=len(exemplar_entries))
        for w in exemplar_warnings:
            _add_warning(w)

    negative_entries: List[Dict[str, Any]] = []
    if use_negatives and negatives_raw:
        for neg_raw in negatives_raw:
            if not isinstance(neg_raw, dict):
                continue
            entry = dict(neg_raw)
            resolved = _resolve_recipe_crop_path(recipe_id, entry.get("crop_path") or entry.get("path"))
            if resolved:
                entry["path"] = resolved
            if entry.get("image_id") is None:
                entry["image_id"] = 0
            entry["bbox"] = [0.5, 0.5, 1.0, 1.0]
            neg_key = _crop_embed_key("neg", entry.get("crop_path") or entry.get("path"))
            if neg_key:
                entry["embed_id"] = neg_key
            if not entry.get("path"):
                _add_warning("missing_crop_negative")
            negative_entries.append(entry)
    negative_embeddings: Dict[str, np.ndarray] = {}
    if use_clip_guard and use_negatives and negative_entries:
        negative_embeddings, _ = _clip_embed_regions(negative_entries, images, max_regions=len(negative_entries))

    ex_mat = None
    if exemplar_embeddings:
        try:
            ex_mat = np.stack(list(exemplar_embeddings.values())).astype(np.float32)
            ex_mat = ex_mat / (np.linalg.norm(ex_mat, axis=1, keepdims=True) + 1e-8)
        except Exception:
            ex_mat = None
    neg_mat = None
    if use_negatives and negative_embeddings:
        try:
            neg_mat = np.stack(list(negative_embeddings.values())).astype(np.float32)
            neg_mat = neg_mat / (np.linalg.norm(neg_mat, axis=1, keepdims=True) + 1e-8)
        except Exception:
            neg_mat = None

    clip_head_active = bool(isinstance(clip_head, dict) and clip_head_target_index is not None)
    if clip_head_active:
        if clip_head_min_prob_override is not None:
            try:
                extra_min = float(clip_head_min_prob_override)
                extra_min = max(0.0, min(1.0, extra_min))
                clip_head_min_prob = max(float(clip_head_min_prob), extra_min)
            except Exception:
                pass
        if clip_head_margin_override is not None:
            try:
                extra_margin = float(clip_head_margin_override)
                extra_margin = max(0.0, min(1.0, extra_margin))
                clip_head_margin = max(float(clip_head_margin), extra_margin)
            except Exception:
                pass
    elif clip_head_min_prob_override is not None or clip_head_margin_override is not None:
        _add_warning("clip_head_override_ignored")
    if use_clip_guard and ex_mat is None:
        _add_warning("clip_missing_exemplars")
        use_clip_guard = False
    head_encoder_type = "clip"
    if isinstance(clip_head, dict):
        raw_encoder = clip_head.get("encoder_type")
        if isinstance(raw_encoder, str) and raw_encoder.strip():
            head_encoder_type = raw_encoder.strip().lower()
    if use_clip_guard and ex_mat is not None and head_encoder_type != "clip":
        _add_warning("clip_guard_disabled_encoder")
        use_clip_guard = False

    _, processor, _ = _ensure_sam3_text_runtime()
    try:
        state = processor.set_image(pil_img)
    except Exception:
        state = None

    def _reset_prompts() -> None:
        try:
            if hasattr(processor, "reset_all_prompts"):
                processor.reset_all_prompts(state)
                return
        except Exception:
            pass
        try:
            if isinstance(state, dict) and isinstance(state.get("backbone_out"), dict):
                for k in ("language_features", "language_mask", "language_embeds"):
                    state["backbone_out"].pop(k, None)
            if isinstance(state, dict):
                for k in ("geometric_prompt", "boxes", "masks", "masks_logits", "scores"):
                    state.pop(k, None)
        except Exception:
            pass

    def _clip_score(feats: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        # feats: (N, D), already normalized
        pos = np.zeros((feats.shape[0],), dtype=np.float32)
        neg = np.zeros((feats.shape[0],), dtype=np.float32)
        if ex_mat is not None:
            try:
                sims = feats @ ex_mat.T
                pos = np.max(sims, axis=1)
            except Exception:
                pos = np.zeros((feats.shape[0],), dtype=np.float32)
        if neg_mat is not None:
            try:
                sims_n = feats @ neg_mat.T
                neg = np.max(sims_n, axis=1)
            except Exception:
                neg = np.zeros((feats.shape[0],), dtype=np.float32)
        score = pos - max(0.0, neg_strength) * neg
        return pos, neg, score

    classes_list = clip_head.get("classes") if isinstance(clip_head, dict) and isinstance(clip_head.get("classes"), list) else []
    clip_head_bg_indices = _clip_head_background_indices(classes_list)
    bg_margin = max(0.0, min(1.0, float(clip_head_background_margin)))
    bg_guard_seed = bool(clip_head_background_guard_seed)
    bg_guard_final = bool(clip_head_background_guard_final)

    def _final_clip_filter(
        dets_in: List[QwenDetection],
        *,
        sim_floor: float,
        head_min_prob: float,
        head_margin: float,
    ) -> List[QwenDetection]:
        if not dets_in:
            return []
        if not ((use_clip_guard and ex_mat is not None) or clip_head_active):
            return dets_in
        dets_in = _dedupe_qwen_detections_iou(
            dets_in, img_w=pil_img.width, img_h=pil_img.height, iou_thresh=out_dedupe_iou
        )
        det_crops: List[Image.Image] = []
        det_refs: List[QwenDetection] = []
        for det in dets_in:
            bbox_xyxy = _bbox_to_xyxy_pixels(det.bbox or [], pil_img.width, pil_img.height)
            if bbox_xyxy is None:
                continue
            x1, y1, x2, y2 = bbox_xyxy
            if x2 <= x1 or y2 <= y1:
                continue
            try:
                det_crops.append(pil_img.crop((x1, y1, x2, y2)))
            except Exception:
                continue
            det_refs.append(det)
        if det_crops:
            if clip_head_active and isinstance(clip_head, dict):
                det_feats = _encode_pil_batch_for_head(det_crops, head=clip_head)
            else:
                det_feats = _clip_encode_pil_batch(det_crops)
        else:
            det_feats = None
        if det_feats is None or det_feats.size == 0:
            _add_warning("clip_unavailable")
            return dets_in
        keep_mask = np.ones((det_feats.shape[0],), dtype=bool)
        if use_clip_guard and ex_mat is not None:
            pos_s, _, score_s = _clip_score(det_feats)
            keep_mask &= (pos_s >= float(sim_floor)) & (score_s >= 0.0)
        if clip_head_active and clip_head_target_index is not None and isinstance(clip_head, dict):
            proba = _clip_head_predict_proba(det_feats, clip_head)
            if proba is not None:
                t_idx = int(clip_head_target_index)
                try:
                    p_target = proba[:, t_idx]
                    if proba.shape[1] > 1:
                        masked = proba.copy()
                        masked[:, t_idx] = -1.0
                        p_other = np.max(masked, axis=1)
                    else:
                        p_other = np.zeros_like(p_target)
                    p_bg = None
                    if clip_head_bg_indices:
                        try:
                            p_bg = np.max(proba[:, clip_head_bg_indices], axis=1)
                        except Exception:
                            p_bg = None
                    for det_obj, p_t, p_o, bg in zip(
                        det_refs,
                        p_target.tolist(),
                        p_other.tolist(),
                        p_bg.tolist() if p_bg is not None else [None] * len(det_refs),
                    ):
                        det_obj.clip_head_prob = float(p_t)
                        det_obj.clip_head_margin = float(p_t - p_o)
                        if bg is not None:
                            det_obj.clip_head_bg_prob = float(bg)
                            det_obj.clip_head_bg_margin = float(p_t - float(bg))
                except Exception:
                    pass
                head_keep = _clip_head_keep_mask(
                    proba,
                    target_index=t_idx,
                    min_prob=float(head_min_prob),
                    margin=float(head_margin),
                    background_indices=clip_head_bg_indices,
                    background_guard=bg_guard_final,
                    background_margin=clip_head_background_margin,
                )
                if head_keep is not None:
                    keep_mask &= head_keep
            else:
                _add_warning("clip_head_unavailable")
        dets_out = [d for d, ok in zip(det_refs, keep_mask.tolist()) if ok]
        if len(dets_out) < len(det_refs):
            _add_warning("clip_fp_filtered")
        if det_refs and not dets_out:
            _add_warning("clip_filtered_all")
        return dets_out

    if recipe_mode == "sam3_steps":
        filtered_final: List[QwenDetection] = []

        def _step_float(step: Dict[str, Any], key: str, default: float) -> float:
            raw = step.get(key)
            if raw is None:
                return default
            try:
                return float(raw)
            except Exception:
                return default

        def _step_int(step: Dict[str, Any], key: str, default: int) -> int:
            raw = step.get(key)
            if raw is None:
                return default
            try:
                return int(raw)
            except Exception:
                return default

        for _, step in enumerate(steps_v2):
            if step.get("enabled") is False:
                continue
            step_prompts_raw: List[str] = []
            if isinstance(step.get("prompts"), list):
                step_prompts_raw.extend([str(p) for p in step.get("prompts") or [] if str(p).strip()])
            if isinstance(step.get("prompt"), str) and step.get("prompt").strip():
                step_prompts_raw.insert(0, str(step.get("prompt")).strip())
            step_prompts = _sanitize_prompts(step_prompts_raw)
            if not step_prompts:
                continue

            step_seed_thr = max(0.0, min(1.0, _step_float(step, "seed_threshold", seed_threshold)))
            step_expand_thr = max(0.0, min(1.0, _step_float(step, "expand_threshold", expand_threshold)))
            step_max_seeds = max(0, min(500, _step_int(step, "max_visual_seeds", max_visual_seeds)))
            step_seed_iou = max(0.0, min(1.0, _step_float(step, "seed_dedupe_iou", seed_dedupe_iou)))
            step_out_iou = max(0.0, min(1.0, _step_float(step, "dedupe_iou", out_dedupe_iou)))
            step_max_results = max(1, min(5000, _step_int(step, "max_results", max_results)))

            clip_seed_cfg = step.get("clip_seed") if isinstance(step.get("clip_seed"), dict) else {}
            clip_final_cfg = step.get("clip_final") if isinstance(step.get("clip_final"), dict) else {}

            seed_sim_floor = 0.0
            if clip_seed_cfg.get("similarity_floor") is not None:
                try:
                    seed_sim_floor = float(clip_seed_cfg.get("similarity_floor"))
                except Exception:
                    seed_sim_floor = 0.0
            seed_sim_floor = max(0.0, min(1.0, seed_sim_floor))

            # Default seed head gate is permissive to avoid wiping out all seeds.
            seed_head_min = 0.0
            if clip_seed_cfg.get("min_prob") is not None:
                try:
                    seed_head_min = float(clip_seed_cfg.get("min_prob"))
                except Exception:
                    seed_head_min = 0.0
            seed_head_min = max(0.0, min(1.0, seed_head_min))
            seed_head_margin = 0.0
            if clip_seed_cfg.get("margin") is not None:
                try:
                    seed_head_margin = float(clip_seed_cfg.get("margin"))
                except Exception:
                    seed_head_margin = 0.0
            seed_head_margin = max(0.0, min(1.0, seed_head_margin))

            final_sim_floor = float(similarity_floor)
            if clip_final_cfg.get("similarity_floor") is not None:
                try:
                    final_sim_floor = max(final_sim_floor, float(clip_final_cfg.get("similarity_floor")))
                except Exception:
                    pass
            final_sim_floor = max(0.0, min(1.0, final_sim_floor))

            final_head_min = float(clip_head_min_prob)
            if clip_final_cfg.get("min_prob") is not None:
                try:
                    final_head_min = max(final_head_min, float(clip_final_cfg.get("min_prob")))
                except Exception:
                    pass
            final_head_min = max(0.0, min(1.0, final_head_min))
            final_head_margin = float(clip_head_margin)
            if clip_final_cfg.get("margin") is not None:
                try:
                    final_head_margin = max(final_head_margin, float(clip_final_cfg.get("margin")))
                except Exception:
                    pass
            final_head_margin = max(0.0, min(1.0, final_head_margin))

            use_crop_bank = bool(use_clip_guard and ex_mat is not None)
            use_head = bool(clip_head_active and clip_head_target_index is not None)

            dets_step = _infer_sam3_greedy_recipe_on_image(
                pil_img=pil_img,
                processor=processor,
                text_prompts=step_prompts,
                exemplar_embeddings=ex_mat if use_crop_bank else None,
                negative_embeddings=neg_mat if (use_crop_bank and use_negatives and neg_mat is not None) else None,
                seed_threshold=step_seed_thr,
                expand_threshold=step_expand_thr,
                max_visual_seeds=step_max_seeds,
                seed_dedupe_iou=step_seed_iou,
                out_dedupe_iou=step_out_iou,
                mask_threshold=mask_threshold,
                min_size=min_size,
                simplify_epsilon=simplify_epsilon,
                max_results=step_max_results,
                negative_strength=float(neg_strength),
                similarity_floor=float(seed_sim_floor),
                clip_head=clip_head if use_head else None,
                clip_head_target_index=int(clip_head_target_index) if use_head else None,
                clip_head_min_prob=float(seed_head_min),
                clip_head_margin=float(seed_head_margin),
                clip_head_background_guard_seed=bg_guard_seed,
                clip_head_background_guard_final=bg_guard_final,
                clip_head_background_margin=clip_head_background_margin,
                final_similarity_floor=float(final_sim_floor),
                final_clip_head_min_prob=float(final_head_min),
                final_clip_head_margin=float(final_head_margin),
                state=state,
            )
            if not dets_step and (use_crop_bank or use_head):
                # Fallback: if CLIP gating wipes everything out, re-run without CLIP so we can still expand,
                # then rely on the final CLIP filter (cleanliness gate) to suppress FPs.
                _add_warning("clip_filtered_all")
                dets_step = _infer_sam3_greedy_recipe_on_image(
                    pil_img=pil_img,
                    processor=processor,
                    text_prompts=step_prompts,
                    exemplar_embeddings=None,
                    negative_embeddings=None,
                    seed_threshold=step_seed_thr,
                    expand_threshold=step_expand_thr,
                    max_visual_seeds=step_max_seeds,
                    seed_dedupe_iou=step_seed_iou,
                    out_dedupe_iou=step_out_iou,
                    mask_threshold=mask_threshold,
                    min_size=min_size,
                    simplify_epsilon=simplify_epsilon,
                    max_results=step_max_results,
                    negative_strength=0.0,
                    similarity_floor=0.0,
                    clip_head=None,
                    clip_head_target_index=None,
                    clip_head_min_prob=0.0,
                    clip_head_margin=0.0,
                    clip_head_background_guard_seed=False,
                    clip_head_background_guard_final=False,
                    clip_head_background_margin=0.0,
                    state=state,
                )
            if not dets_step:
                continue
            filtered_final.extend(dets_step)

        if not filtered_final:
            _add_warning("no_results")
            return []
        filtered_final = _dedupe_qwen_detections_iou(
            filtered_final, img_w=pil_img.width, img_h=pil_img.height, iou_thresh=out_dedupe_iou
        )
    else:
        # 1) Seed with text prompts at low threshold.
        seed_dets: List[QwenDetection] = []
        for p in text_prompts:
            if not p:
                continue
            _reset_prompts()
            try:
                dets = _run_sam3_text_inference(
                    pil_img,
                    p,
                    seed_threshold,
                    mask_threshold,
                    max_results,
                    min_size=min_size if min_size > 0 else None,
                    simplify_epsilon=simplify_epsilon,
                    processor_override=processor,
                    state=state,
                )
            except Exception:
                continue
            seed_dets.extend(dets or [])

        if not seed_dets:
            _add_warning("no_results")
            return []

        # Dedupe seed candidates aggressively before CLIP scoring.
        seed_dets = _dedupe_qwen_detections_iou(seed_dets, img_w=pil_img.width, img_h=pil_img.height, iou_thresh=seed_dedupe_iou)

        # 2) CLIP-filter seed candidates and select a diverse subset to expand.
        seed_boxes_xywh: List[Tuple[float, float, float, float]] = []
        seed_crops: List[Image.Image] = []
        seed_keep_refs: List[QwenDetection] = []
        for det in seed_dets:
            bbox_xyxy = _bbox_to_xyxy_pixels(det.bbox or [], pil_img.width, pil_img.height)
            if bbox_xyxy is None:
                continue
            x1, y1, x2, y2 = bbox_xyxy
            if x2 <= x1 or y2 <= y1:
                continue
            try:
                crop = pil_img.crop((x1, y1, x2, y2))
            except Exception:
                continue
            seed_crops.append(crop)
            seed_boxes_xywh.append((float(x1), float(y1), float(x2 - x1), float(y2 - y1)))
            seed_keep_refs.append(det)
        need_seed_feats = bool(seed_crops) and (use_clip_guard or clip_head_active)
        if need_seed_feats and seed_crops:
            if clip_head_active and isinstance(clip_head, dict):
                feats = _encode_pil_batch_for_head(seed_crops, head=clip_head)
            else:
                feats = _clip_encode_pil_batch(seed_crops)
        else:
            feats = None
        # IMPORTANT: max_visual_seeds should NOT cap how many detections we keep.
        # We keep all filtered seed detections, and use max_visual_seeds only to bound visual expansion/refinement.
        kept_seed_idx_all: List[int] = []
        expand_seed_idx: List[int] = []
        if feats is not None and feats.size:
            keep_mask = np.ones((feats.shape[0],), dtype=bool)
            scores_for_diverse: Optional[np.ndarray] = None
            if use_clip_guard and ex_mat is not None:
                pos_s, _, score_s = _clip_score(feats)
                keep_mask &= (pos_s >= float(similarity_floor)) & (score_s >= 0.0)
                scores_for_diverse = score_s
            if clip_head_active and isinstance(clip_head, dict) and clip_head_target_index is not None:
                proba = _clip_head_predict_proba(feats, clip_head)
                if proba is not None:
                    t_idx = int(clip_head_target_index)
                    try:
                        p_target = proba[:, t_idx]
                        if proba.shape[1] > 1:
                            masked = proba.copy()
                            masked[:, t_idx] = -1.0
                            p_other = np.max(masked, axis=1)
                        else:
                            p_other = np.zeros_like(p_target)
                        p_bg = None
                        if clip_head_bg_indices:
                            try:
                                p_bg = np.max(proba[:, clip_head_bg_indices], axis=1)
                            except Exception:
                                p_bg = None
                        for det_obj, p_t, p_o, bg in zip(
                            seed_keep_refs,
                            p_target.tolist(),
                            p_other.tolist(),
                            p_bg.tolist() if p_bg is not None else [None] * len(seed_keep_refs),
                        ):
                            det_obj.clip_head_prob = float(p_t)
                            det_obj.clip_head_margin = float(p_t - p_o)
                            if bg is not None:
                                det_obj.clip_head_bg_prob = float(bg)
                                det_obj.clip_head_bg_margin = float(p_t - float(bg))
                    except Exception:
                        pass
                    head_keep = _clip_head_keep_mask(
                        proba,
                        target_index=t_idx,
                        min_prob=float(clip_head_min_prob),
                        margin=float(clip_head_margin),
                        background_indices=clip_head_bg_indices,
                        background_guard=bg_guard_seed,
                        background_margin=clip_head_background_margin,
                    )
                    if head_keep is not None:
                        keep_mask &= head_keep
                    scores_for_diverse = proba[:, t_idx]
            kept_seed_idx_all = [i for i, ok in enumerate(keep_mask.tolist()) if ok]
            if not kept_seed_idx_all:
                _add_warning("clip_filtered_all")
                return []
            if max_visual_seeds > 0:
                kept_feats = feats[kept_seed_idx_all]
                kept_scores = scores_for_diverse[kept_seed_idx_all] if scores_for_diverse is not None else None
                diverse_local = _select_diverse_indices(
                    kept_feats,
                    k=min(int(max_visual_seeds), len(kept_seed_idx_all)),
                    scores=kept_scores,
                )
                expand_seed_idx = [kept_seed_idx_all[i] for i in diverse_local]
        else:
            # No CLIP features available: keep all seeds, but only expand from a small top-score subset to bound runtime.
            kept_seed_idx_all = list(range(len(seed_keep_refs)))
            if max_visual_seeds > 0:
                ranked = sorted(
                    range(len(seed_keep_refs)),
                    key=lambda i: (seed_keep_refs[i].score if seed_keep_refs[i].score is not None else 0.0),
                    reverse=True,
                )
                expand_seed_idx = ranked[: max(0, min(int(max_visual_seeds), len(ranked)))]

        kept_seed_dets: List[QwenDetection] = [seed_keep_refs[i] for i in kept_seed_idx_all if 0 <= i < len(seed_keep_refs)]
        if not kept_seed_dets:
            _add_warning("no_results")
            return []

        # 3) Visual expansion from each chosen seed.
        expanded: List[QwenDetection] = []
        for i in expand_seed_idx:
            if i < 0 or i >= len(seed_boxes_xywh):
                continue
            _reset_prompts()
            try:
                dets = _run_sam3_visual_inference(
                    pil_img,
                    seed_boxes_xywh[i],
                    expand_threshold,
                    mask_threshold,
                    max_results,
                    min_size=min_size if min_size > 0 else None,
                    simplify_epsilon=simplify_epsilon,
                    processor_override=processor,
                    state=state,
                )
            except Exception:
                continue
            expanded.extend(dets or [])

        combined = [*kept_seed_dets, *expanded]
        if not combined:
            _add_warning("no_results")
            return []

        # 4) CLIP FP-guard over final detections (pos/neg crop banks).
        filtered_final = _final_clip_filter(
            combined,
            sim_floor=float(similarity_floor),
            head_min_prob=float(clip_head_min_prob),
            head_margin=float(clip_head_margin),
        )

    # 5) Final dedupe + class assignment.
    final = _dedupe_qwen_detections_iou(filtered_final, img_w=pil_img.width, img_h=pil_img.height, iou_thresh=out_dedupe_iou)
    for det in final:
        if det.class_id is None:
            det.class_id = class_id
        if det.class_name is None:
            det.class_name = class_name
        if det.qwen_label is None and class_name:
            det.qwen_label = class_name

    # Optional extra CLIP classifier filter (post-recipe), useful when a recipe is crop-bank-only.
    if extra_clip_classifier_path:
        target_name = str(class_name or "").strip() or str(recipe_target_class_name or "").strip()
        if not target_name:
            _add_warning("extra_clip_missing_class")
        else:
            try:
                classifier_path = _resolve_agent_clip_classifier_path(extra_clip_classifier_path)
                extra_head = _load_clip_head_from_classifier(classifier_path) if classifier_path else None
            except Exception:
                classifier_path = None
                extra_head = None
            if not classifier_path or not isinstance(extra_head, dict):
                _add_warning("extra_clip_classifier_unavailable")
            else:
                extra_classes = extra_head.get("classes") if isinstance(extra_head.get("classes"), list) else []
                extra_bg_indices = _clip_head_background_indices(extra_classes)
                extra_t_idx = _find_clip_head_target_index(extra_classes, target_name)
                if extra_t_idx is None:
                    _add_warning("extra_clip_class_missing")
                else:
                    extra_min = 0.5
                    if extra_clip_min_prob is not None:
                        try:
                            extra_min = float(extra_clip_min_prob)
                        except Exception:
                            extra_min = 0.5
                    extra_min = max(0.0, min(1.0, extra_min))
                    extra_mar = 0.0
                    if extra_clip_margin is not None:
                        try:
                            extra_mar = float(extra_clip_margin)
                        except Exception:
                            extra_mar = 0.0
                    extra_mar = max(0.0, min(1.0, extra_mar))

                    det_crops: List[Image.Image] = []
                    det_refs: List[QwenDetection] = []
                    for det in final:
                        bbox_xyxy = _bbox_to_xyxy_pixels(det.bbox or [], pil_img.width, pil_img.height)
                        if bbox_xyxy is None:
                            continue
                        x1, y1, x2, y2 = bbox_xyxy
                        if x2 <= x1 or y2 <= y1:
                            continue
                        try:
                            det_crops.append(pil_img.crop((x1, y1, x2, y2)))
                        except Exception:
                            continue
                        det_refs.append(det)

                    det_feats = _encode_pil_batch_for_head(det_crops, head=extra_head) if det_crops else None
                    if det_feats is None or det_feats.size == 0:
                        _add_warning("extra_clip_unavailable")
                    else:
                        proba = _clip_head_predict_proba(det_feats, extra_head)
                        keep = (
                            _clip_head_keep_mask(
                                proba,
                                target_index=int(extra_t_idx),
                                min_prob=float(extra_min),
                                margin=float(extra_mar),
                                background_indices=extra_bg_indices,
                                background_guard=bg_guard_final,
                                background_margin=clip_head_background_margin,
                            )
                            if proba is not None
                            else None
                        )
                        if keep is None:
                            _add_warning("extra_clip_unavailable")
                        else:
                            kept = [d for d, ok in zip(det_refs, keep.tolist()) if ok]
                            if len(kept) < len(det_refs):
                                _add_warning("extra_clip_filtered")
                            if det_refs and not kept:
                                _add_warning("extra_clip_filtered_all")
                            final = kept
    return _prune_detections_for_response(final, warnings=warnings)


def _reset_sam3_prompts_for_state(processor: Any, state: Any) -> None:
    """Best-effort prompt reset for a preloaded SAM3 image state."""
    try:
        if hasattr(processor, "reset_all_prompts"):
            processor.reset_all_prompts(state)
            return
    except Exception:
        pass
    try:
        if isinstance(state, dict) and isinstance(state.get("backbone_out"), dict):
            for k in ("language_features", "language_mask", "language_embeds"):
                state["backbone_out"].pop(k, None)
        if isinstance(state, dict):
            for k in ("geometric_prompt", "boxes", "masks", "masks_logits", "scores"):
                state.pop(k, None)
    except Exception:
        pass


def _infer_sam3_greedy_recipe_on_image(
    *,
    pil_img: Image.Image,
    processor: Any,
    text_prompts: Sequence[str],
    exemplar_embeddings: Optional[np.ndarray],
    negative_embeddings: Optional[np.ndarray],
    seed_threshold: float,
    expand_threshold: float,
    max_visual_seeds: int,
    seed_dedupe_iou: float,
    out_dedupe_iou: float,
    mask_threshold: float,
    min_size: int,
    simplify_epsilon: float,
    max_results: int,
    negative_strength: float,
    similarity_floor: float,
    clip_head: Optional[Dict[str, Any]] = None,
    clip_head_target_index: Optional[int] = None,
    clip_head_min_prob: float = 0.5,
    clip_head_margin: float = 0.0,
    clip_head_background_guard_seed: bool = False,
    clip_head_background_guard_final: bool = False,
    clip_head_background_margin: float = 0.0,
    final_similarity_floor: Optional[float] = None,
    final_clip_head_min_prob: Optional[float] = None,
    final_clip_head_margin: Optional[float] = None,
    stats_out: Optional[Dict[str, int]] = None,
    state: Optional[Any] = None,
    clip_device_override: Optional[str] = None,
) -> List[QwenDetection]:
    """
    Greedy SAM3 recipe inference on a single PIL image using *precomputed* CLIP embedding banks.

    This matches the intended "portable recipe" semantics:
    1) low-threshold text prompt(s) to generate candidate boxes
    2) CLIP filter vs positive/negative crop banks to pick diverse seed boxes
    3) SAM3 visual prompting from those seeds to expand detections
    4) CLIP filter + IoU dedupe to suppress dupes/FPs
    """
    prompts = [p for p in (_sanitize_prompts([str(p) for p in text_prompts if str(p).strip()]) or []) if p]
    counts: Dict[str, int] = {
        "text_candidates_total": 0,
        "candidates_after_dedupe": 0,
        "candidates_kept": 0,
        "expand_candidates": 0,
        "expanded_total": 0,
        "final_seed_total": 0,
        "final_expanded_total": 0,
        "final_total": 0,
        "seed_bg_checked": 0,
        "seed_bg_veto": 0,
        "final_bg_checked": 0,
        "final_bg_veto": 0,
    }

    def _emit_stats() -> None:
        if stats_out is None:
            return
        try:
            stats_out.update({k: int(v) for k, v in counts.items()})
        except Exception:
            return

    if not prompts:
        _emit_stats()
        return []
    img_state = state
    if img_state is None:
        try:
            img_state = processor.set_image(pil_img)
        except Exception:
            _emit_stats()
            return []
    else:
        # Ensure no stale prompt state leaks across calls when reusing an image state.
        _reset_sam3_prompts_for_state(processor, img_state)

    ex_mat = exemplar_embeddings
    neg_mat = negative_embeddings
    use_clip = (ex_mat is not None and isinstance(ex_mat, np.ndarray) and ex_mat.size > 0) or (
        isinstance(clip_head, dict) and clip_head_target_index is not None
    )
    seed_similarity_floor = float(similarity_floor)
    final_similarity_floor = seed_similarity_floor if final_similarity_floor is None else float(final_similarity_floor)
    final_clip_head_min_prob = (
        float(clip_head_min_prob) if final_clip_head_min_prob is None else float(final_clip_head_min_prob)
    )
    final_clip_head_margin = (
        float(clip_head_margin) if final_clip_head_margin is None else float(final_clip_head_margin)
    )
    head_encoder_type = "clip"
    if isinstance(clip_head, dict):
        raw_encoder = clip_head.get("encoder_type")
        if isinstance(raw_encoder, str) and raw_encoder.strip():
            head_encoder_type = raw_encoder.strip().lower()
    if ex_mat is not None and head_encoder_type != "clip":
        ex_mat = None
        neg_mat = None
    classes_list = clip_head.get("classes") if isinstance(clip_head, dict) and isinstance(clip_head.get("classes"), list) else []
    clip_head_bg_indices = _clip_head_background_indices(classes_list)
    bg_margin = max(0.0, min(1.0, float(clip_head_background_margin)))
    bg_guard_seed = bool(clip_head_background_guard_seed)
    bg_guard_final = bool(clip_head_background_guard_final)

    def _clip_score(feats: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        pos = np.zeros((feats.shape[0],), dtype=np.float32)
        neg = np.zeros((feats.shape[0],), dtype=np.float32)
        if ex_mat is not None:
            try:
                pos = np.max(feats @ ex_mat.T, axis=1)
            except Exception:
                pos = np.zeros((feats.shape[0],), dtype=np.float32)
        if neg_mat is not None and neg_mat.size:
            try:
                neg = np.max(feats @ neg_mat.T, axis=1)
            except Exception:
                neg = np.zeros((feats.shape[0],), dtype=np.float32)
        score = pos - max(0.0, float(negative_strength)) * neg
        return pos, neg, score

    def _clip_head_filter(feats: np.ndarray) -> Optional[np.ndarray]:
        if not isinstance(clip_head, dict) or clip_head_target_index is None:
            return None
        proba = _clip_head_predict_proba(feats, clip_head)
        if proba is None:
            return None
        return _clip_head_keep_mask(
            proba,
            target_index=int(clip_head_target_index),
            min_prob=float(clip_head_min_prob),
            margin=float(clip_head_margin),
            background_indices=clip_head_bg_indices,
            background_guard=bg_guard_seed,
            background_margin=bg_margin,
        )

    seed_dets: List[QwenDetection] = []
    for prompt in prompts:
        _reset_sam3_prompts_for_state(processor, img_state)
        try:
            dets = _run_sam3_text_inference(
                pil_img,
                prompt,
                float(seed_threshold),
                float(mask_threshold),
                int(max_results) if max_results else None,
                min_size=min_size if min_size > 0 else None,
                simplify_epsilon=simplify_epsilon,
                processor_override=processor,
                state=img_state,
            )
        except Exception:
            continue
        seed_dets.extend(dets or [])
    if not seed_dets:
        _emit_stats()
        return []
    counts["text_candidates_total"] = int(len(seed_dets))

    seed_dets = _dedupe_qwen_detections_iou(seed_dets, img_w=pil_img.width, img_h=pil_img.height, iou_thresh=seed_dedupe_iou)
    if not seed_dets:
        _emit_stats()
        return []
    counts["candidates_after_dedupe"] = int(len(seed_dets))

    seed_boxes_xywh: List[Tuple[float, float, float, float]] = []
    seed_refs: List[QwenDetection] = []
    seed_crops: List[Image.Image] = []
    for det in seed_dets:
        bbox_xyxy = _bbox_to_xyxy_pixels(det.bbox or [], pil_img.width, pil_img.height)
        if bbox_xyxy is None:
            continue
        x1, y1, x2, y2 = bbox_xyxy
        if x2 <= x1 or y2 <= y1:
            continue
        try:
            seed_crops.append(pil_img.crop((x1, y1, x2, y2)))
        except Exception:
            continue
        seed_boxes_xywh.append((float(x1), float(y1), float(x2 - x1), float(y2 - y1)))
        seed_refs.append(det)
    if not seed_refs:
        _emit_stats()
        return []

    # IMPORTANT: max_visual_seeds should NOT cap how many detections we keep. In crowded scenes the
    # text stage can legitimately return hundreds of objects. We keep *all* filtered seed detections
    # and use max_visual_seeds only to bound any optional visual expansion/refinement work.
    kept_seed_indices_all: List[int] = []
    expand_seed_indices: List[int] = []
    had_seed_feats = False
    seed_feats: Optional[np.ndarray] = None
    seed_scores_for_diverse: Optional[np.ndarray] = None

    if use_clip and seed_crops:
        if isinstance(clip_head, dict) and clip_head_target_index is not None:
            seed_feats = _encode_pil_batch_for_head(seed_crops, head=clip_head, device_override=clip_device_override)
        else:
            seed_feats = _clip_encode_pil_batch(seed_crops, device_override=clip_device_override)
        if seed_feats is not None and seed_feats.size:
            had_seed_feats = True
            keep_mask = np.ones((seed_feats.shape[0],), dtype=bool)
            scores_for_diverse: Optional[np.ndarray] = None
            if ex_mat is not None:
                pos_s, _, score_s = _clip_score(seed_feats)
                keep_mask &= (pos_s >= float(seed_similarity_floor)) & (score_s >= 0.0)
                scores_for_diverse = score_s
            if isinstance(clip_head, dict) and clip_head_target_index is not None:
                try:
                    proba = _clip_head_predict_proba(seed_feats, clip_head)
                except Exception:
                    proba = None
                if proba is not None:
                    t_idx = int(clip_head_target_index)
                    p_target: Optional[np.ndarray] = None
                    p_other: Optional[np.ndarray] = None
                    try:
                        p_target = proba[:, t_idx]
                        if proba.shape[1] > 1:
                            masked = proba.copy()
                            masked[:, t_idx] = -1.0
                            p_other = np.max(masked, axis=1)
                        else:
                            p_other = np.zeros_like(p_target)
                        p_bg = None
                        if clip_head_bg_indices:
                            try:
                                p_bg = np.max(proba[:, clip_head_bg_indices], axis=1)
                            except Exception:
                                p_bg = None
                        for det_obj, p_t, p_o, bg in zip(
                            seed_refs,
                            p_target.tolist(),
                            p_other.tolist(),
                            p_bg.tolist() if p_bg is not None else [None] * len(seed_refs),
                        ):
                            det_obj.clip_head_prob = float(p_t)
                            det_obj.clip_head_margin = float(p_t - p_o)
                            if bg is not None:
                                det_obj.clip_head_bg_prob = float(bg)
                                det_obj.clip_head_bg_margin = float(p_t - float(bg))
                    except Exception:
                        pass
                    if p_target is not None and p_other is not None:
                        base_keep = p_target >= float(clip_head_min_prob)
                        if float(clip_head_margin) > 0.0:
                            base_keep &= p_target >= (p_other + float(clip_head_margin))
                        if bg_guard_seed and clip_head_bg_indices:
                            try:
                                p_bg = np.max(proba[:, clip_head_bg_indices], axis=1)
                                bg_fail = p_target < (p_bg + float(bg_margin))
                                counts["seed_bg_checked"] = int(counts.get("seed_bg_checked", 0)) + int(np.sum(base_keep))
                                counts["seed_bg_veto"] = int(counts.get("seed_bg_veto", 0)) + int(np.sum(base_keep & bg_fail))
                            except Exception:
                                pass
                    head_keep = _clip_head_keep_mask(
                        proba,
                        target_index=t_idx,
                        min_prob=float(clip_head_min_prob),
                        margin=float(clip_head_margin),
                        background_indices=clip_head_bg_indices,
                        background_guard=bg_guard_seed,
                        background_margin=bg_margin,
                    )
                    if head_keep is not None:
                        keep_mask &= head_keep
                    if p_target is not None:
                        scores_for_diverse = p_target
            kept_seed_indices_all = [i for i, ok in enumerate(keep_mask.tolist()) if ok]
            counts["candidates_kept"] = int(len(kept_seed_indices_all))
            seed_scores_for_diverse = scores_for_diverse
            if kept_seed_indices_all and max_visual_seeds > 0:
                kept_feats = seed_feats[kept_seed_indices_all]
                kept_scores = (
                    seed_scores_for_diverse[kept_seed_indices_all]
                    if seed_scores_for_diverse is not None
                    else None
                )
                diverse_local = _select_diverse_indices(
                    kept_feats,
                    k=min(int(max_visual_seeds), len(kept_seed_indices_all)),
                    scores=kept_scores,
                )
                expand_seed_indices = [kept_seed_indices_all[i] for i in diverse_local]
                counts["expand_candidates"] = int(len(expand_seed_indices))
    if not kept_seed_indices_all:
        # If we successfully ran CLIP filtering and it rejected everything, return no detections.
        if had_seed_feats:
            _emit_stats()
            return []
        kept_seed_indices_all = list(range(len(seed_refs)))
        counts["candidates_kept"] = int(len(kept_seed_indices_all))
        if max_visual_seeds and int(max_visual_seeds) > 0:
            ranked = sorted(
                range(len(seed_refs)),
                key=lambda i: (seed_refs[i].score if seed_refs[i].score is not None else 0.0),
                reverse=True,
            )
            expand_seed_indices = ranked[: min(int(max_visual_seeds), len(ranked))]
            counts["expand_candidates"] = int(len(expand_seed_indices))
        else:
            expand_seed_indices = []

    kept_seed_dets = [seed_refs[i] for i in kept_seed_indices_all if 0 <= i < len(seed_refs)]

    expanded: List[QwenDetection] = []
    for i in expand_seed_indices:
        if i < 0 or i >= len(seed_boxes_xywh):
            continue
        _reset_sam3_prompts_for_state(processor, img_state)
        try:
            dets = _run_sam3_visual_inference(
                pil_img,
                seed_boxes_xywh[i],
                float(expand_threshold),
                float(mask_threshold),
                int(max_results) if max_results else None,
                min_size=min_size if min_size > 0 else None,
                simplify_epsilon=simplify_epsilon,
                processor_override=processor,
                state=img_state,
            )
        except Exception:
            continue
        expanded.extend(dets or [])
    counts["expanded_total"] = int(len(expanded))

    combined = [*kept_seed_dets, *expanded]
    if not combined:
        _emit_stats()
        return []
    combined = _dedupe_qwen_detections_iou(combined, img_w=pil_img.width, img_h=pil_img.height, iou_thresh=out_dedupe_iou)

    # Avoid re-encoding potentially huge seed sets: seeds are already CLIP-filtered above (and have
    # clip_head_prob/margin attached when head mode is active). Only CLIP-filter the expanded items.
    if use_clip and expanded:
        det_crops: List[Image.Image] = []
        det_refs: List[QwenDetection] = []
        for det in expanded:
            bbox_xyxy = _bbox_to_xyxy_pixels(det.bbox or [], pil_img.width, pil_img.height)
            if bbox_xyxy is None:
                continue
            x1, y1, x2, y2 = bbox_xyxy
            if x2 <= x1 or y2 <= y1:
                continue
            try:
                det_crops.append(pil_img.crop((x1, y1, x2, y2)))
            except Exception:
                continue
            det_refs.append(det)
        if det_crops:
            if isinstance(clip_head, dict) and clip_head_target_index is not None:
                feats2 = _encode_pil_batch_for_head(det_crops, head=clip_head, device_override=clip_device_override)
            else:
                feats2 = _clip_encode_pil_batch(det_crops, device_override=clip_device_override)
        else:
            feats2 = None
        if feats2 is not None and feats2.size:
            keep_mask2 = np.ones((feats2.shape[0],), dtype=bool)
            if ex_mat is not None:
                pos_s, _, score_s = _clip_score(feats2)
                keep_mask2 &= (pos_s >= float(final_similarity_floor)) & (score_s >= 0.0)
            if isinstance(clip_head, dict) and clip_head_target_index is not None:
                try:
                    proba2 = _clip_head_predict_proba(feats2, clip_head)
                except Exception:
                    proba2 = None
                if proba2 is not None:
                    t_idx = int(clip_head_target_index)
                    p_other: Optional[np.ndarray] = None
                    try:
                        p_target = proba2[:, t_idx]
                        if proba2.shape[1] > 1:
                            masked = proba2.copy()
                            masked[:, t_idx] = -1.0
                            p_other = np.max(masked, axis=1)
                        else:
                            p_other = np.zeros_like(p_target)
                        p_bg = None
                        if clip_head_bg_indices:
                            try:
                                p_bg = np.max(proba2[:, clip_head_bg_indices], axis=1)
                            except Exception:
                                p_bg = None
                        for det_obj, p_t, p_o, bg in zip(
                            det_refs,
                            p_target.tolist(),
                            p_other.tolist(),
                            p_bg.tolist() if p_bg is not None else [None] * len(det_refs),
                        ):
                            det_obj.clip_head_prob = float(p_t)
                            det_obj.clip_head_margin = float(p_t - p_o)
                            if bg is not None:
                                det_obj.clip_head_bg_prob = float(bg)
                                det_obj.clip_head_bg_margin = float(p_t - float(bg))
                    except Exception:
                        pass
                    if p_target is not None and p_other is not None:
                        base_keep = p_target >= float(final_clip_head_min_prob)
                        if float(final_clip_head_margin) > 0.0:
                            base_keep &= p_target >= (p_other + float(final_clip_head_margin))
                        if bg_guard_final and clip_head_bg_indices:
                            try:
                                p_bg = np.max(proba2[:, clip_head_bg_indices], axis=1)
                                bg_fail = p_target < (p_bg + float(bg_margin))
                                counts["final_bg_checked"] = int(counts.get("final_bg_checked", 0)) + int(np.sum(base_keep))
                                counts["final_bg_veto"] = int(counts.get("final_bg_veto", 0)) + int(np.sum(base_keep & bg_fail))
                            except Exception:
                                pass
                    head_keep2 = _clip_head_keep_mask(
                        proba2,
                        target_index=t_idx,
                        min_prob=float(final_clip_head_min_prob),
                        margin=float(final_clip_head_margin),
                        background_indices=clip_head_bg_indices,
                        background_guard=bg_guard_final,
                        background_margin=bg_margin,
                    )
                    if head_keep2 is not None:
                        keep_mask2 &= head_keep2
            expanded_filtered = [d for d, ok in zip(det_refs, keep_mask2.tolist()) if ok]
            combined = [*kept_seed_dets, *expanded_filtered]
            combined = _dedupe_qwen_detections_iou(combined, img_w=pil_img.width, img_h=pil_img.height, iou_thresh=out_dedupe_iou)
    try:
        counts["final_seed_total"] = int(
            len([d for d in combined if not (isinstance(d.qwen_label, str) and d.qwen_label.strip().lower() == "visual")])
        )
        counts["final_expanded_total"] = int(
            len([d for d in combined if isinstance(d.qwen_label, str) and d.qwen_label.strip().lower() == "visual"])
        )
    except Exception:
        pass
    counts["final_total"] = int(len(combined))
    _emit_stats()
    return combined


def _build_clip_head_sweep_grid(
    payload: "AgentMiningRequest",
    *,
    base_min_prob: float,
    base_margin: float,
    base_bg_margin: float,
    allow_bg_tune: bool,
    allow_margin_tune: Optional[bool] = None,
) -> Tuple[List[float], List[float], List[float], float]:
    """Return (min_prob candidates, margin candidates, bg_margin candidates, target_precision)."""
    try:
        base_min = float(base_min_prob)
    except Exception:
        base_min = 0.5
    try:
        base_mar = float(base_margin)
    except Exception:
        base_mar = 0.0
    try:
        base_bg = float(base_bg_margin)
    except Exception:
        base_bg = 0.0
    base_min = max(0.0, min(1.0, base_min))
    base_mar = max(0.0, min(1.0, base_mar))
    base_bg = max(0.0, min(1.0, base_bg))
    try:
        target_precision = float(getattr(payload, "clip_head_target_precision", 0.9))
    except Exception:
        target_precision = 0.9
    target_precision = max(0.0, min(1.0, target_precision))

    auto_tune = True
    try:
        auto_tune = bool(getattr(payload, "clip_head_auto_tune", True))
    except Exception:
        auto_tune = True
    bg_auto_tune = True
    try:
        bg_auto_tune = bool(getattr(payload, "clip_head_background_auto_tune", True))
    except Exception:
        bg_auto_tune = True
    margin_auto_tune = True
    try:
        margin_auto_tune = bool(
            getattr(payload, "clip_head_tune_margin", True)
            if allow_margin_tune is None
            else allow_margin_tune
        )
    except Exception:
        margin_auto_tune = True

    if not auto_tune:
        return [base_min], [base_mar], [base_bg], target_precision

    # IMPORTANT: For some classes, head probabilities cluster very close to 0 (especially when crops are small).
    # A coarse 0.05 grid can miss the useful operating region entirely, making auto-tune look “broken”.
    min_probs_raw: List[float] = []
    # Very low thresholds (logistic heads often need these).
    min_probs_raw.extend([0.0, 0.001, 0.002, 0.005])
    # Dense sweep near zero.
    min_probs_raw.extend([round(i * 0.01, 3) for i in range(0, 11)])  # 0.00..0.10
    # Coarser sweep above 0.10.
    min_probs_raw.extend([round(i * 0.05, 3) for i in range(3, 20)])  # 0.15..0.95
    # Extra high-end points.
    min_probs_raw.extend([0.975, 0.99])
    # Always include the user-provided seed/default.
    min_probs_raw.append(base_min)
    min_probs = sorted({float(max(0.0, min(1.0, p))) for p in min_probs_raw})

    margins = [0.0, 0.05, 0.1, 0.2]
    margins.append(base_mar)
    margins = sorted({float(max(0.0, min(1.0, m))) for m in margins})
    if not margin_auto_tune:
        margins = [base_mar]
    bg_margins = [0.0, 0.02, 0.05, 0.1, 0.2]
    bg_margins.append(base_bg)
    bg_margins = sorted({float(max(0.0, min(1.0, m))) for m in bg_margins})
    if not allow_bg_tune or not bg_auto_tune:
        bg_margins = [base_bg]
    return min_probs, margins, bg_margins, target_precision


def _score_head_tuning_candidate(
    *,
    matched: int,
    fps: int,
    precision: float,
    min_prob: float,
    margin: float,
    bg_margin: float,
    target_precision: float,
) -> Tuple[int, float, int, float, float, float]:
    meets_target = bool(matched > 0 and precision >= float(target_precision))
    if meets_target:
        # Priority: reach target precision, then maximize matches (coverage), then minimize FPs.
        # Tie-breaker: prefer the least-restrictive thresholds that still hit the target (more robust).
        return (1, float(matched), -int(fps), float(precision), -float(min_prob), -float(margin), -float(bg_margin))
    # If nothing hits the target, pick the best precision we can get, then maximize matches.
    return (0, float(precision), int(matched), -int(fps), -float(min_prob), -float(margin), -float(bg_margin))


def _update_best_clip_head_sweep_summary(
    *,
    best_summary: Optional[Dict[str, Any]],
    best_key: Optional[Tuple[int, float, int, float, float, float, float]],
    total_gt: int,
    total_images: int,
    matched: int,
    fps: int,
    duplicates: int,
    preds: int,
    det_images: int,
    min_prob: float,
    margin: float,
    bg_margin: float,
    target_precision: float,
    debug: Optional[Dict[str, Any]] = None,
) -> Tuple[Optional[Dict[str, Any]], Optional[Tuple[int, float, int, float, float, float, float]]]:
    recall = matched / total_gt if total_gt else 0.0
    precision = matched / max(1, matched + fps)
    det_rate = det_images / total_images if total_images else 0.0
    key = _score_head_tuning_candidate(
        matched=int(matched),
        fps=int(fps),
        precision=float(precision),
        min_prob=float(min_prob),
        margin=float(margin),
        bg_margin=float(bg_margin),
        target_precision=float(target_precision),
    )
    if best_key is not None and key <= best_key:
        return best_summary, best_key
    summary: Dict[str, Any] = {
        "gts": int(total_gt),
        "matches": int(matched),
        "fps": int(fps),
        "duplicates": int(duplicates),
        "preds": int(preds),
        "precision": float(precision),
        "recall": float(recall),
        "coverage_rate": float(recall),
        "det_rate": float(det_rate),
        "clip_head_min_prob": float(min_prob),
        "clip_head_margin": float(margin),
        "clip_head_background_margin": float(bg_margin),
        "clip_head_target_precision": float(target_precision),
        "clip_head_meets_target_precision": bool(float(precision) >= float(target_precision)),
    }
    if debug is not None:
        summary["debug"] = debug
    return summary, key


def _successive_halving_search(
    *,
    candidates: Sequence[Any],
    budgets: Sequence[int],
    evaluator: Callable[[Any, int], Tuple[Tuple[Any, ...], Any]],
    keep_ratio: float = 0.5,
    log_fn: Optional[Callable[[str], None]] = None,
    log_prefix: str = "",
) -> Tuple[Any, List[Dict[str, Any]]]:
    """
    Deterministic successive-halving controller.

    - candidates: initial list of candidate configs
    - budgets: increasing list of evaluation budgets (ints); larger budgets are more expensive but more reliable
    - evaluator: function(candidate, budget) -> (key, result), where `key` is a comparable tuple; larger is better
    - keep_ratio: fraction of candidates to keep between stages (0 < keep_ratio <= 1)
    """
    if not candidates:
        raise ValueError("no_candidates")
    if not budgets:
        raise ValueError("no_budgets")
    budgets_clean: List[int] = []
    last = 0
    for b in budgets:
        try:
            b_int = int(b)
        except Exception:
            continue
        if b_int <= 0:
            continue
        if b_int <= last:
            raise ValueError("budgets_must_be_increasing")
        budgets_clean.append(b_int)
        last = b_int
    if not budgets_clean:
        raise ValueError("no_valid_budgets")
    try:
        keep = float(keep_ratio)
    except Exception:
        keep = 0.5
    keep = max(0.0, min(1.0, keep))
    if keep <= 0.0:
        raise ValueError("keep_ratio_must_be_positive")

    active: List[Any] = list(candidates)
    history: List[Dict[str, Any]] = []
    for stage_idx, budget in enumerate(budgets_clean):
        evaluated: List[Tuple[Tuple[Any, ...], Any, Any]] = []
        total_candidates = len(active)
        stage_total = len(budgets_clean)
        if log_fn:
            try:
                prefix = f"{log_prefix} " if log_prefix else ""
                log_fn(
                    f"{prefix}Budget pass {stage_idx + 1}/{stage_total}: "
                    f"eval_cap={int(budget)} candidates={total_candidates}"
                )
            except Exception:
                pass
        log_every = max(1, total_candidates // 5) if total_candidates > 0 else 1
        for idx, cand in enumerate(active, start=1):
            key, result = evaluator(cand, int(budget))
            evaluated.append((key, cand, result))
            if log_fn and (idx == total_candidates or idx % log_every == 0):
                try:
                    prefix = f"{log_prefix} " if log_prefix else ""
                    log_fn(f"{prefix}Budget pass {stage_idx + 1}: evaluated {idx}/{total_candidates} candidates")
                except Exception:
                    pass
        evaluated.sort(key=lambda x: x[0], reverse=True)
        best_key = evaluated[0][0] if evaluated else None
        if log_fn and evaluated:
            try:
                prefix = f"{log_prefix} " if log_prefix else ""
                best_result = evaluated[0][2]
                best_summary = best_result.get("summary") if isinstance(best_result, dict) else None
                if isinstance(best_summary, dict):
                    matches = int(best_summary.get("matches") or 0)
                    fps = int(best_summary.get("fps") or 0)
                    precision = float(best_summary.get("precision") or 0.0)
                    min_prob = best_summary.get("clip_head_min_prob")
                    margin = best_summary.get("clip_head_margin")
                    head_bits = []
                    if min_prob is not None:
                        head_bits.append(f"p≥{float(min_prob):.3f}")
                    if margin is not None and float(margin) > 0:
                        head_bits.append(f"Δ≥{float(margin):.3f}")
                    head_str = f" ({' '.join(head_bits)})" if head_bits else ""
                    log_fn(
                        f"{prefix}Budget pass {stage_idx + 1}: best matches={matches} fps={fps} "
                        f"prec={precision:.3f}{head_str}"
                    )
            except Exception:
                pass
        history.append(
            {
                "stage": int(stage_idx),
                "budget": int(budget),
                "n_candidates": int(len(active)),
                "best_key": best_key,
            }
        )
        if stage_idx >= len(budgets_clean) - 1:
            break
        keep_n = max(1, int(math.ceil(len(evaluated) * keep)))
        if log_fn:
            try:
                prefix = f"{log_prefix} " if log_prefix else ""
                log_fn(
                    f"{prefix}Budget pass {stage_idx + 1}: keeping {keep_n}/{len(evaluated)} candidates "
                    f"(best_key={best_key})"
                )
            except Exception:
                pass
        active = [cand for _, cand, _ in evaluated[:keep_n]]

    if not evaluated:
        raise ValueError("no_evaluations")
    return evaluated[0][1], history


def _evaluate_sam3_greedy_recipe(
    *,
    cat_id: int,
    image_ids: Sequence[int],
    images: Dict[int, Dict[str, Any]],
    gt_by_image_cat: Dict[int, Dict[int, List[List[float]]]],
    processor: Any,
    text_prompts: Sequence[str],
    exemplar_embeddings: Optional[np.ndarray],
    negative_embeddings: Optional[np.ndarray],
    clip_head: Optional[Dict[str, Any]] = None,
    clip_head_target_index: Optional[int] = None,
    clip_head_min_prob: float = 0.5,
    clip_head_margin: float = 0.0,
    payload: AgentMiningRequest,
    cancel_event: Optional[threading.Event] = None,
    log_every: int = 25,
    log_fn: Optional[Callable[[str], None]] = None,
) -> Dict[str, Any]:
    total_gt = 0
    for img_id in image_ids:
        total_gt += len((gt_by_image_cat.get(int(img_id)) or {}).get(int(cat_id)) or [])

    bg_guard_seed, bg_guard_final, bg_margin, _bg_apply = _resolve_clip_head_background_settings(payload)
    classes_list = clip_head.get("classes") if isinstance(clip_head, dict) and isinstance(clip_head.get("classes"), list) else []
    bg_indices = _clip_head_background_indices(classes_list)
    allow_bg_tune = bool(bg_indices and getattr(payload, "clip_head_background_guard", False))
    hard_neg_cfg = _resolve_steps_hard_negative_export_config(payload) if export_hard_negatives else {"enabled": False}
    hard_neg_enabled = bool(hard_neg_cfg.get("enabled"))
    try:
        hard_neg_min_prob = float(hard_neg_cfg.get("min_prob") or 0.0)
    except Exception:
        hard_neg_min_prob = 0.0
    try:
        hard_neg_max_crops = int(hard_neg_cfg.get("max_crops") or 0)
    except Exception:
        hard_neg_max_crops = 0
    hard_neg_entries: List[Dict[str, Any]] = []
    hard_neg_lock = threading.Lock()
    classes_list = clip_head.get("classes") if isinstance(clip_head, dict) and isinstance(clip_head.get("classes"), list) else []
    bg_indices = _clip_head_background_indices(classes_list)
    allow_bg_tune = bool(bg_indices and getattr(payload, "clip_head_background_guard", False))

    head_active = bool(isinstance(clip_head, dict) and clip_head_target_index is not None)
    if head_active:
        # Sweep head thresholds without re-running SAM3: run the greedy recipe once with no head thresholding,
        # keep per-detection head probabilities, then apply different thresholds during scoring.
        classes_list = clip_head.get("classes") if isinstance(clip_head, dict) and isinstance(clip_head.get("classes"), list) else []
        bg_indices = _clip_head_background_indices(classes_list)
        allow_bg_tune = bool(bg_indices and getattr(payload, "clip_head_background_guard", False))
        sweep_min_probs, sweep_margins, sweep_bg_margins, target_precision = _build_clip_head_sweep_grid(
            payload,
            base_min_prob=float(clip_head_min_prob),
            base_margin=float(clip_head_margin),
            base_bg_margin=float(bg_margin),
            allow_bg_tune=allow_bg_tune,
        )

        per_image_rows: Dict[int, List[Tuple[float, float, float, Optional[float], Optional[int]]]] = {}
        observed_prob_min: Optional[float] = None
        observed_prob_max: Optional[float] = None
        observed_rows = 0
        observed_with_prob = 0
        for idx, img_id in enumerate(image_ids, start=1):
            if cancel_event is not None and cancel_event.is_set():
                break
            info = images.get(int(img_id)) or {}
            path = info.get("path")
            if not path:
                continue
            try:
                with Image.open(path) as img:
                    pil_img = img.convert("RGB")
            except Exception:
                continue
            dets = _infer_sam3_greedy_recipe_on_image(
                pil_img=pil_img,
                processor=processor,
                text_prompts=text_prompts,
                exemplar_embeddings=exemplar_embeddings,
                negative_embeddings=negative_embeddings,
                seed_threshold=payload.seed_threshold,
                expand_threshold=payload.expand_threshold,
                max_visual_seeds=payload.max_visual_seeds,
                seed_dedupe_iou=payload.seed_dedupe_iou,
                out_dedupe_iou=payload.dedupe_iou,
                mask_threshold=payload.mask_threshold,
                min_size=payload.min_size,
                simplify_epsilon=payload.simplify_epsilon,
                max_results=payload.max_results,
                negative_strength=payload.negative_strength,
                similarity_floor=payload.similarity_score,
                clip_head=clip_head,
                clip_head_target_index=clip_head_target_index,
                clip_head_min_prob=0.0,
                clip_head_margin=0.0,
                clip_head_background_guard_seed=bg_guard_seed,
                clip_head_background_guard_final=bg_guard_final,
                clip_head_background_margin=bg_margin,
            )
            gt_boxes = (gt_by_image_cat.get(int(img_id)) or {}).get(int(cat_id)) or []
            gt_xyxy = [_xywh_to_xyxy(b) for b in gt_boxes]
            rows: List[Tuple[float, float, float, Optional[float], Optional[int]]] = []
            for det in dets:
                bbox = det.bbox or []
                if len(bbox) < 4:
                    continue
                try:
                    det_xyxy = yolo_to_corners(bbox, pil_img.width, pil_img.height)
                except Exception:
                    continue
                best_iou = 0.0
                best_idx: Optional[int] = None
                for j, gt in enumerate(gt_xyxy):
                    iou = _iou_xyxy(det_xyxy, gt)
                    if iou > best_iou:
                        best_iou = iou
                        best_idx = j
                prob = float(det.clip_head_prob) if det.clip_head_prob is not None else 0.0
                margin = float(det.clip_head_margin) if det.clip_head_margin is not None else 0.0
                bg_prob = float(det.clip_head_bg_prob) if det.clip_head_bg_prob is not None else None
                rows.append((prob, margin, float(best_iou), bg_prob, best_idx))
                observed_rows += 1
                if det.clip_head_prob is not None:
                    observed_with_prob += 1
                    if observed_prob_min is None or prob < observed_prob_min:
                        observed_prob_min = float(prob)
                    if observed_prob_max is None or prob > observed_prob_max:
                        observed_prob_max = float(prob)
            per_image_rows[int(img_id)] = rows
            if log_fn and log_every > 0 and idx % log_every == 0:
                try:
                    log_fn(f"Processed {idx}/{len(image_ids)} val images for class_id {cat_id}")
                except Exception:
                    pass

        if log_fn and total_gt:
            try:
                if observed_rows == 0:
                    log_fn(f"CLIP head sweep: no base detections for class_id {cat_id} (gt={total_gt}).")
                elif observed_with_prob == 0:
                    log_fn(f"CLIP head sweep: base dets={observed_rows} but no head probs for class_id {cat_id} (gt={total_gt}).")
            except Exception:
                pass

        best_summary: Optional[Dict[str, Any]] = None
        best_key: Optional[Tuple[int, float, int, float, float, float, float]] = None
        for margin_thr in sweep_margins:
            for min_prob in sweep_min_probs:
                for bg_margin_thr in sweep_bg_margins:
                    matched = 0
                    fps = 0
                    duplicates = 0
                    preds = 0
                    det_images = 0
                    for img_id in image_ids:
                        rows = per_image_rows.get(int(img_id)) or []
                        used: set[int] = set()
                        any_det = False
                        for prob, margin, best_iou, bg_prob, best_idx in rows:
                            if prob < float(min_prob):
                                continue
                            if float(margin_thr) > 0.0 and margin < float(margin_thr):
                                continue
                            if allow_bg_tune and float(bg_margin_thr) > 0.0 and bg_prob is not None:
                                if prob < float(bg_prob) + float(bg_margin_thr):
                                    continue
                            any_det = True
                            preds += 1
                            if best_idx is not None and best_iou >= float(payload.iou_threshold):
                                if best_idx in used:
                                    duplicates += 1
                                else:
                                    used.add(best_idx)
                                    matched += 1
                            else:
                                fps += 1
                        if any_det:
                            det_images += 1
                    best_summary, best_key = _update_best_clip_head_sweep_summary(
                        best_summary=best_summary,
                        best_key=best_key,
                        total_gt=int(total_gt),
                        total_images=int(len(image_ids)),
                        matched=int(matched),
                        fps=int(fps),
                        duplicates=int(duplicates),
                        preds=int(preds),
                        det_images=int(det_images),
                        min_prob=float(min_prob),
                        margin=float(margin_thr),
                        bg_margin=float(bg_margin_thr),
                        target_precision=float(target_precision),
                    )
        if log_fn and total_gt and best_summary is not None:
            try:
                if int(best_summary.get("preds") or 0) == 0 and observed_rows > 0:
                    if observed_prob_max is not None:
                        log_fn(
                            f"CLIP head sweep: class_id {cat_id} prob_range={float(observed_prob_min or observed_prob_max):.3f}..{float(observed_prob_max):.3f} "
                            f"over {observed_rows} dets; tuned min_prob={float(best_summary.get('clip_head_min_prob') or 0.0):.3f}"
                        )
                    else:
                        log_fn(
                            f"CLIP head sweep: class_id {cat_id} had {observed_rows} dets but no head probs; tuned min_prob={float(best_summary.get('clip_head_min_prob') or 0.0):.3f}"
                        )
            except Exception:
                pass
        if best_summary is None:
            best_summary = {
                "gts": total_gt,
                "matches": 0,
                "fps": 0,
                "duplicates": 0,
                "preds": 0,
                "precision": 0.0,
                "recall": 0.0,
                "coverage_rate": 0.0,
                "det_rate": 0.0,
                "clip_head_min_prob": float(clip_head_min_prob),
                "clip_head_margin": float(clip_head_margin),
                "clip_head_background_margin": float(bg_margin),
                "clip_head_target_precision": float(target_precision),
                "clip_head_meets_target_precision": False,
            }
        return best_summary

    matched = 0
    fps = 0
    duplicates = 0
    preds = 0
    det_images = 0
    for idx, img_id in enumerate(image_ids, start=1):
        if cancel_event is not None and cancel_event.is_set():
            break
        info = images.get(int(img_id)) or {}
        path = info.get("path")
        if not path:
            continue
        try:
            with Image.open(path) as img:
                pil_img = img.convert("RGB")
        except Exception:
            continue
        dets = _infer_sam3_greedy_recipe_on_image(
            pil_img=pil_img,
            processor=processor,
            text_prompts=text_prompts,
            exemplar_embeddings=exemplar_embeddings,
            negative_embeddings=negative_embeddings,
            seed_threshold=payload.seed_threshold,
            expand_threshold=payload.expand_threshold,
            max_visual_seeds=payload.max_visual_seeds,
            seed_dedupe_iou=payload.seed_dedupe_iou,
            out_dedupe_iou=payload.dedupe_iou,
            mask_threshold=payload.mask_threshold,
            min_size=payload.min_size,
            simplify_epsilon=payload.simplify_epsilon,
            max_results=payload.max_results,
            negative_strength=payload.negative_strength,
            similarity_floor=payload.similarity_score,
            clip_head=clip_head,
            clip_head_target_index=clip_head_target_index,
            clip_head_min_prob=clip_head_min_prob,
            clip_head_margin=clip_head_margin,
            clip_head_background_guard_seed=bg_guard_seed,
            clip_head_background_guard_final=bg_guard_final,
            clip_head_background_margin=bg_margin,
        )
        if dets:
            det_images += 1
        gt_boxes = (gt_by_image_cat.get(int(img_id)) or {}).get(int(cat_id)) or []
        gt_xyxy = [_xywh_to_xyxy(b) for b in gt_boxes]
        used: set[int] = set()
        for det in dets:
            preds += 1
            bbox = det.bbox or []
            if len(bbox) < 4:
                continue
            try:
                det_xyxy = yolo_to_corners(bbox, pil_img.width, pil_img.height)
            except Exception:
                continue
            best_iou = 0.0
            best_idx = None
            for j, gt in enumerate(gt_xyxy):
                iou = _iou_xyxy(det_xyxy, gt)
                if iou > best_iou:
                    best_iou = iou
                    best_idx = j
            if best_idx is not None and best_iou >= float(payload.iou_threshold):
                if best_idx in used:
                    duplicates += 1
                else:
                    used.add(best_idx)
                    matched += 1
            else:
                fps += 1
        if log_fn and log_every > 0 and idx % log_every == 0:
            try:
                log_fn(f"Processed {idx}/{len(image_ids)} val images for class {cat_id}")
            except Exception:
                pass
    recall = matched / total_gt if total_gt else 0.0
    precision = matched / max(1, matched + fps)
    det_rate = det_images / len(image_ids) if image_ids else 0.0
    return {
        "gts": total_gt,
        "matches": matched,
        "fps": fps,
        "duplicates": duplicates,
        "preds": preds,
        "precision": precision,
        "recall": recall,
        "coverage_rate": recall,
        "det_rate": det_rate,
    }


class _Sam3GreedyEvalWorker:
    def __init__(self, device: torch.device):
        self.device = device
        self.model, self.processor = _build_sam3_text_processor_for_device(device)
        self.lock = threading.Lock()
        self._logged_set_image_failure = False
        self._logged_infer_failures = 0

    def close(self) -> None:
        try:
            del self.processor
        except Exception:  # noqa: BLE001
            pass
        try:
            del self.model
        except Exception:  # noqa: BLE001
            pass
        if torch.cuda.is_available() and self.device.type == "cuda":
            try:
                torch.cuda.empty_cache()
            except Exception:  # noqa: BLE001
                pass

    def process_image(
        self,
        *,
        image_id: int,
        image_path: str,
        class_entries: Sequence[Dict[str, Any]],
        gt_by_image_cat: Dict[int, Dict[int, List[List[float]]]],
        payload: AgentMiningRequest,
        per_class_overrides: Optional[Dict[int, Dict[str, Any]]] = None,
        clip_head: Optional[Dict[str, Any]],
        head_sweep_min_probs: Sequence[float],
        head_sweep_margins: Sequence[float],
        log_fn: Optional[Callable[[str], None]] = None,
        cancel_event: Optional[threading.Event] = None,
    ) -> Dict[int, Dict[str, Any]]:
        if cancel_event is not None and cancel_event.is_set():
            return {}
        try:
            with Image.open(image_path) as img:
                pil_img = img.convert("RGB")
        except Exception:
            return {}

        out: Dict[int, Dict[str, Any]] = {}
        gt_for_image = gt_by_image_cat.get(int(image_id)) or {}
        bg_guard_seed, bg_guard_final, bg_margin, _bg_apply = _resolve_clip_head_background_settings(payload)

        def _counts_init() -> Dict[str, int]:
            return {"matches": 0, "fps": 0, "duplicates": 0, "preds": 0, "det_images": 0}

        with self.lock:
            try:
                state = self.processor.set_image(pil_img)
            except Exception as exc:
                if log_fn and not self._logged_set_image_failure:
                    self._logged_set_image_failure = True
                    try:
                        log_fn(f"SAM3 worker {self.device}: set_image failed for {image_id}: {exc}")
                    except Exception:
                        pass
                return {}

            clip_device_override: Optional[str] = None
            try:
                raw_clip_dev = os.environ.get("AGENT_MINING_CLIP_DEVICE")
            except Exception:
                raw_clip_dev = None
            if raw_clip_dev is not None and str(raw_clip_dev).strip():
                mode = str(raw_clip_dev).strip().lower()
                if mode in {"worker", "per_worker", "per-device", "per_device"}:
                    clip_device_override = str(self.device)
                else:
                    clip_device_override = str(raw_clip_dev).strip()

            for entry in class_entries:
                if cancel_event is not None and cancel_event.is_set():
                    break
                cid = entry.get("id")
                if cid is None:
                    continue
                try:
                    cid_int = int(cid)
                except Exception:
                    continue

                prompts = entry.get("text_prompts") or []
                ex_mat = entry.get("exemplar_embeddings")
                neg_mat = entry.get("negative_embeddings")
                head_target_index = entry.get("clip_head_target_index")
                head_active = bool(isinstance(clip_head, dict) and head_target_index is not None)

                clip_head_min_prob = 0.0 if head_active else float(payload.clip_head_min_prob)
                clip_head_margin = 0.0 if head_active else float(payload.clip_head_margin)

                # Apply per-class tuned overrides (beam search) for the shared greedy knobs.
                seed_thr = float(payload.seed_threshold)
                expand_thr = float(payload.expand_threshold)
                sim_floor = float(payload.similarity_score)
                max_seeds = int(payload.max_visual_seeds)
                if per_class_overrides and isinstance(per_class_overrides, dict):
                    ov = per_class_overrides.get(cid_int)
                    if isinstance(ov, dict):
                        try:
                            if ov.get("seed_threshold") is not None:
                                seed_thr = float(ov.get("seed_threshold"))
                        except Exception:
                            pass
                        try:
                            if ov.get("expand_threshold") is not None:
                                expand_thr = float(ov.get("expand_threshold"))
                        except Exception:
                            pass
                        try:
                            if ov.get("similarity_score") is not None:
                                sim_floor = float(ov.get("similarity_score"))
                        except Exception:
                            pass
                        try:
                            if ov.get("max_visual_seeds") is not None:
                                max_seeds = int(ov.get("max_visual_seeds"))
                        except Exception:
                            pass
                seed_thr = float(max(0.0, min(1.0, seed_thr)))
                expand_thr = float(max(0.0, min(1.0, expand_thr)))
                sim_floor = float(max(0.0, min(1.0, sim_floor)))
                max_seeds = max(0, int(max_seeds))

                try:
                    dets = _infer_sam3_greedy_recipe_on_image(
                        pil_img=pil_img,
                        processor=self.processor,
                        text_prompts=prompts,
                        exemplar_embeddings=ex_mat,
                        negative_embeddings=neg_mat,
                        seed_threshold=seed_thr,
                        expand_threshold=expand_thr,
                        max_visual_seeds=max_seeds,
                        seed_dedupe_iou=payload.seed_dedupe_iou,
                        out_dedupe_iou=payload.dedupe_iou,
                        mask_threshold=payload.mask_threshold,
                        min_size=payload.min_size,
                        simplify_epsilon=payload.simplify_epsilon,
                        max_results=payload.max_results,
                        negative_strength=payload.negative_strength,
                        similarity_floor=sim_floor,
                        clip_head=clip_head,
                        clip_head_target_index=head_target_index,
                        clip_head_min_prob=clip_head_min_prob,
                        clip_head_margin=clip_head_margin,
                        clip_head_background_guard_seed=bg_guard_seed,
                        clip_head_background_guard_final=bg_guard_final,
                        clip_head_background_margin=bg_margin,
                        state=state,
                        clip_device_override=clip_device_override,
                    )
                except torch.cuda.OutOfMemoryError:
                    if log_fn:
                        try:
                            log_fn(f"SAM3 OOM while processing image {image_id} on {self.device}")
                        except Exception:
                            pass
                    raise
                except Exception as exc:
                    if log_fn and self._logged_infer_failures < 3:
                        self._logged_infer_failures += 1
                        try:
                            log_fn(f"SAM3 worker {self.device}: inference failed for class {cid_int} on {image_id}: {exc}")
                        except Exception:
                            pass
                    dets = []

                gt_boxes = gt_for_image.get(cid_int) or []
                gt_xyxy = [_xywh_to_xyxy(b) for b in gt_boxes]

                rows: List[Tuple[float, float, float, Optional[int]]] = []
                debug_base_dets = 0
                debug_with_prob = 0
                debug_prob_min: Optional[float] = None
                debug_prob_max: Optional[float] = None
                if dets:
                    for det in dets:
                        bbox = det.bbox or []
                        if len(bbox) < 4:
                            continue
                        try:
                            det_xyxy = yolo_to_corners(bbox, pil_img.width, pil_img.height)
                        except Exception:
                            continue
                        best_iou = 0.0
                        best_idx: Optional[int] = None
                        for j, gt in enumerate(gt_xyxy):
                            iou = _iou_xyxy(det_xyxy, gt)
                            if iou > best_iou:
                                best_iou = iou
                                best_idx = j
                        prob = float(det.clip_head_prob) if det.clip_head_prob is not None else 0.0
                        margin = float(det.clip_head_margin) if det.clip_head_margin is not None else 0.0
                        rows.append((prob, margin, float(best_iou), best_idx))
                        debug_base_dets += 1
                        if det.clip_head_prob is not None:
                            debug_with_prob += 1
                            if debug_prob_min is None or prob < debug_prob_min:
                                debug_prob_min = float(prob)
                            if debug_prob_max is None or prob > debug_prob_max:
                                debug_prob_max = float(prob)

                if head_active:
                    by_margin: Dict[float, Dict[float, Dict[str, int]]] = {}
                    for margin_thr in head_sweep_margins:
                        by_prob: Dict[float, Dict[str, int]] = {}
                        for min_prob in head_sweep_min_probs:
                            counts = _counts_init()
                            used: set[int] = set()
                            any_det = False
                            for prob, margin, best_iou, best_idx in rows:
                                if prob < float(min_prob):
                                    continue
                                if float(margin_thr) > 0.0 and margin < float(margin_thr):
                                    continue
                                any_det = True
                                counts["preds"] += 1
                                if best_idx is not None and best_iou >= float(payload.iou_threshold):
                                    if best_idx in used:
                                        counts["duplicates"] += 1
                                    else:
                                        used.add(best_idx)
                                        counts["matches"] += 1
                                else:
                                    counts["fps"] += 1
                            if any_det:
                                counts["det_images"] = 1
                            by_prob[float(min_prob)] = counts
                        by_margin[float(margin_thr)] = by_prob
                    out[cid_int] = {
                        "head_active": True,
                        "by_margin": by_margin,
                        "debug": {
                            "base_dets": int(debug_base_dets),
                            "with_prob": int(debug_with_prob),
                            "prob_min": float(debug_prob_min) if debug_prob_min is not None else None,
                            "prob_max": float(debug_prob_max) if debug_prob_max is not None else None,
                        },
                    }
                else:
                    counts = _counts_init()
                    used: set[int] = set()
                    any_det = False
                    for _prob, _margin, best_iou, best_idx in rows:
                        any_det = True
                        counts["preds"] += 1
                        if best_idx is not None and best_iou >= float(payload.iou_threshold):
                            if best_idx in used:
                                counts["duplicates"] += 1
                            else:
                                used.add(best_idx)
                                counts["matches"] += 1
                        else:
                            counts["fps"] += 1
                    if any_det:
                        counts["det_images"] = 1
                    out[cid_int] = {"head_active": False, "counts": counts}
        return out


def _build_sam3_greedy_eval_workers(
    payload: "AgentMiningRequest",
    *,
    log_fn: Optional[Callable[[str], None]] = None,
) -> List["_Sam3GreedyEvalWorker"]:
    base_devices = _resolve_sam3_mining_devices()
    per_dev = 1
    try:
        per_dev = max(1, int(getattr(payload, "max_workers_per_device", 1) or 1))
    except Exception:
        per_dev = 1
    expanded: List[torch.device] = []
    for dev in base_devices:
        expanded.extend([dev] * per_dev)
    max_total = None
    try:
        max_total = getattr(payload, "max_workers", None)
    except Exception:
        max_total = None
    if isinstance(max_total, int) and max_total > 0:
        expanded = expanded[: max_total]
    workers: List[_Sam3GreedyEvalWorker] = []
    for dev in expanded:
        try:
            workers.append(_Sam3GreedyEvalWorker(dev))
        except Exception as exc:  # noqa: BLE001
            if log_fn:
                try:
                    log_fn(f"Failed to initialize SAM3 greedy worker on {dev}: {exc}")
                except Exception:
                    pass
    if not workers:
        raise RuntimeError("sam3_greedy_workers_unavailable")
    return workers


def _evaluate_sam3_greedy_recipes_image_first(
    *,
    class_entries: Sequence[Dict[str, Any]],
    val_ids: Sequence[int],
    images: Dict[int, Dict[str, Any]],
    gt_by_image_cat: Dict[int, Dict[int, List[List[float]]]],
    payload: AgentMiningRequest,
    per_class_overrides: Optional[Dict[int, Dict[str, Any]]] = None,
    clip_head: Optional[Dict[str, Any]] = None,
    cancel_event: Optional[threading.Event] = None,
    log_every: int = 25,
    log_fn: Optional[Callable[[str], None]] = None,
    progress_callback: Optional[Callable[[int, int], None]] = None,
    workers: Optional[List["_Sam3GreedyEvalWorker"]] = None,
    close_workers: bool = True,
) -> Dict[int, Dict[str, Any]]:
    total_images = len(val_ids)
    if total_images <= 0:
        return {}

    # Global greedy sweeps keep background margin fixed to avoid bloating the evaluation grid.
    head_sweep_min_probs, head_sweep_margins, _head_sweep_bg_margins, head_target_precision = _build_clip_head_sweep_grid(
        payload,
        base_min_prob=float(payload.clip_head_min_prob),
        base_margin=float(payload.clip_head_margin),
        base_bg_margin=float(getattr(payload, "clip_head_background_margin", 0.0) or 0.0),
        allow_bg_tune=False,
    )

    # Build image entries upfront (id, path).
    image_entries: List[Tuple[int, str]] = []
    for img_id in val_ids:
        info = images.get(int(img_id)) or {}
        path = info.get("path")
        if path:
            image_entries.append((int(img_id), str(path)))
    skipped = max(0, total_images - len(image_entries))

    # Precompute total GTs per class (for recall denominator).
    total_gt_by_class: Dict[int, int] = {}
    class_name_by_id: Dict[int, str] = {}
    for entry in class_entries:
        cid = entry.get("id")
        if cid is None:
            continue
        try:
            cid_int = int(cid)
        except Exception:
            continue
        class_name_by_id[cid_int] = str(entry.get("name") or f"class_{cid_int}")
        total_gt = 0
        for img_id in val_ids:
            total_gt += len((gt_by_image_cat.get(int(img_id)) or {}).get(cid_int) or [])
        total_gt_by_class[cid_int] = total_gt

    def _counts_init() -> Dict[str, int]:
        return {"matches": 0, "fps": 0, "duplicates": 0, "preds": 0, "det_images": 0}

    agg: Dict[int, Dict[str, Any]] = {}
    for cid_int, total_gt in total_gt_by_class.items():
        head_target_index = None
        for entry in class_entries:
            if int(entry.get("id")) == cid_int:
                head_target_index = entry.get("clip_head_target_index")
                break
        head_active = bool(isinstance(clip_head, dict) and head_target_index is not None)
        if head_active:
            agg[cid_int] = {
                "head_active": True,
                "by_margin": {
                    float(m): {float(p): _counts_init() for p in head_sweep_min_probs} for m in head_sweep_margins
                },
                "total_gt": total_gt,
                "debug": {"base_dets": 0, "with_prob": 0, "prob_min": None, "prob_max": None},
            }
        else:
            agg[cid_int] = {"head_active": False, "counts": _counts_init(), "total_gt": total_gt}

    owned_workers = False
    if workers is None:
        workers = _build_sam3_greedy_eval_workers(payload, log_fn=log_fn)
        owned_workers = True

    try:
        if log_fn:
            try:
                labels = ", ".join(str(w.device) for w in workers)
                extra = f"; skipped {skipped} images missing paths" if skipped else ""
                per_dev = getattr(payload, "max_workers_per_device", 1)
                per_dev_txt = ""
                try:
                    if int(per_dev) > 1:
                        per_dev_txt = f"; up to {int(per_dev)} worker(s) per device"
                except Exception:
                    per_dev_txt = ""
                clip_dev_cfg = ""
                try:
                    raw = os.environ.get("AGENT_MINING_CLIP_DEVICE")
                    if raw is not None and str(raw).strip():
                        clip_dev_cfg = f"; clip_device={str(raw).strip()}"
                except Exception:
                    clip_dev_cfg = ""
                log_fn(
                    f"Starting global image-first sweep (sam3_greedy): {total_images} val images, "
                    f"{len(class_entries)} classes on {len(workers)} worker(s): {labels}{per_dev_txt}{clip_dev_cfg}{extra}"
                )
            except Exception:
                pass

        processed = int(skipped)
        if progress_callback:
            try:
                progress_callback(processed, total_images)
            except Exception:
                pass
        # Use a shared work queue and one thread per worker. Submitting one future per image can
        # accidentally starve some workers (threads can block on a worker lock while other GPUs sit idle).
        work_q: "queue.Queue[Tuple[int, str]]" = queue.Queue()
        for entry in image_entries:
            work_q.put(entry)

        agg_lock = threading.Lock()
        progress_lock = threading.Lock()

        def _merge(per_image: Optional[Dict[int, Dict[str, Any]]]) -> None:
            if not per_image:
                return
            with agg_lock:
                for cid_int, data in per_image.items():
                    dst = agg.get(cid_int)
                    if not dst:
                        continue
                    if data.get("head_active"):
                        by_margin = data.get("by_margin") or {}
                        dst_by_margin = dst.get("by_margin") or {}
                        for m, by_prob in by_margin.items():
                            dst_by_prob = dst_by_margin.get(float(m))
                            if not isinstance(dst_by_prob, dict) or not isinstance(by_prob, dict):
                                continue
                            for p, counts in by_prob.items():
                                dst_counts = dst_by_prob.get(float(p))
                                if not isinstance(dst_counts, dict) or not isinstance(counts, dict):
                                    continue
                                for k in ("matches", "fps", "duplicates", "preds", "det_images"):
                                    try:
                                        dst_counts[k] = int(dst_counts.get(k, 0)) + int(counts.get(k, 0))
                                    except Exception:
                                        continue
                        src_debug = data.get("debug") if isinstance(data, dict) else None
                        dst_debug = dst.get("debug") if isinstance(dst, dict) else None
                        if isinstance(src_debug, dict) and isinstance(dst_debug, dict):
                            try:
                                dst_debug["base_dets"] = int(dst_debug.get("base_dets") or 0) + int(src_debug.get("base_dets") or 0)
                            except Exception:
                                pass
                            try:
                                dst_debug["with_prob"] = int(dst_debug.get("with_prob") or 0) + int(src_debug.get("with_prob") or 0)
                            except Exception:
                                pass
                            src_min = src_debug.get("prob_min")
                            src_max = src_debug.get("prob_max")
                            try:
                                if src_min is not None:
                                    src_min_f = float(src_min)
                                    cur = dst_debug.get("prob_min")
                                    dst_debug["prob_min"] = src_min_f if cur is None else float(min(float(cur), src_min_f))
                            except Exception:
                                pass
                            try:
                                if src_max is not None:
                                    src_max_f = float(src_max)
                                    cur = dst_debug.get("prob_max")
                                    dst_debug["prob_max"] = src_max_f if cur is None else float(max(float(cur), src_max_f))
                            except Exception:
                                pass
                    else:
                        counts = data.get("counts") or {}
                        dst_counts = dst.get("counts")
                        if isinstance(dst_counts, dict) and isinstance(counts, dict):
                            for k in ("matches", "fps", "duplicates", "preds", "det_images"):
                                try:
                                    dst_counts[k] = int(dst_counts.get(k, 0)) + int(counts.get(k, 0))
                                except Exception:
                                    continue

        def _mark_progress() -> None:
            nonlocal processed
            with progress_lock:
                processed += 1
                if progress_callback:
                    try:
                        progress_callback(processed, total_images)
                    except Exception:
                        pass
                if log_fn and log_every > 0 and processed % log_every == 0:
                    try:
                        log_fn(f"Processed {processed}/{total_images} val images (global) for all classes")
                    except Exception:
                        pass

        def _worker_loop(worker: "_Sam3GreedyEvalWorker") -> None:
            while True:
                if cancel_event is not None and cancel_event.is_set():
                    break
                try:
                    img_id, path = work_q.get_nowait()
                except queue.Empty:
                    break
                per_image: Optional[Dict[int, Dict[str, Any]]] = None
                try:
                    per_image = worker.process_image(
                        image_id=img_id,
                        image_path=path,
                        class_entries=class_entries,
                        gt_by_image_cat=gt_by_image_cat,
                        payload=payload,
                        per_class_overrides=per_class_overrides,
                        clip_head=clip_head,
                        head_sweep_min_probs=head_sweep_min_probs,
                        head_sweep_margins=head_sweep_margins,
                        log_fn=log_fn,
                        cancel_event=cancel_event,
                    )
                except torch.cuda.OutOfMemoryError:
                    # Propagate OOM so the caller can fail fast.
                    if cancel_event is not None:
                        try:
                            cancel_event.set()
                        except Exception:
                            pass
                    raise
                except Exception as exc:  # noqa: BLE001
                    if log_fn:
                        try:
                            log_fn(f"SAM3 greedy worker failed: {exc}")
                        except Exception:
                            pass
                    per_image = None
                finally:
                    try:
                        work_q.task_done()
                    except Exception:
                        pass
                _merge(per_image)
                _mark_progress()

        max_workers = max(1, len(workers))
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(_worker_loop, w) for w in workers]
            for future in futures:
                # Propagate OOM and other failures.
                future.result()
    finally:
        if owned_workers and close_workers:
            for w in workers or []:
                try:
                    w.close()
                except Exception:
                    continue

    summaries: Dict[int, Dict[str, Any]] = {}
    for cid_int, data in agg.items():
        total_gt = int(data.get("total_gt") or 0)
        if data.get("head_active"):
            best_summary: Optional[Dict[str, Any]] = None
            best_key: Optional[Tuple[int, float, int, float, float, float, float]] = None
            by_margin = data.get("by_margin") or {}
            for margin_thr in head_sweep_margins:
                by_prob = by_margin.get(float(margin_thr)) if isinstance(by_margin, dict) else None
                if not isinstance(by_prob, dict):
                    continue
                for min_prob in head_sweep_min_probs:
                    counts = by_prob.get(float(min_prob))
                    if not isinstance(counts, dict):
                        continue
                    matched = int(counts.get("matches") or 0)
                    fps = int(counts.get("fps") or 0)
                    duplicates = int(counts.get("duplicates") or 0)
                    preds = int(counts.get("preds") or 0)
                    det_images = int(counts.get("det_images") or 0)
                    best_summary, best_key = _update_best_clip_head_sweep_summary(
                        best_summary=best_summary,
                        best_key=best_key,
                        total_gt=int(total_gt),
                        total_images=int(total_images),
                        matched=int(matched),
                        fps=int(fps),
                        duplicates=int(duplicates),
                        preds=int(preds),
                        det_images=int(det_images),
                        min_prob=float(min_prob),
                        margin=float(margin_thr),
                        bg_margin=float(getattr(payload, "clip_head_background_margin", 0.0) or 0.0),
                        target_precision=float(head_target_precision),
                    )
            if best_summary is None:
                best_summary = {
                    "gts": total_gt,
                    "matches": 0,
                    "fps": 0,
                    "duplicates": 0,
                    "preds": 0,
                    "precision": 0.0,
                    "recall": 0.0,
                    "coverage_rate": 0.0,
                    "det_rate": 0.0,
                    "clip_head_min_prob": float(payload.clip_head_min_prob),
                    "clip_head_margin": float(payload.clip_head_margin),
                    "clip_head_background_margin": float(getattr(payload, "clip_head_background_margin", 0.0) or 0.0),
                    "clip_head_target_precision": float(head_target_precision),
                    "clip_head_meets_target_precision": False,
                }
            if log_fn and total_gt and isinstance(best_summary, dict):
                try:
                    if int(best_summary.get("matches") or 0) == 0:
                        name = class_name_by_id.get(cid_int, f"class_{cid_int}")
                        debug = data.get("debug") if isinstance(data, dict) else None
                        base_dets = int(debug.get("base_dets") or 0) if isinstance(debug, dict) else 0
                        with_prob = int(debug.get("with_prob") or 0) if isinstance(debug, dict) else 0
                        prob_min = debug.get("prob_min") if isinstance(debug, dict) else None
                        prob_max = debug.get("prob_max") if isinstance(debug, dict) else None
                        by_prob0 = by_margin.get(0.0) if isinstance(by_margin, dict) else None
                        counts0 = by_prob0.get(0.0) if isinstance(by_prob0, dict) else None
                        preds0 = int(counts0.get("preds") or 0) if isinstance(counts0, dict) else 0
                        fps0 = int(counts0.get("fps") or 0) if isinstance(counts0, dict) else 0
                        if preds0 <= 0:
                            log_fn(f"Debug head sweep {name}: 0 detections on val split (gt={total_gt}).")
                        elif with_prob <= 0:
                            log_fn(
                                f"Debug head sweep {name}: dets@p>=0.0 preds={preds0} matches=0 fps={fps0}; missing head probs (CLIP encode/proba failed)."
                            )
                        else:
                            rng_txt = ""
                            try:
                                if prob_min is not None and prob_max is not None:
                                    rng_txt = f" prob_range={float(prob_min):.3f}..{float(prob_max):.3f}"
                            except Exception:
                                rng_txt = ""
                            log_fn(
                                f"Debug head sweep {name}: dets@p>=0.0 preds={preds0} matches=0 fps={fps0}; head_probs={with_prob}/{base_dets}{rng_txt}."
                            )
                except Exception:
                    pass
            summaries[cid_int] = best_summary
        else:
            counts = data.get("counts") or {}
            matched = int(counts.get("matches") or 0)
            fps = int(counts.get("fps") or 0)
            duplicates = int(counts.get("duplicates") or 0)
            preds = int(counts.get("preds") or 0)
            det_images = int(counts.get("det_images") or 0)
            recall = matched / total_gt if total_gt else 0.0
            precision = matched / max(1, matched + fps)
            det_rate = det_images / total_images if total_images else 0.0
            summaries[cid_int] = {
                "gts": total_gt,
                "matches": matched,
                "fps": fps,
                "duplicates": duplicates,
                "preds": preds,
                "precision": precision,
                "recall": recall,
                "coverage_rate": recall,
                "det_rate": det_rate,
            }
    return summaries


def _score_greedy_eval_summaries(
    summaries: Dict[int, Dict[str, Any]],
) -> Dict[str, Any]:
    total_gt = 0
    matched = 0
    fps = 0
    preds = 0
    for _cid, s in (summaries or {}).items():
        if not isinstance(s, dict):
            continue
        try:
            total_gt += int(s.get("gts") or 0)
        except Exception:
            pass
        try:
            matched += int(s.get("matches") or 0)
        except Exception:
            pass
        try:
            fps += int(s.get("fps") or 0)
        except Exception:
            pass
        try:
            preds += int(s.get("preds") or 0)
        except Exception:
            pass
    precision = matched / max(1, matched + fps)
    recall = matched / max(1, total_gt)
    f1 = 0.0
    if precision + recall > 1e-9:
        f1 = 2.0 * precision * recall / (precision + recall)
    score_key = (matched, -fps, float(precision))
    return {
        "gts": total_gt,
        "matches": matched,
        "fps": fps,
        "preds": preds,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "key": score_key,
    }


def _gt_instance_key(img_id: int, gt_idx: int) -> int:
    # Pack (image_id, gt_index) into a single int key for compact set operations.
    return (int(img_id) << 20) | int(gt_idx)


def _build_seed_threshold_sweep_grid(
    *,
    base_seed_threshold: float,
    observed_scores: Optional[Sequence[float]] = None,
    limit: int = 64,
) -> List[float]:
    """
    Build a threshold grid for SAM3 seed-stage threshold sweeps.

    Notes:
    - SAM3 text scores can cluster near 0.0, so we include dense points in [0.0..0.1].
    - This is a *seed* sweep grid (SAM score threshold), not a CLIP-head sweep grid.
    """
    try:
        base = float(base_seed_threshold)
    except Exception:
        base = 0.05
    base = max(0.0, min(1.0, base))
    try:
        limit_i = max(4, int(limit))
    except Exception:
        limit_i = 64

    raw: List[float] = []
    raw.extend([0.0, 0.001, 0.002, 0.005])
    raw.extend([round(i * 0.01, 3) for i in range(0, 11)])  # 0.00..0.10
    raw.extend([round(i * 0.05, 3) for i in range(3, 20)])  # 0.15..0.95
    raw.extend([0.975, 0.99, 1.0])
    raw.append(base)

    if observed_scores:
        cleaned_scores: List[float] = []
        for s in observed_scores:
            try:
                val = float(s)
            except Exception:
                continue
            if not (0.0 <= val <= 1.0):
                continue
            cleaned_scores.append(val)
            if len(cleaned_scores) >= 20_000:
                break
        if cleaned_scores:
            try:
                qs = np.quantile(np.asarray(cleaned_scores, dtype=np.float32), np.linspace(0.0, 1.0, num=21))
                raw.extend([float(x) for x in qs.tolist()])
            except Exception:
                pass

    grid = sorted({float(max(0.0, min(1.0, round(v, 4)))) for v in raw})
    if len(grid) > limit_i:
        # Keep low thresholds dense; truncate the tail.
        grid = grid[:limit_i]
    return grid


def _compute_steps_seed_eval_threshold(payload: "AgentMiningRequest") -> float:
    """
    Seed-stage prompt eval threshold used for steps-mode mining.

    This can be lower than the user-configured `seed_threshold` so seed-threshold curves can represent
    operating points below the base threshold.
    """
    try:
        base = float(getattr(payload, "seed_threshold", 0.05))
    except Exception:
        base = 0.05
    base = max(0.0, min(1.0, float(base)))

    floor_raw = getattr(payload, "steps_seed_eval_floor", None)
    if floor_raw is None:
        return base
    try:
        floor = float(floor_raw)
    except Exception:
        return base
    floor = max(0.0, min(1.0, float(floor)))
    return float(min(base, floor))


def _compute_steps_seed_eval_max_results(payload: "AgentMiningRequest") -> int:
    """
    Seed-stage prompt eval max_results override used for steps-mode mining.

    This exists because very low `steps_seed_eval_floor` values can return many detections.
    """
    try:
        base = int(getattr(payload, "max_results", 1000) or 1000)
    except Exception:
        base = 1000
    base = max(1, min(5000, int(base)))

    override_raw = getattr(payload, "steps_seed_eval_max_results", None)
    if override_raw is None:
        return base
    try:
        override = int(override_raw)
    except Exception:
        return base
    override = max(1, min(5000, int(override)))
    return override


def _compute_seed_threshold_curve(
    *,
    gt_best_scores: Mapping[Any, float],
    fp_scores: Sequence[float],
    thresholds: Sequence[float],
) -> List[Dict[str, Any]]:
    """
    Compute seed-stage (matches,fps,precision) at each threshold.

    - `gt_best_scores` maps a GT instance key -> best SAM score among detections matching that GT.
    - `fp_scores` contains SAM scores for detections that did not match any GT (false positives).

    Precision here mirrors our existing seed-stage notion:
      precision = matches / (matches + fps)

    (Duplicates are intentionally not modeled in this helper; they can be tracked separately.)
    """
    thr_list: List[float] = []
    for t in thresholds:
        try:
            thr_list.append(float(t))
        except Exception:
            continue
    thr_list = sorted({max(0.0, min(1.0, round(v, 6))) for v in thr_list})
    if not thr_list:
        return []

    gt_vals: List[float] = []
    for _k, v in (gt_best_scores or {}).items():
        try:
            val = float(v)
        except Exception:
            continue
        if 0.0 <= val <= 1.0:
            gt_vals.append(val)
    gt_arr = np.asarray(gt_vals, dtype=np.float32) if gt_vals else np.zeros((0,), dtype=np.float32)
    gt_arr.sort()

    fp_arr: np.ndarray
    if fp_scores is None:
        fp_arr = np.zeros((0,), dtype=np.float32)
    else:
        try:
            fp_arr = np.asarray(fp_scores, dtype=np.float32)
        except Exception:
            fp_arr = np.asarray(list(fp_scores), dtype=np.float32) if fp_scores else np.zeros((0,), dtype=np.float32)
    if fp_arr.size:
        try:
            fp_arr = fp_arr[(fp_arr >= 0.0) & (fp_arr <= 1.0)]
        except Exception:
            fp_arr = fp_arr
    fp_arr.sort()

    total_gt = int(gt_arr.size)
    total_fp = int(fp_arr.size)

    curve: List[Dict[str, Any]] = []
    for thr in thr_list:
        try:
            thr_key_gt = gt_arr.dtype.type(float(thr))
        except Exception:
            thr_key_gt = float(thr)
        try:
            thr_key_fp = fp_arr.dtype.type(float(thr))
        except Exception:
            thr_key_fp = float(thr)
        gt_start = int(np.searchsorted(gt_arr, thr_key_gt, side="left")) if total_gt else 0
        fp_start = int(np.searchsorted(fp_arr, thr_key_fp, side="left")) if total_fp else 0
        matches = total_gt - gt_start
        fps = total_fp - fp_start
        precision = float(matches) / float(max(1, matches + fps))
        curve.append(
            {
                "threshold": float(thr),
                "matches": int(matches),
                "fps": int(fps),
                "precision": float(precision),
            }
        )
    return curve


def _select_seed_threshold_operating_point(
    curve: Sequence[Dict[str, Any]],
    *,
    min_precision: Optional[float] = None,
    max_fps: Optional[int] = None,
) -> Optional[Dict[str, Any]]:
    """
    Select a single operating point from a seed-threshold curve.

    Selection rule:
      1) Prefer points that satisfy constraints (precision >= min_precision, fps <= max_fps).
      2) Among those, maximize matches, then minimize fps, then prefer lower threshold (less brittle).
      3) If none satisfy, fall back to maximizing precision, then matches, then minimizing fps.
    """
    if not curve:
        return None

    try:
        min_prec = float(min_precision) if min_precision is not None else None
    except Exception:
        min_prec = None
    if min_prec is not None:
        min_prec = max(0.0, min(1.0, min_prec))

    try:
        max_fps_i = int(max_fps) if max_fps is not None else None
    except Exception:
        max_fps_i = None
    if max_fps_i is not None:
        max_fps_i = max(0, max_fps_i)

    def _as_point(p: Dict[str, Any]) -> Tuple[float, int, int, float]:
        try:
            thr = float(p.get("threshold") or 0.0)
        except Exception:
            thr = 0.0
        try:
            matches = int(p.get("matches") or 0)
        except Exception:
            matches = 0
        try:
            fps = int(p.get("fps") or 0)
        except Exception:
            fps = 0
        try:
            precision = float(p.get("precision") or 0.0)
        except Exception:
            precision = 0.0
        return thr, matches, fps, precision

    candidates: List[Dict[str, Any]] = []
    for p in curve:
        if not isinstance(p, dict):
            continue
        thr, matches, fps, precision = _as_point(p)
        ok = True
        if min_prec is not None and precision < float(min_prec):
            ok = False
        if max_fps_i is not None and fps > int(max_fps_i):
            ok = False
        if ok:
            candidates.append(p)

    best: Optional[Dict[str, Any]] = None
    best_key: Optional[Tuple[int, int, float]] = None
    if candidates:
        for p in candidates:
            thr, matches, fps, _precision = _as_point(p)
            # Prefer more matches, then fewer FPs, then lower threshold.
            key = (int(matches), -int(fps), -float(thr))
            if best_key is None or key > best_key:
                best_key = key
                best = p
        return best

    # Fallback: maximize precision, then matches, then minimize fps, then prefer lower threshold.
    best2: Optional[Dict[str, Any]] = None
    best2_key: Optional[Tuple[float, int, int, float]] = None
    for p in curve:
        if not isinstance(p, dict):
            continue
        thr, matches, fps, precision = _as_point(p)
        key = (float(precision), int(matches), -int(fps), -float(thr))
        if best2_key is None or key > best2_key:
            best2_key = key
            best2 = p
    return best2


def _select_seed_threshold_candidate_points(
    curve: Sequence[Dict[str, Any]],
    *,
    max_candidates: int,
    target_precision: Optional[float] = None,
) -> List[Dict[str, Any]]:
    """
    Select up to max_candidates operating points from a seed-threshold curve.

    The curve is typically monotonic in (matches,fps) as threshold increases, so the Pareto frontier can
    be large. We therefore:
      - build a simple frontier in (matches, fps)
      - always include extremes + (optional) a point meeting target precision
      - downsample deterministically
    """
    try:
        max_c = max(1, int(max_candidates))
    except Exception:
        max_c = 6

    pts: List[Dict[str, Any]] = [p for p in (curve or []) if isinstance(p, dict) and p.get("threshold") is not None]
    if not pts:
        return []

    def _pt_key(p: Dict[str, Any]) -> Tuple[float, int, int, float]:
        try:
            thr = float(p.get("threshold") or 0.0)
        except Exception:
            thr = 0.0
        try:
            matches = int(p.get("matches") or 0)
        except Exception:
            matches = 0
        try:
            fps = int(p.get("fps") or 0)
        except Exception:
            fps = 0
        try:
            prec = float(p.get("precision") or 0.0)
        except Exception:
            prec = 0.0
        return (thr, matches, fps, prec)

    # Build a simple frontier in (matches,fps): keep points that improve fps as matches decreases.
    by_matches = sorted(pts, key=lambda p: (-_pt_key(p)[1], _pt_key(p)[2], -_pt_key(p)[3], _pt_key(p)[0]))
    frontier: List[Dict[str, Any]] = []
    best_fps: Optional[int] = None
    for p in by_matches:
        fps = _pt_key(p)[2]
        if best_fps is None or fps < best_fps:
            frontier.append(p)
            best_fps = fps
    frontier = sorted(frontier, key=lambda p: _pt_key(p)[0])

    chosen: List[Dict[str, Any]] = []
    chosen_thr: set[float] = set()

    def _add(p: Optional[Dict[str, Any]]) -> None:
        if not isinstance(p, dict):
            return
        thr = _pt_key(p)[0]
        thr_k = float(round(thr, 6))
        if thr_k in chosen_thr:
            return
        chosen_thr.add(thr_k)
        chosen.append(p)

    # Always include extremes (best coverage / best cleanliness).
    _add(frontier[0] if frontier else None)
    _add(frontier[-1] if frontier else None)

    # If target precision provided, include the best point meeting it (if any).
    try:
        tgt = float(target_precision) if target_precision is not None else None
    except Exception:
        tgt = None
    if tgt is not None:
        tgt = max(0.0, min(1.0, tgt))
        _add(_select_seed_threshold_operating_point(frontier, min_precision=tgt))

    if len(chosen) >= max_c:
        return sorted(chosen, key=lambda p: _pt_key(p)[0])[:max_c]

    # Fill with evenly-spaced frontier points.
    n = len(frontier)
    if n > 0 and max_c > 1:
        slots = max_c
        for i in range(slots):
            if len(chosen) >= max_c:
                break
            if slots == 1:
                idx = 0
            else:
                idx = int(round(i * (n - 1) / float(slots - 1)))
            if 0 <= idx < n:
                _add(frontier[idx])

    return sorted(chosen, key=lambda p: _pt_key(p)[0])[:max_c]


def _summarize_seed_threshold_curve_for_prompt(
    *,
    gt_best_scores: Mapping[Any, float],
    fp_scores: Sequence[float],
    base_seed_threshold: float,
    curve_limit: int = 48,
) -> Dict[str, Any]:
    """
    Build a compact per-prompt seed-threshold curve and choose a default operating point.

    For now, the "recommended" point is:
      - the best matches attainable without reducing seed-stage precision below the base threshold's precision.
    """
    try:
        base_thr = float(base_seed_threshold)
    except Exception:
        base_thr = 0.05
    base_thr = max(0.0, min(1.0, base_thr))

    thresholds = _build_seed_threshold_sweep_grid(
        base_seed_threshold=base_thr,
        observed_scores=None,
        limit=max(8, int(curve_limit)),
    )
    curve = _compute_seed_threshold_curve(gt_best_scores=gt_best_scores, fp_scores=fp_scores, thresholds=thresholds)
    if curve_limit > 0 and len(curve) > int(curve_limit):
        curve = curve[: int(curve_limit)]

    base_point: Optional[Dict[str, Any]] = None
    if curve:
        base_point = min(curve, key=lambda p: abs(float(p.get("threshold") or 0.0) - base_thr))
    try:
        base_precision = float(base_point.get("precision") or 0.0) if base_point else 0.0
    except Exception:
        base_precision = 0.0

    recommended_point = _select_seed_threshold_operating_point(curve, min_precision=base_precision) if curve else None
    try:
        recommended_thr = float(recommended_point.get("threshold")) if recommended_point else base_thr
    except Exception:
        recommended_thr = base_thr

    return {
        "seed_threshold_base": float(base_thr),
        "seed_threshold_base_point": base_point,
        "seed_threshold_curve": curve,
        "seed_threshold_recommended": float(max(0.0, min(1.0, recommended_thr))),
        "seed_threshold_recommended_point": recommended_point,
    }


def _select_steps_from_seed_prompt_stats(
    prompt_stats: Sequence[Dict[str, Any]],
    *,
    max_steps: int,
    target_precision: Optional[float] = None,
    max_candidates_per_prompt: int = 6,
    early_stop: Optional[Dict[str, Any]] = None,
    log_fn: Optional[Callable[[str], None]] = None,
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Greedy set-cover over GT instances, using *seed-stage* matches.

    We primarily maximize new coverage; tie-break on seed precision (higher) and fp count (lower).
    """
    try:
        max_steps_i = max(1, int(max_steps))
    except Exception:
        max_steps_i = 6

    target_precision_f: Optional[float]
    try:
        target_precision_f = float(target_precision) if target_precision is not None else None
    except Exception:
        target_precision_f = None
    if target_precision_f is not None:
        target_precision_f = max(0.0, min(1.0, target_precision_f))

    candidates: List[Dict[str, Any]] = []
    total_gt_keys: set[int] = set()
    for cand in prompt_stats:
        if not isinstance(cand, dict):
            continue
        prompt = str(cand.get("prompt") or "").strip()
        if not prompt:
            continue
        if cand.get("bg_drop"):
            if log_fn:
                try:
                    bg_rate = float(cand.get("bg_veto_rate") or 0.0)
                    bg_checked = int(cand.get("bg_checked") or 0)
                    log_fn(
                        f"[steps] prompt bg-drop: skip '{prompt}' "
                        f"(bg_veto_rate={bg_rate:.2f}, checked={bg_checked})"
                    )
                except Exception:
                    pass
            continue

        curve = cand.get("seed_threshold_curve") if isinstance(cand.get("seed_threshold_curve"), list) else None
        curve_points = curve or []
        candidate_points = _select_seed_threshold_candidate_points(
            curve_points,
            max_candidates=max_candidates_per_prompt,
            target_precision=target_precision_f,
        )

        # If we have no curve (or it was empty), fall back to a single implicit point.
        if not candidate_points:
            thr_fallback: Optional[float] = None
            for key in ("seed_threshold_recommended", "seed_threshold_base"):
                if cand.get(key) is None:
                    continue
                try:
                    thr_fallback = float(cand.get(key))
                    break
                except Exception:
                    continue
            if thr_fallback is None:
                thr_fallback = 0.05
            thr_fallback = max(0.0, min(1.0, float(thr_fallback)))
            candidate_points = [{"threshold": float(thr_fallback), "matches": 0, "fps": int(cand.get("fps") or 0), "precision": 0.0}]

        gt_best_scores = cand.get("gt_best_scores") if isinstance(cand.get("gt_best_scores"), dict) else {}
        for point in candidate_points:
            if not isinstance(point, dict):
                continue
            try:
                selected_thr = float(point.get("threshold"))
            except Exception:
                continue
            selected_thr = max(0.0, min(1.0, selected_thr))

            matched_keys: set[int] = set()
            if gt_best_scores:
                for k, v in gt_best_scores.items():
                    try:
                        k_int = int(k)
                    except Exception:
                        continue
                    try:
                        v_f = float(v)
                    except Exception:
                        continue
                    if v_f >= float(selected_thr):
                        matched_keys.add(k_int)
            else:
                covered = cand.get("matched_keys")
                if isinstance(covered, set):
                    matched_keys = covered

            total_gt_keys |= matched_keys

            try:
                matches = int(point.get("matches") or 0) or len(matched_keys)
            except Exception:
                matches = len(matched_keys)
            try:
                fps = int(point.get("fps") or 0)
            except Exception:
                fps = int(cand.get("fps") or 0)
            try:
                precision = float(point.get("precision") or 0.0)
            except Exception:
                precision = float(matches) / float(max(1, matches + fps))

            candidates.append(
                {
                    **cand,
                    "prompt": prompt,
                    "matched_keys": matched_keys,
                    "matches": int(matches),
                    "fps": int(fps),
                    "precision": float(precision),
                    "selected_seed_threshold": float(selected_thr),
                    "selected_seed_threshold_point": point,
                }
            )

    covered_all: set[int] = set()
    selected: List[Dict[str, Any]] = []
    selected_prompts: set[str] = set()
    remaining = candidates[:]
    early_cfg = early_stop or {"enabled": False}
    new_matches_history: List[int] = []
    total_gt = len(total_gt_keys)
    early_enabled = bool(early_cfg.get("enabled"))
    early_min_steps = int(early_cfg.get("min_steps") or 0)
    early_window = int(early_cfg.get("window") or 1)
    early_increment = float(early_cfg.get("min_increment") or 0.0)
    early_precision_floor = early_cfg.get("precision_floor")
    stop_reason: Optional[str] = None
    triggered = False
    while remaining and len(selected) < max_steps_i:
        best: Optional[Dict[str, Any]] = None
        best_key: Optional[Tuple[int, int, float, int]] = None
        for cand in remaining:
            prompt = str(cand.get("prompt") or "").strip()
            if prompt in selected_prompts:
                continue
            new = cand.get("matched_keys") - covered_all
            new_matches = len(new)
            if new_matches <= 0:
                continue
            precision = float(cand.get("precision") or 0.0)
            fps = int(cand.get("fps") or 0)
            meets = 1
            if target_precision_f is not None:
                meets = 1 if precision >= float(target_precision_f) else 0
            key = (meets, new_matches, precision, -fps)
            if best_key is None or key > best_key:
                best_key = key
                best = cand
        if best is None:
            stop_reason = "no_candidates"
            break
        if early_enabled and target_precision_f is not None and early_precision_floor is not None:
            if len(selected) >= early_min_steps and float(best.get("precision") or 0.0) < float(early_precision_floor):
                stop_reason = "precision_floor"
                triggered = True
                if log_fn:
                    try:
                        log_fn(
                            f"[steps] early-stop: best candidate precision {float(best.get('precision') or 0.0):.3f} "
                            f"below target-{float(early_cfg.get('precision_margin') or 0.0):.2f}"
                        )
                    except Exception:
                        pass
                break
        selected.append(best)
        covered_all |= best.get("matched_keys")
        selected_prompts.add(str(best.get("prompt") or "").strip())
        remaining = [c for c in remaining if str(c.get("prompt") or "").strip() not in selected_prompts]
        if best_key is not None:
            new_matches_history.append(int(best_key[1]))
        if log_fn:
            try:
                log_fn(
                    f"[steps] select step '{best.get('prompt')}' (new_gt={best_key[1] if best_key else 0}, "
                    f"text_thr={float(best.get('selected_seed_threshold') or 0.0):.3f} "
                    f"text_prec={float(best.get('precision') or 0.0):.3f}, text_fps={int(best.get('fps') or 0)})"
                )
            except Exception:
                pass
        if early_enabled and total_gt > 0 and len(selected) >= early_min_steps and early_window > 0:
            window = new_matches_history[-early_window:]
            if window:
                frac = float(sum(window)) / float(total_gt)
                if frac < float(early_increment):
                    stop_reason = "coverage_stall"
                    triggered = True
                    if log_fn:
                        try:
                            log_fn(
                                f"[steps] early-stop: coverage gain {frac * 100:.2f}% over last {len(window)} step(s) "
                                f"(threshold {float(early_increment) * 100:.2f}%)"
                            )
                        except Exception:
                            pass
                    break
    if stop_reason is None:
        if len(selected) >= max_steps_i:
            stop_reason = "max_steps"
        elif not remaining:
            stop_reason = "no_candidates"
        else:
            stop_reason = "complete"
    early_info = {
        "enabled": bool(early_enabled),
        "mode": str(early_cfg.get("mode") or "balanced"),
        "triggered": bool(triggered),
        "reason": str(stop_reason),
        "selected_steps": int(len(selected)),
        "max_steps": int(max_steps_i),
        "min_steps": int(early_min_steps),
        "window": int(early_window),
        "min_increment": float(early_increment),
        "precision_margin": float(early_cfg.get("precision_margin") or 0.0),
    }
    return selected, early_info


def _build_seed_stage_candidate_from_prompt_stat(
    stat: Dict[str, Any],
    *,
    prompt: str,
    fallback_seed_threshold: float,
) -> Optional[Dict[str, Any]]:
    """
    Build a "selected-style" candidate dict for a single prompt from its seed-stage stats.

    This uses the prompt's recommended seed threshold (if available) as the operating point so it can be
    considered during prompt-subset refinement.
    """
    if not isinstance(stat, dict):
        return None
    prompt_s = str(prompt or "").strip()
    if not prompt_s:
        return None

    try:
        thr = float(stat.get("seed_threshold_recommended"))
    except Exception:
        try:
            thr = float(stat.get("seed_threshold_base"))
        except Exception:
            thr = float(fallback_seed_threshold)
    thr = max(0.0, min(1.0, float(thr)))

    gt_best_scores = stat.get("gt_best_scores") if isinstance(stat.get("gt_best_scores"), dict) else {}
    matched_keys: set[int] = set()
    for k, v in (gt_best_scores or {}).items():
        try:
            k_int = int(k)
        except Exception:
            continue
        try:
            v_f = float(v)
        except Exception:
            continue
        if v_f >= float(thr):
            matched_keys.add(k_int)

    curve = stat.get("seed_threshold_curve") if isinstance(stat.get("seed_threshold_curve"), list) else []
    selected_point: Optional[Dict[str, Any]] = None
    if curve:
        try:
            selected_point = min(curve, key=lambda p: abs(float(p.get("threshold") or 0.0) - float(thr)))
        except Exception:
            selected_point = None
    if selected_point is None and isinstance(stat.get("seed_threshold_recommended_point"), dict):
        selected_point = stat.get("seed_threshold_recommended_point")

    try:
        matches = int(selected_point.get("matches") or 0) if selected_point else len(matched_keys)
    except Exception:
        matches = len(matched_keys)
    try:
        fps = int(selected_point.get("fps") or 0) if selected_point else 0
    except Exception:
        fps = 0
    try:
        precision = float(selected_point.get("precision") or 0.0) if selected_point else float(matches) / float(max(1, matches + fps))
    except Exception:
        precision = float(matches) / float(max(1, matches + fps))

    return {
        **stat,
        "prompt": prompt_s,
        "matched_keys": matched_keys,
        "matches": int(matches),
        "fps": int(fps),
        "precision": float(precision),
        "selected_seed_threshold": float(thr),
        "selected_seed_threshold_point": selected_point,
    }


def _refine_steps_prompt_subset_seed_stage(
    prompt_stats: Sequence[Dict[str, Any]],
    selected: Sequence[Dict[str, Any]],
    *,
    max_steps: int,
    target_precision: Optional[float] = None,
    max_iters: int = 6,
    top_k: int = 6,
    base_seed_threshold: float = 0.05,
    log_fn: Optional[Callable[[str], None]] = None,
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Bounded prompt-subset refinement using only seed-stage stats (fast; no extra SAM runs).

    This is intended as a "local search" improvement on top of the greedy set-cover selection:
      - drop redundant steps (no unique GT coverage)
      - try a small number of add/swap moves guided by uncovered GT keys

    NOTE: This does not run full expansion/head tuning. It is a cheap approximation to improve step choice.
    """
    try:
        max_steps_i = max(1, int(max_steps))
    except Exception:
        max_steps_i = 6
    try:
        iters = max(0, int(max_iters))
    except Exception:
        iters = 0
    try:
        k = max(1, int(top_k))
    except Exception:
        k = 6
    try:
        base_thr = float(base_seed_threshold)
    except Exception:
        base_thr = 0.05
    base_thr = max(0.0, min(1.0, float(base_thr)))

    tgt: Optional[float]
    try:
        tgt = float(target_precision) if target_precision is not None else None
    except Exception:
        tgt = None
    if tgt is not None:
        tgt = max(0.0, min(1.0, float(tgt)))

    current: List[Dict[str, Any]] = [c for c in (selected or []) if isinstance(c, dict) and str(c.get("prompt") or "").strip()]
    if not current:
        return [], {"enabled": False, "reason": "no_selected"}

    # Build a pool of one "default operating point" per prompt for add/swap moves.
    pool: Dict[str, Dict[str, Any]] = {}
    for stat in prompt_stats or []:
        if not isinstance(stat, dict):
            continue
        p = str(stat.get("prompt") or "").strip()
        if not p:
            continue
        cand = _build_seed_stage_candidate_from_prompt_stat(stat, prompt=p, fallback_seed_threshold=base_thr)
        if cand:
            pool[p.lower()] = cand

    start_prompts = [str(c.get("prompt") or "") for c in current]

    def _score(cands: Sequence[Dict[str, Any]]) -> Tuple[int, float, int, float, int]:
        covered: set[int] = set()
        fps_total = 0
        for c in cands:
            mk = c.get("matched_keys")
            if isinstance(mk, set):
                covered |= mk
            try:
                fps_total += int(c.get("fps") or 0)
            except Exception:
                pass
        cov = int(len(covered))
        prec = float(cov) / float(max(1, cov + int(fps_total)))
        if tgt is not None and cov > 0 and prec >= float(tgt):
            return (1, float(cov), -int(fps_total), float(prec), -len(cands))
        return (0, float(prec), int(cov), -int(fps_total), -len(cands))

    def _unique_counts(cands: Sequence[Dict[str, Any]]) -> Dict[str, int]:
        counts: Dict[int, int] = {}
        by_prompt: Dict[str, set[int]] = {}
        for c in cands:
            p = str(c.get("prompt") or "").strip()
            mk = c.get("matched_keys") if isinstance(c.get("matched_keys"), set) else set()
            by_prompt[p] = mk
            for k_int in mk:
                counts[int(k_int)] = counts.get(int(k_int), 0) + 1
        uniq: Dict[str, int] = {}
        for p, mk in by_prompt.items():
            uniq[p] = sum(1 for k_int in mk if counts.get(int(k_int), 0) == 1)
        return uniq

    history: List[Dict[str, Any]] = []

    # First, drop purely redundant steps deterministically (no unique coverage).
    changed = True
    while changed and len(current) > 1:
        changed = False
        uniq = _unique_counts(current)
        redundant = [c for c in current if uniq.get(str(c.get("prompt") or "").strip(), 0) <= 0]
        if not redundant:
            break
        # Drop the most "costly" redundant step first (highest fps, then lowest precision).
        redundant = sorted(
            redundant,
            key=lambda c: (
                -int(c.get("fps") or 0),
                float(c.get("precision") or 0.0),
                str(c.get("prompt") or ""),
            ),
        )
        drop = redundant[0]
        before = [str(c.get("prompt") or "") for c in current]
        current = [c for c in current if c is not drop]
        after = [str(c.get("prompt") or "") for c in current]
        history.append({"op": "drop_redundant", "dropped": str(drop.get("prompt") or ""), "before": before, "after": after})
        changed = True

    cur_key = _score(current)

    for _iter in range(iters):
        prompts_now = {str(c.get("prompt") or "").strip().lower() for c in current}
        covered_now: set[int] = set()
        for c in current:
            mk = c.get("matched_keys")
            if isinstance(mk, set):
                covered_now |= mk

        add_pool: List[Dict[str, Any]] = []
        for p_l, cand in pool.items():
            if p_l in prompts_now:
                continue
            mk = cand.get("matched_keys")
            if not isinstance(mk, set):
                continue
            new_cov = len(mk - covered_now)
            if new_cov <= 0:
                continue
            add_pool.append({**cand, "_new_cov": int(new_cov)})
        add_pool = sorted(
            add_pool,
            key=lambda c: (
                int(c.get("_new_cov") or 0),
                float(c.get("precision") or 0.0),
                -int(c.get("fps") or 0),
                str(c.get("prompt") or ""),
            ),
            reverse=True,
        )[:k]

        uniq = _unique_counts(current)
        drop_pool = sorted(
            current,
            key=lambda c: (
                uniq.get(str(c.get("prompt") or "").strip(), 0),
                -int(c.get("fps") or 0),
                float(c.get("precision") or 0.0),
                str(c.get("prompt") or ""),
            ),
        )[:k]

        best_move: Optional[Dict[str, Any]] = None
        best_next: Optional[List[Dict[str, Any]]] = None
        best_key = cur_key

        def _try(next_cands: List[Dict[str, Any]], move: Dict[str, Any]) -> None:
            nonlocal best_move, best_next, best_key
            # Enforce unique prompts + max_steps.
            if len(next_cands) > max_steps_i:
                return
            seen: set[str] = set()
            for c in next_cands:
                p = str(c.get("prompt") or "").strip().lower()
                if not p or p in seen:
                    return
                seen.add(p)
            key = _score(next_cands)
            if key > best_key:
                best_key = key
                best_next = next_cands
                best_move = move

        # Add moves (only if room).
        if len(current) < max_steps_i:
            for add in add_pool:
                _try(current + [add], {"op": "add", "added": str(add.get("prompt") or "")})

        # Swap moves.
        for add in add_pool:
            for drop in drop_pool:
                next_cands = [c for c in current if c is not drop] + [add]
                _try(
                    next_cands,
                    {"op": "swap", "added": str(add.get("prompt") or ""), "dropped": str(drop.get("prompt") or "")},
                )

        if best_next is None or best_move is None:
            break

        before = [str(c.get("prompt") or "") for c in current]
        current = best_next
        after = [str(c.get("prompt") or "") for c in current]
        history.append({**best_move, "before": before, "after": after, "key_before": cur_key, "key_after": best_key})
        cur_key = best_key

        if log_fn:
            try:
                log_fn(f"[steps] refine prompt subset: {history[-1]}")
            except Exception:
                pass

    final_prompts = [str(c.get("prompt") or "") for c in current]
    return current, {
        "enabled": True,
        "mode": "seed_stage_local_search",
        "max_iters": int(iters),
        "top_k": int(k),
        "start_steps": start_prompts,
        "final_steps": final_prompts,
        "history": history,
    }


def _build_steps_recipe_step_list_from_selected_stats(
    selected_stats: Sequence[Dict[str, Any]],
    *,
    prompts_fallback: Optional[Sequence[str]],
    payload: AgentMiningRequest,
) -> Tuple[List[str], List[Dict[str, Any]]]:
    """
    Build the schema-v2 step list for a class, using the selected per-prompt stats.

    This is intentionally tolerant of future fields:
    - if a selected stat dict includes a per-prompt `seed_threshold` (or `selected_seed_threshold`), we
      use it for the corresponding step; otherwise we fall back to `payload.seed_threshold`.
    """
    prompts: List[str] = []
    step_list: List[Dict[str, Any]] = []

    for s in selected_stats or []:
        if not isinstance(s, dict):
            continue
        prompt = str(s.get("prompt") or "").strip()
        if prompt:
            prompts.append(prompt)

    if not prompts and prompts_fallback:
        try:
            fallback = str(list(prompts_fallback)[0]).strip()
        except Exception:
            fallback = ""
        if fallback:
            prompts = [fallback]
            selected_stats = [{}]

    def _pick_seed_threshold(stat: Dict[str, Any]) -> float:
        for key in ("seed_threshold", "selected_seed_threshold"):
            if stat.get(key) is None:
                continue
            try:
                val = float(stat.get(key))
            except Exception:
                continue
            return max(0.0, min(1.0, val))
        try:
            return max(0.0, min(1.0, float(payload.seed_threshold)))
        except Exception:
            return 0.05

    for idx, prompt in enumerate(prompts):
        stat = selected_stats[idx] if idx < len(selected_stats) and isinstance(selected_stats[idx], dict) else {}
        step_list.append(
            {
                "enabled": True,
                "prompt": prompt,
                "seed_threshold": _pick_seed_threshold(stat),
                "expand_threshold": float(payload.expand_threshold),
                "max_visual_seeds": int(getattr(payload, "steps_max_visual_seeds_per_step", 5) or 0),
                "seed_dedupe_iou": float(payload.seed_dedupe_iou),
                "dedupe_iou": float(payload.dedupe_iou),
                "max_results": int(payload.max_results),
            }
        )

    return prompts, step_list


def _mine_seed_prompt_stats_image_first(
    *,
    cat_id: int,
    prompts: Sequence[str],
    val_ids: Sequence[int],
    images: Dict[int, Dict[str, Any]],
    gt_by_image_cat: Dict[int, Dict[int, List[List[float]]]],
    payload: "AgentMiningRequest",
    clip_head: Optional[Dict[str, Any]] = None,
    clip_head_target_index: Optional[int] = None,
    clip_head_bg_indices: Optional[Sequence[int]] = None,
    prompt_bg_drop_cfg: Optional[Dict[str, Any]] = None,
    workers: Sequence["_Sam3GreedyEvalWorker"],
    log_every: int = 50,
    log_fn: Optional[Callable[[str], None]] = None,
    cancel_event: Optional[threading.Event] = None,
    progress_callback: Optional[Callable[[int, int], None]] = None,
) -> List[Dict[str, Any]]:
    """
    Seed-stage evaluation for all prompts (text-only).

    Returns per-prompt stats including a set of matched GT instance keys (for greedy set-cover).
    """
    prompt_list = [str(p).strip() for p in prompts if str(p).strip()]
    if not prompt_list:
        return []

    image_entries: List[Tuple[int, str]] = []
    for img_id in val_ids:
        info = images.get(int(img_id)) or {}
        path = info.get("path")
        if path:
            image_entries.append((int(img_id), str(path)))

    total_images = len(image_entries)
    if total_images <= 0:
        return []

    seed_eval_threshold = _compute_steps_seed_eval_threshold(payload)
    seed_eval_max_results = _compute_steps_seed_eval_max_results(payload)
    if log_fn and seed_eval_threshold < float(payload.seed_threshold) - 1e-9:
        try:
            log_fn(
                f"[steps] Candidate eval floor enabled: base_text_thr={float(payload.seed_threshold):.3f} "
                f"eval_text_thr={float(seed_eval_threshold):.3f} max_results={int(seed_eval_max_results)} (class_id {cat_id})"
            )
        except Exception:
            pass

    bg_cfg = prompt_bg_drop_cfg or {}
    bg_drop_enabled = bool(
        bg_cfg.get("enabled")
        and isinstance(clip_head, dict)
        and clip_head_target_index is not None
        and clip_head_bg_indices
    )
    try:
        bg_margin = float(getattr(payload, "clip_head_background_margin", 0.0) or 0.0)
    except Exception:
        bg_margin = 0.0

    # Global prompt stats. We keep additional per-prompt score distributions so we can build
    # seed-threshold curves without extra SAM3 runs.
    agg: List[Dict[str, Any]] = [
        {
            "prompt": p,
            "matched_keys": set(),
            "gt_best_scores": {},
            "fp_scores": array("f"),
            "fps": 0,
            "duplicates": 0,
            "preds": 0,
            "det_images": 0,
            "bg_checked": 0,
            "bg_veto": 0,
        }
        for p in prompt_list
    ]
    agg_lock = threading.Lock()

    processed = 0
    progress_lock = threading.Lock()

    def _mark_progress() -> None:
        nonlocal processed
        with progress_lock:
            processed += 1
            if progress_callback:
                try:
                    progress_callback(processed, total_images)
                except Exception:
                    pass
            if log_fn and log_every > 0 and processed % log_every == 0:
                try:
                    log_fn(f"[steps] Candidate eval: processed {processed}/{total_images} sample images for class_id {cat_id}")
                except Exception:
                    pass

    work_q: "queue.Queue[Tuple[int, str]]" = queue.Queue()
    for entry in image_entries:
        work_q.put(entry)

    def _worker_loop(worker: "_Sam3GreedyEvalWorker") -> None:
        # Local accumulators to avoid hot global locks.
        local_sets: List[set[int]] = [set() for _ in prompt_list]
        local_best: List[Dict[int, float]] = [{} for _ in prompt_list]
        local_fp_scores: List[array] = [array("f") for _ in prompt_list]
        local_fps = [0 for _ in prompt_list]
        local_dups = [0 for _ in prompt_list]
        local_preds = [0 for _ in prompt_list]
        local_det_imgs = [0 for _ in prompt_list]
        local_bg_checked = [0 for _ in prompt_list]
        local_bg_veto = [0 for _ in prompt_list]
        clip_device_override: Optional[str] = None
        if bg_drop_enabled:
            try:
                raw_clip_dev = os.environ.get("AGENT_MINING_CLIP_DEVICE")
            except Exception:
                raw_clip_dev = None
            if raw_clip_dev is not None and str(raw_clip_dev).strip():
                mode = str(raw_clip_dev).strip().lower()
                if mode in {"worker", "per_worker", "per-device", "per_device"}:
                    clip_device_override = str(worker.device)
                else:
                    clip_device_override = str(raw_clip_dev).strip()

        while True:
            if cancel_event is not None and cancel_event.is_set():
                break
            try:
                img_id, path = work_q.get_nowait()
            except queue.Empty:
                break
            try:
                with Image.open(path) as img:
                    pil_img = img.convert("RGB")
            except Exception:
                try:
                    work_q.task_done()
                except Exception:
                    pass
                _mark_progress()
                continue

            gt_boxes = (gt_by_image_cat.get(int(img_id)) or {}).get(int(cat_id)) or []
            gt_xyxy = [_xywh_to_xyxy(b) for b in gt_boxes]

            with worker.lock:
                try:
                    state = worker.processor.set_image(pil_img)
                except Exception:
                    state = None
                if state is None:
                    try:
                        work_q.task_done()
                    except Exception:
                        pass
                    _mark_progress()
                    continue

                for p_idx, prompt in enumerate(prompt_list):
                    if cancel_event is not None and cancel_event.is_set():
                        break
                    _reset_sam3_prompts_for_state(worker.processor, state)
                    try:
                        dets = _run_sam3_text_inference(
                            pil_img,
                            prompt,
                            float(seed_eval_threshold),
                            float(payload.mask_threshold),
                            int(seed_eval_max_results),
                            min_size=int(payload.min_size) if int(payload.min_size) > 0 else None,
                            simplify_epsilon=float(payload.simplify_epsilon),
                            processor_override=worker.processor,
                            state=state,
                        )
                    except Exception:
                        dets = []
                    if dets:
                        local_det_imgs[p_idx] += 1
                    if bg_drop_enabled and dets and isinstance(clip_head, dict):
                        det_crops: List[Image.Image] = []
                        for det in dets:
                            bbox = det.bbox or []
                            if len(bbox) < 4:
                                continue
                            try:
                                det_xyxy = yolo_to_corners(bbox, pil_img.width, pil_img.height)
                            except Exception:
                                continue
                            x1, y1, x2, y2 = det_xyxy
                            if x2 <= x1 or y2 <= y1:
                                continue
                            try:
                                det_crops.append(pil_img.crop((x1, y1, x2, y2)))
                            except Exception:
                                continue
                        if det_crops and clip_head_target_index is not None and clip_head_bg_indices:
                            try:
                                feats = _encode_pil_batch_for_head(
                                    det_crops, head=clip_head, device_override=clip_device_override
                                )
                            except Exception:
                                feats = None
                            if feats is not None and feats.size:
                                try:
                                    proba = _clip_head_predict_proba(feats, clip_head)
                                except Exception:
                                    proba = None
                                if proba is not None:
                                    try:
                                        t_idx = int(clip_head_target_index)
                                        p_target = proba[:, t_idx]
                                        p_bg = np.max(proba[:, clip_head_bg_indices], axis=1)
                                        local_bg_checked[p_idx] += int(p_target.shape[0])
                                        local_bg_veto[p_idx] += int(np.sum(p_target < (p_bg + float(bg_margin))))
                                    except Exception:
                                        pass
                    used: set[int] = set()
                    for det in dets or []:
                        local_preds[p_idx] += 1
                        try:
                            det_score = float(det.score) if det.score is not None else 0.0
                        except Exception:
                            det_score = 0.0
                        bbox = det.bbox or []
                        if len(bbox) < 4:
                            continue
                        try:
                            det_xyxy = yolo_to_corners(bbox, pil_img.width, pil_img.height)
                        except Exception:
                            continue
                        best_iou = 0.0
                        best_idx: Optional[int] = None
                        for j, gt in enumerate(gt_xyxy):
                            iou = _iou_xyxy(det_xyxy, gt)
                            if iou > best_iou:
                                best_iou = iou
                                best_idx = j
                        if best_idx is not None and best_iou >= float(payload.iou_threshold):
                            gt_key = _gt_instance_key(img_id, best_idx)
                            prev = local_best[p_idx].get(gt_key)
                            if prev is None or det_score > float(prev):
                                local_best[p_idx][gt_key] = float(det_score)
                            if best_idx in used:
                                local_dups[p_idx] += 1
                            else:
                                used.add(best_idx)
                                local_sets[p_idx].add(gt_key)
                        else:
                            local_fps[p_idx] += 1
                            try:
                                local_fp_scores[p_idx].append(float(det_score))
                            except Exception:
                                pass

            try:
                work_q.task_done()
            except Exception:
                pass
            _mark_progress()

        # Merge once per worker thread.
        with agg_lock:
            for i in range(len(prompt_list)):
                agg[i]["matched_keys"].update(local_sets[i])
                # Merge GT best-score maps (max per key).
                dst_best = agg[i].get("gt_best_scores")
                if not isinstance(dst_best, dict):
                    dst_best = {}
                    agg[i]["gt_best_scores"] = dst_best
                for k, v in (local_best[i] or {}).items():
                    try:
                        k_int = int(k)
                    except Exception:
                        continue
                    try:
                        v_f = float(v)
                    except Exception:
                        continue
                    cur = dst_best.get(k_int)
                    if cur is None or v_f > float(cur):
                        dst_best[k_int] = v_f
                # Merge FP score arrays.
                dst_fp = agg[i].get("fp_scores")
                if not isinstance(dst_fp, array):
                    dst_fp = array("f")
                    agg[i]["fp_scores"] = dst_fp
                try:
                    dst_fp.extend(local_fp_scores[i])
                except Exception:
                    pass
                agg[i]["fps"] = int(agg[i]["fps"] or 0) + int(local_fps[i])
                agg[i]["duplicates"] = int(agg[i]["duplicates"] or 0) + int(local_dups[i])
                agg[i]["preds"] = int(agg[i]["preds"] or 0) + int(local_preds[i])
                agg[i]["det_images"] = int(agg[i]["det_images"] or 0) + int(local_det_imgs[i])
                agg[i]["bg_checked"] = int(agg[i]["bg_checked"] or 0) + int(local_bg_checked[i])
                agg[i]["bg_veto"] = int(agg[i]["bg_veto"] or 0) + int(local_bg_veto[i])

    max_workers = max(1, len(workers))
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(_worker_loop, w) for w in workers]
        for fut in futures:
            fut.result()

    out: List[Dict[str, Any]] = []
    for item in agg:
        matched_keys = item.get("matched_keys") if isinstance(item.get("matched_keys"), set) else set()
        gt_best_scores = item.get("gt_best_scores") if isinstance(item.get("gt_best_scores"), dict) else {}
        fp_scores = item.get("fp_scores") if isinstance(item.get("fp_scores"), array) else array("f")
        matches = len(matched_keys)
        fps = int(item.get("fps") or 0)
        preds = int(item.get("preds") or 0)
        det_images = int(item.get("det_images") or 0)
        duplicates = int(item.get("duplicates") or 0)
        precision = matches / max(1, matches + fps)
        det_rate = det_images / total_images if total_images else 0.0
        bg_checked = int(item.get("bg_checked") or 0)
        bg_veto = int(item.get("bg_veto") or 0)
        bg_veto_rate = float(bg_veto) / float(bg_checked) if bg_checked else 0.0
        bg_drop = False
        bg_drop_reason = None
        if bg_drop_enabled:
            min_checked = int(bg_cfg.get("min_checked") or 0)
            drop_rate = float(bg_cfg.get("drop_rate") or 0.0)
            if bg_checked >= min_checked and bg_veto_rate >= drop_rate:
                bg_drop = True
                bg_drop_reason = "background_rate"
        curve_summary = _summarize_seed_threshold_curve_for_prompt(
            gt_best_scores=gt_best_scores,
            fp_scores=fp_scores,
            base_seed_threshold=float(payload.seed_threshold),
        )
        out.append(
            {
                "prompt": item.get("prompt"),
                "matched_keys": matched_keys,
                "gt_best_scores": gt_best_scores,
                "matches": matches,
                "fps": fps,
                "duplicates": duplicates,
                "preds": preds,
                "precision": precision,
                "det_rate": det_rate,
                "bg_checked": bg_checked,
                "bg_veto": bg_veto,
                "bg_veto_rate": bg_veto_rate,
                "bg_drop": bg_drop,
                "bg_drop_reason": bg_drop_reason,
                "seed_eval_threshold_used": float(seed_eval_threshold),
                "seed_eval_max_results_used": int(seed_eval_max_results),
                **(curve_summary or {}),
            }
        )
    return out


def _resolve_steps_early_stop_config(payload: "AgentMiningRequest", *, target_precision: Optional[float]) -> Dict[str, Any]:
    enabled = bool(getattr(payload, "steps_early_stop", False))
    mode = str(getattr(payload, "steps_early_stop_mode", "balanced") or "balanced").lower().strip()
    if mode not in {"conservative", "balanced", "aggressive"}:
        mode = "balanced"
    mode_map = {
        "conservative": {"min_steps": 4, "window": 3, "min_increment": 0.002, "precision_margin": 0.15},
        "balanced": {"min_steps": 3, "window": 2, "min_increment": 0.005, "precision_margin": 0.1},
        "aggressive": {"min_steps": 2, "window": 2, "min_increment": 0.01, "precision_margin": 0.05},
    }
    cfg = mode_map[mode]
    precision_floor = None
    if target_precision is not None:
        try:
            precision_floor = max(0.0, min(1.0, float(target_precision) - float(cfg["precision_margin"])))
        except Exception:
            precision_floor = None
    return {
        "enabled": enabled,
        "mode": mode,
        "min_steps": int(cfg["min_steps"]),
        "window": int(cfg["window"]),
        "min_increment": float(cfg["min_increment"]),
        "precision_floor": precision_floor,
        "precision_margin": float(cfg["precision_margin"]),
    }


def _resolve_steps_prompt_prefilter_config(
    payload: "AgentMiningRequest",
    *,
    allow_prefilter: bool = True,
) -> Dict[str, Any]:
    requested = bool(getattr(payload, "steps_prompt_prefilter", False))
    enabled = bool(requested and allow_prefilter)
    disabled_reason = None
    if requested and not allow_prefilter:
        disabled_reason = "head_encoder_not_clip"
    mode = str(getattr(payload, "steps_prompt_prefilter_mode", "balanced") or "balanced").lower().strip()
    if mode not in {"conservative", "balanced", "aggressive"}:
        mode = "balanced"
    mode_map = {
        "conservative": {"sample_size": 20, "keep_ratio": 0.6},
        "balanced": {"sample_size": 40, "keep_ratio": 0.4},
        "aggressive": {"sample_size": 80, "keep_ratio": 0.25},
    }
    cfg = mode_map[mode]
    return {
        "enabled": enabled,
        "mode": mode,
        "sample_size": int(cfg["sample_size"]),
        "keep_ratio": float(cfg["keep_ratio"]),
        "requested": requested,
        "disabled_reason": disabled_reason,
    }


def _resolve_steps_prompt_bg_drop_config(
    payload: "AgentMiningRequest",
    *,
    allow_drop: bool = True,
) -> Dict[str, Any]:
    requested = bool(getattr(payload, "steps_prompt_bg_drop", False))
    enabled = bool(requested and allow_drop)
    disabled_reason = None
    if requested and not allow_drop:
        disabled_reason = "no_background_classes"
    mode = str(getattr(payload, "steps_prompt_bg_drop_mode", "balanced") or "balanced").lower().strip()
    if mode not in {"conservative", "balanced", "aggressive"}:
        mode = "balanced"
    mode_map = {
        "conservative": {"min_checked": 60, "drop_rate": 0.75},
        "balanced": {"min_checked": 40, "drop_rate": 0.6},
        "aggressive": {"min_checked": 20, "drop_rate": 0.45},
    }
    cfg = mode_map[mode]
    return {
        "enabled": enabled,
        "mode": mode,
        "min_checked": int(cfg["min_checked"]),
        "drop_rate": float(cfg["drop_rate"]),
        "requested": requested,
        "disabled_reason": disabled_reason,
    }


def _resolve_steps_hard_negative_export_config(payload: "AgentMiningRequest") -> Dict[str, Any]:
    enabled = bool(getattr(payload, "steps_hard_negative_export", False))
    try:
        max_crops = int(getattr(payload, "steps_hard_negative_max_crops", 0) or 0)
    except Exception:
        max_crops = 0
    max_crops = max(0, int(max_crops))
    try:
        min_prob = float(getattr(payload, "steps_hard_negative_min_prob", 0.0) or 0.0)
    except Exception:
        min_prob = 0.0
    min_prob = max(0.0, min(1.0, float(min_prob)))
    enabled = bool(enabled and max_crops > 0)
    return {
        "enabled": enabled,
        "max_crops": int(max_crops),
        "min_prob": float(min_prob),
    }


def _estimate_steps_speed_factor(payload: "AgentMiningRequest", *, allow_prefilter: bool = True) -> float:
    early_enabled = bool(getattr(payload, "steps_early_stop", False))
    early_mode = str(getattr(payload, "steps_early_stop_mode", "balanced") or "balanced").lower().strip()
    if early_mode not in {"conservative", "balanced", "aggressive"}:
        early_mode = "balanced"
    prefilter_enabled = bool(getattr(payload, "steps_prompt_prefilter", False) and allow_prefilter)
    prefilter_mode = str(getattr(payload, "steps_prompt_prefilter_mode", "balanced") or "balanced").lower().strip()
    if prefilter_mode not in {"conservative", "balanced", "aggressive"}:
        prefilter_mode = "balanced"
    bg_drop_enabled = bool(getattr(payload, "steps_prompt_bg_drop", False))
    bg_drop_mode = str(getattr(payload, "steps_prompt_bg_drop_mode", "balanced") or "balanced").lower().strip()
    if bg_drop_mode not in {"conservative", "balanced", "aggressive"}:
        bg_drop_mode = "balanced"
    early_factor = 1.0
    if early_enabled:
        early_factor = 0.9 if early_mode == "conservative" else 0.65 if early_mode == "aggressive" else 0.8
    prefilter_factor = 1.0
    if prefilter_enabled:
        prefilter_factor = 0.85 if prefilter_mode == "conservative" else 0.55 if prefilter_mode == "aggressive" else 0.7
    bg_drop_factor = 1.0
    if bg_drop_enabled:
        bg_drop_factor = 0.92 if bg_drop_mode == "conservative" else 0.7 if bg_drop_mode == "aggressive" else 0.82
    return float(early_factor * prefilter_factor * bg_drop_factor)


def _estimate_agent_global_optimizer_image_evals(
    *,
    val_images: int,
    eval_caps: Sequence[int],
    keep_ratio: float,
    rounds: int,
    max_trials: int,
    mutations_per_round: int,
) -> Tuple[int, List[int], bool]:
    parsed = []
    for cap in eval_caps or []:
        try:
            b = int(cap)
        except Exception:
            continue
        if b > 0:
            parsed.append(b)
    budgets = sorted(set(parsed))
    if not budgets:
        return 0, [], True
    try:
        val_n = max(1, int(val_images))
    except Exception:
        val_n = int(max(1, val_images))
    try:
        keep = max(0.0, min(1.0, float(keep_ratio)))
    except Exception:
        keep = 0.5
    try:
        rounds_i = max(1, int(rounds))
    except Exception:
        rounds_i = 1
    try:
        max_trials_i = max(1, int(max_trials))
    except Exception:
        max_trials_i = 1
    try:
        mutations_i = max(1, int(mutations_per_round))
    except Exception:
        mutations_i = 1
    candidates_per_round = 1
    if max_trials_i > 1:
        max_mut = max(1, min(mutations_i, max_trials_i - 1))
        candidates_per_round = 1 + max_mut
    total = 0
    for _round in range(rounds_i):
        active = max(1, candidates_per_round)
        for idx, budget in enumerate(budgets):
            eff_budget = min(int(budget), val_n)
            total += int(active) * int(eff_budget)
            if idx < len(budgets) - 1:
                active = max(1, int(math.ceil(active * keep)))
    return int(total), budgets, False


def _collect_clip_prefilter_crops(
    *,
    cat_id: int,
    eval_ids: Sequence[int],
    images: Dict[int, Dict[str, Any]],
    gt_by_image_cat: Dict[int, Dict[int, List[List[float]]]],
    sample_size: int,
    seed: int,
) -> List[Image.Image]:
    candidates: List[Tuple[int, List[float]]] = []
    eval_set = set(int(i) for i in eval_ids)
    for img_id, cat_map in gt_by_image_cat.items():
        if int(img_id) not in eval_set:
            continue
        bboxes = cat_map.get(int(cat_id)) or []
        for bbox in bboxes:
            if not bbox or len(bbox) < 4:
                continue
            candidates.append((int(img_id), list(bbox)))
    if not candidates:
        return []
    rng = random.Random(int(seed))
    rng.shuffle(candidates)
    sample_size = max(1, int(sample_size))
    picks = candidates[: min(sample_size, len(candidates))]
    crops: List[Image.Image] = []
    image_cache: Dict[int, Image.Image] = {}
    for img_id, bbox in picks:
        info = images.get(int(img_id)) or {}
        path = info.get("path")
        if not path:
            continue
        pil_img = image_cache.get(int(img_id))
        if pil_img is None:
            try:
                pil_img = Image.open(path).convert("RGB")
            except Exception:
                continue
            image_cache[int(img_id)] = pil_img
        try:
            x0, y0, w, h = bbox[:4]
            x1 = int(max(0.0, float(x0)))
            y1 = int(max(0.0, float(y0)))
            x2 = int(min(float(pil_img.width), float(x0) + float(w)))
            y2 = int(min(float(pil_img.height), float(y0) + float(h)))
        except Exception:
            continue
        if x2 <= x1 or y2 <= y1:
            continue
        try:
            crops.append(pil_img.crop((x1, y1, x2, y2)))
        except Exception:
            continue
    return crops


def _prefilter_prompts_with_clip(
    prompts: Sequence[str],
    *,
    keep_prompts: Sequence[str],
    cat_id: int,
    class_name: str,
    eval_ids: Sequence[int],
    images: Dict[int, Dict[str, Any]],
    gt_by_image_cat: Dict[int, Dict[int, List[List[float]]]],
    clip_model_name: Optional[str],
    sample_size: int,
    keep_ratio: float,
    seed: int,
    log_fn: Optional[Callable[[str], None]] = None,
) -> List[str]:
    prompt_list: List[str] = []
    seen: set[str] = set()
    for p in prompts:
        key = str(p).strip()
        if not key:
            continue
        low = key.lower()
        if low in seen:
            continue
        seen.add(low)
        prompt_list.append(key)
    if not prompt_list:
        return []

    keep_set = {str(p).strip().lower() for p in keep_prompts if str(p).strip()}
    extra_indices = [idx for idx, p in enumerate(prompt_list) if p.lower() not in keep_set]
    if not extra_indices:
        return prompt_list

    crops = _collect_clip_prefilter_crops(
        cat_id=cat_id,
        eval_ids=eval_ids,
        images=images,
        gt_by_image_cat=gt_by_image_cat,
        sample_size=sample_size,
        seed=seed,
    )
    if not crops:
        if log_fn:
            try:
                log_fn(f"[steps] CLIP prefilter skipped for {class_name}: no GT crops in sample.")
            except Exception:
                pass
        return prompt_list

    img_emb = _clip_encode_pil_batch(crops, clip_model_override=clip_model_name)
    text_emb = _clip_encode_text_batch(prompt_list, clip_model_override=clip_model_name)
    if img_emb is None or text_emb is None:
        if log_fn:
            try:
                log_fn(f"[steps] CLIP prefilter skipped for {class_name}: CLIP embeddings unavailable.")
            except Exception:
                pass
        return prompt_list

    try:
        sims = np.matmul(text_emb, img_emb.T)
        scores = sims.max(axis=1) if sims.size else np.zeros(len(prompt_list), dtype=np.float32)
    except Exception:
        if log_fn:
            try:
                log_fn(f"[steps] CLIP prefilter skipped for {class_name}: similarity computation failed.")
            except Exception:
                pass
        return prompt_list

    keep_ratio = max(0.05, min(float(keep_ratio), 1.0))
    extra_count = len(extra_indices)
    keep_count = max(1, int(round(extra_count * keep_ratio)))
    keep_count = min(extra_count, keep_count)
    ranked = sorted(extra_indices, key=lambda i: float(scores[i]), reverse=True)
    kept_extra = set(ranked[:keep_count])

    filtered = [p for idx, p in enumerate(prompt_list) if idx in kept_extra or p.lower() in keep_set]
    if log_fn:
        try:
            log_fn(
                f"[steps] CLIP prefilter {class_name}: kept {len(filtered)}/{len(prompt_list)} prompts "
                f"(base {len(prompt_list) - extra_count} + filtered {len(kept_extra)})"
            )
        except Exception:
            pass
    return filtered


def _normalize_steps_for_head_tuning(
    steps: Sequence[Dict[str, Any]],
    *,
    payload: "AgentMiningRequest",
) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    for step in steps or []:
        if not isinstance(step, dict):
            continue
        if step.get("enabled") is False:
            continue
        prompt = str(step.get("prompt") or "").strip()
        if not prompt:
            continue

        try:
            seed_thr = float(step.get("seed_threshold") if step.get("seed_threshold") is not None else payload.seed_threshold)
        except Exception:
            seed_thr = float(payload.seed_threshold)
        seed_thr = max(0.0, min(1.0, float(seed_thr)))

        try:
            expand_thr = float(
                step.get("expand_threshold") if step.get("expand_threshold") is not None else payload.expand_threshold
            )
        except Exception:
            expand_thr = float(payload.expand_threshold)
        expand_thr = max(0.0, min(1.0, float(expand_thr)))

        try:
            max_seeds = int(
                step.get("max_visual_seeds")
                if step.get("max_visual_seeds") is not None
                else getattr(payload, "steps_max_visual_seeds_per_step", 0)
            )
        except Exception:
            max_seeds = int(getattr(payload, "steps_max_visual_seeds_per_step", 0) or 0)
        max_seeds = max(0, int(max_seeds))

        try:
            seed_iou = float(step.get("seed_dedupe_iou") if step.get("seed_dedupe_iou") is not None else payload.seed_dedupe_iou)
        except Exception:
            seed_iou = float(payload.seed_dedupe_iou)
        seed_iou = max(0.0, min(1.0, float(seed_iou)))

        try:
            out_iou = float(step.get("dedupe_iou") if step.get("dedupe_iou") is not None else payload.dedupe_iou)
        except Exception:
            out_iou = float(payload.dedupe_iou)
        out_iou = max(0.0, min(1.0, float(out_iou)))

        try:
            max_results = int(step.get("max_results") if step.get("max_results") is not None else payload.max_results)
        except Exception:
            max_results = int(payload.max_results)
        max_results = max(1, int(max_results))

        out.append(
            {
                "prompt": prompt,
                "seed_threshold": float(seed_thr),
                "expand_threshold": float(expand_thr),
                "max_visual_seeds": int(max_seeds),
                "seed_dedupe_iou": float(seed_iou),
                "dedupe_iou": float(out_iou),
                "max_results": int(max_results),
            }
        )
    return out


def _tune_clip_head_for_selected_steps_image_first(
    *,
    cat_id: int,
    class_name: Optional[str] = None,
    steps: Sequence[Dict[str, Any]],
    val_ids: Sequence[int],
    images: Dict[int, Dict[str, Any]],
    gt_by_image_cat: Dict[int, Dict[int, List[List[float]]]],
    payload: "AgentMiningRequest",
    clip_head: Dict[str, Any],
    clip_head_target_index: int,
    workers: Sequence["_Sam3GreedyEvalWorker"],
    log_every: int = 50,
    log_fn: Optional[Callable[[str], None]] = None,
    cancel_event: Optional[threading.Event] = None,
    progress_callback: Optional[Callable[[int, int], None]] = None,
    export_hard_negatives: bool = True,
) -> Dict[str, Any]:
    """
    Run the full step pipeline (seed->diverse->expand) for the selected prompts, then sweep CLIP-head
    thresholds (min_prob/margin) to find a cleanliness operating point on the val split.
    """
    steps_norm = _normalize_steps_for_head_tuning(steps, payload=payload)
    if not steps_norm:
        return {"gts": 0, "matches": 0, "fps": 0, "duplicates": 0, "preds": 0, "precision": 0.0, "recall": 0.0, "det_rate": 0.0}

    image_entries: List[Tuple[int, str]] = []
    for img_id in val_ids:
        info = images.get(int(img_id)) or {}
        path = info.get("path")
        if path:
            image_entries.append((int(img_id), str(path)))
    total_images = len(image_entries)
    if total_images <= 0:
        return {"gts": 0, "matches": 0, "fps": 0, "duplicates": 0, "preds": 0, "precision": 0.0, "recall": 0.0, "det_rate": 0.0}

    total_gt = 0
    for img_id in val_ids:
        total_gt += len((gt_by_image_cat.get(int(img_id)) or {}).get(int(cat_id)) or [])

    bg_guard_seed, bg_guard_final, bg_margin, _bg_apply = _resolve_clip_head_background_settings(payload)
    classes_list = clip_head.get("classes") if isinstance(clip_head, dict) and isinstance(clip_head.get("classes"), list) else []
    bg_indices = _clip_head_background_indices(classes_list)
    allow_bg_tune = bool(bg_indices and getattr(payload, "clip_head_background_guard", False))
    hard_neg_cfg = _resolve_steps_hard_negative_export_config(payload) if export_hard_negatives else {"enabled": False}
    hard_neg_enabled = bool(hard_neg_cfg.get("enabled"))
    try:
        hard_neg_min_prob = float(hard_neg_cfg.get("min_prob") or 0.0)
    except Exception:
        hard_neg_min_prob = 0.0
    try:
        hard_neg_max_crops = int(hard_neg_cfg.get("max_crops") or 0)
    except Exception:
        hard_neg_max_crops = 0
    hard_neg_entries: List[Dict[str, Any]] = []
    hard_neg_lock = threading.Lock()

    flow_keys = (
        "text_candidates_total",
        "candidates_after_dedupe",
        "candidates_kept",
        "expand_candidates",
        "expanded_total",
        "final_seed_total",
        "final_expanded_total",
        "final_total",
        "seed_bg_checked",
        "seed_bg_veto",
        "final_bg_checked",
        "final_bg_veto",
    )
    step_flow_totals: List[Dict[str, Any]] = []
    for step_cfg in steps_norm:
        step_flow_totals.append({"prompt": str(step_cfg.get("prompt") or ""), **{k: 0 for k in flow_keys}})

    sweep_min_probs, sweep_margins, sweep_bg_margins, target_precision = _build_clip_head_sweep_grid(
        payload,
        base_min_prob=float(payload.clip_head_min_prob),
        base_margin=float(payload.clip_head_margin),
        base_bg_margin=float(bg_margin),
        allow_bg_tune=allow_bg_tune,
    )

    def _counts_init() -> Dict[str, int]:
        return {"matches": 0, "fps": 0, "duplicates": 0, "preds": 0, "det_images": 0}

    agg: Dict[float, Dict[float, Dict[float, Dict[str, int]]]] = {
        float(m): {float(bg): {float(p): _counts_init() for p in sweep_min_probs} for bg in sweep_bg_margins}
        for m in sweep_margins
    }
    debug = {"base_dets": 0, "with_prob": 0, "prob_min": None, "prob_max": None}

    agg_lock = threading.Lock()
    processed = 0
    progress_lock = threading.Lock()

    def _mark_progress() -> None:
        nonlocal processed
        with progress_lock:
            processed += 1
            if progress_callback:
                try:
                    progress_callback(processed, total_images)
                except Exception:
                    pass
            if log_fn and log_every > 0 and processed % log_every == 0:
                try:
                    log_fn(f"[steps] Final tune: processed {processed}/{total_images} val images for class_id {cat_id}")
                except Exception:
                    pass

    work_q: "queue.Queue[Tuple[int, str]]" = queue.Queue()
    for entry in image_entries:
        work_q.put(entry)

    def _resolve_clip_device(worker_dev: torch.device) -> Optional[str]:
        try:
            raw = os.environ.get("AGENT_MINING_CLIP_DEVICE")
        except Exception:
            raw = None
        if raw is None or not str(raw).strip():
            return None
        mode = str(raw).strip().lower()
        if mode in {"worker", "per_worker", "per-device", "per_device"}:
            return str(worker_dev)
        return str(raw).strip()

    def _worker_loop(worker: "_Sam3GreedyEvalWorker") -> None:
        local: Dict[float, Dict[float, Dict[float, Dict[str, int]]]] = {
            float(m): {float(bg): {float(p): _counts_init() for p in sweep_min_probs} for bg in sweep_bg_margins}
            for m in sweep_margins
        }
        local_debug = {"base_dets": 0, "with_prob": 0, "prob_min": None, "prob_max": None}
        local_step_flow: List[Dict[str, int]] = [{k: 0 for k in flow_keys} for _ in steps_norm]
        local_set_image_failures = 0
        local_infer_failures = 0
        clip_device_override = _resolve_clip_device(worker.device)
        local_hard_negs: List[Dict[str, Any]] = []
        local_hard_cap = max(0, int(hard_neg_max_crops)) * 2 if hard_neg_enabled else 0

        while True:
            if cancel_event is not None and cancel_event.is_set():
                break
            try:
                img_id, path = work_q.get_nowait()
            except queue.Empty:
                break
            try:
                with Image.open(path) as img:
                    pil_img = img.convert("RGB")
            except Exception:
                try:
                    work_q.task_done()
                except Exception:
                    pass
                _mark_progress()
                continue

            gt_boxes = (gt_by_image_cat.get(int(img_id)) or {}).get(int(cat_id)) or []
            gt_xyxy = [_xywh_to_xyxy(b) for b in gt_boxes]

            rows: List[Tuple[float, float, float, Optional[float], Optional[int]]] = []
            with worker.lock:
                try:
                    state = worker.processor.set_image(pil_img)
                except Exception as exc:
                    if log_fn and local_set_image_failures < 3:
                        local_set_image_failures += 1
                        try:
                            log_fn(f"[steps] Final tune: set_image failed on {worker.device} for img {img_id}: {exc}")
                        except Exception:
                            pass
                    state = None
                if state is not None:
                    for step_idx, step_cfg in enumerate(steps_norm):
                        if cancel_event is not None and cancel_event.is_set():
                            break
                        prompt = str(step_cfg.get("prompt") or "").strip()
                        if not prompt:
                            continue
                        flow_stats: Dict[str, int] = {}
                        try:
                            dets_step = _infer_sam3_greedy_recipe_on_image(
                                pil_img=pil_img,
                                processor=worker.processor,
                                text_prompts=[prompt],
                                exemplar_embeddings=None,
                                negative_embeddings=None,
                                seed_threshold=float(step_cfg["seed_threshold"]),
                                expand_threshold=float(step_cfg["expand_threshold"]),
                                max_visual_seeds=int(step_cfg["max_visual_seeds"]),
                                seed_dedupe_iou=float(step_cfg["seed_dedupe_iou"]),
                                out_dedupe_iou=float(step_cfg["dedupe_iou"]),
                                mask_threshold=float(payload.mask_threshold),
                                min_size=int(payload.min_size),
                                simplify_epsilon=float(payload.simplify_epsilon),
                                max_results=int(step_cfg["max_results"]),
                                negative_strength=0.0,
                                similarity_floor=0.0,
                                clip_head=clip_head,
                                clip_head_target_index=int(clip_head_target_index),
                                clip_head_min_prob=0.0,
                                clip_head_margin=0.0,
                                clip_head_background_guard_seed=bg_guard_seed,
                                clip_head_background_guard_final=bg_guard_final,
                                clip_head_background_margin=bg_margin,
                                stats_out=flow_stats,
                                state=state,
                                clip_device_override=clip_device_override,
                            )
                        except Exception as exc:
                            if log_fn and local_infer_failures < 3:
                                local_infer_failures += 1
                                try:
                                    log_fn(f"[steps] Final tune: step failed for '{prompt}' on img {img_id}: {exc}")
                                except Exception:
                                    pass
                            dets_step = []
                        if 0 <= step_idx < len(local_step_flow):
                            for key in flow_keys:
                                try:
                                    local_step_flow[step_idx][key] += int(flow_stats.get(key) or 0)
                                except Exception:
                                    continue
                        for det in dets_step or []:
                            bbox = det.bbox or []
                            if len(bbox) < 4:
                                continue
                            try:
                                det_xyxy = yolo_to_corners(bbox, pil_img.width, pil_img.height)
                            except Exception:
                                continue
                            best_iou = 0.0
                            best_idx: Optional[int] = None
                            for j, gt in enumerate(gt_xyxy):
                                iou = _iou_xyxy(det_xyxy, gt)
                                if iou > best_iou:
                                    best_iou = iou
                                    best_idx = j
                            prob = float(det.clip_head_prob) if det.clip_head_prob is not None else 0.0
                            margin = float(det.clip_head_margin) if det.clip_head_margin is not None else 0.0
                            bg_prob = float(det.clip_head_bg_prob) if det.clip_head_bg_prob is not None else None
                            if hard_neg_enabled and hard_neg_max_crops > 0:
                                is_fp = best_idx is None or float(best_iou) < float(payload.iou_threshold)
                                if is_fp:
                                    score = prob
                                    if det.clip_head_prob is None:
                                        try:
                                            score = float(det.score) if det.score is not None else 0.0
                                        except Exception:
                                            score = 0.0
                                    if float(score) >= float(hard_neg_min_prob):
                                        local_hard_negs.append(
                                            {
                                                "image_id": int(img_id),
                                                "image_path": str(path),
                                                "bbox_xyxy": [float(det_xyxy[0]), float(det_xyxy[1]), float(det_xyxy[2]), float(det_xyxy[3])],
                                                "score": float(score),
                                                "clip_prob": float(prob) if det.clip_head_prob is not None else None,
                                                "clip_bg_prob": float(bg_prob) if bg_prob is not None else None,
                                                "clip_margin": float(margin) if det.clip_head_margin is not None else None,
                                                "prompt": str(prompt),
                                            }
                                        )
                                        if local_hard_cap and len(local_hard_negs) > local_hard_cap:
                                            local_hard_negs.sort(key=lambda e: float(e.get("score") or 0.0), reverse=True)
                                            local_hard_negs[:] = local_hard_negs[:local_hard_cap]
                            rows.append((prob, margin, float(best_iou), bg_prob, best_idx))
                            local_debug["base_dets"] += 1
                            if det.clip_head_prob is not None:
                                local_debug["with_prob"] += 1
                                cur_min = local_debug.get("prob_min")
                                cur_max = local_debug.get("prob_max")
                                try:
                                    local_debug["prob_min"] = prob if cur_min is None else float(min(float(cur_min), prob))
                                except Exception:
                                    local_debug["prob_min"] = prob
                                try:
                                    local_debug["prob_max"] = prob if cur_max is None else float(max(float(cur_max), prob))
                                except Exception:
                                    local_debug["prob_max"] = prob

            for margin_thr in sweep_margins:
                for bg_margin_thr in sweep_bg_margins:
                    for min_prob in sweep_min_probs:
                        counts = local[float(margin_thr)][float(bg_margin_thr)][float(min_prob)]
                        used: set[int] = set()
                        any_det = False
                        for prob, margin, best_iou, bg_prob, best_idx in rows:
                            if prob < float(min_prob):
                                continue
                            if float(margin_thr) > 0.0 and margin < float(margin_thr):
                                continue
                            if allow_bg_tune and float(bg_margin_thr) > 0.0 and bg_prob is not None:
                                if prob < float(bg_prob) + float(bg_margin_thr):
                                    continue
                            any_det = True
                            counts["preds"] += 1
                            if best_idx is not None and best_iou >= float(payload.iou_threshold):
                                if best_idx in used:
                                    counts["duplicates"] += 1
                                else:
                                    used.add(best_idx)
                                    counts["matches"] += 1
                            else:
                                counts["fps"] += 1
                        if any_det:
                            counts["det_images"] += 1

            try:
                work_q.task_done()
            except Exception:
                pass
            _mark_progress()

        with agg_lock:
            for m, by_bg in local.items():
                for bg, by_prob in by_bg.items():
                    for p, counts in by_prob.items():
                        dst = agg[float(m)][float(bg)][float(p)]
                        for k in ("matches", "fps", "duplicates", "preds", "det_images"):
                            try:
                                dst[k] = int(dst.get(k, 0)) + int(counts.get(k, 0))
                            except Exception:
                                continue
            for idx, stats in enumerate(local_step_flow):
                if idx >= len(step_flow_totals):
                    continue
                for key in flow_keys:
                    try:
                        step_flow_totals[idx][key] = int(step_flow_totals[idx].get(key, 0)) + int(stats.get(key, 0))
                    except Exception:
                        continue
            try:
                debug["base_dets"] = int(debug.get("base_dets") or 0) + int(local_debug.get("base_dets") or 0)
            except Exception:
                pass
            try:
                debug["with_prob"] = int(debug.get("with_prob") or 0) + int(local_debug.get("with_prob") or 0)
            except Exception:
                pass
            for bound in ("prob_min", "prob_max"):
                val = local_debug.get(bound)
                if val is None:
                    continue
                try:
                    val_f = float(val)
                except Exception:
                    continue
                cur = debug.get(bound)
                try:
                    if cur is None:
                        debug[bound] = val_f
                    elif bound == "prob_min":
                        debug[bound] = float(min(float(cur), val_f))
                    else:
                        debug[bound] = float(max(float(cur), val_f))
                except Exception:
                    debug[bound] = val_f
            if hard_neg_enabled and local_hard_negs:
                with hard_neg_lock:
                    hard_neg_entries.extend(local_hard_negs)
                    if hard_neg_max_crops and len(hard_neg_entries) > hard_neg_max_crops:
                        hard_neg_entries.sort(key=lambda e: float(e.get("score") or 0.0), reverse=True)
                        hard_neg_entries[:] = hard_neg_entries[: int(hard_neg_max_crops)]

    max_workers = max(1, len(workers))
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(_worker_loop, w) for w in workers]
        for fut in futures:
            fut.result()

    best_summary: Optional[Dict[str, Any]] = None
    best_key: Optional[Tuple[int, float, int, float, float, float, float]] = None
    for margin_thr in sweep_margins:
        for bg_margin_thr in sweep_bg_margins:
            for min_prob in sweep_min_probs:
                counts = agg.get(float(margin_thr), {}).get(float(bg_margin_thr), {}).get(float(min_prob)) or {}
                matched = int(counts.get("matches") or 0)
                fps = int(counts.get("fps") or 0)
                duplicates = int(counts.get("duplicates") or 0)
                preds = int(counts.get("preds") or 0)
                det_images = int(counts.get("det_images") or 0)
                best_summary, best_key = _update_best_clip_head_sweep_summary(
                    best_summary=best_summary,
                    best_key=best_key,
                    total_gt=int(total_gt),
                    total_images=int(total_images),
                    matched=int(matched),
                    fps=int(fps),
                    duplicates=int(duplicates),
                    preds=int(preds),
                    det_images=int(det_images),
                    min_prob=float(min_prob),
                    margin=float(margin_thr),
                    bg_margin=float(bg_margin_thr),
                    target_precision=float(target_precision),
                    debug=debug,
                )
    if best_summary is None:
        best_summary = {
            "gts": total_gt,
            "matches": 0,
            "fps": 0,
            "duplicates": 0,
            "preds": 0,
            "precision": 0.0,
            "recall": 0.0,
            "coverage_rate": 0.0,
            "det_rate": 0.0,
            "clip_head_min_prob": float(payload.clip_head_min_prob),
            "clip_head_margin": float(payload.clip_head_margin),
            "clip_head_background_margin": float(bg_margin),
            "clip_head_target_precision": float(target_precision),
            "clip_head_meets_target_precision": False,
            "debug": debug,
        }
    if step_flow_totals:
        best_summary["similarity_flow"] = {
            "images": int(total_images),
            "steps": step_flow_totals,
        }
        try:
            seed_bg_checked = sum(int(entry.get("seed_bg_checked") or 0) for entry in step_flow_totals)
            seed_bg_veto = sum(int(entry.get("seed_bg_veto") or 0) for entry in step_flow_totals)
            final_bg_checked = sum(int(entry.get("final_bg_checked") or 0) for entry in step_flow_totals)
            final_bg_veto = sum(int(entry.get("final_bg_veto") or 0) for entry in step_flow_totals)
            best_summary["bg_veto_seed"] = int(seed_bg_veto)
            best_summary["bg_veto_final"] = int(final_bg_veto)
            best_summary["bg_veto_rate_seed"] = float(seed_bg_veto) / float(seed_bg_checked) if seed_bg_checked else 0.0
            best_summary["bg_veto_rate_final"] = float(final_bg_veto) / float(final_bg_checked) if final_bg_checked else 0.0
        except Exception:
            pass
        if log_fn and total_images > 0:
            try:
                for entry in step_flow_totals:
                    prompt = str(entry.get("prompt") or "").strip()
                    if not prompt:
                        continue
                    text_total = int(entry.get("text_candidates_total") or 0)
                    kept_total = int(entry.get("candidates_kept") or 0)
                    expand_total = int(entry.get("expand_candidates") or 0)
                    expanded_total = int(entry.get("expanded_total") or 0)
                    final_seed_total = int(entry.get("final_seed_total") or 0)
                    final_expanded_total = int(entry.get("final_expanded_total") or 0)
                    final_total = int(entry.get("final_total") or 0)
                    seed_bg_checked = int(entry.get("seed_bg_checked") or 0)
                    seed_bg_veto = int(entry.get("seed_bg_veto") or 0)
                    final_bg_checked = int(entry.get("final_bg_checked") or 0)
                    final_bg_veto = int(entry.get("final_bg_veto") or 0)
                    bg_bits = ""
                    if seed_bg_checked > 0 or final_bg_checked > 0:
                        bg_bits = (
                            f" bg_seed={seed_bg_veto}/{seed_bg_checked} "
                            f"bg_final={final_bg_veto}/{final_bg_checked}"
                        )
                    no_text = " (no text candidates)" if text_total == 0 else ""
                    log_fn(
                        "[steps] Similarity flow "
                        f"'{prompt}': text={text_total} kept={kept_total} "
                        f"expand={expand_total} expanded={expanded_total} "
                        f"final_seed={final_seed_total} final_expanded={final_expanded_total} final={final_total} "
                        f"(avg/img text={text_total/total_images:.2f} "
                        f"kept={kept_total/total_images:.2f} "
                        f"expand={expand_total/total_images:.2f} "
                        f"expanded={expanded_total/total_images:.2f} "
                        f"final_seed={final_seed_total/total_images:.2f} "
                        f"final_expanded={final_expanded_total/total_images:.2f} "
                        f"final={final_total/total_images:.2f})"
                        + bg_bits
                        + no_text
                    )
            except Exception:
                pass
    if hard_neg_enabled:
        export_info = _export_hard_negative_replay(
            dataset_id=str(payload.dataset_id),
            class_id=int(cat_id),
            class_name=str(class_name or f"class_{cat_id}"),
            entries=hard_neg_entries,
            max_crops=int(hard_neg_max_crops),
            log_fn=log_fn,
        )
        if isinstance(export_info, dict):
            best_summary["hard_negative_export"] = export_info
        else:
            best_summary["hard_negative_export"] = {
                "enabled": True,
                "count": 0,
                "max_crops": int(hard_neg_max_crops),
            }
    elif export_hard_negatives and isinstance(best_summary, dict):
        best_summary["hard_negative_export"] = {
            "enabled": False,
            "count": 0,
            "max_crops": int(hard_neg_max_crops),
        }
    try:
        best_summary["clip_head_background_guard"] = bool(getattr(payload, "clip_head_background_guard", False))
        if best_summary.get("clip_head_background_margin") is None:
            best_summary["clip_head_background_margin"] = float(
                getattr(payload, "clip_head_background_margin", 0.0) or 0.0
            )
        best_summary["clip_head_background_apply"] = str(getattr(payload, "clip_head_background_apply", "final") or "final")
        best_summary["clip_head_background_penalty"] = float(getattr(payload, "clip_head_background_penalty", 0.0) or 0.0)
    except Exception:
        pass
    return best_summary


def _tune_steps_tier1_knobs_image_first(
    *,
    cat_id: int,
    steps: Sequence[Dict[str, Any]],
    val_ids: Sequence[int],
    images: Dict[int, Dict[str, Any]],
    gt_by_image_cat: Dict[int, Dict[int, List[List[float]]]],
    payload: "AgentMiningRequest",
    clip_head: Dict[str, Any],
    clip_head_target_index: int,
    workers: Sequence["_Sam3GreedyEvalWorker"],
    log_fn: Optional[Callable[[str], None]] = None,
    cancel_event: Optional[threading.Event] = None,
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Bounded Tier-1 grid search for steps-mode mining.

    Tunes a small grid over:
      - visual expansion score (expand_threshold)
      - max_visual_seeds (per step)

    Candidates are ranked via the steps CLIP-head sweep evaluator (precision/coverage/FP tradeoff),
    using small subsets of the validation split.
    """

    def _log(msg: str) -> None:
        if log_fn:
            try:
                log_fn(msg)
            except Exception:
                pass

    steps_norm = _normalize_steps_for_head_tuning(steps, payload=payload)
    if not steps_norm:
        return [], {"enabled": False, "reason": "no_steps"}

    try:
        enabled = bool(getattr(payload, "steps_optimize_tier1", False))
    except Exception:
        enabled = False
    if not enabled:
        return list(steps), {"enabled": False, "reason": "disabled"}

    total_val = len(val_ids)
    if total_val <= 0:
        return list(steps), {"enabled": False, "reason": "no_val_images"}

    try:
        eval_cap = int(getattr(payload, "steps_optimize_tier1_eval_cap", 200) or 0)
    except Exception:
        eval_cap = 200
    eval_cap = max(1, min(int(eval_cap), int(total_val)))

    try:
        max_trials = int(getattr(payload, "steps_optimize_tier1_max_trials", 9) or 0)
    except Exception:
        max_trials = 9
    max_trials = max(1, int(max_trials))

    try:
        base_expand = float(steps_norm[0].get("expand_threshold", payload.expand_threshold))
    except Exception:
        base_expand = float(payload.expand_threshold)
    base_expand = max(0.0, min(1.0, float(base_expand)))
    try:
        base_max_seeds = int(
            steps_norm[0].get("max_visual_seeds", getattr(payload, "steps_max_visual_seeds_per_step", 0))
        )
    except Exception:
        base_max_seeds = int(getattr(payload, "steps_max_visual_seeds_per_step", 0) or 0)
    base_max_seeds = max(0, int(base_max_seeds))

    expand_raw = [base_expand + d for d in (-0.2, -0.1, -0.05, 0.0, 0.05, 0.1, 0.2)]
    expand_vals = sorted({float(max(0.0, min(1.0, round(v, 6)))) for v in expand_raw})
    if float(base_expand) not in expand_vals:
        expand_vals.append(float(base_expand))
        expand_vals = sorted(set(expand_vals))

    seeds_raw = [base_max_seeds + d for d in (-8, -4, -2, 0, 2, 4, 8)]
    seeds_vals = sorted({max(0, min(500, int(v))) for v in seeds_raw})
    if int(base_max_seeds) not in seeds_vals:
        seeds_vals.append(int(base_max_seeds))
        seeds_vals = sorted(set(seeds_vals))

    def _select_near_base(vals: Sequence[Any], base: Any, n: int) -> List[Any]:
        if n <= 0:
            return []
        vals_list = list(vals)
        if not vals_list:
            return []
        if n >= len(vals_list):
            return vals_list
        try:
            base_idx = vals_list.index(base)
        except Exception:
            base_idx = len(vals_list) // 2
        picked: List[int] = [base_idx]
        left = base_idx - 1
        right = base_idx + 1
        while len(picked) < n and (left >= 0 or right < len(vals_list)):
            if left >= 0:
                picked.append(left)
                left -= 1
                if len(picked) >= n:
                    break
            if right < len(vals_list):
                picked.append(right)
                right += 1
        picked_sorted = sorted(set(picked))[:n]
        return [vals_list[i] for i in picked_sorted]

    n_expand = max(1, int(round(math.sqrt(max_trials))))
    n_expand = min(n_expand, len(expand_vals))
    n_seeds = max(1, max_trials // n_expand)
    n_seeds = min(n_seeds, len(seeds_vals))
    while n_expand * n_seeds > max_trials and n_seeds > 1:
        n_seeds -= 1

    expand_pick = _select_near_base(expand_vals, float(base_expand), n_expand)
    seeds_pick = _select_near_base(seeds_vals, int(base_max_seeds), n_seeds)

    candidates: List[Dict[str, Any]] = [
        {"expand_threshold": float(e), "max_visual_seeds": int(s)} for e in expand_pick for s in seeds_pick
    ]
    candidates.sort(
        key=lambda c: (
            0
            if (
                float(c.get("expand_threshold")) == float(base_expand)
                and int(c.get("max_visual_seeds")) == int(base_max_seeds)
            )
            else 1,
            float(c.get("expand_threshold")),
            int(c.get("max_visual_seeds")),
        )
    )
    candidates = candidates[: max(1, int(max_trials))]

    stage1 = max(1, int(eval_cap // 4))
    budgets = [int(eval_cap)] if stage1 >= eval_cap else [int(stage1), int(eval_cap)]

    _, _, _, target_precision = _build_clip_head_sweep_grid(
        payload,
        base_min_prob=float(payload.clip_head_min_prob),
        base_margin=float(payload.clip_head_margin),
        base_bg_margin=float(getattr(payload, "clip_head_background_margin", 0.0) or 0.0),
        allow_bg_tune=False,
    )

    def _apply_candidate(candidate: Dict[str, Any]) -> List[Dict[str, Any]]:
        try:
            expand_thr = float(candidate.get("expand_threshold"))
        except Exception:
            expand_thr = float(base_expand)
        expand_thr = max(0.0, min(1.0, float(expand_thr)))
        try:
            max_seeds = int(candidate.get("max_visual_seeds"))
        except Exception:
            max_seeds = int(base_max_seeds)
        max_seeds = max(0, min(500, int(max_seeds)))
        out: List[Dict[str, Any]] = []
        for step in steps or []:
            if not isinstance(step, dict):
                continue
            s2 = dict(step)
            s2["expand_threshold"] = float(expand_thr)
            s2["max_visual_seeds"] = int(max_seeds)
            out.append(s2)
        return out

    def _eval_candidate(candidate: Dict[str, Any], budget: int) -> Tuple[Tuple[Any, ...], Dict[str, Any]]:
        if cancel_event is not None and cancel_event.is_set():
            raise RuntimeError("cancelled")
        steps_candidate = _apply_candidate(candidate)
        subset = val_ids[: max(1, min(int(budget), int(total_val)))]
        summary = _tune_clip_head_for_selected_steps_image_first(
            cat_id=cat_id,
            class_name=None,
            steps=steps_candidate,
            val_ids=subset,
            images=images,
            gt_by_image_cat=gt_by_image_cat,
            payload=payload,
            clip_head=clip_head,
            clip_head_target_index=int(clip_head_target_index),
            workers=workers,
            log_every=0,
            log_fn=None,
            cancel_event=cancel_event,
            progress_callback=None,
            export_hard_negatives=False,
        )
        try:
            matched = int(summary.get("matches") or 0)
        except Exception:
            matched = 0
        try:
            fps = int(summary.get("fps") or 0)
        except Exception:
            fps = 0
        try:
            precision = float(summary.get("precision") or 0.0)
        except Exception:
            precision = 0.0
        bg_penalty = 0.0
        try:
            bg_penalty = float(getattr(payload, "clip_head_background_penalty", 0.0) or 0.0)
        except Exception:
            bg_penalty = 0.0
        bg_rate = 0.0
        try:
            bg_rate = float(summary.get("bg_veto_rate_final") or 0.0)
        except Exception:
            bg_rate = 0.0
        precision_adj = float(precision)
        if bg_penalty > 0.0 and bg_rate > 0.0:
            precision_adj = max(0.0, float(precision) - float(bg_penalty) * float(bg_rate))
            try:
                summary["bg_penalty_applied"] = float(bg_penalty)
                summary["bg_penalty_precision"] = float(precision_adj)
            except Exception:
                pass
        try:
            min_prob = float(summary.get("clip_head_min_prob") or 0.0)
        except Exception:
            min_prob = 0.0
        try:
            margin = float(summary.get("clip_head_margin") or 0.0)
        except Exception:
            margin = 0.0
        try:
            bg_margin = float(summary.get("clip_head_background_margin") or 0.0)
        except Exception:
            bg_margin = 0.0
        key = _score_head_tuning_candidate(
            matched=matched,
            fps=fps,
            precision=precision_adj,
            min_prob=min_prob,
            margin=margin,
            bg_margin=bg_margin,
            target_precision=float(target_precision),
        )
        return key, {"candidate": dict(candidate), "summary": summary, "key": key, "budget": int(budget)}

    _log(
        f"[steps] Tier-1 grid search: trying up to {len(candidates)} configs (eval_cap={eval_cap}, budgets={budgets}) for class_id {cat_id} "
        f"(visual_thr≈{base_expand:.3f}, max_seeds≈{base_max_seeds})"
    )
    best_candidate, history = _successive_halving_search(
        candidates=candidates,
        budgets=budgets,
        evaluator=_eval_candidate,
        keep_ratio=0.5,
    )
    tuned_steps = _apply_candidate(best_candidate)
    _log(
        f"[steps] Tier-1 grid search: selected visual_thr={float(best_candidate.get('expand_threshold')):.3f} "
        f"max_seeds={int(best_candidate.get('max_visual_seeds'))} for class_id {cat_id}"
    )
    return tuned_steps, {
        "enabled": True,
        "base": {"expand_threshold": float(base_expand), "max_visual_seeds": int(base_max_seeds)},
        "selected": {
            "expand_threshold": float(best_candidate.get("expand_threshold")),
            "max_visual_seeds": int(best_candidate.get("max_visual_seeds")),
        },
        "eval_cap": int(eval_cap),
        "max_trials": int(max_trials),
        "history": history,
    }


def _build_steps_tier2_candidate_grid(
    *,
    base_seed_dedupe_iou: float,
    base_dedupe_iou: float,
    max_trials: int,
) -> List[Dict[str, Any]]:
    """
    Build a bounded candidate grid for Tier-2 steps-mode tuning.

    Tier-2 currently tunes dedupe IoUs (global-per-class, applied to all steps):
      - seed_dedupe_iou
      - dedupe_iou
    """
    try:
        base_seed = float(base_seed_dedupe_iou)
    except Exception:
        base_seed = 0.9
    try:
        base_out = float(base_dedupe_iou)
    except Exception:
        base_out = 0.5
    base_seed = max(0.0, min(1.0, float(base_seed)))
    base_out = max(0.0, min(1.0, float(base_out)))

    try:
        max_trials_i = max(1, int(max_trials))
    except Exception:
        max_trials_i = 12

    seed_raw = [base_seed + d for d in (-0.3, -0.2, -0.1, -0.05, 0.0, 0.05, 0.1, 0.2)]
    out_raw = [base_out + d for d in (-0.3, -0.2, -0.1, -0.05, 0.0, 0.05, 0.1, 0.2, 0.3)]
    seed_vals = sorted({float(max(0.0, min(1.0, round(v, 6)))) for v in seed_raw})
    out_vals = sorted({float(max(0.0, min(1.0, round(v, 6)))) for v in out_raw})
    if float(base_seed) not in seed_vals:
        seed_vals.append(float(base_seed))
        seed_vals = sorted(set(seed_vals))
    if float(base_out) not in out_vals:
        out_vals.append(float(base_out))
        out_vals = sorted(set(out_vals))

    def _select_near_base(vals: Sequence[float], base: float, n: int) -> List[float]:
        if n <= 0:
            return []
        vals_list = list(vals)
        if not vals_list:
            return []
        if n >= len(vals_list):
            return vals_list
        try:
            base_idx = vals_list.index(float(base))
        except Exception:
            base_idx = len(vals_list) // 2
        picked: List[int] = [base_idx]
        left = base_idx - 1
        right = base_idx + 1
        while len(picked) < n and (left >= 0 or right < len(vals_list)):
            if left >= 0:
                picked.append(left)
                left -= 1
                if len(picked) >= n:
                    break
            if right < len(vals_list):
                picked.append(right)
                right += 1
        picked_sorted = sorted(set(picked))[:n]
        return [vals_list[i] for i in picked_sorted]

    n_seed = max(1, int(round(math.sqrt(max_trials_i))))
    n_seed = min(n_seed, len(seed_vals))
    n_out = max(1, max_trials_i // n_seed)
    n_out = min(n_out, len(out_vals))
    while n_seed * n_out > max_trials_i and n_out > 1:
        n_out -= 1
    while n_seed * n_out > max_trials_i and n_seed > 1:
        n_seed -= 1

    seed_sel = _select_near_base(seed_vals, float(base_seed), n_seed)
    out_sel = _select_near_base(out_vals, float(base_out), n_out)

    candidates: List[Dict[str, Any]] = []
    for s in seed_sel:
        for o in out_sel:
            candidates.append({"seed_dedupe_iou": float(s), "dedupe_iou": float(o)})
    candidates = sorted(
        candidates,
        key=lambda c: (
            0
            if (
                float(c.get("seed_dedupe_iou")) == float(base_seed)
                and float(c.get("dedupe_iou")) == float(base_out)
            )
            else 1,
            float(c.get("seed_dedupe_iou")),
            float(c.get("dedupe_iou")),
        ),
    )
    return candidates[:max_trials_i]


def _tune_steps_tier2_knobs_image_first(
    *,
    cat_id: int,
    steps: Sequence[Dict[str, Any]],
    val_ids: Sequence[int],
    images: Dict[int, Dict[str, Any]],
    gt_by_image_cat: Dict[int, Dict[int, List[List[float]]]],
    payload: "AgentMiningRequest",
    clip_head: Dict[str, Any],
    clip_head_target_index: int,
    workers: Sequence["_Sam3GreedyEvalWorker"],
    log_fn: Optional[Callable[[str], None]] = None,
    cancel_event: Optional[threading.Event] = None,
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Bounded Tier-2 tuning for steps-mode mining.

    Tunes a small grid over:
      - seed_dedupe_iou
      - dedupe_iou

    Candidates are ranked via the steps CLIP-head sweep evaluator (precision/coverage/FP tradeoff),
    using capped subsets of the validation split.
    """

    def _log(msg: str) -> None:
        if log_fn:
            try:
                log_fn(msg)
            except Exception:
                pass

    steps_norm = _normalize_steps_for_head_tuning(steps, payload=payload)
    if not steps_norm:
        return [], {"enabled": False, "reason": "no_steps"}

    try:
        enabled = bool(getattr(payload, "steps_optimize_tier2", False))
    except Exception:
        enabled = False
    if not enabled:
        return list(steps), {"enabled": False, "reason": "disabled"}

    total_val = len(val_ids)
    if total_val <= 0:
        return list(steps), {"enabled": False, "reason": "no_val_images"}

    try:
        eval_cap = int(getattr(payload, "steps_optimize_tier2_eval_cap", 200) or 0)
    except Exception:
        eval_cap = 200
    eval_cap = max(1, min(int(eval_cap), int(total_val)))

    try:
        max_trials = int(getattr(payload, "steps_optimize_tier2_max_trials", 12) or 0)
    except Exception:
        max_trials = 12
    max_trials = max(1, int(max_trials))

    try:
        base_seed_iou = float(steps_norm[0].get("seed_dedupe_iou", payload.seed_dedupe_iou))
    except Exception:
        base_seed_iou = float(payload.seed_dedupe_iou)
    base_seed_iou = max(0.0, min(1.0, float(base_seed_iou)))
    try:
        base_out_iou = float(steps_norm[0].get("dedupe_iou", payload.dedupe_iou))
    except Exception:
        base_out_iou = float(payload.dedupe_iou)
    base_out_iou = max(0.0, min(1.0, float(base_out_iou)))

    candidates = _build_steps_tier2_candidate_grid(
        base_seed_dedupe_iou=float(base_seed_iou),
        base_dedupe_iou=float(base_out_iou),
        max_trials=int(max_trials),
    )
    if not candidates:
        return list(steps), {"enabled": False, "reason": "no_candidates"}

    stage1 = max(1, int(eval_cap // 4))
    budgets = [int(eval_cap)] if stage1 >= eval_cap else [int(stage1), int(eval_cap)]

    _, _, _, target_precision = _build_clip_head_sweep_grid(
        payload,
        base_min_prob=float(payload.clip_head_min_prob),
        base_margin=float(payload.clip_head_margin),
        base_bg_margin=float(getattr(payload, "clip_head_background_margin", 0.0) or 0.0),
        allow_bg_tune=False,
    )

    def _apply_candidate(candidate: Dict[str, Any]) -> List[Dict[str, Any]]:
        try:
            seed_iou = float(candidate.get("seed_dedupe_iou"))
        except Exception:
            seed_iou = float(base_seed_iou)
        seed_iou = max(0.0, min(1.0, float(seed_iou)))
        try:
            out_iou = float(candidate.get("dedupe_iou"))
        except Exception:
            out_iou = float(base_out_iou)
        out_iou = max(0.0, min(1.0, float(out_iou)))

        out_steps: List[Dict[str, Any]] = []
        for step in steps or []:
            if not isinstance(step, dict):
                continue
            s2 = dict(step)
            s2["seed_dedupe_iou"] = float(seed_iou)
            s2["dedupe_iou"] = float(out_iou)
            out_steps.append(s2)
        return out_steps

    def _eval_candidate(candidate: Dict[str, Any], budget: int) -> Tuple[Tuple[Any, ...], Dict[str, Any]]:
        if cancel_event is not None and cancel_event.is_set():
            raise RuntimeError("cancelled")
        steps_candidate = _apply_candidate(candidate)
        subset = val_ids[: max(1, min(int(budget), int(total_val)))]
        summary = _tune_clip_head_for_selected_steps_image_first(
            cat_id=cat_id,
            class_name=None,
            steps=steps_candidate,
            val_ids=subset,
            images=images,
            gt_by_image_cat=gt_by_image_cat,
            payload=payload,
            clip_head=clip_head,
            clip_head_target_index=int(clip_head_target_index),
            workers=workers,
            log_every=0,
            log_fn=None,
            cancel_event=cancel_event,
            progress_callback=None,
            export_hard_negatives=False,
        )
        try:
            matched = int(summary.get("matches") or 0)
        except Exception:
            matched = 0
        try:
            fps = int(summary.get("fps") or 0)
        except Exception:
            fps = 0
        try:
            precision = float(summary.get("precision") or 0.0)
        except Exception:
            precision = 0.0
        try:
            min_prob = float(summary.get("clip_head_min_prob") or 0.0)
        except Exception:
            min_prob = 0.0
        try:
            margin = float(summary.get("clip_head_margin") or 0.0)
        except Exception:
            margin = 0.0
        try:
            bg_margin = float(summary.get("clip_head_background_margin") or 0.0)
        except Exception:
            bg_margin = 0.0
        key = _score_head_tuning_candidate(
            matched=matched,
            fps=fps,
            precision=precision,
            min_prob=min_prob,
            margin=margin,
            bg_margin=bg_margin,
            target_precision=float(target_precision),
        )
        return key, {"candidate": dict(candidate), "summary": summary, "key": key, "budget": int(budget)}

    _log(
        f"[steps] Tier-2 tuning: trying up to {len(candidates)} configs (eval_cap={eval_cap}, budgets={budgets}) for class_id {cat_id} "
        f"(seed_iou≈{base_seed_iou:.3f}, out_iou≈{base_out_iou:.3f})"
    )
    best_candidate, history = _successive_halving_search(
        candidates=candidates,
        budgets=budgets,
        evaluator=_eval_candidate,
        keep_ratio=0.5,
    )
    tuned_steps = _apply_candidate(best_candidate)
    _log(
        f"[steps] Tier-2 tuning: selected seed_iou={float(best_candidate.get('seed_dedupe_iou')):.3f} "
        f"out_iou={float(best_candidate.get('dedupe_iou')):.3f} for class_id {cat_id}"
    )
    return tuned_steps, {
        "enabled": True,
        "base": {"seed_dedupe_iou": float(base_seed_iou), "dedupe_iou": float(base_out_iou)},
        "selected": {
            "seed_dedupe_iou": float(best_candidate.get("seed_dedupe_iou")),
            "dedupe_iou": float(best_candidate.get("dedupe_iou")),
        },
        "eval_cap": int(eval_cap),
        "max_trials": int(max_trials),
        "history": history,
    }


def _stable_sample_ids(
    ids: Sequence[int],
    *,
    cap: int,
    seed: int,
    salt: str,
) -> List[int]:
    """Deterministically sample up to cap ids, independent of thread scheduling."""
    try:
        cap_i = int(cap)
    except Exception:
        cap_i = 0
    if cap_i <= 0:
        return []
    uniq = sorted({int(i) for i in ids})
    if not uniq:
        return []
    if cap_i >= len(uniq):
        return uniq

    scored: List[Tuple[bytes, int]] = []
    for i in uniq:
        h = hashlib.sha256(f"{salt}:{seed}:{i}".encode("utf-8")).digest()
        scored.append((h, int(i)))
    scored.sort(key=lambda t: (t[0], t[1]))
    return [i for _h, i in scored[:cap_i]]


def _clamp01(val: Any, default: float) -> float:
    try:
        f = float(val)
    except Exception:
        f = float(default)
    return float(max(0.0, min(1.0, f)))


def _clamp_int(val: Any, default: int, *, lo: int, hi: int) -> int:
    try:
        i = int(val)
    except Exception:
        i = int(default)
    return int(max(int(lo), min(int(hi), i)))


def _steps_candidate_signature(steps: Sequence[Dict[str, Any]]) -> Tuple[Tuple[Any, ...], ...]:
    """Canonical signature for deduping step candidates (order-sensitive)."""
    out: List[Tuple[Any, ...]] = []
    for step in steps or []:
        if not isinstance(step, dict):
            continue
        if not bool(step.get("enabled", True)):
            continue
        prompt = str(step.get("prompt") or "").strip()
        if not prompt:
            continue
        sig = (
            prompt,
            float(round(_clamp01(step.get("seed_threshold"), 0.05), 6)),
            float(round(_clamp01(step.get("expand_threshold"), 0.3), 6)),
            int(_clamp_int(step.get("max_visual_seeds"), 5, lo=0, hi=500)),
            float(round(_clamp01(step.get("seed_dedupe_iou"), 0.9), 6)),
            float(round(_clamp01(step.get("dedupe_iou"), 0.5), 6)),
            int(_clamp_int(step.get("max_results"), 1000, lo=1, hi=5000)),
        )
        out.append(sig)
    return tuple(out)


def _seed_threshold_candidates_for_prompt_stat(
    stat: Dict[str, Any],
    *,
    max_candidates: int,
    target_precision: Optional[float],
    fallback_seed_threshold: float,
) -> List[float]:
    curve = stat.get("seed_threshold_curve") if isinstance(stat.get("seed_threshold_curve"), list) else []
    points = _select_seed_threshold_candidate_points(
        curve,
        max_candidates=max(1, int(max_candidates)),
        target_precision=target_precision,
    )
    vals: List[float] = []
    for p in points:
        if not isinstance(p, dict):
            continue
        if p.get("threshold") is None:
            continue
        try:
            vals.append(float(p.get("threshold")))
        except Exception:
            continue
    # Always include the recommended/base thresholds when present.
    for k in ("seed_threshold_recommended", "seed_threshold_base", "selected_seed_threshold"):
        if stat.get(k) is None:
            continue
        try:
            vals.append(float(stat.get(k)))
        except Exception:
            continue
    if not vals:
        vals = [float(fallback_seed_threshold)]
    vals = sorted({float(round(_clamp01(v, fallback_seed_threshold), 6)) for v in vals})
    return vals


def _default_step_for_prompt(
    *,
    prompt: str,
    prompt_stat: Optional[Dict[str, Any]],
    payload: "AgentMiningRequest",
) -> Dict[str, Any]:
    base_seed = float(getattr(payload, "seed_threshold", 0.05) or 0.05)
    seed_thr = None
    if isinstance(prompt_stat, dict):
        for k in ("selected_seed_threshold", "seed_threshold_recommended", "seed_threshold_base"):
            if prompt_stat.get(k) is None:
                continue
            try:
                seed_thr = float(prompt_stat.get(k))
                break
            except Exception:
                continue
    seed_thr = float(_clamp01(seed_thr, base_seed))
    return {
        "enabled": True,
        "prompt": str(prompt),
        "seed_threshold": float(seed_thr),
        "expand_threshold": float(_clamp01(getattr(payload, "expand_threshold", 0.3), 0.3)),
        "max_visual_seeds": int(_clamp_int(getattr(payload, "steps_max_visual_seeds_per_step", 5), 5, lo=0, hi=500)),
        "seed_dedupe_iou": float(_clamp01(getattr(payload, "seed_dedupe_iou", 0.9), 0.9)),
        "dedupe_iou": float(_clamp01(getattr(payload, "dedupe_iou", 0.5), 0.5)),
        "max_results": int(_clamp_int(getattr(payload, "max_results", 1000), 1000, lo=1, hi=5000)),
    }


def _normalize_steps_candidate_steps(
    steps: Sequence[Dict[str, Any]],
    *,
    payload: "AgentMiningRequest",
) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    seen: set[str] = set()
    for step in steps or []:
        if not isinstance(step, dict):
            continue
        if not bool(step.get("enabled", True)):
            continue
        prompt = str(step.get("prompt") or "").strip()
        if not prompt:
            continue
        key = prompt.lower()
        if key in seen:
            continue
        seen.add(key)
        out.append(
            {
                "enabled": True,
                "prompt": prompt,
                "seed_threshold": float(_clamp01(step.get("seed_threshold"), getattr(payload, "seed_threshold", 0.05))),
                "expand_threshold": float(_clamp01(step.get("expand_threshold"), getattr(payload, "expand_threshold", 0.3))),
                "max_visual_seeds": int(
                    _clamp_int(
                        step.get("max_visual_seeds"),
                        int(getattr(payload, "steps_max_visual_seeds_per_step", 5) or 0),
                        lo=0,
                        hi=500,
                    )
                ),
                "seed_dedupe_iou": float(_clamp01(step.get("seed_dedupe_iou"), getattr(payload, "seed_dedupe_iou", 0.9))),
                "dedupe_iou": float(_clamp01(step.get("dedupe_iou"), getattr(payload, "dedupe_iou", 0.5))),
                "max_results": int(_clamp_int(step.get("max_results"), int(getattr(payload, "max_results", 1000) or 1000), lo=1, hi=5000)),
            }
        )
    return out


def _build_steps_global_prompt_pool(
    seed_stats: Sequence[Dict[str, Any]],
    *,
    max_pool: int,
) -> List[str]:
    """
    Deterministically pick a bounded prompt pool for the global optimizer.

    We prioritize high-coverage prompts from seed-stage stats.
    """
    rows: List[Tuple[int, float, int, str]] = []
    for stat in seed_stats or []:
        if not isinstance(stat, dict):
            continue
        prompt = str(stat.get("prompt") or "").strip()
        if not prompt:
            continue
        try:
            matches = int(stat.get("matches") or 0)
        except Exception:
            matches = 0
        try:
            precision = float(stat.get("precision") or 0.0)
        except Exception:
            precision = 0.0
        try:
            fps = int(stat.get("fps") or 0)
        except Exception:
            fps = 0
        rows.append((int(matches), float(precision), int(fps), prompt))
    rows.sort(key=lambda r: (-r[0], -r[1], r[2], r[3]))
    prompts = [p for *_rest, p in rows]
    uniq: List[str] = []
    seen: set[str] = set()
    for p in prompts:
        k = p.lower()
        if k in seen:
            continue
        seen.add(k)
        uniq.append(p)
        if len(uniq) >= max(1, int(max_pool)):
            break
    return uniq


def _generate_steps_global_mutations(
    *,
    base_candidate: Dict[str, Any],
    seed_stats: Sequence[Dict[str, Any]],
    payload: "AgentMiningRequest",
    max_mutations: int,
    target_precision: Optional[float],
    enable_max_results: bool,
    enable_ordering: bool,
) -> List[Dict[str, Any]]:
    """
    Deterministically generate a bounded neighborhood of candidates around base_candidate.

    This is intentionally conservative and order-stable so the optimizer is reproducible.
    """
    base_steps = _normalize_steps_candidate_steps(base_candidate.get("steps") or [], payload=payload)
    if not base_steps:
        return []

    try:
        max_steps = int(getattr(payload, "steps_max_steps_per_recipe", 6) or 6)
    except Exception:
        max_steps = 6
    max_steps = max(1, min(50, int(max_steps)))

    prompt_pool = _build_steps_global_prompt_pool(seed_stats, max_pool=max(12, max_steps * 6))
    stats_by_prompt = {str(s.get("prompt") or "").strip().lower(): s for s in seed_stats if isinstance(s, dict)}

    existing = {str(s.get("prompt") or "").strip().lower() for s in base_steps}

    base_seed = float(getattr(payload, "seed_threshold", 0.05) or 0.05)
    candidates: List[Dict[str, Any]] = []
    seen_sig: set[Tuple[Tuple[Any, ...], ...]] = set()

    def _add(steps: List[Dict[str, Any]], mutation: Dict[str, Any]) -> None:
        nonlocal candidates
        if len(candidates) >= int(max_mutations):
            return
        norm = _normalize_steps_candidate_steps(steps, payload=payload)
        if not norm:
            return
        sig = _steps_candidate_signature(norm)
        if sig in seen_sig:
            return
        seen_sig.add(sig)
        candidates.append({"steps": norm, "mutation": mutation, "sig": sig})

    # 1) Ordering moves (swap adjacent).
    if enable_ordering and len(base_steps) > 1:
        for i in range(len(base_steps) - 1):
            swapped = list(base_steps)
            swapped[i], swapped[i + 1] = swapped[i + 1], swapped[i]
            _add(swapped, {"op": "swap_order", "i": int(i)})

    # 2) Drop moves.
    if len(base_steps) > 1:
        for i, step in enumerate(base_steps):
            dropped = [s for j, s in enumerate(base_steps) if j != i]
            _add(dropped, {"op": "drop_step", "prompt": str(step.get("prompt") or ""), "i": int(i)})

    # 3) Add moves (append).
    if len(base_steps) < max_steps:
        for p in prompt_pool:
            k = p.lower()
            if k in existing:
                continue
            stat = stats_by_prompt.get(k)
            added = list(base_steps) + [_default_step_for_prompt(prompt=p, prompt_stat=stat, payload=payload)]
            _add(added, {"op": "add_step", "prompt": p})

    # 4) Swap prompt moves.
    for i, step in enumerate(base_steps):
        for p in prompt_pool:
            k = p.lower()
            if k in existing:
                continue
            stat = stats_by_prompt.get(k)
            swapped = list(base_steps)
            swapped[i] = _default_step_for_prompt(prompt=p, prompt_stat=stat, payload=payload)
            _add(swapped, {"op": "swap_prompt", "i": int(i), "dropped": str(step.get("prompt") or ""), "added": p})

    # 5) Per-step seed-threshold moves (from curve candidates).
    for i, step in enumerate(base_steps):
        p = str(step.get("prompt") or "").strip()
        if not p:
            continue
        stat = stats_by_prompt.get(p.lower())
        if not isinstance(stat, dict):
            continue
        thr_candidates = _seed_threshold_candidates_for_prompt_stat(
            stat,
            max_candidates=6,
            target_precision=target_precision,
            fallback_seed_threshold=base_seed,
        )
        cur_thr = float(_clamp01(step.get("seed_threshold"), base_seed))
        for thr in thr_candidates:
            thr_f = float(_clamp01(thr, base_seed))
            if abs(thr_f - cur_thr) < 1e-9:
                continue
            mutated = list(base_steps)
            s2 = dict(mutated[i])
            s2["seed_threshold"] = float(thr_f)
            mutated[i] = s2
            _add(mutated, {"op": "seed_threshold", "i": int(i), "prompt": p, "seed_threshold": float(thr_f)})

    # 6) Per-step visual expansion score moves (small grid around current).
    for i, step in enumerate(base_steps):
        cur = float(_clamp01(step.get("expand_threshold"), getattr(payload, "expand_threshold", 0.3)))
        for d in (-0.2, -0.1, -0.05, 0.05, 0.1, 0.2):
            nxt = float(_clamp01(cur + float(d), cur))
            if abs(nxt - cur) < 1e-9:
                continue
            mutated = list(base_steps)
            s2 = dict(mutated[i])
            s2["expand_threshold"] = float(nxt)
            mutated[i] = s2
            _add(mutated, {"op": "expand_threshold", "i": int(i), "prompt": str(step.get("prompt") or ""), "expand_threshold": float(nxt)})

    # 7) Per-step max_visual_seeds moves.
    for i, step in enumerate(base_steps):
        cur = int(_clamp_int(step.get("max_visual_seeds"), int(getattr(payload, "steps_max_visual_seeds_per_step", 5) or 0), lo=0, hi=500))
        for d in (-8, -4, -2, 2, 4, 8):
            nxt = int(_clamp_int(cur + int(d), cur, lo=0, hi=500))
            if nxt == cur:
                continue
            mutated = list(base_steps)
            s2 = dict(mutated[i])
            s2["max_visual_seeds"] = int(nxt)
            mutated[i] = s2
            _add(mutated, {"op": "max_visual_seeds", "i": int(i), "prompt": str(step.get("prompt") or ""), "max_visual_seeds": int(nxt)})

    # 8) Per-step IoU moves.
    for i, step in enumerate(base_steps):
        seed_iou = float(_clamp01(step.get("seed_dedupe_iou"), getattr(payload, "seed_dedupe_iou", 0.9)))
        out_iou = float(_clamp01(step.get("dedupe_iou"), getattr(payload, "dedupe_iou", 0.5)))
        for d in (-0.2, -0.1, -0.05, 0.05, 0.1, 0.2):
            nxt = float(_clamp01(seed_iou + float(d), seed_iou))
            if abs(nxt - seed_iou) >= 1e-9:
                mutated = list(base_steps)
                s2 = dict(mutated[i])
                s2["seed_dedupe_iou"] = float(nxt)
                mutated[i] = s2
                _add(mutated, {"op": "seed_dedupe_iou", "i": int(i), "prompt": str(step.get("prompt") or ""), "seed_dedupe_iou": float(nxt)})
        for d in (-0.2, -0.1, -0.05, 0.05, 0.1, 0.2, 0.3):
            nxt = float(_clamp01(out_iou + float(d), out_iou))
            if abs(nxt - out_iou) < 1e-9:
                continue
            mutated = list(base_steps)
            s2 = dict(mutated[i])
            s2["dedupe_iou"] = float(nxt)
            mutated[i] = s2
            _add(mutated, {"op": "dedupe_iou", "i": int(i), "prompt": str(step.get("prompt") or ""), "dedupe_iou": float(nxt)})

    # 9) Per-step max_results moves (optional).
    if enable_max_results:
        for i, step in enumerate(base_steps):
            cur = int(_clamp_int(step.get("max_results"), int(getattr(payload, "max_results", 1000) or 1000), lo=1, hi=5000))
            grid = [10, 25, 50, 100, 200, 500, 1000, 2000]
            for nxt in grid:
                nxt_i = int(_clamp_int(nxt, cur, lo=1, hi=5000))
                if nxt_i == cur:
                    continue
                mutated = list(base_steps)
                s2 = dict(mutated[i])
                s2["max_results"] = int(nxt_i)
                mutated[i] = s2
                _add(mutated, {"op": "max_results", "i": int(i), "prompt": str(step.get("prompt") or ""), "max_results": int(nxt_i)})

    return candidates[: max(1, int(max_mutations))]


def _run_steps_global_successive_halving_rounds(
    *,
    base_candidate: Any,
    budgets: Sequence[int],
    keep_ratio: float,
    rounds: int,
    max_trials: int,
    mutate: Callable[[Any, int], Sequence[Any]],
    evaluator: Callable[[Any, int], Tuple[Tuple[Any, ...], Any]],
    log_fn: Optional[Callable[[str], None]] = None,
    log_prefix: str = "",
) -> Tuple[Any, List[Dict[str, Any]]]:
    """
    Pure controller logic for the global optimizer (testable without GPUs).

    Repeats successive-halving over a bounded neighborhood for multiple rounds, carrying forward
    the best candidate each time.
    """
    try:
        rounds_i = max(1, int(rounds))
    except Exception:
        rounds_i = 1
    try:
        max_trials_i = max(1, int(max_trials))
    except Exception:
        max_trials_i = 1
    try:
        keep_f = float(keep_ratio)
    except Exception:
        keep_f = 0.5

    best = base_candidate
    history: List[Dict[str, Any]] = []
    for r in range(int(rounds_i)):
        neighborhood = list(mutate(best, int(r)) or [])
        candidates = [best] + neighborhood
        candidates = candidates[: max_trials_i]
        if log_fn:
            try:
                prefix = f"{log_prefix} " if log_prefix else ""
                log_fn(
                    f"{prefix}Mutation round {r + 1}/{rounds_i}: {len(candidates)} candidate(s), "
                    f"eval_caps={list(budgets)} keep_ratio={float(keep_f):.2f}"
                )
            except Exception:
                pass
        best, sh_history = _successive_halving_search(
            candidates=candidates,
            budgets=budgets,
            evaluator=evaluator,
            keep_ratio=float(keep_f),
            log_fn=log_fn,
            log_prefix=f"{log_prefix} Mutation round {r + 1} —",
        )
        history.append({"round": int(r), "successive_halving": sh_history, "n_candidates": int(len(candidates))})
    return best, history


def _tune_steps_global_optimizer_image_first(
    *,
    cat_id: int,
    steps: Sequence[Dict[str, Any]],
    seed_stats: Sequence[Dict[str, Any]],
    val_ids: Sequence[int],
    images: Dict[int, Dict[str, Any]],
    gt_by_image_cat: Dict[int, Dict[int, List[List[float]]]],
    payload: "AgentMiningRequest",
    clip_head: Dict[str, Any],
    clip_head_target_index: int,
    workers: Sequence["_Sam3GreedyEvalWorker"],
    log_fn: Optional[Callable[[str], None]] = None,
    cancel_event: Optional[threading.Event] = None,
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Global optimizer wrapper for steps-mode mining (image-first, multi-GPU friendly).
    """

    def _log(msg: str) -> None:
        if not log_fn:
            return
        try:
            log_fn(msg)
        except Exception:
            return

    steps_norm = _normalize_steps_candidate_steps(steps, payload=payload)
    if not steps_norm:
        return list(steps), {"enabled": False, "reason": "no_steps"}

    total_val = len(val_ids)
    if total_val <= 0:
        return list(steps_norm), {"enabled": False, "reason": "no_val_images"}

    try:
        enabled = bool(getattr(payload, "steps_optimize_global", False))
    except Exception:
        enabled = False
    if not enabled:
        return list(steps_norm), {"enabled": False, "reason": "disabled"}

    # Budgets (eval caps) for successive halving.
    caps_raw: List[int] = []
    try:
        raw = getattr(payload, "steps_optimize_global_eval_caps", None)
        if isinstance(raw, list):
            caps_raw = [int(x) for x in raw]
    except Exception:
        caps_raw = []
    if not caps_raw:
        caps_raw = [50, 200, 1000]
    budgets = sorted({max(1, min(int(total_val), int(c))) for c in caps_raw if int(c) > 0})
    if not budgets:
        budgets = [int(total_val)]

    try:
        max_trials = int(getattr(payload, "steps_optimize_global_max_trials", 36) or 0)
    except Exception:
        max_trials = 36
    max_trials = max(1, int(max_trials))

    try:
        keep_ratio = float(getattr(payload, "steps_optimize_global_keep_ratio", 0.5) or 0.5)
    except Exception:
        keep_ratio = 0.5
    keep_ratio = max(0.1, min(0.9, float(keep_ratio)))

    try:
        rounds = int(getattr(payload, "steps_optimize_global_rounds", 2) or 0)
    except Exception:
        rounds = 2
    rounds = max(1, int(rounds))

    try:
        mutations_per_round = int(getattr(payload, "steps_optimize_global_mutations_per_round", 24) or 0)
    except Exception:
        mutations_per_round = 24
    mutations_per_round = max(1, int(mutations_per_round))

    enable_max_results = bool(getattr(payload, "steps_optimize_global_enable_max_results", False))
    enable_ordering = bool(getattr(payload, "steps_optimize_global_enable_ordering", False))

    _, _, _, target_precision = _build_clip_head_sweep_grid(
        payload,
        base_min_prob=float(payload.clip_head_min_prob),
        base_margin=float(payload.clip_head_margin),
        base_bg_margin=float(getattr(payload, "clip_head_background_margin", 0.0) or 0.0),
        allow_bg_tune=False,
    )

    # Precompute deterministic val subsets for each budget.
    seed = int(getattr(payload, "split_seed", 42) or 42)
    val_subsets: Dict[int, List[int]] = {}
    for b in budgets:
        val_subsets[int(b)] = _stable_sample_ids(val_ids, cap=int(b), seed=seed, salt=f"steps_global:{cat_id}:{b}")

    base_candidate: Dict[str, Any] = {
        "steps": steps_norm,
        "mutation": {"op": "base"},
        "sig": _steps_candidate_signature(steps_norm),
    }

    def _mutate(best_candidate: Any, round_idx: int) -> Sequence[Any]:
        if cancel_event is not None and cancel_event.is_set():
            raise RuntimeError("cancelled")
        cand = best_candidate if isinstance(best_candidate, dict) else {}
        max_mut = max(1, min(int(mutations_per_round), max(1, int(max_trials) - 1)))
        return _generate_steps_global_mutations(
            base_candidate=cand,
            seed_stats=seed_stats,
            payload=payload,
            max_mutations=int(max_mut),
            target_precision=float(target_precision),
            enable_max_results=bool(enable_max_results),
            enable_ordering=bool(enable_ordering),
        )

    def _eval(cand: Any, budget: int) -> Tuple[Tuple[Any, ...], Any]:
        if cancel_event is not None and cancel_event.is_set():
            raise RuntimeError("cancelled")
        c = cand if isinstance(cand, dict) else {}
        steps_c = c.get("steps") if isinstance(c.get("steps"), list) else []
        steps_c = _normalize_steps_candidate_steps(steps_c, payload=payload)
        subset = val_subsets.get(int(budget)) or list(val_ids)[: max(1, min(int(budget), int(total_val)))]
        summary = _tune_clip_head_for_selected_steps_image_first(
            cat_id=cat_id,
            class_name=None,
            steps=steps_c,
            val_ids=subset,
            images=images,
            gt_by_image_cat=gt_by_image_cat,
            payload=payload,
            clip_head=clip_head,
            clip_head_target_index=int(clip_head_target_index),
            workers=workers,
            log_every=0,
            log_fn=None,
            cancel_event=cancel_event,
            progress_callback=None,
            export_hard_negatives=False,
        )
        try:
            matched = int(summary.get("matches") or 0)
        except Exception:
            matched = 0
        try:
            fps = int(summary.get("fps") or 0)
        except Exception:
            fps = 0
        try:
            precision = float(summary.get("precision") or 0.0)
        except Exception:
            precision = 0.0
        try:
            min_prob = float(summary.get("clip_head_min_prob") or 0.0)
        except Exception:
            min_prob = 0.0
        try:
            margin = float(summary.get("clip_head_margin") or 0.0)
        except Exception:
            margin = 0.0
        try:
            bg_margin = float(summary.get("clip_head_background_margin") or 0.0)
        except Exception:
            bg_margin = 0.0
        key = _score_head_tuning_candidate(
            matched=matched,
            fps=fps,
            precision=precision,
            min_prob=min_prob,
            margin=margin,
            bg_margin=bg_margin,
            target_precision=float(target_precision),
        )
        return key, {"summary": summary, "candidate": c, "key": key, "budget": int(budget)}

    log_prefix = "[recipe-mining][multi-step][global-optimizer]"
    _log(
        f"{log_prefix} setup: rounds={rounds} max_trials={max_trials} eval_caps={budgets} keep_ratio={keep_ratio} "
        f"mutations/round={mutations_per_round} (ordering={enable_ordering}, max_results={enable_max_results}) "
        f"for class_id {cat_id}"
    )

    best, round_history = _run_steps_global_successive_halving_rounds(
        base_candidate=base_candidate,
        budgets=budgets,
        keep_ratio=float(keep_ratio),
        rounds=int(rounds),
        max_trials=int(max_trials),
        mutate=_mutate,
        evaluator=_eval,
        log_fn=_log,
        log_prefix=log_prefix,
    )
    best_steps = _normalize_steps_candidate_steps(best.get("steps") if isinstance(best, dict) else [], payload=payload)
    if isinstance(best, dict) and isinstance(best.get("mutation"), dict):
        try:
            _log(f"{log_prefix} selected mutation: {best.get('mutation')}")
        except Exception:
            pass

    info = {
        "enabled": True,
        "algorithm": "sam3_steps_global_v1",
        "version": 1,
        "eval_caps": budgets,
        "keep_ratio": float(keep_ratio),
        "max_trials": int(max_trials),
        "rounds": int(rounds),
        "mutations_per_round": int(mutations_per_round),
        "enable_ordering": bool(enable_ordering),
        "enable_max_results": bool(enable_max_results),
        "history": round_history,
        "selected_signature": _steps_candidate_signature(best_steps),
    }
    if isinstance(best, dict) and isinstance(best.get("mutation"), dict):
        info["selected_mutation"] = best.get("mutation")
    return best_steps, info


def _beam_tune_sam3_greedy_params(
    *,
    class_entries: Sequence[Dict[str, Any]],
    val_ids: Sequence[int],
    images: Dict[int, Dict[str, Any]],
    gt_by_image_cat: Dict[int, Dict[int, List[List[float]]]],
    payload: AgentMiningRequest,
    clip_head: Optional[Dict[str, Any]],
    workers: List[_Sam3GreedyEvalWorker],
    log_fn: Optional[Callable[[str], None]] = None,
    cancel_event: Optional[threading.Event] = None,
) -> Tuple[AgentMiningRequest, Dict[int, Dict[str, Any]]]:
    """
    Beam search (v2): tune shared greedy parameters per class on a capped subset of val images.

    We keep the eval loop image-first and evaluate each proposal across all classes in one pass
    (reusing loaded SAM3 workers), then pick the best proposal independently per class. This avoids
    GPU load/unload churn while still producing per-class tuned knobs for recipe export.
    """

    def _log(msg: str) -> None:
        if not log_fn:
            return
        try:
            log_fn(msg)
        except Exception:
            return

    if not val_ids:
        return payload, {}

    class_ids: List[int] = []
    class_names: Dict[int, str] = {}
    val_gt_by_id: Dict[int, int] = {}
    for entry in class_entries:
        try:
            cid_int = int(entry.get("id"))
        except Exception:
            continue
        class_ids.append(cid_int)
        class_names[cid_int] = str(entry.get("name") or f"class_{cid_int}")
        try:
            val_gt_by_id[cid_int] = int(entry.get("val_gt") or 0)
        except Exception:
            val_gt_by_id[cid_int] = 0
    class_ids = list(dict.fromkeys(class_ids))
    if not class_ids:
        return payload, {}

    # Deterministic sample of validation images for the search.
    cap = max(1, min(int(payload.beam_eval_cap), len(val_ids)))
    sample_ids = list(val_ids)
    if cap < len(sample_ids):
        rng = random.Random(int(payload.split_seed))
        rng.shuffle(sample_ids)
        sample_ids = sample_ids[:cap]

    beam_k = max(1, min(int(payload.beam_width), 16))
    rounds = max(1, min(int(payload.beam_rounds), 10))
    min_improve = max(0.0, min(float(payload.beam_min_improve), 1.0))

    # Start at the user-provided settings, then explore neighbors.
    base_seed = float(payload.seed_threshold)
    base_expand = float(payload.expand_threshold)
    base_sim = float(payload.similarity_score)
    base_seeds = int(payload.max_visual_seeds)

    def _clamp01(v: float) -> float:
        return max(0.0, min(1.0, float(v)))

    def _clamp_seeds(v: int) -> int:
        return max(0, min(500, int(v)))

    delta_thr = 0.05
    delta_expand = 0.05
    delta_sim = 0.05
    delta_seeds = 10

    def _make_key(seed_thr: float, expand_thr: float, sim: float, seeds: int) -> Tuple[float, float, float, int]:
        return (round(_clamp01(seed_thr), 4), round(_clamp01(expand_thr), 4), round(_clamp01(sim), 4), _clamp_seeds(seeds))

    evaluated: Dict[Tuple[float, float, float, int], Dict[int, Dict[str, Any]]] = {}

    # Seed proposals (keep small).
    proposals: List[Tuple[float, float, float, int]] = [
        _make_key(base_seed, base_expand, base_sim, base_seeds),
        _make_key(base_seed - delta_thr, base_expand, base_sim, base_seeds),
        _make_key(base_seed + delta_thr, base_expand, base_sim, base_seeds),
        _make_key(base_seed, base_expand - delta_expand, base_sim, base_seeds),
        _make_key(base_seed, base_expand + delta_expand, base_sim, base_seeds),
        _make_key(base_seed, base_expand, base_sim - delta_sim, base_seeds),
        _make_key(base_seed, base_expand, base_sim + delta_sim, base_seeds),
        _make_key(base_seed, base_expand, base_sim, base_seeds - delta_seeds),
        _make_key(base_seed, base_expand, base_sim, base_seeds + delta_seeds),
    ]
    proposals = list(dict.fromkeys(proposals))

    prev_best_avg_f1: Optional[float] = None

    _log(
        f"Beam search enabled (per-class): tuning on {len(sample_ids)}/{len(val_ids)} val images; "
        f"k={beam_k} rounds={rounds} eps={min_improve}"
    )

    for round_idx in range(rounds):
        if cancel_event is not None and cancel_event.is_set():
            break
        new_props = [p for p in proposals if p not in evaluated]
        if not new_props:
            break
        _log(f"Beam round {round_idx + 1}/{rounds}: evaluating {len(new_props)} proposal(s)…")
        for seed_thr, expand_thr, sim, seeds in new_props:
            if cancel_event is not None and cancel_event.is_set():
                break
            variant = payload.copy(
                update={
                    "seed_threshold": float(seed_thr),
                    "expand_threshold": float(expand_thr),
                    "similarity_score": float(sim),
                    "max_visual_seeds": int(seeds),
                }
            )
            summaries = _evaluate_sam3_greedy_recipes_image_first(
                class_entries=class_entries,
                val_ids=sample_ids,
                images=images,
                gt_by_image_cat=gt_by_image_cat,
                payload=variant,
                clip_head=clip_head,
                cancel_event=cancel_event,
                log_every=0,
                log_fn=None,
                progress_callback=None,
                workers=workers,
                    close_workers=False,
                )
            evaluated[(seed_thr, expand_thr, sim, seeds)] = summaries

        # Rank per-class (best proposal per class) based on lexicographic key.
        best_by_class: Dict[int, Tuple[Tuple[float, float, float, int], Tuple[int, int, float], float]] = {}
        for cid in class_ids:
            scored_for_class: List[Tuple[Tuple[float, float, float, int], Tuple[int, int, float], float]] = []
            for prop, summaries in evaluated.items():
                s = summaries.get(int(cid)) if isinstance(summaries, dict) else None
                if not isinstance(s, dict):
                    continue
                try:
                    matched = int(s.get("matches") or 0)
                except Exception:
                    matched = 0
                try:
                    fps = int(s.get("fps") or 0)
                except Exception:
                    fps = 0
                try:
                    precision = float(s.get("precision"))
                except Exception:
                    precision = matched / max(1, matched + fps)
                try:
                    recall = float(s.get("recall"))
                except Exception:
                    gts = int(s.get("gts") or 0)
                    recall = matched / gts if gts else 0.0
                f1 = 0.0
                if precision + recall > 1e-9:
                    f1 = 2.0 * precision * recall / (precision + recall)
                key = (matched, -fps, float(precision))
                scored_for_class.append((prop, key, float(f1)))
            if not scored_for_class:
                continue
            scored_for_class.sort(key=lambda x: x[1], reverse=True)
            best_by_class[int(cid)] = scored_for_class[0]

        if not best_by_class:
            _log("Beam search produced no valid per-class scores; falling back to Greedy settings.")
            break

        # Compute an aggregate progress metric for early-stop (average F1 across classes with scores).
        avg_f1 = 0.0
        try:
            avg_f1 = float(sum(v[2] for v in best_by_class.values()) / max(1, len(best_by_class)))
        except Exception:
            avg_f1 = 0.0
        _log(
            f"Beam aggregate: avg_F1={avg_f1:.3f} across {len(best_by_class)}/{len(class_ids)} classes "
            f"({len(evaluated)} total proposal(s) evaluated)"
        )
        if prev_best_avg_f1 is not None and (avg_f1 - prev_best_avg_f1) < min_improve:
            _log("Beam early-stop: average improvement below ε.")
            break
        prev_best_avg_f1 = avg_f1

        # Propose neighbors around top-k (shrink step each round).
        delta_thr *= 0.5
        delta_expand *= 0.5
        delta_sim *= 0.5
        delta_seeds = max(1, int(delta_seeds * 0.5))
        next_props: List[Tuple[float, float, float, int]] = []
        # Generate neighbors around a small set of "anchor" proposals (bounded by beam_k).
        votes: Dict[Tuple[float, float, float, int], Dict[str, int]] = {}
        for cid, best in best_by_class.items():
            prop = best[0]
            row = votes.get(prop)
            if row is None:
                row = {"votes": 0, "gt": 0}
                votes[prop] = row
            row["votes"] += 1
            row["gt"] += int(val_gt_by_id.get(int(cid), 0) or 0)
        anchors = sorted(votes.items(), key=lambda kv: (kv[1].get("votes", 0), kv[1].get("gt", 0)), reverse=True)
        anchor_props = [p for p, _info in anchors[:beam_k]] if anchors else []
        for seed_thr, expand_thr, sim, seeds in sorted(anchor_props):
            # Axis-aligned neighbors only (keeps proposal count bounded).
            next_props.extend(
                [
                    _make_key(seed_thr - delta_thr, expand_thr, sim, int(seeds)),
                    _make_key(seed_thr + delta_thr, expand_thr, sim, int(seeds)),
                    _make_key(seed_thr, expand_thr - delta_expand, sim, int(seeds)),
                    _make_key(seed_thr, expand_thr + delta_expand, sim, int(seeds)),
                    _make_key(seed_thr, expand_thr, sim - delta_sim, int(seeds)),
                    _make_key(seed_thr, expand_thr, sim + delta_sim, int(seeds)),
                    _make_key(seed_thr, expand_thr, sim, int(seeds) - delta_seeds),
                    _make_key(seed_thr, expand_thr, sim, int(seeds) + delta_seeds),
                ]
            )
        proposals = list(dict.fromkeys(next_props))

    if not evaluated:
        _log("Beam search produced no valid proposals; falling back to Greedy settings.")
        return payload, {}

    per_class_overrides: Dict[int, Dict[str, Any]] = {}
    for cid in class_ids:
        best: Optional[Tuple[Tuple[float, float, float, int], Tuple[int, int, float], float]] = None
        best_key: Optional[Tuple[int, int, float]] = None
        for prop, summaries in evaluated.items():
            s = summaries.get(int(cid)) if isinstance(summaries, dict) else None
            if not isinstance(s, dict):
                continue
            try:
                matched = int(s.get("matches") or 0)
            except Exception:
                matched = 0
            try:
                fps = int(s.get("fps") or 0)
            except Exception:
                fps = 0
            try:
                precision = float(s.get("precision"))
            except Exception:
                precision = matched / max(1, matched + fps)
            key = (matched, -fps, float(precision))
            if best_key is None or key > best_key:
                best_key = key
                try:
                    recall = float(s.get("recall"))
                except Exception:
                    gts = int(s.get("gts") or 0)
                    recall = matched / gts if gts else 0.0
                f1 = 0.0
                if precision + recall > 1e-9:
                    f1 = 2.0 * precision * recall / (precision + recall)
                best = (prop, key, float(f1))
        if best is None:
            continue
        seed_thr, expand_thr, sim, seeds = best[0]
        per_class_overrides[int(cid)] = {
            "seed_threshold": float(seed_thr),
            "expand_threshold": float(expand_thr),
            "similarity_score": float(sim),
            "max_visual_seeds": int(seeds),
        }

    # Log the final per-class selection (kept concise).
    if per_class_overrides:
        for cid in class_ids:
            ov = per_class_overrides.get(int(cid))
            if not isinstance(ov, dict):
                continue
            name = class_names.get(int(cid)) or f"class_{cid}"
            try:
                _log(
                    f"Beam selected for {name} (id={int(cid)}): text_thr={float(ov['seed_threshold']):.3f} "
                    f"visual_thr={float(ov['expand_threshold']):.3f} sim={float(ov['similarity_score']):.3f} "
                    f"max_seeds={int(ov['max_visual_seeds'])}"
                )
            except Exception:
                continue

    return payload, per_class_overrides


def _detections_to_eval_cache(
    detections: Sequence[Dict[str, Any]],
    images: Dict[int, Dict[str, Any]],
) -> Dict[int, List[Tuple[float, float, float, float, Optional[float]]]]:
    """Convert detection dicts into the format expected by _evaluate_prompt_candidate cached_detections."""
    by_image: Dict[int, List[Tuple[float, float, float, float, Optional[float]]]] = {}
    for det in detections:
        try:
            img_id = int(det.get("image_id"))
        except Exception:
            continue
        bbox = det.get("bbox")
        if not bbox or len(bbox) < 4:
            continue
        info = images.get(img_id)
        if not info:
            continue
        width = info.get("width")
        height = info.get("height")
        if width is None or height is None:
            path = info.get("path")
            if path:
                try:
                    with Image.open(path) as pil_img:
                        width = pil_img.width
                        height = pil_img.height
                        info["width"] = width
                        info["height"] = height
                except Exception:
                    continue
            else:
                continue
        try:
            x1, y1, x2, y2 = _yolo_to_xyxy(int(width), int(height), bbox)
        except Exception:
            continue
        score = det.get("score")
        by_image.setdefault(img_id, []).append((x1, y1, x2, y2, score))
    return by_image


def _load_agent_mining_sample(dataset_id: str) -> Optional[Dict[str, Any]]:
    sample_path = _agent_mining_meta_dir(dataset_id) / "sample.json"
    if not sample_path.exists():
        return None
    try:
        with sample_path.open("r", encoding="utf-8") as fp:
            data = json.load(fp)
            data["_path"] = str(sample_path)
            return data
    except Exception:
        return None


def _ensure_agent_mining_sample(
    dataset_id: str,
    dataset_root: Path,
    *,
    sample_size: int,
    seed: int,
) -> Dict[str, Any]:
    coco, _, images = _load_coco_index(dataset_root)
    categories = coco.get("categories") or []
    dataset_signature = _compute_dataset_signature(dataset_id, dataset_root, images, categories)
    image_ids = [int(img.get("id", idx)) for idx, img in enumerate(coco.get("images") or []) if "id" in img or idx >= 0]
    if not image_ids:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_mining_no_images")
    dir_signature = _compute_dir_signature(dataset_root)
    sample_size = max(1, int(sample_size))
    cached = _load_agent_mining_sample(dataset_id)
    if cached:
        cached_seed = cached.get("seed")
        cached_size = cached.get("sample_size")
        if (
            cached_seed is not None
            and cached_size is not None
            and cached.get("signature") == dir_signature
            and cached.get("dataset_signature") == dataset_signature
            and int(cached_seed) == int(seed)
            and int(cached_size) == int(sample_size)
        ):
            return {**cached, "_cached": True}
    rng = random.Random(seed)
    rng.shuffle(image_ids)
    total = len(image_ids)
    if sample_size >= total:
        sample_ids = list(image_ids)
    else:
        sample_ids = image_ids[:sample_size]
    if not sample_ids:
        raise HTTPException(status_code=HTTP_412_PRECONDITION_FAILED, detail="agent_sample_empty")
    sample = {
        "sample_ids": sample_ids,
        "sample_size": int(sample_size),
        "seed": int(seed),
        "total_images": int(total),
        "signature": dir_signature,
        "dataset_signature": dataset_signature,
        "created_at": time.time(),
    }
    sample_path = _agent_mining_meta_dir(dataset_id) / "sample.json"
    try:
        with sample_path.open("w", encoding="utf-8") as fp:
            json.dump(sample, fp)
    except Exception:
        logger.exception("Failed to persist agent mining sample to %s", sample_path)
    return {**sample, "_cached": False}


def _agent_mining_cache_key(
    *,
    class_id: Optional[int] = None,
    prompt: Optional[str],
    visual_ref: Optional[Dict[str, Any]],
    threshold: float,
    mask_threshold: float,
    min_size: int,
    simplify: float,
    max_results: int,
    similarity_score: Optional[float] = None,
    context: Optional[str] = None,
) -> str:
    visual_key = ""
    if visual_ref:
        visual_key = f"{visual_ref.get('image_id','')}:{','.join(map(str, visual_ref.get('bbox') or []))}"
    key_parts = [
        f"class={class_id}" if class_id is not None else "class=?",
        prompt or "",
        visual_key,
        f"thr={threshold:.4f}",
        f"mthr={mask_threshold:.4f}",
        f"min={min_size}",
        f"simplify={simplify:.4f}",
        f"max={max_results}",
    ]
    if similarity_score is not None:
        key_parts.append(f"sim={similarity_score:.4f}")
    if context:
        key_parts.append(f"context={context}")
    return _stable_hash(key_parts)


def _agent_mining_cache_paths(cache_dir: Path, key: str) -> Tuple[Path, Path]:
    """Return (gz_path, legacy_json_path) for a cache key."""
    return cache_dir / f"{key}.json.gz", cache_dir / f"{key}.json"


def _slim_detections(detections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Keep only the minimal fields needed downstream to cut disk usage."""
    kept_fields = {"image_id", "bbox", "score", "label", "class_id", "class_idx"}
    slimmed: List[Dict[str, Any]] = []
    for det in detections or []:
        try:
            slim = {k: det.get(k) for k in kept_fields if k in det}
            # ensure bbox is JSON-serializable list
            if "bbox" in slim and isinstance(slim["bbox"], tuple):
                slim["bbox"] = list(slim["bbox"])
            slimmed.append(slim)
        except Exception:
            continue
    return slimmed


def _load_agent_mining_detections(cache_dir: Path, key: str) -> Optional[List[Dict[str, Any]]]:
    path_gz, path_json = _agent_mining_cache_paths(cache_dir, key)
    path = path_gz if path_gz.exists() else path_json if path_json.exists() else None
    if path is None:
        return None
    try:
        if path.suffix == ".gz":
            with gzip.open(path, "rt", encoding="utf-8") as fp:
                return json.load(fp)
        with path.open("r", encoding="utf-8") as fp:
            return json.load(fp)
    except Exception:
        return None


def _save_agent_mining_detections(cache_dir: Path, key: str, detections: List[Dict[str, Any]]) -> None:
    cache_dir.mkdir(parents=True, exist_ok=True)
    path_gz, path_json = _agent_mining_cache_paths(cache_dir, key)
    # remove legacy uncompressed file if present
    if path_json.exists():
        try:
            path_json.unlink()
        except Exception:
            logger.debug("Failed to remove legacy cache file %s", path_json)
    slimmed = _slim_detections(detections)
    try:
        with tempfile.NamedTemporaryFile("wb", delete=False, dir=cache_dir, suffix=".tmp") as fp:
            with gzip.GzipFile(fileobj=fp, mode="w") as gz:
                gz.write(json.dumps(slimmed).encode("utf-8"))
            tmp_name = fp.name
        Path(tmp_name).replace(path_gz)
    except Exception:
        logger.exception("Failed to persist agent mining detections to %s", path_gz)
    _enforce_agent_mining_cache_limits(cache_dir, allow_when_running=True)


def _agent_cache_running_jobs() -> bool:
    with AGENT_MINING_JOBS_LOCK:
        return any(j.status == "running" for j in AGENT_MINING_JOBS.values())


def _enforce_agent_mining_cache_limits(cache_root: Path, *, allow_when_running: bool = False) -> Dict[str, int]:
    """
    Enforce TTL and total size limits on a cache directory. Returns stats.
    """
    stats = {"deleted_files": 0, "deleted_bytes": 0}
    if not cache_root.exists():
        return stats
    if _agent_cache_running_jobs() and not allow_when_running:
        return stats
    ttl_seconds = max(0, AGENT_MINING_CACHE_TTL_HOURS) * 3600
    now = time.time()
    files: List[Tuple[Path, float, int]] = []
    try:
        for p in cache_root.rglob("*"):
            if p.is_file():
                try:
                    stat = p.stat()
                    files.append((p, stat.st_mtime, stat.st_size))
                except Exception:
                    continue
    except Exception:
        return stats
    # TTL purge first
    for path, mtime, size in files:
        try:
            if ttl_seconds and (now - mtime) > ttl_seconds:
                path.unlink(missing_ok=True)
                stats["deleted_bytes"] += size
                stats["deleted_files"] += 1
        except Exception:
            continue
    # Recompute remaining and total size
    remaining: List[Tuple[Path, float, int]] = []
    total_size = 0
    try:
        for p in cache_root.rglob("*"):
            if not p.is_file():
                continue
            try:
                stat = p.stat()
                total_size += stat.st_size
                remaining.append((p, stat.st_mtime, stat.st_size))
            except Exception:
                continue
    except Exception:
        return stats
    if AGENT_MINING_CACHE_MAX_BYTES > 0 and total_size > AGENT_MINING_CACHE_MAX_BYTES:
        remaining.sort(key=lambda t: t[1])  # oldest first
        for path, _, size in remaining:
            if total_size <= AGENT_MINING_CACHE_MAX_BYTES:
                break
            try:
                path.unlink(missing_ok=True)
                total_size -= size
                stats["deleted_bytes"] += size
                stats["deleted_files"] += 1
            except Exception:
                continue
    # Remove empty directories to keep the tree clean.
    try:
        for d in sorted({p.parent for p, _, _ in remaining}, key=lambda p: len(p.parts), reverse=True):
            if d.exists():
                try:
                    if not any(d.iterdir()):
                        d.rmdir()
                except Exception:
                    continue
    except Exception:
        pass
    return stats


def _collect_agent_mining_detections(
    *,
    images: Dict[int, Dict[str, Any]],
    image_ids: Sequence[int],
    prompt: Optional[str],
    visual_ref: Optional[Dict[str, Any]],
    threshold: float,
    mask_threshold: float,
    min_size: int,
    simplify: float,
    max_results: int,
    cache_dir: Path,
    context: Optional[str] = None,
) -> List[Dict[str, Any]]:
    cache_key = _agent_mining_cache_key(
        prompt=prompt,
        visual_ref=visual_ref,
        threshold=threshold,
        mask_threshold=mask_threshold,
        min_size=min_size,
        simplify=simplify,
        max_results=max_results,
        context=context,
    )
    cached = _load_agent_mining_detections(cache_dir, cache_key)
    if cached is not None:
        return cached
    start_ts = time.time()
    results: List[Dict[str, Any]] = []
    for img_id in image_ids:
        img_info = images.get(img_id)
        if not img_info:
            continue
        img_path = img_info.get("path")
        if not img_path:
            continue
        try:
            pil_img = Image.open(img_path).convert("RGB")
        except Exception:
            logger.exception("Agent mining failed to open image %s", img_path)
            continue
        detections: List[QwenDetection]
        try:
            if visual_ref:
                bbox = visual_ref.get("bbox") if isinstance(visual_ref, dict) else None
                if not bbox or len(bbox) < 4:
                    continue
                detections = _run_sam3_visual_inference(
                    pil_img,
                    (
                        float(bbox[0]),
                        float(bbox[1]),
                        float(bbox[2]),
                        float(bbox[3]),
                    ),
                    threshold,
                    mask_threshold,
                    max_results,
                    min_size=min_size if min_size > 0 else None,
                    simplify_epsilon=simplify,
                )
            else:
                detections = _run_sam3_text_inference(
                    pil_img,
                    prompt or "",
                    threshold,
                    mask_threshold,
                    max_results,
                    min_size=min_size if min_size > 0 else None,
                    simplify_epsilon=simplify,
                )
        except Exception:
            logger.exception("Agent mining prompt failed for image %s", img_id)
            continue
        for det in detections:
            det_dict = det.dict()
            det_dict["image_id"] = img_id
            results.append(det_dict)
    _save_agent_mining_detections(cache_dir, cache_key, results)
    try:
        elapsed = time.time() - start_ts
        logger.info(
            "Agent mining collected %d detections for prompt=%s visual=%s over %d images in %.2fs",
            len(results),
            prompt if prompt else "",
            bool(visual_ref),
            len(image_ids),
            elapsed,
        )
    except Exception:
        pass
    return results


def _build_sam3_text_processor_for_device(device: torch.device) -> Tuple[Any, Any]:
    if SAM3_NATIVE_IMAGE_IMPORT_ERROR is not None or build_sam3_image_model is None or Sam3ImageProcessor is None:
        raise RuntimeError(f"sam3_text_unavailable:{SAM3_NATIVE_IMAGE_IMPORT_ERROR}")
    # Use the full device string (e.g. "cuda:1") so multi-GPU mining doesn't accidentally pin all
    # workers to cuda:0 during model construction.
    device_str = str(device) if device.type == "cuda" else "cpu"
    try:
        model = build_sam3_image_model(
            checkpoint_path=active_sam3_checkpoint,
            device=device_str,
            load_from_HF=active_sam3_checkpoint is None,
            enable_segmentation=True,
            bpe_path=str(SAM3_BPE_PATH),
        )
        if device:
            model = model.to(device)
        _sam3_clear_device_pinned_caches(model)
        processor = Sam3ImageProcessor(model, device=device_str)
        return model, processor
    except Exception as exc:  # noqa: BLE001
        raise RuntimeError(f"sam3_text_load_failed:{exc}") from exc


class _Sam3MiningWorker:
    def __init__(self, device: torch.device):
        self.device = device
        self.model, self.processor = _build_sam3_text_processor_for_device(device)
        self.lock = threading.Lock()

    def close(self) -> None:
        try:
            del self.processor
        except Exception:  # noqa: BLE001
            pass
        try:
            del self.model
        except Exception:  # noqa: BLE001
            pass
        if torch.cuda.is_available() and self.device.type == "cuda":
            try:
                torch.cuda.empty_cache()
            except Exception:  # noqa: BLE001
                pass

    def process_image(
        self,
        image_id: int,
        pil_img: Image.Image,
        tasks: Sequence[Dict[str, Any]],
        *,
        min_threshold: float,
        mask_threshold: float,
        max_results: int,
        min_size: int,
        simplify: float,
        return_masks: bool = False,
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Run all requested tasks against a single preloaded image on this worker.

        Important: SAM3 state is *not* safe to reuse across prompts without clearing previous
        prompts/results. We keep the expensive image backbone in `state["backbone_out"]` but
        reset text/geometric prompts between tasks.

        Visual tasks in Agent Mining are "seeded visual prompts": we do not use a fixed bbox from
        the training image. Instead, we:
        1) run (or reuse) a text prompt on the current image to get candidate boxes,
        2) pick the best seed box by CLIP similarity to the exemplar crop embedding,
        3) run SAM3 geometric prompt using that seed box.
        """
        if not tasks:
            return {}
        with self.lock:
            try:
                self.processor.set_confidence_threshold(float(min_threshold))
            except Exception:
                pass
            state = self.processor.set_image(pil_img)

            def _reset_prompts() -> None:
                try:
                    if hasattr(self.processor, "reset_all_prompts"):
                        self.processor.reset_all_prompts(state)
                        return
                except Exception:
                    pass
                # Best-effort fallback if upstream changes.
                try:
                    if isinstance(state.get("backbone_out"), dict):
                        for k in ("language_features", "language_mask", "language_embeds"):
                            state["backbone_out"].pop(k, None)
                    for k in ("geometric_prompt", "boxes", "masks", "masks_logits", "scores"):
                        state.pop(k, None)
                except Exception:
                    pass

            outputs: Dict[str, List[Dict[str, Any]]] = {}
            text_dets_by_prompt: Dict[str, List[QwenDetection]] = {}

            # Run all text tasks first so visual tasks can reuse them for seeding.
            text_tasks = [t for t in tasks if t.get("type") == "text"]
            visual_tasks = [t for t in tasks if t.get("type") != "text"]

            for task in text_tasks:
                task_id = task.get("id")
                if not task_id:
                    continue
                prompt_text = (task.get("prompt") or "").strip()
                _reset_prompts()
                det_masks: Optional[List[np.ndarray]] = None
                try:
                    if return_masks:
                        dets, det_masks = _run_sam3_text_inference(
                            pil_img,
                            prompt_text,
                            min_threshold,
                            mask_threshold,
                            max_results,
                            min_size=min_size if min_size > 0 else None,
                            simplify_epsilon=simplify,
                            processor_override=self.processor,
                            state=state,
                            return_masks=True,
                        )
                    else:
                        dets = _run_sam3_text_inference(
                            pil_img,
                            prompt_text,
                            min_threshold,
                            mask_threshold,
                            max_results,
                            min_size=min_size if min_size > 0 else None,
                            simplify_epsilon=simplify,
                            processor_override=self.processor,
                            state=state,
                        )
                except Exception:
                    continue
                if prompt_text:
                    text_dets_by_prompt[prompt_text] = dets
                det_dicts: List[Dict[str, Any]] = []
                for det_idx, det in enumerate(dets):
                    try:
                        det_data = det.dict()
                    except Exception:
                        continue
                    det_data["image_id"] = image_id
                    if return_masks and det_masks is not None:
                        if 0 <= det_idx < len(det_masks):
                            det_data["mask_array"] = det_masks[det_idx]
                    det_dicts.append(det_data)
                outputs[task_id] = det_dicts

            # Lazy CLIP seed embedding cache per seed prompt for this image.
            seed_cache: Dict[str, Tuple[List[Tuple[float, float, float, float]], np.ndarray]] = {}

            def _encode_clip_batch(crops: List[Image.Image]) -> Optional[np.ndarray]:
                model, preprocess = _ensure_clip_backbone_for_mining()
                if model is None or preprocess is None or not crops:
                    return None
                try:
                    with clip_lock:
                        inp = torch.stack([preprocess(c) for c in crops], dim=0).to(device)
                        try:
                            target_dtype = next(model.parameters()).dtype
                        except Exception:
                            target_dtype = torch.float32
                        if inp.dtype != target_dtype:
                            inp = inp.to(dtype=target_dtype)
                        with torch.no_grad():
                            feats = model.encode_image(inp)
                        feats = feats.to(dtype=torch.float32, device="cpu")
                    feats = feats / (feats.norm(dim=-1, keepdim=True) + 1e-8)
                    return feats.cpu().numpy()
                except Exception as exc:
                    logger.debug("CLIP batch encode failed: %s", exc)
                    return None

            def _seed_candidates_for_prompt(prompt_text: str) -> List[QwenDetection]:
                prompt_text = (prompt_text or "").strip()
                if not prompt_text:
                    return []
                existing = text_dets_by_prompt.get(prompt_text)
                if existing is not None:
                    return existing
                # If the seed prompt wasn't part of the task slice, run it on-demand.
                _reset_prompts()
                try:
                    dets = _run_sam3_text_inference(
                        pil_img,
                        prompt_text,
                        min_threshold,
                        mask_threshold,
                        max_results,
                        min_size=min_size if min_size > 0 else None,
                        simplify_epsilon=simplify,
                        processor_override=self.processor,
                        state=state,
                    )
                except Exception:
                    dets = []
                text_dets_by_prompt[prompt_text] = dets
                return dets

            def _best_seed_bbox_xywh(
                seed_prompt: str,
                *,
                exemplar_vec: Optional[np.ndarray],
            ) -> Optional[Tuple[float, float, float, float]]:
                dets = _seed_candidates_for_prompt(seed_prompt)
                if not dets:
                    return None
                # Prefer CLIP-based seed selection when an exemplar embedding is present.
                if exemplar_vec is None:
                    # Fallback: pick highest-scoring text detection.
                    best = max(dets, key=lambda d: (d.score if d.score is not None else 0.0))
                    try:
                        left, top, right, bottom = yolo_to_corners(best.bbox, pil_img.width, pil_img.height)
                    except Exception:
                        return None
                    return float(left), float(top), float(right - left), float(bottom - top)

                seed_prompt_key = (seed_prompt or "").strip()
                cached = seed_cache.get(seed_prompt_key)
                if cached is None:
                    # Only embed a small top-k for performance.
                    ranked = sorted(dets, key=lambda d: (d.score if d.score is not None else 0.0), reverse=True)
                    top_k = ranked[:50]
                    bboxes_xywh: List[Tuple[float, float, float, float]] = []
                    crops: List[Image.Image] = []
                    for det in top_k:
                        bbox = det.bbox or []
                        if len(bbox) < 4:
                            continue
                        try:
                            left, top, right, bottom = yolo_to_corners(bbox, pil_img.width, pil_img.height)
                        except Exception:
                            continue
                        if right <= left or bottom <= top:
                            continue
                        try:
                            crop = pil_img.crop((left, top, right, bottom))
                        except Exception:
                            continue
                        bboxes_xywh.append((float(left), float(top), float(right - left), float(bottom - top)))
                        crops.append(crop)
                    feats = _encode_clip_batch(crops)
                    if feats is None or feats.size == 0:
                        seed_cache[seed_prompt_key] = (bboxes_xywh, np.zeros((0, 1), dtype=np.float32))
                    else:
                        seed_cache[seed_prompt_key] = (bboxes_xywh, feats)
                    cached = seed_cache.get(seed_prompt_key)
                bboxes_xywh, feats = cached if cached is not None else ([], np.zeros((0, 1), dtype=np.float32))
                if feats is None or feats.size == 0 or not bboxes_xywh:
                    return None
                try:
                    ex = np.asarray(exemplar_vec, dtype=np.float32).reshape(-1)
                    ex = ex / (np.linalg.norm(ex) + 1e-8)
                    sims = feats @ ex.reshape(-1, 1)
                    sims = sims.squeeze(-1)
                    idx_best = int(np.argmax(sims)) if sims.size else -1
                except Exception:
                    idx_best = -1
                if idx_best < 0 or idx_best >= len(bboxes_xywh):
                    return None
                return bboxes_xywh[idx_best]

            for task in visual_tasks:
                task_id = task.get("id")
                if not task_id:
                    continue
                # Legacy: fixed bbox (pixel xywh) provided directly.
                bbox = task.get("bbox")
                seed_bbox: Optional[Tuple[float, float, float, float]] = None
                if bbox and len(bbox) >= 4:
                    try:
                        seed_bbox = (float(bbox[0]), float(bbox[1]), float(bbox[2]), float(bbox[3]))
                    except Exception:
                        seed_bbox = None
                if seed_bbox is None:
                    seed_prompt = task.get("seed_prompt") or ""
                    exemplar_vec = task.get("exemplar_vec")
                    seed_bbox = _best_seed_bbox_xywh(seed_prompt, exemplar_vec=exemplar_vec)
                if seed_bbox is None:
                    outputs[task_id] = []
                    continue
                _reset_prompts()
                det_masks: Optional[List[np.ndarray]] = None
                try:
                    if return_masks:
                        dets, det_masks = _run_sam3_visual_inference(
                            pil_img,
                            seed_bbox,
                            min_threshold,
                            mask_threshold,
                            max_results,
                            min_size=min_size if min_size > 0 else None,
                            simplify_epsilon=simplify,
                            processor_override=self.processor,
                            state=state,
                            return_masks=True,
                        )
                    else:
                        dets = _run_sam3_visual_inference(
                            pil_img,
                            seed_bbox,
                            min_threshold,
                            mask_threshold,
                            max_results,
                            min_size=min_size if min_size > 0 else None,
                            simplify_epsilon=simplify,
                            processor_override=self.processor,
                            state=state,
                        )
                except Exception:
                    continue
                det_dicts: List[Dict[str, Any]] = []
                for det_idx, det in enumerate(dets):
                    try:
                        det_data = det.dict()
                    except Exception:
                        continue
                    det_data["image_id"] = image_id
                    if return_masks and det_masks is not None:
                        if 0 <= det_idx < len(det_masks):
                            det_data["mask_array"] = det_masks[det_idx]
                    det_dicts.append(det_data)
                outputs[task_id] = det_dicts
            return outputs


class _Sam1SegWorker:
    def __init__(self, device: torch.device):
        model = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH)
        if device:
            try:
                model = model.to(device)
            except Exception:
                pass
        self.device = device
        self.predictor = SamPredictor(model)
        self.lock = threading.Lock()

    def close(self) -> None:
        try:
            del self.predictor
        except Exception:
            pass
        if torch.cuda.is_available() and self.device and self.device.type == "cuda":
            try:
                torch.cuda.empty_cache()
            except Exception:
                pass

    def process_image(
        self,
        image_id: int,
        pil_img: Image.Image,
        tasks: Sequence[Dict[str, Any]],
        *,
        simplify: float,
        min_size: int = 0,
        **_: Any,
    ) -> Dict[str, List[Dict[str, Any]]]:
        outputs: Dict[str, List[Dict[str, Any]]] = {}
        if not tasks:
            return outputs
        np_img = np.array(pil_img.convert("RGB"))
        with self.lock:
            try:
                self.predictor.set_image(np_img)
            except Exception:
                return outputs
            for task in tasks:
                task_id = task.get("id")
                bbox = task.get("bbox")
                class_idx = task.get("class_idx")
                fallback = task.get("fallback_poly")
                if not task_id or not bbox or len(bbox) < 4:
                    continue
                x, y, w, h = bbox
                xyxy = np.array([x, y, x + w, y + h])
                try:
                    masks, scores, _ = self.predictor.predict(
                        box=xyxy[None, :],
                        multimask_output=True,
                        return_logits=False,
                    )
                except Exception:
                    continue
                if masks is None or len(masks) == 0:
                    continue
                scores_arr = np.asarray(scores) if scores is not None else None
                idx_best = int(np.argmax(scores_arr)) if scores_arr is not None and scores_arr.size else 0
                mask_arr = masks[idx_best]
                area = float(np.count_nonzero(mask_arr))
                if min_size and area < float(min_size):
                    continue
                outputs[task_id] = [
                    {
                        "image_id": image_id,
                        "mask_array": mask_arr,
                        "score": float(scores_arr[idx_best]) if scores_arr is not None and scores_arr.size else None,
                        "class_idx": class_idx,
                        "fallback_poly": fallback,
                    }
                ]
        return outputs


class _Sam3MiningPool:
    def __init__(self, devices: Sequence[torch.device]):
        self.workers: List[_Sam3MiningWorker] = []
        for dev in devices:
            try:
                self.workers.append(_Sam3MiningWorker(dev))
            except Exception as exc:  # noqa: BLE001
                logger.warning("Failed to initialize SAM3 mining worker on %s: %s", dev, exc)
        if not self.workers:
            raise RuntimeError("sam3_mining_workers_unavailable")

    def close(self) -> None:
        for worker in self.workers:
            try:
                worker.close()
            except Exception:
                continue

    def run(
        self,
        image_entries: Sequence[Tuple[int, str]],
        tasks: Sequence[Dict[str, Any]],
        *,
        min_threshold: float,
        mask_threshold: float,
        max_results: int,
        min_size: int,
        simplify: float,
        cancel_event: Optional[threading.Event] = None,
        progress_callback: Optional[Callable[[int], None]] = None,
        return_masks: bool = False,
    ) -> Dict[str, List[Dict[str, Any]]]:
        if not tasks or not image_entries:
            return {task.get("id"): [] for task in tasks if task.get("id")}
        results: Dict[str, List[Dict[str, Any]]] = {task.get("id"): [] for task in tasks if task.get("id")}
        max_workers = max(1, len(self.workers))

        # Fast path: single worker, run sequentially to avoid thread overhead and potential native race conditions.
        if max_workers == 1:
            worker = self.workers[0]
            for idx, (img_id, path) in enumerate(image_entries, start=1):
                if cancel_event is not None and cancel_event.is_set():
                    break
                try:
                    with Image.open(path) as img:
                        pil_img = img.convert("RGB")
                except Exception:
                    continue
                if cancel_event is not None and cancel_event.is_set():
                    break
                partial = worker.process_image(
                    img_id,
                    pil_img,
                    tasks,
                    min_threshold=min_threshold,
                    mask_threshold=mask_threshold,
                    max_results=max_results,
                    min_size=min_size,
                    simplify=simplify,
                    return_masks=return_masks,
                )
                for key, dets in partial.items():
                    if key is None or not dets:
                        continue
                    results.setdefault(key, []).extend(dets)
                if progress_callback:
                    try:
                        progress_callback(idx)
                    except Exception:
                        pass
            return results

        processed = 0
        proc_lock = threading.Lock()

        def _run_task(worker: _Sam3MiningWorker, img_id: int, path: str) -> Dict[str, List[Dict[str, Any]]]:
            if cancel_event is not None and cancel_event.is_set():
                return {}
            if not path:
                return {}
            try:
                with Image.open(path) as img:
                    pil_img = img.convert("RGB")
            except Exception:
                return {}
            if cancel_event is not None and cancel_event.is_set():
                return {}
            return worker.process_image(
                img_id,
                pil_img,
                tasks,
                min_threshold=min_threshold,
                mask_threshold=mask_threshold,
                max_results=max_results,
                min_size=min_size,
                simplify=simplify,
                return_masks=return_masks,
            )

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for idx, (img_id, path) in enumerate(image_entries):
                if cancel_event is not None and cancel_event.is_set():
                    break
                worker = self.workers[idx % max_workers]
                futures.append(executor.submit(_run_task, worker, img_id, path))
            for future in as_completed(futures):
                try:
                    partial = future.result()
                except Exception as exc:  # noqa: BLE001
                    logger.debug("Agent mining worker failed: %s", exc)
                    continue
                if cancel_event is not None and cancel_event.is_set():
                    break
                for key, dets in partial.items():
                    if key is None:
                        continue
                    if dets:
                        results.setdefault(key, []).extend(dets)
                if progress_callback:
                    with proc_lock:
                        processed += 1
                        try:
                            progress_callback(processed)
                        except Exception:
                            pass
        return results


def _collect_agent_mining_detections_image_first(
    *,
    candidates: Sequence[Dict[str, Any]],
    thresholds: Sequence[float],
    similarity_scores: Optional[Sequence[float]],
    images: Dict[int, Dict[str, Any]],
    image_ids: Sequence[int],
    mask_threshold: float,
    min_size: int,
    simplify: float,
    max_results: int,
    cache_dir: Path,
    pool: _Sam3MiningPool,
    use_cache: bool = True,
    cancel_event: Optional[threading.Event] = None,
    progress_callback: Optional[Callable[[int], None]] = None,
    cache_context: Optional[str] = None,
) -> Tuple[Dict[str, Dict[float, Dict[Optional[float], List[Dict[str, Any]]]]], Dict[str, Any]]:
    """
    Collect detections for all candidates over all images in an image-first manner. We run the
    lowest threshold once per image and reuse scores to materialize higher-threshold variants.
    """
    thresholds_list = [float(t) for t in thresholds] if thresholds else [0.3]
    thresholds_list = [t for t in thresholds_list if 0.0 <= t <= 1.0]
    if not thresholds_list:
        thresholds_list = [0.3]
    sim_list = [float(s) for s in (similarity_scores or []) if 0.0 <= float(s) <= 1.0]
    if not sim_list:
        sim_list = [None]
    min_threshold = min(thresholds_list)
    results: Dict[str, Dict[float, Dict[Optional[float], List[Dict[str, Any]]]]] = {}
    missing: List[Dict[str, Any]] = []
    # For streaming flush
    executed_keys: set[str] = set()
    executed_keys_with_dets: set[str] = set()
    cached_pairs = 0
    executed_pairs = 0
    executed_pairs_with_dets = 0
    # First try to satisfy from cache.
    if use_cache:
        for cand in candidates:
            cand_id = cand.get("id")
            if not cand_id:
                continue
            results[cand_id] = {}
            all_cached = True
            for thr in thresholds_list:
                for sim in sim_list:
                    cached = _load_agent_mining_detections(
                        cache_dir,
                        _agent_mining_cache_key(
                            class_id=cand.get("class_id"),
                            prompt=cand.get("prompt"),
                            visual_ref=cand.get("visual_ref"),
                            threshold=thr,
                            mask_threshold=mask_threshold,
                            min_size=min_size,
                            simplify=simplify,
                            max_results=max_results,
                            similarity_score=sim,
                            context=cache_context,
                        ),
                    )
                    if cached is not None:
                        results[cand_id].setdefault(thr, {})[sim] = cached
                        cached_pairs += 1
                    else:
                        all_cached = False
            if not all_cached:
                missing.append(cand)
    else:
        missing = list(candidates)
        for cand in candidates:
            cand_id = cand.get("id")
            if not cand_id:
                continue
            results[cand_id] = {}
    if missing and (cancel_event is None or not cancel_event.is_set()):
        image_entries: List[Tuple[int, str]] = []
        for img_id in image_ids:
            info = images.get(img_id) or {}
            path = info.get("path")
            if path:
                image_entries.append((img_id, path))
        # Remove stale cache files for the keys we will regenerate to avoid duplicate appends across runs.
        keys_to_reset: set[str] = set()
        for cand in missing:
            for thr in thresholds_list:
                for sim in sim_list:
                    cache_key = _agent_mining_cache_key(
                        class_id=cand.get("class_id"),
                        prompt=cand.get("prompt"),
                        visual_ref=cand.get("visual_ref"),
                        threshold=thr,
                        mask_threshold=mask_threshold,
                        min_size=min_size,
                        simplify=simplify,
                        max_results=max_results,
                        similarity_score=sim,
                        context=cache_context,
                    )
                    keys_to_reset.add(cache_key)
        for key in keys_to_reset:
            path_gz, path_json = _agent_mining_cache_paths(cache_dir, key)
            for p in (path_gz, path_json):
                if p.exists():
                    try:
                        p.unlink()
                    except Exception:
                        logger.debug("Failed to remove stale cache file %s", p)
        # Stream in chunks to limit memory, flushing each chunk to cache.
        # Keep chunk_size small and also cap how many candidates are evaluated per chunk to limit RAM/VRAM.
        chunk_size = 4
        max_cands_per_chunk = 16
        accumulator: Dict[str, List[Dict[str, Any]]] = {}
        processed_total = 0

        def flush_accumulator() -> None:
            nonlocal accumulator
            if not accumulator:
                return
            for key, items in accumulator.items():
                if cancel_event is not None and cancel_event.is_set():
                    break
                # We already cleared stale cache entries for these keys above,
                # so write the accumulated detections directly to avoid repeated read/extend churn.
                _save_agent_mining_detections(cache_dir, key, items)
            accumulator = {}

        for start in range(0, len(image_entries), chunk_size):
            if cancel_event is not None and cancel_event.is_set():
                break
            batch_entries = image_entries[start : start + chunk_size]
            # Optionally slice candidates to keep per-chunk work bounded.
            cand_chunks = [missing[i : i + max_cands_per_chunk] for i in range(0, len(missing), max_cands_per_chunk)]
            logger.info(
                "[agent-mining] chunk %d/%d images %d-%d/%d (candidates=%d, thresholds=%d)",
                (start // chunk_size) + 1,
                math.ceil(len(image_entries) / chunk_size),
                start + 1,
                min(start + len(batch_entries), len(image_entries)),
                len(image_entries),
                len(missing),
                len(thresholds_list),
            )
            for cand_slice in cand_chunks:
                if cancel_event is not None and cancel_event.is_set():
                    break
                try:
                    pooled = pool.run(
                        batch_entries,
                        cand_slice,
                        min_threshold=min_threshold,
                        mask_threshold=mask_threshold,
                        max_results=max_results,
                        min_size=min_size,
                        simplify=simplify,
                        cancel_event=cancel_event,
                        progress_callback=None,
                    )
                except torch.cuda.OutOfMemoryError:
                    logger.warning(
                        "SAM3 mining OOM; retrying slice with reduced candidate batch (cand_slice=%d)",
                        len(cand_slice),
                    )
                    if torch.cuda.is_available():
                        try:
                            torch.cuda.empty_cache()
                        except Exception:
                            pass
                    # Retry one-by-one to limp forward
                    pooled = {}
                    for single_cand in cand_slice:
                        try:
                            partial_single = pool.run(
                                batch_entries,
                                [single_cand],
                                min_threshold=min_threshold,
                                mask_threshold=mask_threshold,
                                max_results=max_results,
                                min_size=min_size,
                                simplify=simplify,
                                cancel_event=cancel_event,
                                progress_callback=None,
                            )
                            pooled.update(partial_single)
                        except torch.cuda.OutOfMemoryError:
                            logger.error("SAM3 mining OOM even on single candidate; aborting batch.")
                            raise
                for cand in cand_slice:
                    cand_id = cand.get("id")
                    if not cand_id:
                        continue
                    base_dets = pooled.get(cand_id, [])
                    for thr in thresholds_list:
                        filtered = [
                            det
                            for det in base_dets
                            if ((det.get("score") is None and thr <= min_threshold) or (det.get("score") or 0.0) >= thr)
                        ]
                        for sim in sim_list:
                            cache_key = _agent_mining_cache_key(
                                class_id=cand.get("class_id"),
                                prompt=cand.get("prompt"),
                                visual_ref=cand.get("visual_ref"),
                                threshold=thr,
                                mask_threshold=mask_threshold,
                                min_size=min_size,
                                simplify=simplify,
                                max_results=max_results,
                                similarity_score=sim,
                                context=cache_context,
                            )
                            accumulator.setdefault(cache_key, []).extend(filtered)
                            executed_keys.add(cache_key)
                            if filtered:
                                executed_keys_with_dets.add(cache_key)
                flush_accumulator()
                try:
                    # Free pooled results explicitly to lower peak RAM.
                    pooled.clear()
                except Exception:
                    pass
                # Give the GPU a chance to release memory between slices.
                if torch.cuda.is_available():
                    try:
                        torch.cuda.empty_cache()
                    except Exception:
                        pass
            processed_total += len(batch_entries)
            if progress_callback:
                try:
                    progress_callback(processed_total)
                except Exception:
                    pass
        flush_accumulator()
        # Reload from cache for all missing candidates.
        for cand in missing:
            cand_id = cand.get("id")
            if not cand_id:
                continue
            for thr in thresholds_list:
                for sim in sim_list:
                    cache_key = _agent_mining_cache_key(
                        class_id=cand.get("class_id"),
                        prompt=cand.get("prompt"),
                        visual_ref=cand.get("visual_ref"),
                        threshold=thr,
                        mask_threshold=mask_threshold,
                        min_size=min_size,
                        simplify=simplify,
                        max_results=max_results,
                        similarity_score=sim,
                        context=cache_context,
                    )
                    cached = _load_agent_mining_detections(cache_dir, cache_key) or []
                    results.setdefault(cand_id, {}).setdefault(thr, {})[sim] = cached
        executed_pairs += len(executed_keys)
        executed_pairs_with_dets += len(executed_keys_with_dets)
    stats = {
        "images": len(image_ids),
        "candidates": len(candidates),
        "thresholds": len(thresholds_list),
        "cached_pairs": cached_pairs,
        "executed_pairs": executed_pairs,
        "executed_pairs_with_dets": executed_pairs_with_dets,
    }
    logger.info(
        "[agent-mining] global sweep done: images=%d candidates=%d thresholds=%d cached=%d executed=%d det_pairs=%d",
        len(image_ids),
        len(candidates),
        len(thresholds_list),
        cached_pairs,
        executed_pairs,
        executed_pairs_with_dets,
    )
    return results, stats


def _clip_embed_regions(
    regions: List[Dict[str, Any]],
    images: Dict[int, Dict[str, Any]],
    *,
    max_regions: int = 256,
) -> Tuple[Dict[str, np.ndarray], List[str]]:
    """
    Embed cropped regions (image_id + YOLO bbox) with raw CLIP. Returns (id->embedding, warnings).
    """
    model, preprocess = _ensure_clip_backbone_for_mining()
    warnings: List[str] = []
    if model is None or preprocess is None:
        warnings.append("clip_unavailable")
        return {}, warnings
    embeddings: Dict[str, np.ndarray] = {}
    device_to_use = device if isinstance(device, str) or isinstance(device, torch.device) else "cpu"
    def _encode_image(image: Image.Image) -> Optional[np.ndarray]:
        try:
            target_dtype = next(model.parameters()).dtype
        except Exception:
            target_dtype = torch.float32
        try:
            inp = preprocess(image).unsqueeze(0).to(device_to_use)
            if inp.dtype != target_dtype:
                inp = inp.to(dtype=target_dtype)
            with torch.no_grad():
                feats = model.encode_image(inp)
            feats = feats.to(dtype=torch.float32, device="cpu")
            feats = feats / (feats.norm(dim=-1, keepdim=True) + 1e-8)
            return feats.squeeze(0).cpu().numpy()
        except Exception as exc:
            logger.debug("CLIP encode failed: %s", exc)
            return None
    for entry in regions[:max_regions]:
        try:
            img_id = int(entry.get("image_id"))
        except Exception:
            continue
        bbox = entry.get("bbox")
        if not bbox or len(bbox) < 4:
            continue
        info = images.get(img_id) or {}
        path = entry.get("path") or info.get("path")
        if not path:
            crop_path = entry.get("crop_path")
            if crop_path:
                try:
                    rel = Path(str(crop_path))
                except Exception:
                    rel = None
                if rel and not rel.is_absolute() and ".." not in rel.parts:
                    candidate = (AGENT_MINING_RECIPES_ROOT / rel).resolve()
                    if (
                        candidate.exists()
                        and candidate.is_file()
                        and _path_is_within_root(candidate, AGENT_MINING_RECIPES_ROOT.resolve())
                    ):
                        path = str(candidate)
        if not path:
            continue
        try:
            pil_img = Image.open(path).convert("RGB")
        except Exception:
            continue
        try:
            bbox4 = list(map(float, bbox[:4]))
            if all(0.0 <= v <= 1.0 for v in bbox4):  # YOLO (cx, cy, w, h) normalized
                left, top, right, bottom = yolo_to_corners(bbox4, pil_img.width, pil_img.height)
            else:  # treat as COCO-style pixel xywh (x, y, w, h)
                x, y, w, h = map(float, bbox[:4])
                left = int(round(x))
                top = int(round(y))
                right = int(round(x + w))
                bottom = int(round(y + h))
                left = max(0, min(pil_img.width, left))
                top = max(0, min(pil_img.height, top))
                right = max(left, min(pil_img.width, right))
                bottom = max(top, min(pil_img.height, bottom))
            if right <= left or bottom <= top:
                continue
            crop = pil_img.crop((left, top, right, bottom))
            emb = _encode_image(crop)
            if emb is None:
                continue
            key = entry.get("embed_id") or f"{img_id}:{left},{top},{right},{bottom}"
            embeddings[key] = emb
        except Exception as exc:
            logger.debug("CLIP embed crop failed: %s", exc)
            continue
    if not embeddings:
        warnings.append("clip_embedding_empty")
    return embeddings, warnings


def _clip_fp_filter_detections(
    detections: List[Dict[str, Any]],
    *,
    exemplar_embeddings: Dict[str, np.ndarray],
    negative_embeddings: Optional[Dict[str, np.ndarray]] = None,
    negative_strength: float = 0.0,
    images: Dict[int, Dict[str, Any]],
    similarity_floor: float,
    max_regions: int = 512,
) -> Tuple[List[Dict[str, Any]], List[str]]:
    """Filter detections whose CLIP similarity to any exemplar is below the floor."""
    warnings: List[str] = []
    if not exemplar_embeddings:
        return detections, warnings
    model, preprocess = _ensure_clip_backbone_for_mining()
    if model is None or preprocess is None:
        warnings.append("clip_unavailable")
        return detections, warnings
    filtered: List[Dict[str, Any]] = []
    exemplars_mat = np.stack(list(exemplar_embeddings.values()))
    ex_norm = np.linalg.norm(exemplars_mat, axis=1, keepdims=True) + 1e-8
    exemplars_mat = exemplars_mat / ex_norm

    neg_mat = None
    if negative_embeddings:
        neg_mat = np.stack(list(negative_embeddings.values()))
        neg_norm = np.linalg.norm(neg_mat, axis=1, keepdims=True) + 1e-8
        neg_mat = neg_mat / neg_norm
    device_to_use = device if isinstance(device, str) or isinstance(device, torch.device) else "cpu"
    def _encode_image(image: Image.Image) -> Optional[np.ndarray]:
        try:
            target_dtype = next(model.parameters()).dtype
        except Exception:
            target_dtype = torch.float32
        try:
            inp = preprocess(image).unsqueeze(0).to(device_to_use)
            if inp.dtype != target_dtype:
                inp = inp.to(dtype=target_dtype)
            with torch.no_grad():
                feats = model.encode_image(inp)
            feats = feats.to(dtype=torch.float32, device="cpu")
            feats = feats / (feats.norm(dim=-1, keepdim=True) + 1e-8)
            return feats.squeeze(0).cpu().numpy()
        except Exception as exc:
            logger.debug("CLIP encode failed: %s", exc)
            return None
    for det in detections[:max_regions]:
        bbox = det.get("bbox")
        if not bbox or len(bbox) < 4:
            continue
        try:
            img_id = int(det.get("image_id"))
        except Exception:
            continue
        info = images.get(img_id) or {}
        path = info.get("path")
        if not path:
            continue
        try:
            pil_img = Image.open(path).convert("RGB")
        except Exception:
            continue
        try:
            bbox4 = list(map(float, bbox[:4]))
            if all(0.0 <= v <= 1.0 for v in bbox4):  # YOLO (cx, cy, w, h) normalized
                left, top, right, bottom = yolo_to_corners(bbox4, pil_img.width, pil_img.height)
            else:  # COCO-style pixel xywh
                x, y, w, h = map(float, bbox[:4])
                left = int(round(x))
                top = int(round(y))
                right = int(round(x + w))
                bottom = int(round(y + h))
                left = max(0, min(pil_img.width, left))
                top = max(0, min(pil_img.height, top))
                right = max(left, min(pil_img.width, right))
                bottom = max(top, min(pil_img.height, bottom))
            if right <= left or bottom <= top:
                continue
            crop = pil_img.crop((left, top, right, bottom))
            emb = _encode_image(crop)
            if emb is None:
                continue
            sims = (exemplars_mat @ emb.reshape(-1, 1)).squeeze()
            max_sim = float(np.max(sims)) if sims.size else 0.0
            max_neg_sim = 0.0
            if neg_mat is not None:
                neg_sims = (neg_mat @ emb.reshape(-1, 1)).squeeze()
                max_neg_sim = float(np.max(neg_sims)) if neg_sims.size else 0.0
            score = max_sim - max(0.0, negative_strength) * max_neg_sim
            if max_sim >= similarity_floor and score >= 0:
                filtered.append(det)
        except Exception as exc:
            logger.debug("CLIP filter crop failed: %s", exc)
            continue
    if len(filtered) < len(detections):
        warnings.append("clip_fp_filtered")
    return filtered if filtered else detections, warnings


def _k_center_select(regions: List[Dict[str, Any]], embeds: Dict[str, np.ndarray], k: int) -> List[Dict[str, Any]]:
    """Greedy k-center over normalized embeddings keyed by region['embed_id']."""
    if k <= 0 or not regions or not embeds:
        return []
    keyed = [(r, embeds.get(r.get("embed_id", ""))) for r in regions if r.get("embed_id") in embeds]
    keyed = [(r, e) for r, e in keyed if e is not None]
    if not keyed:
        return []
    vecs = np.stack([e for _, e in keyed])
    areas = np.array([float(r.get("area", 0.0)) for r, _ in keyed])
    start_idx = int(np.argmax(areas))
    selected_indices = [start_idx]
    dists = np.ones(len(keyed), dtype=np.float32)
    dists *= np.inf
    dists = np.minimum(dists, 1.0 - (vecs @ vecs[start_idx]))
    while len(selected_indices) < min(k, len(keyed)):
        next_idx = int(np.argmax(dists))
        if not np.isfinite(dists[next_idx]):
            break
        selected_indices.append(next_idx)
        dists = np.minimum(dists, 1.0 - (vecs @ vecs[next_idx]))
    return [keyed[i][0] for i in selected_indices]


def _persist_qwen_run_metadata(
    result_path: Path,
    config: QwenTrainingConfig,
    training_result: QwenTrainingResult,
) -> Dict[str, Any]:
    dataset_meta = training_result.metadata or {}
    metadata = {
        "id": config.run_name or result_path.name,
        "label": config.run_name or result_path.name,
        "system_prompt": config.system_prompt,
        "dataset_context": dataset_meta.get("context", ""),
        "classes": dataset_meta.get("classes", []) or [],
        "model_id": config.model_id,
        "training_mode": getattr(config, "training_mode", None),
        "model_family": "qwen3",
        "min_pixels": config.min_pixels,
        "max_pixels": config.max_pixels,
        "max_length": config.max_length,
        "lora_rank": config.lora_rank,
        "lora_alpha": config.lora_alpha,
        "lora_dropout": config.lora_dropout,
        "lora_target_modules": list(config.lora_target_modules or []),
        "created_at": time.time(),
        "latest_checkpoint": training_result.latest_checkpoint,
        "source_dataset": config.dataset_root,
    }
    _write_qwen_metadata(result_path / QWEN_METADATA_FILENAME, metadata)
    return metadata


def _persist_qwen_dataset_metadata(dataset_root: Path, metadata: Dict[str, Any]) -> None:
    meta_path = dataset_root / QWEN_METADATA_FILENAME
    try:
        with meta_path.open("w", encoding="utf-8") as handle:
            json.dump(metadata, handle, ensure_ascii=False, indent=2)
    except Exception as exc:  # noqa: BLE001
        logger.warning("Failed to write Qwen dataset metadata for %s: %s", dataset_root, exc)


def _list_qwen_dataset_entries() -> List[Dict[str, Any]]:
    entries: List[Dict[str, Any]] = []
    if not QWEN_DATASET_ROOT.exists():
        return entries
    for path in QWEN_DATASET_ROOT.iterdir():
        if not path.is_dir():
            continue
        metadata = _load_qwen_dataset_metadata(path)
        if not metadata:
            continue
        metadata, signature = _ensure_qwen_dataset_signature(path, metadata)
        entry = {
            "id": metadata.get("id") or path.name,
            "label": metadata.get("label") or path.name,
            "dataset_root": str(path),
            "created_at": metadata.get("created_at"),
            "image_count": metadata.get("image_count"),
            "train_count": metadata.get("train_count"),
            "val_count": metadata.get("val_count"),
            "classes": metadata.get("classes", []),
            "context": metadata.get("context", ""),
            "signature": signature,
            "type": metadata.get("type", "bbox"),
        }
        entries.append(entry)
    entries.sort(key=lambda item: item.get("created_at") or 0, reverse=True)
    return entries


def _load_qwen_run_metadata(run_dir: Path) -> Optional[Dict[str, Any]]:
    meta_path = run_dir / QWEN_METADATA_FILENAME
    if not meta_path.exists():
        return None
    try:
        with meta_path.open("r", encoding="utf-8") as handle:
            data = json.load(handle)
            if isinstance(data, dict):
                data.setdefault("id", run_dir.name)
                return data
    except Exception as exc:  # noqa: BLE001
        logger.warning("Failed to read Qwen metadata from %s: %s", meta_path, exc)
    return None


def _infer_qwen_run_metadata_from_artifacts(run_dir: Path) -> Optional[Dict[str, Any]]:
    latest_dir = run_dir / "latest"
    if not latest_dir.exists():
        return None
    dataset_dir = QWEN_DATASET_ROOT / run_dir.name
    dataset_meta = _load_qwen_dataset_metadata(dataset_dir) or {}
    adapter_config_path = latest_dir / "adapter_config.json"
    adapter_meta: Dict[str, Any] = {}
    if adapter_config_path.exists():
        try:
            with adapter_config_path.open("r", encoding="utf-8") as handle:
                data = json.load(handle)
                if isinstance(data, dict):
                    adapter_meta = data
        except Exception as exc:  # noqa: BLE001
            logger.warning("Failed to read adapter config for %s: %s", adapter_config_path, exc)
    base_model_id = (
        adapter_meta.get("base_model_name_or_path")
        or dataset_meta.get("model_id")
        or QWEN_MODEL_NAME
    )
    model_family = "qwen3" if "qwen3" in str(base_model_id).lower() else "legacy"
    metadata = {
        "id": run_dir.name,
        "label": dataset_meta.get("label") or dataset_meta.get("id") or run_dir.name,
        "system_prompt": dataset_meta.get("system_prompt", DEFAULT_SYSTEM_PROMPT),
        "dataset_context": dataset_meta.get("context", ""),
        "classes": dataset_meta.get("classes", []) or [],
        "model_id": base_model_id,
        "training_mode": dataset_meta.get("training_mode"),
        "model_family": model_family,
        "min_pixels": dataset_meta.get("min_pixels", QWEN_MIN_PIXELS),
        "max_pixels": dataset_meta.get("max_pixels", QWEN_MAX_PIXELS),
        "lora_rank": adapter_meta.get("r"),
        "lora_alpha": adapter_meta.get("lora_alpha"),
        "lora_dropout": adapter_meta.get("lora_dropout"),
        "lora_target_modules": adapter_meta.get("target_modules", []),
        "created_at": dataset_meta.get("created_at") or run_dir.stat().st_mtime,
        "latest_checkpoint": str(latest_dir),
        "source_dataset": str(dataset_dir) if dataset_dir.exists() else None,
    }
    return metadata


def _load_or_repair_qwen_run_metadata(run_dir: Path) -> Optional[Dict[str, Any]]:
    metadata = _load_qwen_run_metadata(run_dir)
    if metadata:
        return metadata
    inferred = _infer_qwen_run_metadata_from_artifacts(run_dir)
    if inferred:
        _write_qwen_metadata(run_dir / QWEN_METADATA_FILENAME, inferred)
    return inferred


def _load_qwen_dataset_metadata(dataset_dir: Path) -> Optional[Dict[str, Any]]:
    meta_path = dataset_dir / QWEN_METADATA_FILENAME
    if not meta_path.exists():
        return None
    try:
        with meta_path.open("r", encoding="utf-8") as handle:
            data = json.load(handle)
            if isinstance(data, dict):
                updated = False
                if "id" not in data:
                    data["id"] = dataset_dir.name
                    updated = True
                if "type" not in data:
                    data["type"] = "bbox"
                    updated = True
                if updated:
                    _persist_qwen_dataset_metadata(dataset_dir, data)
                return data
    except Exception as exc:  # noqa: BLE001
        logger.warning("Failed to read Qwen dataset metadata from %s: %s", meta_path, exc)
    return None


def _normalize_qwen_image_rel(src_path: Path, dataset_root: Path, fallback: Path) -> Path:
    try:
        rel = src_path.relative_to(dataset_root)
    except Exception:
        rel = fallback
    parts = list(rel.parts)
    if parts and parts[0] in {"train", "val"}:
        parts = parts[1:]
    if parts and parts[0] == "images":
        parts = parts[1:]
    if not parts:
        parts = [fallback.name]
    return Path(*parts)


def _prepare_qwen_training_split(
    dataset_root: Path,
    job_id: str,
    *,
    random_split: bool,
    val_percent: float,
    split_seed: int,
    train_limit: Optional[int] = None,
    val_limit: Optional[int] = None,
    log_messages: Optional[List[str]] = None,
) -> Path:
    if not random_split:
        return dataset_root
    meta = _load_qwen_dataset_metadata(dataset_root) or {}
    entries: List[Dict[str, Any]] = []
    for split in ("train", "val"):
        jsonl_path = dataset_root / split / "annotations.jsonl"
        if not jsonl_path.exists():
            continue
        try:
            with jsonl_path.open("r", encoding="utf-8") as handle:
                for line in handle:
                    raw = line.strip()
                    if not raw:
                        continue
                    try:
                        payload = json.loads(raw)
                    except Exception:
                        continue
                    image_rel = payload.get("image")
                    if not isinstance(image_rel, str) or not image_rel.strip():
                        continue
                    rel_path = Path(image_rel.strip())
                    candidates = [
                        dataset_root / split / rel_path,
                        dataset_root / split / "images" / rel_path,
                        dataset_root / "images" / rel_path,
                        dataset_root / "train" / rel_path,
                        dataset_root / "val" / rel_path,
                        dataset_root / rel_path,
                        dataset_root / "train" / "images" / rel_path,
                        dataset_root / "val" / "images" / rel_path,
                    ]
                    src_path = next((p for p in candidates if p.exists()), None)
                    if src_path is None:
                        continue
                    normalized_rel = _normalize_qwen_image_rel(src_path, dataset_root, rel_path)
                    entries.append(
                        {
                            "raw": raw,
                            "image_rel": normalized_rel,
                            "src": src_path,
                        }
                    )
        except Exception as exc:  # noqa: BLE001
            logger.warning("Failed to read Qwen annotations from %s: %s", jsonl_path, exc)
    if not entries:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="qwen_training_no_annotations")
    rnd = random.Random(split_seed)
    rnd.shuffle(entries)
    total = len(entries)
    vp = max(0.0, min(float(val_percent), 0.9))
    val_count = int(total * vp)
    if val_count <= 0 and total > 1:
        val_count = 1
    if val_limit is not None and val_limit > 0:
        val_count = min(val_count if val_count > 0 else val_limit, val_limit, total - 1 if total > 1 else total)
    val_entries = entries[:val_count]
    train_entries = entries[val_count:]
    if train_limit is not None and train_limit > 0:
        train_entries = train_entries[:train_limit]
    if not train_entries or not val_entries:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="qwen_training_split_empty")
    split_root = (QWEN_JOB_ROOT / "splits" / job_id).resolve()
    split_root.parent.mkdir(parents=True, exist_ok=True)
    if split_root.exists():
        shutil.rmtree(split_root, ignore_errors=True)
    (split_root / "train" / "images").mkdir(parents=True, exist_ok=True)
    (split_root / "val" / "images").mkdir(parents=True, exist_ok=True)
    counts = {"train": 0, "val": 0}
    for split_name, split_entries in (("train", train_entries), ("val", val_entries)):
        ann_path = split_root / split_name / "annotations.jsonl"
        with ann_path.open("w", encoding="utf-8") as handle:
            for entry in split_entries:
                dst = split_root / split_name / "images" / entry["image_rel"]
                _link_or_copy_file(entry["src"], dst)
                try:
                    record = json.loads(entry["raw"])
                except Exception:
                    record = {"image": ""}
                record["image"] = entry["image_rel"].as_posix()
                handle.write(json.dumps(record, ensure_ascii=False) + "\n")
                counts[split_name] += 1
    new_meta = {
        **meta,
        "id": meta.get("id") or dataset_root.name,
        "label": meta.get("label") or meta.get("id") or dataset_root.name,
        "classes": meta.get("classes") or _load_qwen_labelmap(dataset_root),
        "context": meta.get("context") or "",
        "created_at": meta.get("created_at") or time.time(),
        "train_count": counts["train"],
        "val_count": counts["val"],
        "image_count": counts["train"] + counts["val"],
    }
    _persist_qwen_dataset_metadata(split_root, new_meta)
    split_summary = (
        f"Qwen split: {counts['train']} train / {counts['val']} val "
        f"(seed={split_seed}, val_percent={vp:.2f}, src={dataset_root}) -> {split_root}"
    )
    logger.info(split_summary)
    if log_messages is not None:
        log_messages.append(split_summary)
    return split_root


def _list_qwen_model_entries() -> List[Dict[str, Any]]:
    entries: List[Dict[str, Any]] = []
    for path in QWEN_JOB_ROOT.iterdir():
        if not path.is_dir() or path.name == QWEN_DATASET_ROOT.name:
            continue
        latest = path / "latest"
        if not latest.exists():
            continue
        metadata = _load_or_repair_qwen_run_metadata(path)
        if not metadata:
            continue
        entries.append(
            {
                "id": metadata.get("id") or path.name,
                "label": metadata.get("label") or metadata.get("run_name") or path.name,
                "path": str(latest),
                "created_at": metadata.get("created_at"),
                "metadata": metadata,
                "type": "trained",
            }
        )
    entries.sort(key=lambda item: item.get("created_at") or 0, reverse=True)
    return entries


def _get_qwen_model_entry(model_id: str) -> Optional[Dict[str, Any]]:
    for entry in _list_qwen_model_entries():
        if entry.get("id") == model_id:
            return entry
    return None


def _build_qwen_config(payload: QwenTrainRequest, job_id: str, job_logs: Optional[List[str]] = None) -> QwenTrainingConfig:
    if QWEN_TRAINING_IMPORT_ERROR is not None or QwenTrainingConfig is None:
        raise HTTPException(
            status_code=HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"qwen_training_unavailable:{QWEN_TRAINING_IMPORT_ERROR}",
        )
    dataset_root = _resolve_sam3_or_qwen_dataset(str(payload.dataset_id))
    if not dataset_root.is_dir():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="dataset_root_not_found")
    qwen_meta = _load_qwen_dataset_metadata(dataset_root)
    if not qwen_meta:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="dataset_qwen_not_ready")
    if not (dataset_root / "train" / "annotations.jsonl").exists() or not (dataset_root / "val" / "annotations.jsonl").exists():
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="dataset_qwen_not_ready")
    val_percent = payload.val_percent if payload.val_percent is not None else 0.3
    split_seed = int(payload.split_seed) if payload.split_seed is not None else 42
    random_split = payload.random_split if payload.random_split is not None else True
    train_limit = int(payload.train_limit) if payload.train_limit is not None and payload.train_limit > 0 else None
    val_limit = int(payload.val_limit) if payload.val_limit is not None and payload.val_limit > 0 else None
    dataset_root = _prepare_qwen_training_split(
        dataset_root,
        job_id,
        random_split=random_split,
        val_percent=val_percent,
        split_seed=split_seed,
        train_limit=train_limit,
        val_limit=val_limit,
        log_messages=job_logs,
    )
    train_dir = dataset_root / "train"
    val_dir = dataset_root / "val"
    if not train_dir.is_dir() or not val_dir.is_dir():
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="dataset_missing_train_val")
    meta = _load_qwen_dataset_metadata(dataset_root) or {}
    if meta.get("train_count", 0) <= 0 or meta.get("val_count", 0) <= 0:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="dataset_empty_split")
    if not meta.get("classes"):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="dataset_missing_classes")
    run_name = payload.run_name or f"qwen_run_{job_id}"
    result_path = (QWEN_JOB_ROOT / run_name).resolve()
    if not str(result_path).startswith(str(QWEN_JOB_ROOT.resolve())):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="run_path_invalid")
    if result_path.exists():
        raise HTTPException(status_code=HTTP_409_CONFLICT, detail="run_name_exists")
    system_prompt = (payload.system_prompt or DEFAULT_SYSTEM_PROMPT).strip() or DEFAULT_SYSTEM_PROMPT
    training_mode = payload.training_mode or "official_lora"
    cfg_kwargs: Dict[str, Any] = {
        "dataset_root": str(dataset_root),
        "result_path": str(result_path),
        "model_id": payload.model_id or QWEN_MODEL_NAME,
        "run_name": run_name,
        "system_prompt": system_prompt,
        "training_mode": training_mode,
    }
    if payload.devices is not None:
        try:
            device_ids = _parse_device_ids_string(payload.devices)
        except ValueError as exc:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=str(exc)) from exc
        if device_ids is not None:
            _validate_cuda_device_ids(device_ids)
            cfg_kwargs["devices"] = ",".join(str(device) for device in device_ids)
    defaults = {
        "batch_size": payload.batch_size,
        "max_epochs": payload.max_epochs,
        "lr": payload.lr,
        "accumulate_grad_batches": payload.accumulate_grad_batches,
        "warmup_steps": payload.warmup_steps,
        "num_workers": payload.num_workers,
        "lora_rank": payload.lora_rank,
        "lora_alpha": payload.lora_alpha,
        "lora_dropout": payload.lora_dropout,
        "lora_target_modules": payload.lora_target_modules,
        "log_every_n_steps": payload.log_every_n_steps,
        "min_pixels": payload.min_pixels,
        "max_pixels": payload.max_pixels,
        "max_length": payload.max_length,
        "seed": payload.seed,
    }
    for key, value in defaults.items():
        if value is not None:
            cfg_kwargs[key] = value
    config = QwenTrainingConfig(**cfg_kwargs)
    estimate_mb, estimate_note = _estimate_qwen_vram_mb(
        config.model_id,
        config.training_mode,
        max_pixels=config.max_pixels,
        batch_size=config.batch_size,
    )
    config.vram_estimate_mb = estimate_mb
    config.vram_estimate_note = estimate_note
    if job_logs is not None and estimate_mb:
        gpu_total_mb = None
        if torch.cuda.is_available():
            try:
                _, total_bytes = torch.cuda.mem_get_info()
                gpu_total_mb = _bytes_to_mb(int(total_bytes))
            except Exception:
                gpu_total_mb = None
        gpu_note = f" (GPU total ~{gpu_total_mb:.0f} MB)" if gpu_total_mb else ""
        job_logs.append(
            f"Estimated VRAM: ~{estimate_mb:.0f} MB for {config.model_id} ({config.training_mode}){gpu_note}."
        )
        if estimate_note:
            job_logs.append(f"VRAM note: {estimate_note}")
    return config


def _normalise_relative_path(name: Optional[str]) -> Path:
    candidate = (name or "").replace("\\", "/")
    path = Path(candidate)
    parts = []
    for part in path.parts:
        if part in ("", ".", ".."):
            continue
        if part.endswith(":"):
            continue
        parts.append(part)
    if not parts:
        fallback = Path(candidate).name or f"file_{uuid.uuid4().hex}"
        parts = [fallback]
    return Path(*parts)


def _safe_extract_zip(
    zf: zipfile.ZipFile,
    dest_dir: Path,
    *,
    strip_root: bool = False,
    max_bytes_per_file: Optional[int] = None,
    total_quota_bytes: Optional[int] = None,
) -> None:
    """Safely extract a zip into dest_dir, rejecting paths that escape."""
    dest_dir = dest_dir.resolve()
    extracted_bytes = 0
    for member in zf.namelist():
        # Skip directories explicitly; we'll create as needed.
        if not member or member.endswith("/"):
            continue
        try:
            info = zf.getinfo(member)
            file_size = getattr(info, "file_size", 0)
        except KeyError:
            file_size = 0
        if max_bytes_per_file and file_size > max_bytes_per_file:
            raise HTTPException(status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="zip_entry_too_large")
        if total_quota_bytes and extracted_bytes + file_size > total_quota_bytes:
            raise HTTPException(status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="zip_quota_exceeded")
        rel = _normalise_relative_path(member)
        if strip_root and len(rel.parts) > 1:
            rel = Path(*rel.parts[1:])
        target = (dest_dir / rel).resolve()
        if not str(target).startswith(str(dest_dir)):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="zip_entry_invalid_path")
        target.parent.mkdir(parents=True, exist_ok=True)
        with zf.open(member, "r") as src, target.open("wb") as dst:
            shutil.copyfileobj(src, dst)
        extracted_bytes += max(file_size, 0)


def _link_or_copy_file(src: Path, dst: Path, *, overwrite: bool = False) -> None:
    dst.parent.mkdir(parents=True, exist_ok=True)
    try:
        if src.resolve() == dst.resolve():
            return
    except Exception:
        pass
    if dst.exists():
        if not overwrite:
            return
        try:
            dst.unlink()
        except Exception:
            return
    try:
        os.link(src, dst)
    except Exception:
        shutil.copy2(src, dst)


def _dir_size_bytes(path: Path) -> int:
    if not path.exists():
        return 0
    total = 0
    for p in path.rglob("*"):
        try:
            if p.is_file():
                total += p.stat().st_size
        except Exception:
            continue
    return total


def _purge_directory(path: Path) -> int:
    if not path.exists():
        return 0
    deleted = 0
    for p in sorted(path.rglob("*"), key=lambda x: len(x.parts), reverse=True):
        try:
            if p.is_file():
                deleted += p.stat().st_size
                p.unlink()
            elif p.is_dir():
                p.rmdir()
        except Exception:
            continue
    try:
        path.rmdir()
    except Exception:
        pass
    return deleted


def _get_clip_dataset_job(job_id: str) -> ClipDatasetUploadJob:
    with CLIP_DATASET_JOBS_LOCK:
        job = CLIP_DATASET_JOBS.get(job_id)
    if not job:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="clip_dataset_job_not_found")
    return job


def _pop_clip_dataset_job(job_id: str) -> ClipDatasetUploadJob:
    with CLIP_DATASET_JOBS_LOCK:
        job = CLIP_DATASET_JOBS.pop(job_id, None)
    if not job:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="clip_dataset_job_not_found")
    return job


def _get_qwen_dataset_job(job_id: str) -> QwenDatasetUploadJob:
    with QWEN_DATASET_JOBS_LOCK:
        job = QWEN_DATASET_JOBS.get(job_id)
    if not job:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="qwen_dataset_job_not_found")
    return job


def _pop_qwen_dataset_job(job_id: str) -> QwenDatasetUploadJob:
    with QWEN_DATASET_JOBS_LOCK:
        job = QWEN_DATASET_JOBS.pop(job_id, None)
    if not job:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="qwen_dataset_job_not_found")
    return job


def _get_qwen_job(job_id: str) -> QwenTrainingJob:
    with QWEN_TRAINING_JOBS_LOCK:
        job = QWEN_TRAINING_JOBS.get(job_id)
        if not job:
            raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="qwen_job_not_found")
        return job


@app.post("/clip/dataset/init")
def clip_dataset_init():
    with CLIP_DATASET_JOBS_LOCK:
        active_roots = {str(j.root_dir) for j in CLIP_DATASET_JOBS.values()}
    _purge_staging_dirs(CLIP_DATASET_UPLOAD_ROOT, active_roots=active_roots)
    job_id = uuid.uuid4().hex
    root = (CLIP_DATASET_UPLOAD_ROOT / job_id).resolve()
    images_dir = root / "images"
    labels_dir = root / "labels"
    images_dir.mkdir(parents=True, exist_ok=True)
    labels_dir.mkdir(parents=True, exist_ok=True)
    job = ClipDatasetUploadJob(job_id=job_id, root_dir=root, images_dir=images_dir, labels_dir=labels_dir)
    with CLIP_DATASET_JOBS_LOCK:
        CLIP_DATASET_JOBS[job_id] = job
    return {"job_id": job_id}


@app.post("/clip/dataset/chunk")
async def clip_dataset_chunk(
    job_id: str = Form(...),
    kind: str = Form(...),
    relative_path: Optional[str] = Form(None),
    file: UploadFile = File(...),
):
    job = _get_clip_dataset_job(job_id)
    if job.completed:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="clip_dataset_job_finalized")
    kind_lower = kind.strip().lower()
    if kind_lower not in {"image", "label"}:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="invalid_chunk_kind")
    filename = relative_path or file.filename or f"{kind_lower}_{uuid.uuid4().hex}"
    normalised = _normalise_relative_path(filename)
    target_dir = job.images_dir if kind_lower == "image" else job.labels_dir
    dest_path = (target_dir / normalised).resolve()
    if not str(dest_path).startswith(str(job.root_dir)):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="invalid_relative_path")
    await _write_upload_file(
        file,
        dest_path,
        max_bytes=CLIP_DATASET_CHUNK_MAX_BYTES,
        quota_root=job.root_dir,
        quota_limit=CLIP_DATASET_UPLOAD_QUOTA_BYTES,
        allow_overwrite=True,  # allow idempotent retries for same relative_path
    )
    with CLIP_DATASET_JOBS_LOCK:
        if kind_lower == "image":
            job.image_count += 1
        else:
            job.label_count += 1
    return {"status": "ok", "images": job.image_count, "labels": job.label_count}


@app.post("/clip/dataset/finalize")
def clip_dataset_finalize(job_id: str = Form(...)):
    job = _pop_clip_dataset_job(job_id)
    job.completed = True
    if job.image_count == 0:
        shutil.rmtree(job.root_dir, ignore_errors=True)
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="dataset_images_missing")
    return {
        "images_path": str(job.images_dir),
        "labels_path": str(job.labels_dir),
        "temp_dir": str(job.root_dir),
        "images": job.image_count,
        "labels": job.label_count,
    }


@app.post("/clip/dataset/cancel")
def clip_dataset_cancel(job_id: str = Form(...)):
    job = None
    with CLIP_DATASET_JOBS_LOCK:
        job = CLIP_DATASET_JOBS.pop(job_id, None)
    if job:
        shutil.rmtree(job.root_dir, ignore_errors=True)
    return {"status": "cancelled"}


@app.post("/qwen/dataset/init")
def qwen_dataset_init(run_name: Optional[str] = Form(None)):
    with QWEN_DATASET_JOBS_LOCK:
        active_roots = {str(j.root_dir) for j in QWEN_DATASET_JOBS.values()}
    # Limit purge to staging_* dirs to avoid deleting real datasets.
    _purge_staging_dirs(QWEN_DATASET_ROOT, active_roots=active_roots, prefix="staging_")
    job_id = uuid.uuid4().hex
    staging_dir = (QWEN_DATASET_ROOT / f"staging_{job_id}").resolve()
    train_dir = staging_dir / "train"
    val_dir = staging_dir / "val"
    train_dir.mkdir(parents=True, exist_ok=True)
    val_dir.mkdir(parents=True, exist_ok=True)
    train_annotations = train_dir / "annotations.jsonl"
    val_annotations = val_dir / "annotations.jsonl"
    train_annotations.touch()
    val_annotations.touch()
    job = QwenDatasetUploadJob(
        job_id=job_id,
        root_dir=staging_dir,
        train_dir=train_dir,
        val_dir=val_dir,
        train_annotations=train_annotations,
        val_annotations=val_annotations,
        run_name=run_name,
    )
    with QWEN_DATASET_JOBS_LOCK:
        QWEN_DATASET_JOBS[job_id] = job
    logger.info("[qwen-dataset %s] init run_name=%s root=%s", job_id[:8], run_name or "", staging_dir)
    return {"job_id": job_id}


@app.post("/qwen/dataset/chunk")
async def qwen_dataset_chunk(
    job_id: str = Form(...),
    split: str = Form(...),
    image_name: Optional[str] = Form(None),
    annotation_line: Optional[str] = Form(None),
    file: UploadFile = File(...),
):
    job = _get_qwen_dataset_job(job_id)
    if job.completed:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="qwen_dataset_job_finalized")
    split_lower = (split or "").strip().lower()
    if split_lower not in {"train", "val"}:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="invalid_split")
    target_dir = job.train_dir if split_lower == "train" else job.val_dir
    target_annotations = job.train_annotations if split_lower == "train" else job.val_annotations
    name = image_name or file.filename or f"{split_lower}_{uuid.uuid4().hex}"
    normalised = _normalise_relative_path(name)
    dest_path = (target_dir / normalised).resolve()
    if not str(dest_path).startswith(str(job.root_dir)):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="invalid_relative_path")
    await _write_upload_file(
        file,
        dest_path,
        max_bytes=QWEN_DATASET_CHUNK_MAX_BYTES,
        quota_root=job.root_dir,
        quota_limit=QWEN_DATASET_UPLOAD_QUOTA_BYTES,
        allow_overwrite=True,  # allow idempotent retries
    )
    line = (annotation_line or "").strip()
    if not line:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="annotation_required")
    with target_annotations.open("a", encoding="utf-8") as handle:
        handle.write(line.rstrip("\n") + "\n")
    with QWEN_DATASET_JOBS_LOCK:
        if split_lower == "train":
            job.train_count += 1
        else:
            job.val_count += 1
        train_count = job.train_count
        val_count = job.val_count
    size_bytes = None
    try:
        size_bytes = dest_path.stat().st_size
    except OSError:
        size_bytes = None
    logger.info(
        "[qwen-dataset %s] chunk split=%s image=%s size=%sB train=%d val=%d",
        job_id[:8],
        split_lower,
        normalised,
        size_bytes if size_bytes is not None else "unknown",
        train_count,
        val_count,
    )
    return {"status": "ok", "train": train_count, "val": val_count}


@app.post("/qwen/dataset/finalize")
def qwen_dataset_finalize(
    job_id: str = Form(...),
    metadata: str = Form(...),
    run_name: Optional[str] = Form(None),
):
    job = _pop_qwen_dataset_job(job_id)
    try:
        meta_obj = json.loads(metadata)
        if not isinstance(meta_obj, dict):
            raise ValueError("metadata_not_dict")
    except Exception as exc:  # noqa: BLE001
        shutil.rmtree(job.root_dir, ignore_errors=True)
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"metadata_invalid:{exc}") from exc
    meta_path = job.root_dir / "dataset_meta.json"
    try:
        with meta_path.open("w", encoding="utf-8") as handle:
            json.dump(meta_obj, handle, ensure_ascii=False, indent=2)
    except Exception as exc:  # noqa: BLE001
        shutil.rmtree(job.root_dir, ignore_errors=True)
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"metadata_write_failed:{exc}") from exc
    signature = _compute_dir_signature(job.root_dir)
    existing = _find_qwen_dataset_by_signature(signature)
    if existing is not None:
        shutil.rmtree(job.root_dir, ignore_errors=True)
        existing_meta = _load_qwen_dataset_metadata(existing) or {}
        existing_meta, _ = _ensure_qwen_dataset_signature(existing, existing_meta)
        logger.info(
            "[qwen-dataset %s] reused existing dataset=%s (signature match)",
            job_id[:8],
            existing.name,
        )
        return {
            "dataset_root": str(existing),
            "run_name": existing.name,
            "metadata": existing_meta,
            "reused": True,
        }
    desired_name = run_name or job.run_name or f"dataset_{job_id}"
    safe_name = re.sub(r"[^A-Za-z0-9._-]", "_", desired_name).strip("_") or f"dataset_{job_id}"
    dest_dir = (QWEN_DATASET_ROOT / safe_name).resolve()
    if dest_dir.exists():
        # Never overwrite an existing dataset on disk. If a name collision happens
        # (common with defaults like "qwen_dataset"), pick a unique suffix.
        alt_name = f"{safe_name}_{job_id[:6]}"
        dest_dir = (QWEN_DATASET_ROOT / alt_name).resolve()
        if dest_dir.exists():
            alt_name = f"{safe_name}_{job_id[:6]}_{uuid.uuid4().hex[:4]}"
            dest_dir = (QWEN_DATASET_ROOT / alt_name).resolve()
        safe_name = alt_name
    shutil.move(str(job.root_dir), str(dest_dir))
    job.completed = True
    qwen_format_version = "qwen3_conversation_v1"
    dataset_meta = {
        "id": safe_name,
        "label": meta_obj.get("label") or safe_name,
        "classes": meta_obj.get("classes") or [],
        "context": meta_obj.get("context") or "",
        "created_at": time.time(),
        "image_count": job.train_count + job.val_count,
        "record_count": job.train_count + job.val_count,
        "train_count": job.train_count,
        "val_count": job.val_count,
        "signature": signature,
        "qwen": {
            "format_version": qwen_format_version,
            "model_family": "qwen3",
        },
    }
    _persist_qwen_dataset_metadata(dest_dir, dataset_meta)
    logger.info(
        "[qwen-dataset %s] finalized dataset=%s train=%d val=%d meta=%s",
        job_id[:8],
        safe_name,
        job.train_count,
        job.val_count,
        json.dumps(meta_obj, ensure_ascii=False),
    )
    return {"dataset_root": str(dest_dir), "run_name": safe_name, "metadata": dataset_meta, "reused": False}


@app.post("/qwen/dataset/cancel")
def qwen_dataset_cancel(job_id: str = Form(...)):
    job = None
    with QWEN_DATASET_JOBS_LOCK:
        job = QWEN_DATASET_JOBS.pop(job_id, None)
    if job:
        shutil.rmtree(job.root_dir, ignore_errors=True)
        logger.info("[qwen-dataset %s] cancelled and cleaned up %s", job_id[:8], job.root_dir)
    return {"status": "cancelled"}


@app.get("/qwen/datasets")
def list_qwen_datasets():
    return _list_qwen_dataset_entries()


@app.delete("/qwen/datasets/{dataset_id}")
def delete_qwen_dataset(dataset_id: str):
    target = (QWEN_DATASET_ROOT / dataset_id).resolve()
    if not str(target).startswith(str(QWEN_DATASET_ROOT.resolve())) or not target.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="qwen_dataset_not_found")
    shutil.rmtree(target, ignore_errors=True)
    _purge_dataset_artifacts(dataset_id)
    return {"status": "deleted"}


def _find_yolo_dataset_root(extracted_dir: Path) -> Optional[Path]:
    candidates: List[Path] = [extracted_dir]
    for child in extracted_dir.iterdir():
        if child.is_dir():
            candidates.append(child)
    for candidate in candidates:
        labelmap_path = candidate / "labelmap.txt"
        train_images = candidate / "train" / "images"
        train_labels = candidate / "train" / "labels"
        val_images = candidate / "val" / "images"
        val_labels = candidate / "val" / "labels"
        root_images = candidate / "images"
        root_labels = candidate / "labels"
        if not labelmap_path.exists():
            continue
        if (train_images.exists() and train_labels.exists()) or (root_images.exists() and root_labels.exists()):
            return candidate
    return None


def _count_images_in_dir(images_dir: Path) -> int:
    if not images_dir.exists():
        return 0
    exts = {".jpg", ".jpeg", ".png", ".bmp", ".webp", ".tif", ".tiff"}
    count = 0
    for path in images_dir.rglob("*"):
        if path.is_file() and path.suffix.lower() in exts:
            count += 1
    return count


def _count_dataset_images(dataset_root: Path) -> int:
    train_images = dataset_root / "train" / "images"
    val_images = dataset_root / "val" / "images"
    root_images = dataset_root / "images"
    count = 0
    if train_images.exists():
        count += _count_images_in_dir(train_images)
        if val_images.exists():
            count += _count_images_in_dir(val_images)
        return count
    if root_images.exists():
        return _count_images_in_dir(root_images)
    return 0


def _count_caption_labels(dataset_root: Path) -> Tuple[int, bool]:
    captions_dir = dataset_root / "text_labels"
    if not captions_dir.exists():
        return 0, False
    stems: set[str] = set()
    jsonl_path = captions_dir / "captions.jsonl"
    if jsonl_path.exists():
        try:
            with jsonl_path.open("r", encoding="utf-8") as handle:
                for line in handle:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        payload = json.loads(line)
                    except Exception:
                        continue
                    image_name = payload.get("image")
                    if isinstance(image_name, str) and image_name.strip():
                        stems.add(Path(image_name).stem)
        except Exception:
            pass
    for path in captions_dir.rglob("*.txt"):
        if path.name == "captions.jsonl":
            continue
        stems.add(path.stem)
    return len(stems), True


def _caption_image_stem(image_name: str) -> str:
    cleaned = Path(str(image_name or "").strip()).name
    stem = Path(cleaned).stem
    if not stem:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="caption_image_invalid")
    return stem


def _resolve_caption_dataset_root(dataset_id: str) -> Path:
    entry = _resolve_dataset_entry(dataset_id)
    dataset_root: Optional[Path] = None
    if entry and entry.get("dataset_root"):
        dataset_root = Path(entry["dataset_root"]).resolve()
    if not dataset_root:
        dataset_root = _resolve_sam3_or_qwen_dataset(dataset_id)
    if not dataset_root or not dataset_root.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="dataset_not_found")
    return dataset_root


def _caption_file_path(dataset_root: Path, image_name: str, *, create_dir: bool) -> Path:
    captions_dir = dataset_root / "text_labels"
    if create_dir:
        captions_dir.mkdir(parents=True, exist_ok=True)
    stem = _caption_image_stem(image_name)
    return captions_dir / f"{stem}.txt"


def _save_caption_for_image(dataset_id: str, image_name: str, caption: str) -> None:
    dataset_root = _resolve_caption_dataset_root(dataset_id)
    caption_path = _caption_file_path(dataset_root, image_name, create_dir=True)
    caption_path.write_text(f"{caption.strip()}\n", encoding="utf-8")


class CaptionLabelRequest(BaseModel):
    caption: str = ""


def _infer_yolo_dataset_type(labels_dir: Path, fallback: str = "bbox") -> str:
    if not labels_dir.exists():
        return fallback
    try:
        for txt in labels_dir.rglob("*.txt"):
            with txt.open("r", encoding="utf-8") as handle:
                for line in handle:
                    parts = line.strip().split()
                    if len(parts) > 5:
                        return "seg"
    except Exception:
        return fallback
    return fallback


class QwenDatasetBuildRequest(BaseModel):
    force: bool = False
    context: Optional[str] = None


class DatasetGlossaryPayload(BaseModel):
    glossary: Optional[str] = None


class GlossaryLibraryPayload(BaseModel):
    name: str = ""
    glossary: Optional[str] = None


def _build_qwen_instruction(context_text: str, class_names: List[str]) -> str:
    parts: List[str] = []
    context_text = (context_text or "").strip()
    if context_text:
        parts.append(f"This image shows {context_text}.")
    cleaned = [str(name).strip() for name in (class_names or []) if str(name).strip()]
    if cleaned:
        parts.append(f"Objects of interest: {', '.join(cleaned)}.")
    return " ".join(parts).strip()


def _build_qwen_training_prompt(context_text: str, class_names: List[str], mode: str) -> str:
    instruction = _build_qwen_instruction(context_text, class_names)
    parts: List[str] = []
    if instruction:
        parts.append(instruction)
    parts.append("Return detections for every labeled object.")
    if mode == "point":
        parts.append(
            'Return a JSON object named "detections". Each detection must include "label" and "point" as [x,y] pixel coordinates near the object center. '
            'If nothing is present, respond with {"detections": []}. Respond with JSON only.'
        )
    else:
        parts.append(
            'Return a JSON object named "detections". Each detection must include "label" and "bbox" as [x1,y1,x2,y2] pixel coordinates (integers). '
            'If nothing is present, respond with {"detections": []}. Respond with JSON only.'
        )
    return " ".join(parts).strip()


def _qwen_build_output_payload(detections: List[Dict[str, Any]], mode: str) -> str:
    items: List[Dict[str, Any]] = []
    for det in detections:
        label = det.get("label")
        if not label:
            continue
        if mode == "point":
            point = det.get("point")
            if point:
                items.append({"label": label, "point": point})
        else:
            bbox = det.get("bbox")
            if bbox:
                items.append({"label": label, "bbox": bbox})
    return json.dumps({"detections": items}, ensure_ascii=False)


def _atomic_write_text(path: Path, content: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp_path = path.with_name(f".{path.name}.tmp_{uuid.uuid4().hex[:8]}")
    try:
        tmp_path.write_text(content, encoding="utf-8")
        tmp_path.replace(path)
    finally:
        try:
            tmp_path.unlink(missing_ok=True)
        except Exception:
            pass


def _atomic_write_json(path: Path, payload: Dict[str, Any]) -> None:
    _atomic_write_text(path, json.dumps(payload, ensure_ascii=False, indent=2))


def _iter_yolo_images(images_dir: Path) -> List[Path]:
    if not images_dir.exists():
        return []
    exts = {".jpg", ".jpeg", ".png", ".bmp", ".webp", ".tif", ".tiff"}
    return sorted([p for p in images_dir.rglob("*") if p.is_file() and p.suffix.lower() in exts])


def _load_image_size(image_path: Path) -> Tuple[int, int]:
    try:
        with Image.open(image_path) as im:
            width, height = im.size
            return int(width), int(height)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"image_read_failed:{image_path.name}:{exc}") from exc


def _qwen_det_from_yolo(
    tokens: List[str],
    *,
    labelmap: List[str],
    width: int,
    height: int,
) -> Optional[Dict[str, Any]]:
    if not tokens:
        return None
    try:
        class_idx = int(float(tokens[0]))
    except Exception:
        return None
    if class_idx < 0 or class_idx >= len(labelmap):
        return None
    coords: List[float] = []
    for part in tokens[1:]:
        try:
            coords.append(float(part))
        except Exception:
            return None
    if len(coords) < 4:
        return None
    if len(coords) > 4:
        if len(coords) < 6 or len(coords) % 2 != 0:
            return None
        xs = coords[0::2]
        ys = coords[1::2]
        if not xs or not ys:
            return None
        x1n = max(0.0, min(1.0, min(xs)))
        x2n = max(0.0, min(1.0, max(xs)))
        y1n = max(0.0, min(1.0, min(ys)))
        y2n = max(0.0, min(1.0, max(ys)))
    else:
        cx, cy, w, h = coords[:4]
        if w <= 0 or h <= 0:
            return None
        x1n = cx - w / 2.0
        x2n = cx + w / 2.0
        y1n = cy - h / 2.0
        y2n = cy + h / 2.0
        x1n = max(0.0, min(1.0, x1n))
        x2n = max(0.0, min(1.0, x2n))
        y1n = max(0.0, min(1.0, y1n))
        y2n = max(0.0, min(1.0, y2n))
    if x2n <= x1n or y2n <= y1n:
        return None
    x1 = int(round(x1n * width))
    x2 = int(round(x2n * width))
    y1 = int(round(y1n * height))
    y2 = int(round(y2n * height))
    x1 = max(0, min(width, x1))
    x2 = max(0, min(width, x2))
    y1 = max(0, min(height, y1))
    y2 = max(0, min(height, y2))
    if x2 <= x1 or y2 <= y1:
        return None
    cxp = int(round((x1 + x2) / 2.0))
    cyp = int(round((y1 + y2) / 2.0))
    return {
        "label": labelmap[class_idx],
        "bbox": [x1, y1, x2, y2],
        "point": [cxp, cyp],
    }


def _build_qwen_dataset_from_yolo(dataset_root: Path, *, context_text: str = "", force: bool = False) -> Dict[str, Any]:
    labelmap_path = dataset_root / "labelmap.txt"
    if not labelmap_path.exists():
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="labelmap_txt_missing")
    try:
        labelmap = [line.strip() for line in labelmap_path.read_text(encoding="utf-8").splitlines() if line.strip()]
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"labelmap_txt_invalid:{exc}") from exc
    if not labelmap:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="dataset_missing_classes")

    train_images = dataset_root / "train" / "images"
    train_labels = dataset_root / "train" / "labels"
    if not train_images.exists() or not train_labels.exists():
        root_images = dataset_root / "images"
        root_labels = dataset_root / "labels"
        if root_images.exists() and root_labels.exists():
            train_images = root_images
            train_labels = root_labels
        else:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="yolo_layout_not_found")

    val_images = dataset_root / "val" / "images"
    val_labels = dataset_root / "val" / "labels"

    train_list = _iter_yolo_images(train_images)
    val_list = _iter_yolo_images(val_images) if val_images.exists() else []
    if not train_list and not val_list:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="dataset_missing_images")

    # Ensure both splits have at least one entry by duplicating across splits when needed.
    if not train_list and val_list:
        train_list = [val_list[0]]
    if not val_list and train_list:
        val_list = [train_list[0]]

    instruction = _build_qwen_instruction(context_text, labelmap)
    qwen_format_version = "qwen3_conversation_v1"
    total_detections = 0

    def _label_path_for_image(img_path: Path, primary_images: Path, primary_labels: Path) -> Tuple[Path, Path]:
        candidates: List[Tuple[Path, Path]] = [(primary_images, primary_labels)]
        if train_images != primary_images:
            candidates.append((train_images, train_labels))
        if val_images.exists() and val_labels.exists() and val_images != primary_images:
            candidates.append((val_images, val_labels))
        for img_dir, lbl_dir in candidates:
            try:
                rel = img_path.relative_to(img_dir)
            except Exception:
                continue
            return rel, lbl_dir / rel.with_suffix(".txt")
        return Path(img_path.name), primary_labels / Path(img_path.stem + ".txt")

    def _write_split(split_name: str, images_dir: Path, labels_dir: Path, images_list: List[Path]) -> int:
        nonlocal total_detections
        out_dir = dataset_root / split_name
        out_dir.mkdir(parents=True, exist_ok=True)
        jsonl_path = out_dir / "annotations.jsonl"
        if jsonl_path.exists() and not force:
            try:
                meta = _load_sam3_dataset_metadata(dataset_root) or _load_registry_dataset_metadata(dataset_root) or {}
                qwen_meta = meta.get("qwen") or {}
                if qwen_meta.get("format_version") == qwen_format_version:
                    with jsonl_path.open("r", encoding="utf-8") as handle:
                        return sum(1 for _ in handle if _.strip())
            except Exception:
                pass
            # Assume existing annotations are valid; count entries quickly.
            try:
                with jsonl_path.open("r", encoding="utf-8") as handle:
                    return sum(1 for _ in handle if _.strip())
            except Exception:
                pass
        tmp_path = jsonl_path.with_name(f".{jsonl_path.name}.tmp_{uuid.uuid4().hex[:8]}")
        count = 0
        try:
            with tmp_path.open("w", encoding="utf-8") as handle:
                for img_path in images_list:
                    rel, label_path = _label_path_for_image(img_path, images_dir, labels_dir)
                    width, height = _load_image_size(img_path)
                    detections: List[Dict[str, Any]] = []
                    if label_path.exists():
                        try:
                            for raw in label_path.read_text(encoding="utf-8").splitlines():
                                raw = raw.strip()
                                if not raw:
                                    continue
                                det = _qwen_det_from_yolo(raw.split(), labelmap=labelmap, width=width, height=height)
                                if det:
                                    detections.append(det)
                        except Exception:
                            detections = []
                    total_detections += len(detections)
                    for mode in ("bbox", "point"):
                        prompt_text = _build_qwen_training_prompt(instruction, labelmap, mode)
                        output_text = _qwen_build_output_payload(detections, mode)
                        record = {
                            "image": str(rel.as_posix()),
                            "conversations": [
                                {"from": "human", "value": f"<image>\n{prompt_text}"},
                                {"from": "gpt", "value": output_text},
                            ],
                        }
                        count += 1
            tmp_path.replace(jsonl_path)
        finally:
            try:
                tmp_path.unlink(missing_ok=True)
            except Exception:
                pass
        return count

    train_count = _write_split("train", train_images, train_labels, train_list)
    val_count = _write_split(
        "val",
        val_images if val_images.exists() else train_images,
        val_labels if val_labels.exists() else train_labels,
        val_list,
    )
    if total_detections <= 0:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="dataset_missing_labels")

    meta = _load_sam3_dataset_metadata(dataset_root) or _load_registry_dataset_metadata(dataset_root) or {}
    dataset_type = str(meta.get("type") or "bbox")
    meta_payload = {
        "id": meta.get("id") or dataset_root.name,
        "label": meta.get("label") or meta.get("id") or dataset_root.name,
        "classes": labelmap,
        "context": context_text,
        "created_at": time.time(),
        "image_count": len(train_list) + len(val_list),
        "record_count": train_count + val_count,
        "train_count": train_count,
        "val_count": val_count,
        "type": dataset_type,
        "qwen": {
            "format_version": qwen_format_version,
            "model_family": "qwen3",
        },
    }
    _atomic_write_json(dataset_root / QWEN_METADATA_FILENAME, meta_payload)
    return meta_payload


@app.post("/datasets/{dataset_id}/build/qwen")
def build_dataset_qwen_artifact(
    dataset_id: str,
    payload: QwenDatasetBuildRequest = Body(default_factory=QwenDatasetBuildRequest),
):
    dataset_root = _resolve_sam3_or_qwen_dataset(dataset_id)
    labelmap_path = dataset_root / "labelmap.txt"
    train_images = dataset_root / "train" / "images"
    train_labels = dataset_root / "train" / "labels"
    root_images = dataset_root / "images"
    root_labels = dataset_root / "labels"
    has_yolo = labelmap_path.exists() and (
        (train_images.exists() and train_labels.exists())
        or (root_images.exists() and root_labels.exists())
    )
    if not has_yolo:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="yolo_layout_missing")
    entry = None
    for candidate in _list_all_datasets(prefer_registry=True):
        if dataset_id in (candidate.get("id"), candidate.get("signature")):
            entry = candidate
            break
    context_text = ""
    if payload and payload.context is not None:
        context_text = str(payload.context)
    elif entry:
        context_text = str(entry.get("context") or "")
    force = bool(payload.force) if payload else False
    meta = _build_qwen_dataset_from_yolo(dataset_root, context_text=context_text, force=force)
    return {"status": "ready", "dataset_id": entry.get("id") if entry else dataset_id, "qwen_metadata": meta}


@app.post("/datasets/upload")
async def upload_dataset_zip(
    file: UploadFile = File(...),
    dataset_id: Optional[str] = Form(None),
    dataset_type: Optional[str] = Form(None),
    context: Optional[str] = Form(None),
):
    # Purge stale staging before creating a new one.
    _purge_staging_dirs(DATASET_UPLOAD_ROOT, active_roots=set(), prefix="dataset_upload_")
    filename = file.filename or "dataset.zip"
    safe_name = _safe_run_name(dataset_id, Path(filename).stem or f"dataset_{uuid.uuid4().hex[:6]}")
    tmp_root = Path(tempfile.mkdtemp(prefix="dataset_upload_", dir=str(DATASET_UPLOAD_ROOT)))
    zip_path = tmp_root / "payload.zip"
    try:
        await _write_upload_file(file, zip_path, max_bytes=DATASET_ZIP_MAX_BYTES)
        extracted_dir = tmp_root / "extracted"
        extracted_dir.mkdir(parents=True, exist_ok=True)
        with zipfile.ZipFile(zip_path, "r") as zf:
            _safe_extract_zip(
                zf,
                extracted_dir,
                max_bytes_per_file=DATASET_ZIP_ENTRY_MAX_BYTES,
                total_quota_bytes=DATASET_ZIP_MAX_BYTES,
            )
        dataset_root = _find_yolo_dataset_root(extracted_dir)
        if not dataset_root:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="yolo_layout_not_found")
        # Normalize layout: accept either train/val splits or root-level images/labels (no split).
        train_images = dataset_root / "train" / "images"
        train_labels = dataset_root / "train" / "labels"
        root_images = dataset_root / "images"
        root_labels = dataset_root / "labels"
        if root_images.exists() and root_labels.exists() and not train_images.exists():
            (dataset_root / "train").mkdir(parents=True, exist_ok=True)
            shutil.move(str(root_images), str(train_images.parent))
            shutil.move(str(root_labels), str(train_labels.parent))
        if not train_images.exists() or not train_labels.exists():
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="train_split_missing")
        target_dir = (DATASET_REGISTRY_ROOT / safe_name).resolve()
        if not str(target_dir).startswith(str(DATASET_REGISTRY_ROOT.resolve())):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="dataset_path_invalid")
        if target_dir.exists():
            raise HTTPException(status_code=HTTP_409_CONFLICT, detail="dataset_exists")
        # Ensure labelmap.txt exists and read classes.
        labelmap_path = dataset_root / "labelmap.txt"
        if not labelmap_path.exists():
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="labelmap_txt_missing")
        try:
            with labelmap_path.open("r", encoding="utf-8") as handle:
                labelmap = [line.strip() for line in handle if line.strip()]
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"labelmap_txt_invalid:{exc}") from exc
        if not labelmap:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="labelmap_empty")
        _validate_yolo_label_ids(train_labels, len(labelmap))
        shutil.move(str(dataset_root), str(target_dir))
        dataset_kind = (dataset_type or "").strip().lower() or _infer_yolo_dataset_type(target_dir / "train" / "labels", "bbox")
        if dataset_kind not in {"bbox", "seg"}:
            dataset_kind = "bbox"
        train_count = _count_images_in_dir(target_dir / "train" / "images")
        val_dir = target_dir / "val" / "images"
        val_count = _count_images_in_dir(val_dir) if val_dir.exists() else 0
        image_count = train_count + val_count
        signature = _compute_dir_signature(target_dir)
        metadata = {
            "id": safe_name,
            "label": safe_name,
            "dataset_root": str(target_dir),
            "type": dataset_kind,
            "source": "upload",
            "created_at": time.time(),
            "image_count": image_count,
            "train_count": train_count,
            "val_count": val_count,
            "context": (context or "").strip(),
            "classes": labelmap,
            "signature": signature,
        }
        _persist_sam3_dataset_metadata(target_dir, metadata)
        # Build COCO JSONs immediately so Qwen/SAM3 consumers have them ready.
        try:
            coco_meta = _convert_yolo_dataset_to_coco(target_dir)
            metadata.update(
                {
                    "coco_train_json": coco_meta.get("coco_train_json"),
                    "coco_val_json": coco_meta.get("coco_val_json"),
                }
            )
            _persist_sam3_dataset_metadata(target_dir, metadata)
        except Exception as exc:  # noqa: BLE001
            logger.warning("[dataset-upload] failed COCO conversion for %s: %s", safe_name, exc)
        logger.info(
            "[dataset-upload] stored=%s type=%s train=%d val=%d classes=%d",
            safe_name,
            dataset_kind,
            train_count,
            val_count,
            len(labelmap),
        )
        return metadata
    finally:
        shutil.rmtree(tmp_root, ignore_errors=True)


@app.get("/datasets")
def list_datasets():
    return _list_sam3_datasets()


def _load_dataset_glossary(dataset_root: Path) -> str:
    sam_meta = _load_sam3_dataset_metadata(dataset_root) or {}
    qwen_meta = _load_qwen_dataset_metadata(dataset_root) or {}
    raw = sam_meta.get("labelmap_glossary") or qwen_meta.get("labelmap_glossary")
    return _normalize_labelmap_glossary(raw)


def _persist_dataset_glossary(dataset_root: Path, glossary_text: str) -> None:
    updated = False
    sam_meta = _load_sam3_dataset_metadata(dataset_root)
    if sam_meta is not None:
        sam_meta["labelmap_glossary"] = glossary_text
        _persist_sam3_dataset_metadata(dataset_root, sam_meta)
        updated = True
    qwen_meta = _load_qwen_dataset_metadata(dataset_root)
    if qwen_meta is not None:
        qwen_meta["labelmap_glossary"] = glossary_text
        _persist_qwen_dataset_metadata(dataset_root, qwen_meta)
        updated = True
    if not updated:
        fallback = {
            "id": dataset_root.name,
            "label": dataset_root.name,
            "created_at": time.time(),
            "labelmap_glossary": glossary_text,
        }
        _persist_sam3_dataset_metadata(dataset_root, fallback)


def _normalize_glossary_name(name: str) -> str:
    return re.sub(r"\\s+", " ", str(name or "").strip())


def _glossary_key(name: str) -> str:
    return _normalize_glossary_name(name).lower()


def _load_glossary_library() -> List[Dict[str, Any]]:
    if not GLOSSARY_LIBRARY_PATH.exists():
        return []
    try:
        with GLOSSARY_LIBRARY_PATH.open("r", encoding="utf-8") as handle:
            data = json.load(handle)
    except Exception:
        return []
    entries = data.get("glossaries") if isinstance(data, dict) else data
    return list(entries) if isinstance(entries, list) else []


def _persist_glossary_library(entries: List[Dict[str, Any]]) -> None:
    payload = {"glossaries": entries}
    tmp_path = GLOSSARY_LIBRARY_PATH.with_suffix(".tmp")
    with tmp_path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle, indent=2, ensure_ascii=True)
    tmp_path.replace(GLOSSARY_LIBRARY_PATH)


def _find_glossary_entry(entries: List[Dict[str, Any]], name: str) -> Optional[Dict[str, Any]]:
    key = _glossary_key(name)
    if not key:
        return None
    for entry in entries:
        if _glossary_key(entry.get("name")) == key:
            return entry
    return None


def _upsert_glossary_entry(name: str, glossary_text: str) -> Dict[str, Any]:
    normalized_name = _normalize_glossary_name(name)
    if not normalized_name:
        raise ValueError("glossary_name_required")
    glossary_text = _normalize_labelmap_glossary(glossary_text)
    with GLOSSARY_LIBRARY_LOCK:
        entries = _load_glossary_library()
        entry = _find_glossary_entry(entries, normalized_name)
        now = time.time()
        if entry:
            entry["name"] = normalized_name
            entry["glossary"] = glossary_text
            entry["updated_at"] = now
        else:
            entry = {
                "name": normalized_name,
                "glossary": glossary_text,
                "created_at": now,
                "updated_at": now,
            }
            entries.append(entry)
        entries.sort(key=lambda item: _glossary_key(item.get("name")))
        _persist_glossary_library(entries)
    return entry


def _delete_glossary_entry(name: str) -> bool:
    normalized_name = _normalize_glossary_name(name)
    if not normalized_name:
        return False
    with GLOSSARY_LIBRARY_LOCK:
        entries = _load_glossary_library()
        before = len(entries)
        entries = [entry for entry in entries if _glossary_key(entry.get("name")) != _glossary_key(normalized_name)]
        if len(entries) == before:
            return False
        _persist_glossary_library(entries)
    return True


@app.get("/datasets/{dataset_id}/glossary")
def get_dataset_glossary(dataset_id: str):
    dataset_root = _resolve_sam3_or_qwen_dataset(dataset_id)
    glossary = _load_dataset_glossary(dataset_root)
    if not glossary:
        labelmap = _discover_yolo_labelmap(dataset_root) or _load_qwen_labelmap(dataset_root)
        if labelmap:
            glossary = _default_agent_glossary_for_labelmap(labelmap)
    return {"dataset_id": dataset_id, "glossary": glossary}


@app.post("/datasets/{dataset_id}/glossary")
def update_dataset_glossary(dataset_id: str, payload: DatasetGlossaryPayload = Body(default_factory=DatasetGlossaryPayload)):
    dataset_root = _resolve_sam3_or_qwen_dataset(dataset_id)
    glossary_text = _normalize_labelmap_glossary(payload.glossary)
    _persist_dataset_glossary(dataset_root, glossary_text)
    return {"dataset_id": dataset_id, "glossary": glossary_text, "saved": True}


@app.get("/glossaries")
def list_glossaries():
    entries = _load_glossary_library()
    response = []
    for entry in entries:
        response.append(
            {
                "name": entry.get("name"),
                "created_at": entry.get("created_at"),
                "updated_at": entry.get("updated_at"),
            }
        )
    return response


@app.get("/glossaries/{glossary_name}")
def get_glossary(glossary_name: str):
    entries = _load_glossary_library()
    entry = _find_glossary_entry(entries, glossary_name)
    if not entry:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="glossary_not_found")
    return {
        "name": entry.get("name"),
        "glossary": _normalize_labelmap_glossary(entry.get("glossary")),
        "created_at": entry.get("created_at"),
        "updated_at": entry.get("updated_at"),
    }


@app.post("/glossaries")
def upsert_glossary(payload: GlossaryLibraryPayload = Body(default_factory=GlossaryLibraryPayload)):
    name = _normalize_glossary_name(payload.name)
    if not name:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="glossary_name_required")
    glossary_text = _normalize_labelmap_glossary(payload.glossary)
    try:
        entry = _upsert_glossary_entry(name, glossary_text)
    except ValueError as exc:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=str(exc))
    return {
        "name": entry.get("name"),
        "glossary": entry.get("glossary"),
        "created_at": entry.get("created_at"),
        "updated_at": entry.get("updated_at"),
        "saved": True,
    }


@app.delete("/glossaries/{glossary_name}")
def delete_glossary(glossary_name: str):
    ok = _delete_glossary_entry(glossary_name)
    if not ok:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="glossary_not_found")
    return {"name": glossary_name, "deleted": True}


@app.delete("/datasets/{dataset_id}")
def delete_dataset(dataset_id: str):
    target_entry = None
    for entry in _list_all_datasets(prefer_registry=True):
        if dataset_id in (entry.get("id"), entry.get("signature")):
            target_entry = entry
            break
    if not target_entry:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="dataset_not_found")
    target_path = Path(target_entry["dataset_root"]).resolve()
    allowed_roots = {
        DATASET_REGISTRY_ROOT.resolve(),
        SAM3_DATASET_ROOT.resolve(),
        QWEN_DATASET_ROOT.resolve(),
    }
    if not any(str(target_path).startswith(str(root)) for root in allowed_roots):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="dataset_path_invalid")
    try:
        shutil.rmtree(target_path, ignore_errors=False)
    except FileNotFoundError:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="dataset_not_found")
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=str(exc))
    _purge_dataset_artifacts(dataset_id)
    return {"status": "deleted", "dataset_id": dataset_id}


def _cleanup_stream_and_dir(stream: Any, staging_dir: Path) -> None:
    try:
        stream.close()
    except Exception:
        pass
    shutil.rmtree(staging_dir, ignore_errors=True)


@app.get("/datasets/{dataset_id}/download")
def download_dataset(dataset_id: str):
    target_entry = None
    for entry in _list_all_datasets(prefer_registry=True):
        if dataset_id in (entry.get("id"), entry.get("signature")):
            target_entry = entry
            break
    if not target_entry:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="dataset_not_found")
    target_path = Path(target_entry["dataset_root"]).resolve()
    allowed_roots = {
        DATASET_REGISTRY_ROOT.resolve(),
        SAM3_DATASET_ROOT.resolve(),
        QWEN_DATASET_ROOT.resolve(),
    }
    if not any(str(target_path).startswith(str(root)) for root in allowed_roots):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="dataset_path_invalid")
    staging_dir = Path(tempfile.mkdtemp(prefix="dataset_export_", dir=str(DATASET_UPLOAD_ROOT)))
    safe_name = re.sub(r"[^A-Za-z0-9._-]", "_", (target_entry.get("id") or target_path.name)).strip("_") or target_path.name
    zip_path = staging_dir / f"{safe_name}.zip"
    try:
        with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_STORED) as zf:
            root_name = Path(target_path.name)
            for path in sorted(target_path.rglob("*")):
                if not path.is_file():
                    continue
                rel = path.relative_to(target_path)
                zf.write(path, arcname=str(root_name / rel))
        stream = zip_path.open("rb")
    except Exception as exc:  # noqa: BLE001
        shutil.rmtree(staging_dir, ignore_errors=True)
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"dataset_export_failed:{exc}") from exc
    filename = zip_path.name
    headers = {"Content-Disposition": f'attachment; filename="{filename}"'}
    return StreamingResponse(
        stream,
        media_type="application/zip",
        headers=headers,
        background=BackgroundTask(_cleanup_stream_and_dir, stream, staging_dir),
    )


@app.get("/datasets/{dataset_id}/text_labels/{image_name}")
def get_caption_label(dataset_id: str, image_name: str):
    dataset_root = _resolve_caption_dataset_root(dataset_id)
    caption_path = _caption_file_path(dataset_root, image_name, create_dir=False)
    if not caption_path.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="caption_not_found")
    try:
        caption = caption_path.read_text(encoding="utf-8").strip()
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=str(exc))
    return {"image": Path(image_name).name, "caption": caption}


@app.post("/datasets/{dataset_id}/text_labels/{image_name}")
def save_caption_label(dataset_id: str, image_name: str, payload: CaptionLabelRequest):
    dataset_root = _resolve_caption_dataset_root(dataset_id)
    caption_path = _caption_file_path(dataset_root, image_name, create_dir=True)
    caption = (payload.caption or "").strip()
    if not caption:
        deleted = False
        if caption_path.exists():
            try:
                caption_path.unlink()
                deleted = True
            except Exception as exc:  # noqa: BLE001
                raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=str(exc))
        return {"image": Path(image_name).name, "caption": "", "saved": False, "deleted": deleted}
    try:
        caption_path.write_text(f"{caption}\n", encoding="utf-8")
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=str(exc))
    return {"image": Path(image_name).name, "caption": caption, "saved": True}


@app.get("/sam3/datasets")
def list_sam3_datasets():
    return _list_sam3_datasets()


@app.get("/sam3/datasets/{dataset_id}/classes")
def get_sam3_dataset_classes(dataset_id: str):
    dataset_root = _resolve_sam3_or_qwen_dataset(dataset_id)
    coco, _, _ = _load_coco_index(dataset_root)
    categories = coco.get("categories") or []
    classes: List[str] = []
    class_ids: List[int] = []
    for idx, cat in enumerate(categories):
        try:
            cid = int(cat.get("id", idx))
        except Exception:
            cid = idx
        class_ids.append(cid)
        classes.append(str(cat.get("name", f"class_{cid}")))
    return {"dataset_id": dataset_id, "classes": classes, "class_ids": class_ids}


@app.post("/sam3/datasets/{dataset_id}/convert")
def sam3_convert_dataset(dataset_id: str):
    dataset_root = _resolve_sam3_or_qwen_dataset(dataset_id)
    annotations_path = dataset_root / "train" / "annotations.jsonl"
    train_images = dataset_root / "train" / "images"
    train_labels = dataset_root / "train" / "labels"
    if annotations_path.exists():
        meta = _convert_qwen_dataset_to_coco(dataset_root)
    elif train_images.exists() and train_labels.exists():
        meta = _convert_yolo_dataset_to_coco(dataset_root)
    else:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="sam3_dataset_type_unsupported")
    return meta


def _find_coco_split(dataset_root: Path) -> Tuple[Path, Path]:
    """Return (annotations_path, images_dir) preferring val split, then train."""
    val_ann = dataset_root / "val" / "_annotations.coco.json"
    if val_ann.exists():
        images_dir = val_ann.parent / "images"
        if not images_dir.exists():
            images_dir = val_ann.parent
        return val_ann, images_dir
    train_ann = dataset_root / "train" / "_annotations.coco.json"
    if train_ann.exists():
        images_dir = train_ann.parent / "images"
        if not images_dir.exists():
            images_dir = train_ann.parent
        return train_ann, images_dir
    raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="coco_annotations_missing")


def _load_coco_index(dataset_root: Path) -> Tuple[Dict[str, Any], Dict[int, Dict[int, List[List[float]]]], Dict[int, Dict[str, Any]]]:
    ann_paths: List[Tuple[Path, Path]] = []
    for split in ("train", "val"):
        ann_file = dataset_root / split / "_annotations.coco.json"
        if ann_file.exists():
            images_dir = ann_file.parent / "images"
            if not images_dir.exists():
                images_dir = ann_file.parent
            ann_paths.append((ann_file, images_dir))
    if not ann_paths:
        ann_path, images_dir = _find_coco_split(dataset_root)
        ann_paths = [(ann_path, images_dir)]
    images: Dict[int, Dict[str, Any]] = {}
    gt_by_image_cat: Dict[int, Dict[int, List[List[float]]]] = {}
    coco_merged: Dict[str, Any] = {"images": [], "annotations": [], "categories": []}
    categories_map: Dict[int, Dict[str, Any]] = {}
    for ann_path, images_dir in ann_paths:
        try:
            with ann_path.open("r", encoding="utf-8") as handle:
                coco = json.load(handle)
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"coco_load_failed:{exc}") from exc
        for cat in coco.get("categories", []) or []:
            try:
                cid = int(cat.get("id"))
            except Exception:
                continue
            categories_map.setdefault(cid, cat)
        for img in coco.get("images", []) or []:
            try:
                img_id = int(img["id"])
            except Exception:
                continue
            path = (images_dir / img["file_name"]).resolve()
            images[img_id] = {**img, "path": path}
        for ann in coco.get("annotations", []) or []:
            try:
                img_id = int(ann["image_id"])
                cat_id = int(ann["category_id"])
            except Exception:
                continue
            bbox = ann.get("bbox")
            if bbox is None:
                continue
            gt_by_image_cat.setdefault(img_id, {}).setdefault(cat_id, []).append(list(bbox))
    coco_merged["categories"] = sorted(categories_map.values(), key=lambda c: int(c.get("id", 0)))
    coco_merged["images"] = list(
        {
            img_id: {
                "id": img_id,
                "file_name": str(img.get("file_name") or Path(img.get("path", "")).name),
                "width": img.get("width"),
                "height": img.get("height"),
            }
            for img_id, img in images.items()
        }.values()
    )
    # Rebuild annotations from gt_by_image_cat so downstream uses the merged mapping.
    annotations: List[Dict[str, Any]] = []
    ann_id = 1
    for img_id, cat_map in gt_by_image_cat.items():
        for cat_id, boxes in cat_map.items():
            for bbox in boxes:
                annotations.append(
                    {
                        "id": ann_id,
                        "image_id": img_id,
                        "category_id": cat_id,
                        "bbox": bbox,
                    }
                )
                ann_id += 1
    coco_merged["annotations"] = annotations
    return coco_merged, gt_by_image_cat, images


def _yolo_to_xyxy(width: int, height: int, bbox: Sequence[float]) -> Tuple[float, float, float, float]:
    cx, cy, bw, bh = map(float, bbox[:4])
    x1 = max(0.0, (cx - bw / 2.0) * width)
    y1 = max(0.0, (cy - bh / 2.0) * height)
    x2 = min(float(width), (cx + bw / 2.0) * width)
    y2 = min(float(height), (cy + bh / 2.0) * height)
    return x1, y1, x2, y2


def _xywh_to_xyxy(bbox: Sequence[float]) -> Tuple[float, float, float, float]:
    x, y, w, h = map(float, bbox[:4])
    return x, y, x + w, y + h


def _xyxy_to_yolo_norm(width: int, height: int, x1: float, y1: float, x2: float, y2: float) -> Tuple[float, float, float, float]:
    w = max(0.0, x2 - x1)
    h = max(0.0, y2 - y1)
    cx = (x1 + x2) / 2.0
    cy = (y1 + y2) / 2.0
    if width <= 0 or height <= 0:
        return 0.0, 0.0, 0.0, 0.0
    return cx / float(width), cy / float(height), w / float(width), h / float(height)


def _xyxy_to_qwen_bbox(width: int, height: int, x1: float, y1: float, x2: float, y2: float) -> Tuple[float, float, float, float]:
    if width <= 0 or height <= 0:
        return 0.0, 0.0, 0.0, 0.0
    qx1 = max(0.0, min(1000.0, x1 / float(width) * 1000.0))
    qy1 = max(0.0, min(1000.0, y1 / float(height) * 1000.0))
    qx2 = max(0.0, min(1000.0, x2 / float(width) * 1000.0))
    qy2 = max(0.0, min(1000.0, y2 / float(height) * 1000.0))
    if qx1 > qx2:
        qx1, qx2 = qx2, qx1
    if qy1 > qy2:
        qy1, qy2 = qy2, qy1
    return qx1, qy1, qx2, qy2


def _qwen_bbox_to_xyxy(width: int, height: int, bbox_2d: Sequence[float]) -> Tuple[float, float, float, float]:
    if len(bbox_2d) < 4 or width <= 0 or height <= 0:
        return 0.0, 0.0, 0.0, 0.0
    qx1, qy1, qx2, qy2 = map(float, bbox_2d[:4])
    qx1 = max(0.0, min(1000.0, qx1))
    qy1 = max(0.0, min(1000.0, qy1))
    qx2 = max(0.0, min(1000.0, qx2))
    qy2 = max(0.0, min(1000.0, qy2))
    if qx1 > qx2:
        qx1, qx2 = qx2, qx1
    if qy1 > qy2:
        qy1, qy2 = qy2, qy1
    x1 = qx1 / 1000.0 * float(width)
    y1 = qy1 / 1000.0 * float(height)
    x2 = qx2 / 1000.0 * float(width)
    y2 = qy2 / 1000.0 * float(height)
    return x1, y1, x2, y2


def _remap_window_xyxy_to_full(xyxy: Sequence[float], window_xyxy: Sequence[float]) -> Tuple[float, float, float, float]:
    x1, y1, x2, y2 = map(float, xyxy[:4])
    wx1, wy1, wx2, wy2 = map(float, window_xyxy[:4])
    return x1 + wx1, y1 + wy1, x2 + wx1, y2 + wy1


def _coord_roundtrip_smoke() -> None:
    samples = [
        (1024, 768, (100.0, 50.0, 300.0, 170.0)),
        (1920, 1080, (10.0, 20.0, 190.0, 260.0)),
        (800, 800, (0.0, 0.0, 799.0, 799.0)),
    ]
    tol = 0.5
    for width, height, xyxy in samples:
        qwen = _xyxy_to_qwen_bbox(width, height, *xyxy)
        xyxy_rt = _qwen_bbox_to_xyxy(width, height, qwen)
        err_qwen = max(abs(a - b) for a, b in zip(xyxy, xyxy_rt))
        yolo = _xyxy_to_yolo_norm(width, height, *xyxy)
        xyxy_yolo = _yolo_to_xyxy(width, height, yolo)
        err_yolo = max(abs(a - b) for a, b in zip(xyxy, xyxy_yolo))
        if err_qwen > tol or err_yolo > tol:
            logger.warning("coord_roundtrip_mismatch width=%s height=%s err_qwen=%.3f err_yolo=%.3f", width, height, err_qwen, err_yolo)


def _iou(box_a: Tuple[float, float, float, float], box_b: Tuple[float, float, float, float]) -> float:
    ax1, ay1, ax2, ay2 = box_a
    bx1, by1, bx2, by2 = box_b
    inter_x1 = max(ax1, bx1)
    inter_y1 = max(ay1, by1)
    inter_x2 = min(ax2, bx2)
    inter_y2 = min(ay2, by2)
    inter_w = max(0.0, inter_x2 - inter_x1)
    inter_h = max(0.0, inter_y2 - inter_y1)
    inter_area = inter_w * inter_h
    if inter_area <= 0:
        return 0.0
    area_a = max(0.0, ax2 - ax1) * max(0.0, ay2 - ay1)
    area_b = max(0.0, bx2 - bx1) * max(0.0, by2 - by1)
    union = area_a + area_b - inter_area
    if union <= 0:
        return 0.0
    return inter_area / union


def _humanize_class_name(name: str) -> str:
    return re.sub(r"[\\-_]+", " ", name).strip()


def _generate_prompt_variants_for_class(class_name: str, max_synonyms: int, use_qwen: bool) -> List[str]:
    base: List[str] = []
    cleaned = class_name.strip()
    if cleaned:
        base.append(cleaned)
    human = _humanize_class_name(cleaned)
    if human and human.lower() != cleaned.lower():
        base.append(human)
    base_lower = {b.lower() for b in base if b}
    base_words = []
    for entry in base_lower:
        base_words.extend(re.split(r"[\\s_\\-]+", entry))
    base_words = [w for w in base_words if w]
    variants: List[str] = []
    if use_qwen and max_synonyms > 0:
        try:
            text = _generate_prompt_text(
                (
                    f"Generate up to {max_synonyms} alternative, common English labels for the object class "
                    f"'{human or cleaned}'. Each label must be 1-3 full words, each word at least 3 letters. "
                    "No abbreviations, no partial/truncated words, no numbering, no JSON. Avoid repeating the original name. "
                    "Use labels typical of object-detection datasets (e.g., car -> car, automobile, sedan; "
                    "person -> person, human, individual; utility pole -> utility pole, telephone pole, power pole). "
                    "Return a single comma-separated list."
                ),
                max_new_tokens=96,
            )
            raw_parts = re.split(r"[\\n;,]+", text)
            for part in raw_parts:
                normalized = part.strip().strip('"').strip("'")
                if not normalized:
                    continue
                if any(ch in normalized for ch in "{}[]:\""):
                    continue
                alpha_chars = re.sub(r"[^A-Za-z]", "", normalized)
                if len(alpha_chars) < 3:
                    continue
                if len(normalized) > 40:
                    continue
                if not re.search(r"[A-Za-z]", normalized):
                    continue
                words = normalized.split()
                if len(words) > 4:
                    continue
                if any(len(w) < 3 for w in words):
                    continue
                lowered = normalized.lower()
                fragment_of_base = any(
                    (lowered != b and lowered.startswith(b[: max(1, len(b) - 2)]))
                    or (b.startswith(lowered) and (len(b) - len(lowered) <= 2))
                    for b in base_lower
                )
                if fragment_of_base:
                    continue
                # Drop obvious truncated tokens relative to base words.
                truncated_token = False
                for w in words:
                    lw = w.lower()
                    for bw in base_words:
                        if not bw:
                            continue
                        if bw.startswith(lw) and len(bw) - len(lw) >= 2:
                            truncated_token = True
                            break
                        if lw.startswith(bw) and len(lw) - len(bw) >= 3:
                            truncated_token = True
                            break
                    if truncated_token:
                        break
                if truncated_token:
                    continue
                variants.append(normalized)
        except Exception as exc:  # noqa: BLE001
            logger.warning("Prompt helper: Qwen generation failed for %s: %s", class_name, exc)
    seen = set()
    ordered: List[str] = []
    for item in [*base, *variants]:
        key = item.lower()
        if key in seen:
            continue
        seen.add(key)
        ordered.append(item)
    if max_synonyms <= 0:
        return ordered[:1]
    # Always keep the first/base entry, then up to max_synonyms additional candidates.
    head = ordered[:1]
    tail = ordered[1 : 1 + max_synonyms]
    return head + tail


def _suggest_prompts_for_dataset(payload: PromptHelperSuggestRequest) -> Dict[str, Any]:
    dataset_root = _resolve_sam3_or_qwen_dataset(payload.dataset_id)
    coco, _, _ = _load_coco_index(dataset_root)
    categories = coco.get("categories") or []
    cat_to_images: Dict[int, set[int]] = {}
    cat_to_gts: Dict[int, int] = {}
    for ann in coco.get("annotations", []):
        try:
            cat_id = int(ann["category_id"])
            img_id = int(ann["image_id"])
        except Exception:
            continue
        cat_to_images.setdefault(cat_id, set()).add(img_id)
        cat_to_gts[cat_id] = cat_to_gts.get(cat_id, 0) + 1
    classes: List[Dict[str, Any]] = []
    for idx, cat in enumerate(categories):
        cat_id = int(cat.get("id", idx))
        class_name = str(cat.get("name", f"class_{cat_id}"))
        prompts = _generate_prompt_variants_for_class(class_name, payload.max_synonyms, payload.use_qwen)
        classes.append(
            {
                "class_id": cat_id,
                "class_name": class_name,
                "default_prompts": prompts,
                "image_count": len(cat_to_images.get(cat_id, set())),
                "gt_count": cat_to_gts.get(cat_id, 0),
            }
        )
    return {
        "dataset_id": payload.dataset_id,
        "config": payload.dict(),
        "classes": classes,
    }


def _list_prompt_helper_presets() -> List[Dict[str, Any]]:
    presets: List[Dict[str, Any]] = []
    for path in PROMPT_HELPER_PRESET_ROOT.glob("*.json"):
        try:
            with path.open("r", encoding="utf-8") as handle:
                data = json.load(handle)
            presets.append(data)
        except Exception:
            continue
    presets.sort(key=lambda p: p.get("created_at", 0), reverse=True)
    return presets


def _load_prompt_helper_preset(preset_id: str) -> Dict[str, Any]:
    path = (PROMPT_HELPER_PRESET_ROOT / f"{preset_id}.json").resolve()
    if not str(path).startswith(str(PROMPT_HELPER_PRESET_ROOT.resolve())) or not path.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="prompt_helper_preset_not_found")
    try:
        with path.open("r", encoding="utf-8") as handle:
            return json.load(handle)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"prompt_helper_preset_load_failed:{exc}") from exc


def _save_prompt_helper_preset(label: str, dataset_id: str, prompts_by_class: Dict[int, List[str]]) -> Dict[str, Any]:
    preset_id = f"phset_{uuid.uuid4().hex[:8]}"
    created_at = time.time()
    payload = {
        "id": preset_id,
        "label": label or preset_id,
        "dataset_id": dataset_id_clean or "portable",
        "created_at": created_at,
        "prompts_by_class": prompts_by_class,
    }
    path = (PROMPT_HELPER_PRESET_ROOT / f"{preset_id}.json").resolve()
    if not str(path).startswith(str(PROMPT_HELPER_PRESET_ROOT.resolve())):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="prompt_helper_preset_path_invalid")
    with path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle, ensure_ascii=False, indent=2)
    return payload


def _sample_images_for_category(cat_id: int, img_ids: List[int], sample_size: int, seed: int) -> List[int]:
    if not img_ids:
        return []
    rnd = random.Random(seed + cat_id * 9973)
    if len(img_ids) <= sample_size:
        return list(img_ids)
    return rnd.sample(img_ids, sample_size)


def _sample_negative_images(cat_id: int, all_img_ids: List[int], cat_to_images: Dict[int, set[int]], sample_size: int, seed: int) -> List[int]:
    """Pick images that do NOT contain the target category."""
    negative_pool = [img_id for img_id in all_img_ids if img_id not in cat_to_images.get(cat_id, set())]
    if not negative_pool or sample_size <= 0:
        return []
    rnd = random.Random(seed + cat_id * 15391)
    if len(negative_pool) <= sample_size:
        return negative_pool
    return rnd.sample(negative_pool, sample_size)


def _evaluate_prompt_for_class(
    prompt: str,
    *,
    cat_id: int,
    image_ids: List[int],
    gt_by_image_cat: Dict[int, Dict[int, List[List[float]]]],
    images: Dict[int, Dict[str, Any]],
    score_threshold: float,
    max_dets: int,
    iou_threshold: float,
    image_cache: Dict[int, Image.Image],
) -> Dict[str, Any]:
    total_gt = 0
    total_preds = 0
    matches = 0
    det_images = 0
    iou_sum = 0.0
    score_sum = 0.0
    matched_scores = 0
    for img_id in image_ids:
        info = images.get(img_id)
        if not info:
            continue
        path = info.get("path")
        width = info.get("width")
        height = info.get("height")
        if not path or width is None or height is None:
            continue
        gts = [*gt_by_image_cat.get(img_id, {}).get(cat_id, [])]
        gt_boxes = [_xywh_to_xyxy(b) for b in gts]
        total_gt += len(gt_boxes)
        if not gt_boxes:
            continue
        try:
            pil_img = image_cache[img_id]
        except KeyError:
            try:
                pil_img = Image.open(path).convert("RGB")
            except Exception:
                continue
            image_cache[img_id] = pil_img
        preds = _run_sam3_text_inference(
            pil_img,
            prompt,
            threshold=score_threshold,
            mask_threshold=0.0,
            limit=max_dets,
        )
        pred_boxes: List[Tuple[float, float, float, float, Optional[float]]] = []
        for det in preds:
            try:
                x1, y1, x2, y2 = _yolo_to_xyxy(pil_img.width, pil_img.height, det.bbox)
                pred_boxes.append((x1, y1, x2, y2, det.score))
            except Exception:
                continue
        if not pred_boxes:
            continue
        pred_boxes.sort(key=lambda b: (b[4] if b[4] is not None else 0.0), reverse=True)
        total_preds += len(pred_boxes)
        gt_used = [False] * len(gt_boxes)
        matched_in_image = 0
        for x1, y1, x2, y2, score in pred_boxes:
            best_iou = 0.0
            best_idx = -1
            for idx, gt_box in enumerate(gt_boxes):
                if gt_used[idx]:
                    continue
                iou = _iou((x1, y1, x2, y2), gt_box)
                if iou > best_iou:
                    best_iou = iou
                    best_idx = idx
            if best_iou >= iou_threshold and best_idx >= 0:
                gt_used[best_idx] = True
                matches += 1
                matched_in_image += 1
                iou_sum += best_iou
                if score is not None:
                    score_sum += score
                    matched_scores += 1
        if matched_in_image > 0:
            det_images += 1
    precision = matches / total_preds if total_preds else 0.0
    recall = matches / total_gt if total_gt else 0.0
    det_rate = det_images / len(image_ids) if image_ids else 0.0
    avg_iou = iou_sum / matches if matches else None
    avg_score = score_sum / matched_scores if matched_scores else None
    f1 = (2 * precision * recall) / (precision + recall + 1e-8) if (precision + recall) > 0 else 0.0
    overall_score = f1 * (0.5 + 0.5 * det_rate)
    fps = max(0, total_preds - matches)
    return {
        "prompt": prompt,
        "precision": precision,
        "recall": recall,
        "det_rate": det_rate,
        "avg_iou": avg_iou,
        "avg_score": avg_score,
        "score": overall_score,
        "f1": f1,
        "preds": total_preds,
        "matches": matches,
        "gts": total_gt,
        "fps": fps,
    }


class PromptHelperSuggestRequest(BaseModel):
    dataset_id: str
    max_synonyms: int = Field(3, ge=0, le=10)
    use_qwen: bool = True

class PromptHelperPreset(BaseModel):
    id: str
    label: str
    dataset_id: str
    created_at: float
    prompts_by_class: Dict[int, List[str]]


class PromptHelperRequest(BaseModel):
    dataset_id: str
    sample_per_class: int = Field(10, ge=1, le=1000)
    max_synonyms: int = Field(3, ge=0, le=10)
    score_threshold: float = Field(0.2, ge=0.0, le=1.0)
    max_dets: int = Field(100, ge=1, le=2000)
    iou_threshold: float = Field(0.5, ge=0.0, le=1.0)
    seed: int = 42
    use_qwen: bool = True
    # Optional explicit prompts provided by the user; key is category_id.
    prompts_by_class: Optional[Dict[int, List[str]]] = None


class PromptHelperSearchRequest(BaseModel):
    dataset_id: str
    sample_per_class: int = Field(20, ge=1, le=2000)
    negatives_per_class: int = Field(20, ge=0, le=2000)
    score_threshold: float = Field(0.2, ge=0.0, le=1.0)
    max_dets: int = Field(100, ge=1, le=2000)
    iou_threshold: float = Field(0.5, ge=0.0, le=1.0)
    seed: int = 42
    precision_floor: float = Field(0.9, ge=0.0, le=1.0)
    prompts_by_class: Dict[int, List[str]]
    class_id: Optional[int] = None


class PromptRecipePrompt(BaseModel):
    prompt: str
    thresholds: Optional[List[float]] = None


class PromptRecipeRequest(BaseModel):
    dataset_id: str
    class_id: int
    prompts: List[PromptRecipePrompt]
    sample_size: int = Field(30, ge=1, le=5000)
    negatives: int = Field(0, ge=0, le=5000)
    max_dets: int = Field(100, ge=1, le=2000)
    iou_threshold: float = Field(0.5, ge=0.0, le=1.0)
    seed: int = 42
    score_threshold: float = Field(0.2, ge=0.0, le=1.0)
    threshold_candidates: Optional[List[float]] = None


class PromptRecipeExpandRequest(BaseModel):
    dataset_id: str
    class_id: int
    base_prompts: List[str]
    max_new: int = Field(10, ge=0, le=50)


class AgentMiningRequest(BaseModel):
    dataset_id: str
    classes: Optional[List[int]] = None
    eval_image_count: int = Field(
        100,
        ge=1,
        le=50_000,
        description="How many images to sample for mining and scoring (smaller = faster, larger = more reliable).",
    )
    split_seed: int = 42

    # Search strategy: steps-only (schema v2).
    search_mode: Literal["steps"] = "steps"
    reuse_cache: bool = True

    # Multi-step ("steps") mining knobs.
    steps_max_steps_per_recipe: int = Field(
        6,
        ge=1,
        le=50,
        description="Maximum number of prompt steps to select per class when search_mode='steps'.",
    )
    steps_max_visual_seeds_per_step: int = Field(
        10,
        ge=0,
        le=500,
        description="Max similarity candidates expanded per step when search_mode='steps' (per-image, per-step).",
    )
    steps_optimize_tier1: bool = Field(
        False,
        description=(
            "When search_mode='steps' and a pretrained CLIP head is provided, enable bounded Tier-1 grid search of "
            "the SAM3 visual expansion score (`expand_threshold`) and `steps_max_visual_seeds_per_step` "
            "(uses small validation subsets to rank "
            "candidates, then re-tunes on the full validation split for the final recipe)."
        ),
    )
    steps_optimize_tier1_eval_cap: int = Field(
        200,
        ge=10,
        le=50_000,
        description="Max number of sampled images used during Tier-1 grid search stages (search_mode='steps').",
    )
    steps_optimize_tier1_max_trials: int = Field(
        9,
        ge=1,
        le=256,
        description="Max candidate configurations evaluated during Tier-1 grid search (search_mode='steps').",
    )
    steps_seed_eval_floor: Optional[float] = Field(
        None,
        ge=0.0,
        le=1.0,
        description=(
            "When search_mode='steps', optionally run the text-candidate evaluation at a lower threshold than "
            "`seed_threshold` so candidate curves can include values below the base. If unset, defaults to "
            "the base `seed_threshold` (behavior unchanged)."
        ),
    )
    steps_seed_eval_max_results: Optional[int] = Field(
        None,
        ge=1,
        le=5000,
        description=(
            "When search_mode='steps' and `steps_seed_eval_floor` is set low, this optional override caps the number "
            "of text-candidate detections returned per prompt per image (safety valve). If unset, uses `max_results`."
        ),
    )
    steps_early_stop: bool = Field(
        True,
        description=(
            "When search_mode='steps', enable early-stop heuristics that stop adding more prompt steps when "
            "coverage saturates or candidates are far below the cleanliness target (faster, may miss rare cases)."
        ),
    )
    steps_early_stop_mode: Literal["conservative", "balanced", "aggressive"] = Field(
        "balanced",
        description=(
            "Early-stop strictness: conservative keeps searching longer, aggressive stops sooner. "
            "Balanced is a good default."
        ),
    )
    steps_prompt_prefilter: bool = Field(
        True,
        description=(
            "When search_mode='steps', use CLIP similarity to prefilter weak prompts before running SAM3. "
            "This speeds up mining by reducing prompt evals (recommended)."
        ),
    )
    steps_prompt_prefilter_mode: Literal["conservative", "balanced", "aggressive"] = Field(
        "balanced",
        description=(
            "CLIP prompt prefilter strength: conservative keeps more prompts, aggressive keeps fewer (faster). "
            "Balanced is a good default."
        ),
    )
    steps_prompt_bg_drop: bool = Field(
        True,
        description=(
            "When search_mode='steps', drop prompts that mostly trigger CLIP background classes "
            "(auto-skip noisy prompts before building steps). Requires a head with __bg_* classes."
        ),
    )
    steps_prompt_bg_drop_mode: Literal["conservative", "balanced", "aggressive"] = Field(
        "balanced",
        description=(
            "Background-drop strictness: conservative drops fewer prompts, aggressive drops more."
        ),
    )
    steps_optimize_tier2: bool = Field(
        False,
        description=(
            "When search_mode='steps' and a pretrained CLIP head is provided, enable bounded Tier-2 tuning of "
            "`seed_dedupe_iou` and `dedupe_iou` (applied globally per class across all steps)."
        ),
    )
    steps_optimize_tier2_eval_cap: int = Field(
        200,
        ge=10,
        le=50_000,
        description="Max number of sampled images used during Tier-2 tuning stages (search_mode='steps').",
    )
    steps_optimize_tier2_max_trials: int = Field(
        12,
        ge=1,
        le=256,
        description="Max candidate configurations evaluated during Tier-2 tuning (search_mode='steps').",
    )
    steps_refine_prompt_subset: bool = Field(
        False,
        description=(
            "When search_mode='steps', enable a bounded seed-stage local search that can add/drop/swap prompt steps "
            "after the initial greedy set-cover selection. This is a fast heuristic pass (no extra SAM runs)."
        ),
    )
    steps_refine_prompt_subset_max_iters: int = Field(
        6,
        ge=0,
        le=100,
        description="Max number of refinement iterations when steps_refine_prompt_subset is enabled (search_mode='steps').",
    )
    steps_refine_prompt_subset_top_k: int = Field(
        6,
        ge=1,
        le=50,
        description="How many add/swap candidates to consider per refinement iteration (search_mode='steps').",
    )
    steps_optimize_global: bool = Field(
        False,
        description=(
            "When search_mode='steps' and a pretrained CLIP head is provided, enable a budgeted global optimizer that "
            "jointly searches the whole recipe configuration (prompt subset + per-step knobs + optional ordering). "
            "This is the highest-quality mode but can be much slower than Tier-1 grid search or Tier-2."
        ),
    )
    steps_optimize_global_eval_caps: List[int] = Field(
        default_factory=lambda: [50, 200, 1000],
        description=(
            "When steps_optimize_global is enabled, sampled-image caps for successive-halving stages. "
            "Example: [50, 200, 1000] means candidates are first ranked on 50 sampled images, then the best subset is "
            "re-ranked on 200, then finalized on up to 1000 (or the full sample if smaller)."
        ),
    )
    steps_optimize_global_max_trials: int = Field(
        36,
        ge=1,
        le=4096,
        description="When steps_optimize_global is enabled, cap on candidate configs evaluated per class (stage-1).",
    )
    steps_optimize_global_keep_ratio: float = Field(
        0.5,
        ge=0.1,
        le=0.9,
        description="When steps_optimize_global is enabled, prune ratio per successive-halving stage (0.5 keeps top half).",
    )
    steps_optimize_global_rounds: int = Field(
        2,
        ge=1,
        le=20,
        description="When steps_optimize_global is enabled, how many mutation rounds to run (each round uses successive halving).",
    )
    steps_optimize_global_mutations_per_round: int = Field(
        24,
        ge=1,
        le=10_000,
        description="When steps_optimize_global is enabled, how many mutated candidates to generate per round (per class).",
    )
    steps_optimize_global_max_steps_mutated: int = Field(
        2,
        ge=1,
        le=10,
        description="When steps_optimize_global is enabled, maximum number of steps whose knobs can change in a single mutation.",
    )
    steps_optimize_global_enable_max_results: bool = Field(
        False,
        description="When steps_optimize_global is enabled, include max_results tuning (can impact precision/recall and runtime).",
    )
    steps_optimize_global_enable_ordering: bool = Field(
        False,
        description="When steps_optimize_global is enabled, include step ordering mutations (usually minor effect; can matter with caps).",
    )

    # Mining concurrency controls (SAM3 workers).
    # By default we fan out across all CUDA devices; this controls how many workers we run per device.
    max_workers_per_device: int = Field(1, ge=1, le=8)
    # Optional global cap on the total number of workers (after per-device expansion).
    max_workers: Optional[int] = Field(None, ge=1, le=256)

    # Prompt mining (Qwen).
    text_prompts_by_class: Optional[Dict[int, List[str]]] = None
    prompt_llm_max_prompts: int = Field(10, ge=0, le=50)
    prompt_max_new_tokens: int = Field(160, ge=16, le=800)
    # Deprecated: hint text injected into prompts (no longer used).
    class_hints: Optional[Dict[str, str]] = None
    # Optional: extra user-provided prompt phrases per class (merged into the prompt list).
    extra_prompts_by_class: Optional[Dict[str, List[str]]] = None

    # Pretrained CLIP head (LogReg) used for filtering/tuning during recipe mining.
    # NOTE: Agent Mining requires this; jobs will be rejected if missing.
    # This should point at a classifier file on the backend (typically under uploads/classifiers/).
    clip_head_classifier_path: Optional[str] = None
    clip_head_min_prob: float = Field(
        0.5,
        ge=0.0,
        le=1.0,
        description="Minimum probability for the target class when using an embedded CLIP head.",
    )
    clip_head_margin: float = Field(
        0.0,
        ge=0.0,
        le=1.0,
        description="Require target prob to exceed best other class by this margin when using an embedded CLIP head.",
    )
    clip_head_auto_tune: bool = Field(
        True,
        description=(
            "When using a pretrained CLIP head, auto-tune per-class min_prob/margin on the sampled images "
            "(recommended). If disabled, uses the fixed min_prob/margin values above."
        ),
    )
    clip_head_tune_margin: bool = Field(
        True,
        description=(
            "When auto-tuning a pretrained CLIP head, also tune the margin. "
            "Disable this to lock the margin while still tuning min_prob."
        ),
    )
    clip_head_target_precision: float = Field(
        0.75,
        ge=0.0,
        le=1.0,
        description=(
            "When auto-tuning a pretrained CLIP head, prefer thresholds that reach this target precision on "
            "the sampled images (higher = fewer false positives, lower recall)."
        ),
    )
    clip_head_background_guard: bool = Field(
        True,
        description=(
            "When enabled, reject detections that score higher for a background class than the target class "
            "(uses the __bg_* classes trained from negative crops)."
        ),
    )
    clip_head_background_margin: float = Field(
        0.0,
        ge=0.0,
        le=1.0,
        description=(
            "Optional margin when using background suppression: require the target class to beat the best "
            "background class by at least this amount."
        ),
    )
    clip_head_background_auto_tune: bool = Field(
        True,
        description=(
            "When using a pretrained CLIP head, auto-tune per-class background margin during recipe mining "
            "(recommended when __bg_* classes are present)."
        ),
    )
    clip_head_background_apply: Literal["seed", "final", "both"] = Field(
        "final",
        description=(
            "Where to apply background suppression: seed candidates only, final detections only, or both."
        ),
    )
    clip_head_background_penalty: float = Field(
        0.0,
        ge=0.0,
        le=2.0,
        description=(
            "Optional penalty weight applied during optimization to prefer recipes with lower background confusion."
        ),
    )

    steps_hard_negative_export: bool = Field(
        True,
        description=(
            "Export a capped set of hard-negative crops (false positives) during recipe mining so they can be "
            "replayed as background examples in future CLIP training."
        ),
    )
    steps_hard_negative_max_crops: int = Field(
        200,
        ge=0,
        le=5000,
        description="Maximum hard-negative crops to export per class (0 disables export).",
    )
    steps_hard_negative_min_prob: float = Field(
        0.1,
        ge=0.0,
        le=1.0,
        description="Minimum CLIP target probability for a false positive to be exported as a hard negative.",
    )

    # Greedy SAM3 recipe parameters.
    seed_threshold: float = Field(
        0.02,
        ge=0.0,
        le=1.0,
        description="Base text-candidate score threshold for the initial SAM3 text prompt pass (per-step thresholds are auto-selected from candidate curves by default).",
    )
    expand_threshold: float = Field(
        0.15,
        ge=0.0,
        le=1.0,
        description="SAM3 visual expansion score threshold used during similarity-based search.",
    )
    max_visual_seeds: int = Field(
        25,
        ge=0,
        le=500,
        description="Max candidate boxes to expand per step (used to bound similarity-based search).",
    )
    seed_dedupe_iou: float = Field(0.9, ge=0.0, le=1.0)
    dedupe_iou: float = Field(0.5, ge=0.0, le=1.0)
    mask_threshold: float = Field(0.5, ge=0.0, le=1.0)
    similarity_score: float = Field(0.25, ge=0.0, le=1.0)
    max_results: int = Field(1000, ge=1, le=5000)
    min_size: int = Field(0, ge=0, le=10_000)
    simplify_epsilon: float = Field(0.0, ge=0.0, le=1_000.0)

    # Evaluation / debug.
    iou_threshold: float = Field(0.5, ge=0.0, le=1.0)


def _expand_prompts_with_prompt_llm(
    class_name: str,
    base_prompts: List[str],
    max_new: int,
    log_fn: Optional[Callable[[str], None]] = None,
    max_new_tokens: int = 128,
) -> List[str]:
    """Use Qwen text-only generation to brainstorm additional prompt variants for a class."""
    cleaned_base = _sanitize_prompts(base_prompts)
    if max_new <= 0 or not cleaned_base:
        return []

    seen = {p.lower() for p in cleaned_base}
    suggestions: List[str] = []
    try:
        known_list_str = ", ".join(cleaned_base)

        def _log(msg: str) -> None:
            if log_fn:
                try:
                    log_fn(msg)
                except Exception:
                    pass

        def _run_brainstorm_with_retries(remaining: int, round_idx: int) -> List[str]:
            """Try up to 3 times (initial + 2 critiques) to get a clean list."""
            base_prompt = [
                "Generate diverse noun-phrase prompts for open-vocabulary object detection with SAM3.",
                f"Target class: '{_humanize_class_name(class_name)}'.",
                f"Known good prompts: {known_list_str}.",
            ]
            base_prompt.extend(
                [
                    f"Propose up to {remaining} NEW, concrete object names (1-3 words) that strictly describe this class.",
                    "Rules: letters/spaces/hyphens only; no numbers; no punctuation beyond commas between items; no adjectives alone; avoid repeats.",
                    "Return ONLY a comma-separated list. Example: thing one, thing two, thing three",
                ]
            )
            last_text = ""
            for attempt in range(3):
                prompt_lines = list(base_prompt)
                if attempt > 0:
                    prompt_lines.extend(
                        [
                            f"Previous output was invalid: {last_text or '(empty)'}",
                            f"Try again. Respond ONLY with up to {remaining} comma-separated noun phrases (1-3 words, letters/spaces/hyphens).",
                            "No commentary.",
                        ]
                    )
                prompt_text = "\n".join(prompt_lines)
                text = _generate_prompt_text(prompt_text, max_new_tokens=max_new_tokens)
                last_text = text
                if not text:
                    _log(f"Qwen prompt expansion (class={class_name}, round {round_idx + 1}, attempt {attempt + 1}) returned empty/invalid")
                    continue
                parsed = _parse_prompt_candidates(text, seen, remaining)
                if parsed and len(parsed) > remaining:
                    parsed = parsed[:remaining]
                _log(
                    f"Qwen prompt expansion (class={class_name}, round {round_idx + 1}, attempt {attempt + 1}"
                    "): "
                    f"{', '.join(parsed) if parsed else text}"
                )
                if parsed:
                    return parsed
                # If the only issue is duplication, don't treat it as a hard failure.
                dup_check = _parse_prompt_candidates(text, set(), remaining)
                if dup_check:
                    _log(
                        f"Qwen prompt expansion (class={class_name}, round {round_idx + 1}, attempt {attempt + 1}) "
                        "produced only duplicates; keeping existing list."
                    )
                    return []
                _log(f"Qwen prompt expansion (class={class_name}, round {round_idx + 1}, attempt {attempt + 1}) yielded no valid candidates")
            _log(f"Qwen prompt expansion (class={class_name}, round {round_idx + 1}) failed after 3 attempts")
            return []

        for round_idx in range(3):  # allow a few brainstorming rounds
            if len(suggestions) >= max_new:
                break
            remaining = max_new - len(suggestions)
            parsed = _run_brainstorm_with_retries(remaining, round_idx)
            if not parsed:
                continue
            suggestions.extend(parsed)
            if len(suggestions) >= max_new:
                break
    except Exception as exc:  # noqa: BLE001
        logger.warning("Prompt recipe: Qwen expansion failed for %s: %s", class_name, exc)
        suggestions = []
    # Final sanitize + dedupe. We intentionally avoid extra model passes here so this
    # helper stays lightweight and doesn't contend with SAM3 for GPU memory.
    reviewed = _sanitize_prompts([*cleaned_base, *suggestions])
    reviewed_lower = {p.lower() for p in cleaned_base}
    final_new: List[str] = []
    for p in reviewed:
        low = p.lower()
        if low in reviewed_lower:
            continue
        reviewed_lower.add(low)
        final_new.append(p)
        if len(final_new) >= max_new:
            break
    # Fallback: if nothing survived, return the cleaned base prompts only.
    if not final_new:
        if log_fn:
            try:
                log_fn(f"Qwen prompts fell back to base for {class_name}")
            except Exception:
                pass
        return cleaned_base
    return final_new


def _sanitize_prompts(prompts: List[str]) -> List[str]:
    cleaned: List[str] = []
    seen = set()
    for p in prompts:
        if not isinstance(p, str):
            continue
        val = p.strip()
        if not val:
            continue
        words = val.split()
        if not (1 <= len(words) <= 4):
            continue
        if any(len(w) <= 1 for w in words):
            continue
        key = val.lower()
        if key in seen:
            continue
        seen.add(key)
        cleaned.append(val)
    return cleaned


def _refine_prompts_with_qwen(prompts: List[str]) -> List[str]:
    prompts = _sanitize_prompts(prompts)
    if not prompts or Qwen3VLForConditionalGeneration is None or QWEN_IMPORT_ERROR:
        return prompts
    try:
        prompt_text = (
            "You are validating candidate noun phrases for open-vocabulary object detection. "
            "Keep only entries that are concrete object-like noun phrases (1-4 words, nouns included). "
            "Reject fragments, verbs, partial words, or unrelated terms. "
            "Respond ONLY as a comma-separated list, no numbering, no explanations, ending with STOP.\n"
            f"Candidates: {', '.join(prompts)}"
        )
        text = _generate_prompt_text(prompt_text, max_new_tokens=160)
        if not text:
            return prompts
        parts = [t.strip() for t in re.split(r"[,\\n]+", text) if t.strip() and t.strip().upper() != "STOP"]
        cleaned = _sanitize_prompts(parts)
        return cleaned or prompts
    except Exception:
        return prompts


def _qwen_self_filter_prompts(class_name: str, prompts: List[str]) -> List[str]:
    """Ask Qwen to self-critique the candidate prompts against the target class and return only credible entries."""
    prompts = _sanitize_prompts(prompts)
    if not prompts or Qwen3VLForConditionalGeneration is None or QWEN_IMPORT_ERROR:
        return prompts
    try:
        prompt_text = (
            "You are double-checking candidate noun phrases for object detection. "
            f"Target class: '{_humanize_class_name(class_name)}'. "
            "From the list, keep ONLY phrases that clearly describe that class (synonyms or sub-types). "
            "Drop anything ambiguous, misspelled, or unrelated. "
            "Return ONLY a comma-separated list, no explanations, ending with STOP.\n"
            f"Candidates: {', '.join(prompts)}"
        )
        text = _generate_prompt_text(prompt_text, max_new_tokens=160)
        if not text:
            return prompts
        parts = [t.strip() for t in re.split(r"[,\\n]+", text) if t.strip() and t.strip().upper() != "STOP"]
        cleaned = _sanitize_prompts(parts)
        return cleaned or prompts
    except Exception:
        return prompts


def _normalize_recipe_thresholds(thresholds: Optional[List[float]], fallback: float, limit: int = 20) -> List[float]:
    values = thresholds if thresholds is not None else [fallback]
    cleaned: List[float] = []
    seen = set()
    for raw in values:
        try:
            val = float(raw)
        except Exception:
            continue
        if val < 0.0 or val > 1.0:
            continue
        key = round(val, 4)
        if key in seen:
            continue
        seen.add(key)
        cleaned.append(val)
        if len(cleaned) >= limit:
            break
    return cleaned


def _expand_midpoints(values: List[float], *, fine_step: float = 0.05, clamp: Tuple[float, float] = (0.0, 1.0), limit: int = 20) -> List[float]:
    """Given a sorted list, add midpoints and small +/- offsets for coarse-to-fine sweeps."""
    if not values:
        return values
    lo, hi = clamp
    base = sorted({v for v in values if lo <= v <= hi})
    extras: List[float] = []
    for a, b in zip(base, base[1:]):
        mid = (a + b) / 2.0
        extras.append(mid)
    if fine_step > 0:
        for v in base:
            extras.append(v + fine_step)
            extras.append(v - fine_step)
    merged = sorted({v for v in [*base, *extras] if lo <= v <= hi})
    if len(merged) > limit:
        merged = merged[:limit]
    return merged


def _build_gt_index_for_class(
    gt_by_image_cat: Dict[int, Dict[int, List[List[float]]]], target_class: int
) -> Tuple[Dict[int, List[Tuple[str, Tuple[float, float, float, float]]]], set[str], Dict[int, int]]:
    gt_index: Dict[int, List[Tuple[str, Tuple[float, float, float, float]]]] = {}
    all_keys: set[str] = set()
    per_image_counts: Dict[int, int] = {}
    for img_id, by_cat in gt_by_image_cat.items():
        boxes = by_cat.get(target_class)
        if not boxes:
            continue
        entries: List[Tuple[str, Tuple[float, float, float, float]]] = []
        for idx, bbox in enumerate(boxes):
            key = f"{img_id}:{idx}"
            entries.append((key, _xywh_to_xyxy(bbox)))
            all_keys.add(key)
        gt_index[img_id] = entries
        per_image_counts[img_id] = len(entries)
    return gt_index, all_keys, per_image_counts


def _evaluate_prompt_candidate(
    prompt: str,
    threshold: float,
    *,
    cat_id: int,
    image_ids: List[int],
    gt_index: Dict[int, List[Tuple[str, Tuple[float, float, float, float]]]],
    other_gt_index: Optional[Dict[int, List[Tuple[str, Tuple[float, float, float, float]]]]] = None,
    images: Dict[int, Dict[str, Any]],
    iou_threshold: float,
    max_dets: int,
    image_cache: Dict[int, Image.Image],
    cached_detections: Optional[Dict[int, List[Tuple[float, float, float, float, Optional[float]]]]] = None,
) -> Dict[str, Any]:
    total_gt = sum(len(gt_index.get(img_id, [])) for img_id in image_ids)
    total_preds = 0
    conflicts = 0
    matches = 0
    fps = 0
    det_images = 0
    iou_sum = 0.0
    score_sum = 0.0
    matched_scores = 0
    matched_gt_keys: set[str] = set()
    matches_by_image: Dict[int, Dict[str, Any]] = {}
    for img_id in image_ids:
        info = images.get(img_id)
        if not info:
            continue
        path = info.get("path")
        width = info.get("width")
        height = info.get("height")
        if not path or width is None or height is None:
            continue
        gts = gt_index.get(img_id, [])
        gt_used = [False] * len(gts)
        pred_boxes: List[Tuple[float, float, float, float, Optional[float]]] = []
        if cached_detections is not None:
            pred_boxes = cached_detections.get(img_id, [])
        if not pred_boxes:
            try:
                pil_img = image_cache[img_id]
            except KeyError:
                try:
                    pil_img = Image.open(path).convert("RGB")
                except Exception:
                    continue
                image_cache[img_id] = pil_img
            preds = _run_sam3_text_inference(
                pil_img,
                prompt,
                threshold=threshold,
                mask_threshold=0.0,
                limit=max_dets,
            )
            for det in preds:
                try:
                    x1, y1, x2, y2 = _yolo_to_xyxy(pil_img.width, pil_img.height, det.bbox)
                    pred_boxes.append((x1, y1, x2, y2, det.score))
                except Exception:
                    continue
        if not pred_boxes:
            continue
        filtered = [b for b in pred_boxes if (b[4] if b[4] is not None else 0.0) >= threshold]
        filtered.sort(key=lambda b: (b[4] if b[4] is not None else 0.0), reverse=True)
        pred_boxes = filtered[:max_dets] if max_dets else filtered
        total_preds += len(pred_boxes)
        matched_in_image = 0
        fp_in_image = 0
        matched_keys: List[str] = []
        for x1, y1, x2, y2, score in pred_boxes:
            if other_gt_index:
                other_hits = other_gt_index.get(img_id, [])
                conflict_found = False
                for _, other_box in other_hits:
                    if _iou((x1, y1, x2, y2), other_box) >= iou_threshold:
                        conflicts += 1
                        conflict_found = True
                        break
                if conflict_found:
                    continue
            total_preds += 1
            best_iou = 0.0
            best_idx = -1
            for idx, (_, gt_box) in enumerate(gts):
                if gt_used[idx]:
                    continue
                iou = _iou((x1, y1, x2, y2), gt_box)
                if iou > best_iou:
                    best_iou = iou
                    best_idx = idx
            if best_iou >= iou_threshold and best_idx >= 0:
                gt_used[best_idx] = True
                matches += 1
                matched_in_image += 1
                matched_key = gts[best_idx][0]
                matched_keys.append(matched_key)
                matched_gt_keys.add(matched_key)
                iou_sum += best_iou
                if score is not None:
                    score_sum += score
                    matched_scores += 1
            else:
                fp_in_image += 1
        fps += fp_in_image
        if matched_in_image > 0:
            det_images += 1
        if matched_keys or fp_in_image:
            matches_by_image[img_id] = {"matched": matched_keys, "fps": fp_in_image}
    precision = matches / total_preds if total_preds else 0.0
    recall = matches / total_gt if total_gt else 0.0
    det_rate = det_images / len(image_ids) if image_ids else 0.0
    avg_iou = iou_sum / matches if matches else None
    avg_score = score_sum / matched_scores if matched_scores else None
    f1 = (2 * precision * recall) / (precision + recall + 1e-8) if (precision + recall) > 0 else 0.0
    overall_score = f1 * (0.5 + 0.5 * det_rate)
    return {
        "prompt": prompt,
        "threshold": threshold,
        "precision": precision,
        "recall": recall,
        "det_rate": det_rate,
        "avg_iou": avg_iou,
        "avg_score": avg_score,
        "score": overall_score,
        "f1": f1,
        "preds": total_preds,
        "matches": matches,
        "gts": total_gt,
        "fps": fps,
        "conflicts": conflicts,
        "det_images": det_images,
        "matched_gt_keys": matched_gt_keys,
        "matches_by_image": matches_by_image,
    }


def _collect_prompt_detections(
    prompt: str,
    min_threshold: float,
    *,
    image_ids: List[int],
    images: Dict[int, Dict[str, Any]],
    image_cache: Dict[int, Image.Image],
    max_dets: int,
) -> Dict[int, List[Tuple[float, float, float, float, Optional[float]]]]:
    results: Dict[int, List[Tuple[float, float, float, float, Optional[float]]]] = {}
    for img_id in image_ids:
        info = images.get(img_id)
        if not info:
            continue
        path = info.get("path")
        width = info.get("width")
        height = info.get("height")
        if not path or width is None or height is None:
            continue
        try:
            pil_img = image_cache[img_id]
        except KeyError:
            try:
                pil_img = Image.open(path).convert("RGB")
            except Exception:
                continue
            image_cache[img_id] = pil_img
        preds = _run_sam3_text_inference(
            pil_img,
            prompt,
            threshold=min_threshold,
            mask_threshold=0.0,
            limit=max_dets,
        )
        boxes: List[Tuple[float, float, float, float, Optional[float]]] = []
        for det in preds:
            try:
                x1, y1, x2, y2 = _yolo_to_xyxy(pil_img.width, pil_img.height, det.bbox)
                boxes.append((x1, y1, x2, y2, det.score))
            except Exception:
                continue
        if boxes:
            results[img_id] = boxes
    return results


def _build_prompt_recipe(
    candidates: List[Dict[str, Any]],
    all_gt_keys: set[str],
    per_image_gt: Dict[int, int],
    images: Dict[int, Dict[str, Any]],
    image_ids: List[int],
    gt_index: Dict[int, List[Tuple[str, Tuple[float, float, float, float]]]],
) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
    # Pick the best threshold per prompt (highest matched GTs, then lowest FPs, then higher precision).
    best_by_prompt: Dict[str, Dict[str, Any]] = {}
    for cand in candidates:
        prompt = cand.get("prompt") or ""
        matched_count = len(cand.get("matched_gt_keys") or [])
        fps = cand.get("fps", 0)
        precision = cand.get("precision", 0.0)
        current = best_by_prompt.get(prompt)
        if current is None:
            best_by_prompt[prompt] = cand
            continue
        curr_matched = len(current.get("matched_gt_keys") or [])
        curr_fps = current.get("fps", 0)
        curr_precision = current.get("precision", 0.0)
        better = (matched_count, -fps, precision) > (curr_matched, -curr_fps, curr_precision)
        if better:
            best_by_prompt[prompt] = cand
    ordered_candidates = list(best_by_prompt.values())

    # Simulate per-image early stop: run prompts only on images with uncovered GTs; negatives always contribute FPs.
    remaining_by_image: Dict[int, set[str]] = {
        img_id: {key for key, _ in entries} for img_id, entries in gt_index.items() if entries
    }
    remaining_total = sum(len(v) for v in remaining_by_image.values())
    active_images = {img_id for img_id, keys in remaining_by_image.items() if keys}
    negative_images = {img_id for img_id in image_ids if per_image_gt.get(img_id, 0) == 0}
    steps: List[Dict[str, Any]] = []
    total_fps = 0
    total_duplicates = 0
    used_prompt_keys: set[Tuple[str, float]] = set()
    while remaining_total > 0 and ordered_candidates:
        best = None
        best_score = (-1, -1, -1, -1)
        for cand in ordered_candidates:
            prompt_key = (cand.get("prompt"), cand.get("threshold"))
            if prompt_key in used_prompt_keys:
                continue
            matches_by_image = cand.get("matches_by_image") or {}
            step_gain = 0
            step_fps = 0
            step_duplicates = 0
            step_matches_total = 0
            step_hits_by_image: Dict[int, Dict[str, Any]] = {}
            for img_id in negative_images:
                img_hits = matches_by_image.get(img_id)
                if not img_hits:
                    continue
                matched_total = len(img_hits.get("matched") or [])
                fps_count = max(0, img_hits.get("fps", 0))
                step_matches_total += matched_total
                step_fps += fps_count
                if matched_total or fps_count:
                    step_hits_by_image[img_id] = {
                        "matched": [],
                        "matched_total": matched_total,
                        "fps": fps_count,
                    }
            for img_id in active_images:
                img_hits = matches_by_image.get(img_id)
                if not img_hits:
                    continue
                matched_list = img_hits.get("matched") or []
                matched_set = set(matched_list)
                unmatched = remaining_by_image.get(img_id, set())
                new_hits = matched_set & unmatched
                matched_total = len(matched_set)
                fps_count = max(0, img_hits.get("fps", 0))
                step_gain += len(new_hits)
                step_duplicates += max(0, matched_total - len(new_hits))
                step_matches_total += matched_total
                step_fps += fps_count
                step_hits_by_image[img_id] = {
                    "matched": list(new_hits),
                    "matched_total": matched_total,
                    "fps": fps_count,
                }
            if step_gain <= 0:
                continue
            zero_fp = step_fps == 0
            score_tuple = (
                1 if zero_fp else 0,
                step_gain,
                -step_fps,
                cand.get("precision", 0.0),
            )
            if score_tuple > best_score:
                best = (cand, step_hits_by_image, step_gain, step_fps, step_duplicates, step_matches_total)
                best_score = score_tuple
        if not best:
            break
        cand, step_hits_by_image, gain, step_fps, duplicate_hits, step_matches_total = best
        prompt_key = (cand.get("prompt"), cand.get("threshold"))
        used_prompt_keys.add(prompt_key)
        for img_id, hit_info in step_hits_by_image.items():
            new_hits = set(hit_info.get("matched") or [])
            if not new_hits:
                continue
            current_unmatched = remaining_by_image.get(img_id)
            if current_unmatched is None:
                continue
            remaining_by_image[img_id] = current_unmatched - new_hits
        active_images = {img_id for img_id, keys in remaining_by_image.items() if keys}
        remaining_total = max(0, remaining_total - gain)
        total_duplicates += duplicate_hits
        total_fps += max(0, step_fps)
        covered_after = len(all_gt_keys) - remaining_total
        cum_coverage = covered_after / len(all_gt_keys) if all_gt_keys else 0.0
        preds_in_step = step_matches_total + step_fps
        seq_precision = step_matches_total / preds_in_step if preds_in_step else 0.0
        steps.append(
            {
                "prompt": cand.get("prompt"),
                "threshold": cand.get("threshold"),
                "gain": gain,
                "matches": step_matches_total,
                "fps": step_fps,
                "precision": seq_precision,
                "recall": cand.get("recall"),
                "det_rate": cand.get("det_rate"),
                "avg_iou": cand.get("avg_iou"),
                "avg_score": cand.get("avg_score"),
                "duplicates": duplicate_hits,
                "covered_after": covered_after,
                "cum_coverage": cum_coverage,
                "cum_fps": total_fps,
                "_matches_by_image": step_hits_by_image,
                "_prompt_precision": cand.get("precision"),
                "similarity_score": cand.get("similarity_score"),
            }
        )
    coverage_rate = (len(all_gt_keys) - remaining_total) / len(all_gt_keys) if all_gt_keys else 0.0
    recipe = {
        "steps": [
            {
                **{k: v for k, v in step.items() if not k.startswith("_")},
                "coverage_after": (step.get("covered_after", 0) / len(all_gt_keys)) if all_gt_keys else 0.0,
            }
            for step in steps
        ],
        "summary": {
            "total_gt": len(all_gt_keys),
            "covered": len(all_gt_keys) - remaining_total,
            "coverage_rate": coverage_rate,
            "fps": total_fps,
            "duplicates": total_duplicates,
        },
    }
    coverage_by_image: List[Dict[str, Any]] = []
    coverage_map: Dict[int, Dict[str, Any]] = {}
    for img_id in image_ids:
        info = images.get(img_id, {})
        remaining_keys = remaining_by_image.get(img_id, set())
        is_positive = per_image_gt.get(img_id, 0) > 0
        entry = {
            "image_id": img_id,
            "file_name": info.get("file_name"),
            "gt": per_image_gt.get(img_id, 0),
            "hits": [],
            "type": "pos" if is_positive else "neg",
            "covered": per_image_gt.get(img_id, 0) == 0 or len(remaining_keys) == 0,
        }
        coverage_map[img_id] = entry
        coverage_by_image.append(entry)
    for idx, step in enumerate(steps):
        matches_by_image = step.get("_matches_by_image") or {}
        for img_id, img_info in matches_by_image.items():
            target = coverage_map.get(img_id)
            if not target:
                continue
            matched_list = img_info.get("matched") or []
            fp_count = img_info.get("fps", 0)
            target["hits"].append(
                {
                    "step": idx,
                    "prompt": step.get("prompt"),
                    "threshold": step.get("threshold"),
                    "matched": len(matched_list),
                    "fps": fp_count,
                }
            )
    return recipe, coverage_by_image


def _serialize_prompt_helper_job(job: PromptHelperJob) -> Dict[str, Any]:
    return {
        "job_id": job.job_id,
        "status": job.status,
        "message": job.message,
        "progress": job.progress,
        "total_steps": job.total_steps,
        "completed_steps": job.completed_steps,
        "created_at": job.created_at,
        "updated_at": job.updated_at,
        "request": job.request,
        "result": job.result,
        "logs": job.logs,
        "error": job.error,
    }


def _serialize_agent_mining_job(job: AgentMiningJob) -> Dict[str, Any]:
    return {
        "job_id": job.job_id,
        "status": job.status,
        "message": job.message,
        "progress": job.progress,
        "created_at": job.created_at,
        "updated_at": job.updated_at,
        "request": job.request,
        "result": job.result,
        "logs": job.logs,
        "error": job.error,
    }


def _serialize_calibration_job(job: CalibrationJob) -> Dict[str, Any]:
    return {
        "job_id": job.job_id,
        "status": job.status,
        "message": job.message,
        "phase": job.phase,
        "progress": job.progress,
        "processed": job.processed,
        "total": job.total,
        "created_at": job.created_at,
        "updated_at": job.updated_at,
        "request": job.request,
        "result": job.result,
        "error": job.error,
    }


def _calibration_update(job: CalibrationJob, **kwargs: Any) -> None:
    for key, value in kwargs.items():
        setattr(job, key, value)
    job.updated_at = time.time()


def _calibration_write_record_atomic(path: Path, record: Dict[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp_path = path.with_suffix(path.suffix + ".tmp")
    tmp_path.write_text(json.dumps(record, ensure_ascii=False))
    tmp_path.replace(path)


def _calibration_prepass_worker(
    device_index: int,
    tasks: List[Tuple[str, str]],
    dataset_id: str,
    labelmap: List[str],
    glossary: str,
    prepass_payload_dict: Dict[str, Any],
    cancel_event: Optional[Any],
    progress_queue: Optional[Any],
) -> None:
    # NOTE: This runs in a separate process; re-bind device prefs here.
    try:
        if torch.cuda.is_available():
            torch.cuda.set_device(device_index)
    except Exception:
        pass
    try:
        global SAM3_DEVICE_PREF
        SAM3_DEVICE_PREF = f"cuda:{device_index}" if torch.cuda.is_available() else "cpu"
    except Exception:
        pass
    try:
        prepass_payload = QwenPrepassRequest(**prepass_payload_dict)
    except Exception:
        return
    dataset_root = _resolve_sam3_or_qwen_dataset(dataset_id)
    for image_name, cache_path in tasks:
        if cancel_event is not None and cancel_event.is_set():
            break
        img_path = None
        for split in ("val", "train"):
            candidate = dataset_root / split / image_name
            if candidate.exists():
                img_path = candidate
                break
        if img_path is None:
            if progress_queue is not None:
                progress_queue.put(1)
            continue
        try:
            with Image.open(img_path) as img:
                pil_img = img.convert("RGB")
        except Exception:
            if progress_queue is not None:
                progress_queue.put(1)
            continue
        try:
            image_token = _calibration_cache_image(pil_img, prepass_payload.sam_variant)
            result = _agent_run_deep_prepass(
                prepass_payload,
                pil_img=pil_img,
                image_token=image_token,
                labelmap=labelmap,
                glossary=glossary,
                trace_writer=None,
                trace_full_writer=None,
                trace_readable=None,
            )
            detections = list(result.get("detections") or [])
            warnings = list(result.get("warnings") or [])
            record = {
                "image": image_name,
                "dataset_id": dataset_id,
                "detections": detections,
                "warnings": warnings,
            }
            _calibration_write_record_atomic(Path(cache_path), record)
        except Exception:
            pass
        if progress_queue is not None:
            progress_queue.put(1)


def _calibration_list_images(dataset_id: str) -> List[str]:
    dataset_root = _resolve_sam3_or_qwen_dataset(dataset_id)
    images: List[str] = []
    for split in ("val", "train"):
        split_root = dataset_root / split
        if not split_root.exists():
            continue
        for entry in sorted(split_root.iterdir()):
            if entry.is_file() and entry.suffix.lower() in {".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff"}:
                images.append(entry.name)
    return images


def _calibration_sample_images(images: List[str], *, max_images: int, seed: int) -> List[str]:
    if max_images <= 0 or len(images) <= max_images:
        return list(images)
    rng = random.Random(seed)
    picks = list(images)
    rng.shuffle(picks)
    return picks[:max_images]


def _calibration_cache_image(pil_img: Image.Image, sam_variant: Optional[str]) -> str:
    np_img = np.ascontiguousarray(np.array(pil_img.convert("RGB")))
    token = hashlib.md5(np_img.tobytes()).hexdigest()
    _store_preloaded_image(token, np_img, _default_variant(sam_variant))
    return token


def _calibration_hash_payload(payload: Dict[str, Any]) -> str:
    serialized = json.dumps(payload, sort_keys=True, ensure_ascii=True)
    return hashlib.sha1(serialized.encode("utf-8")).hexdigest()


def _calibration_safe_link(src: Path, dest: Path) -> None:
    try:
        if dest.is_symlink() and not dest.exists():
            dest.unlink()
        if dest.exists() or dest.is_symlink():
            return
        dest.parent.mkdir(parents=True, exist_ok=True)
        os.symlink(str(src.resolve()), dest)
    except Exception:
        try:
            if dest.exists():
                return
            shutil.copy2(src, dest)
        except Exception:
            pass


def _run_calibration_job(job: CalibrationJob, payload: CalibrationRequest) -> None:
    with CALIBRATION_JOBS_LOCK:
        CALIBRATION_JOBS[job.job_id] = job
    _calibration_update(job, status="running", message="Selecting images…", phase="select_images", request=payload.dict())
    output_dir = CALIBRATION_ROOT / job.job_id
    output_dir.mkdir(parents=True, exist_ok=True)
    try:
        _require_sam3_for_prepass(True, True)
        _prepare_for_training()
        labelmap, glossary = _agent_load_labelmap_meta(payload.dataset_id)
        if not labelmap:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="calibration_labelmap_missing")
        images = _calibration_list_images(payload.dataset_id)
        if not images:
            raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="calibration_images_missing")
        max_images = int(payload.max_images or 0)
        seed = int(payload.seed or 0)
        selected = _calibration_sample_images(images, max_images=max_images, seed=seed)
        total = len(selected)
        _calibration_update(job, total=total, processed=0, progress=0.0)

        prepass_path = output_dir / "prepass.jsonl"
        CALIBRATION_CACHE_ROOT.mkdir(parents=True, exist_ok=True)
        if not payload.classifier_id and not isinstance(active_classifier_head, dict):
            raise HTTPException(status_code=HTTP_412_PRECONDITION_FAILED, detail="calibration_classifier_required")
        prepass_payload = QwenPrepassRequest(
            dataset_id=payload.dataset_id,
            enable_yolo=True,
            enable_rfdetr=True,
            sam_variant="sam3",
            enable_sam3_text=True,
            enable_sam3_similarity=True,
            prepass_caption=False,
            prepass_only=True,
            prepass_finalize=False,
            prepass_keep_all=True,
            sam3_text_synonym_budget=None
            if payload.sam3_text_synonym_budget is None
            else int(payload.sam3_text_synonym_budget),
            sam3_text_window_extension=bool(payload.sam3_text_window_extension)
            if payload.sam3_text_window_extension is not None
            else True,
            sam3_text_window_mode=payload.sam3_text_window_mode or "grid",
            sam3_text_window_size=payload.sam3_text_window_size,
            sam3_text_window_overlap=payload.sam3_text_window_overlap,
            prepass_sam3_text_thr=float(payload.prepass_sam3_text_thr or 0.2),
            prepass_similarity_score=float(payload.prepass_similarity_score or 0.3),
            similarity_min_exemplar_score=float(payload.similarity_min_exemplar_score or 0.6),
            similarity_window_extension=bool(payload.similarity_window_extension),
            sam3_score_thr=float(payload.sam3_score_thr or 0.2),
            sam3_mask_threshold=float(payload.sam3_mask_threshold or 0.2),
            detector_conf=float(payload.detector_conf or 0.45),
            sahi_window_size=payload.sahi_window_size,
            sahi_overlap_ratio=payload.sahi_overlap_ratio,
            classifier_id=payload.classifier_id,
            scoreless_iou=float(payload.scoreless_iou or 0.0),
            iou=float(payload.dedupe_iou or 0.75),
        )

        labelmap_hash = hashlib.sha1(",".join(labelmap).encode("utf-8")).hexdigest()
        glossary_hash = hashlib.sha1((glossary or "").encode("utf-8")).hexdigest()
        selected_hash = hashlib.sha1(json.dumps(selected, sort_keys=True).encode("utf-8")).hexdigest()
        prepass_config = {
            "sam3_text_synonym_budget": None
            if payload.sam3_text_synonym_budget is None
            else int(payload.sam3_text_synonym_budget),
            "sam3_text_window_extension": payload.sam3_text_window_extension,
            "sam3_text_window_mode": payload.sam3_text_window_mode,
            "sam3_text_window_size": payload.sam3_text_window_size,
            "sam3_text_window_overlap": payload.sam3_text_window_overlap,
            "prepass_sam3_text_thr": float(payload.prepass_sam3_text_thr or 0.2),
            "prepass_similarity_score": float(payload.prepass_similarity_score or 0.3),
            "similarity_min_exemplar_score": float(payload.similarity_min_exemplar_score or 0.6),
            "similarity_window_extension": bool(payload.similarity_window_extension),
            "sam3_score_thr": float(payload.sam3_score_thr or 0.2),
            "sam3_mask_threshold": float(payload.sam3_mask_threshold or 0.2),
            "detector_conf": float(payload.detector_conf or 0.45),
            "sahi_window_size": payload.sahi_window_size,
            "sahi_overlap_ratio": payload.sahi_overlap_ratio,
            "scoreless_iou": float(payload.scoreless_iou or 0.0),
            "dedupe_iou": float(payload.dedupe_iou or 0.75),
        }
        prepass_config_key = _calibration_hash_payload(
            {
                "dataset_id": payload.dataset_id,
                "labelmap_hash": labelmap_hash,
                "glossary_hash": glossary_hash,
                "prepass": prepass_config,
            }
        )
        prepass_key = _calibration_hash_payload(
            {
                "prepass_config_key": prepass_config_key,
                "selected_hash": selected_hash,
            }
        )
        prepass_cache_dir = CALIBRATION_CACHE_ROOT / "prepass" / prepass_config_key
        image_cache_dir = prepass_cache_dir / "images"
        image_cache_dir.mkdir(parents=True, exist_ok=True)
        
        prepass_cache_meta = prepass_cache_dir / "prepass.meta.json"

        def _safe_image_cache_name(image_name: str) -> str:
            safe = re.sub(r"[^A-Za-z0-9_.-]+", "_", image_name)
            if safe != image_name:
                suffix = hashlib.sha1(image_name.encode("utf-8")).hexdigest()[:8]
                safe = f"{safe}_{suffix}"
            return safe

        def _cache_path_for_image(image_name: str) -> Path:
            return image_cache_dir / f"{_safe_image_cache_name(image_name)}.json"

        def _load_cached_record(image_name: str) -> Optional[Dict[str, Any]]:
            path = _cache_path_for_image(image_name)
            if not path.exists():
                return None
            try:
                return json.loads(path.read_text())
            except Exception:
                return None

        def _write_cached_record(image_name: str, record: Dict[str, Any]) -> None:
            path = _cache_path_for_image(image_name)
            _calibration_write_record_atomic(path, record)

        cached_records: Dict[str, Dict[str, Any]] = {}
        for image_name in selected:
            cached = _load_cached_record(image_name)
            if cached:
                cached_records[image_name] = cached

        processed = len(cached_records)
        if processed:
            _calibration_update(
                job,
                message="Using cached prepass (partial)…",
                phase="prepass",
                processed=processed,
                progress=processed / total if total else 1.0,
            )

        if processed < total:
            _calibration_update(job, message="Running deep prepass…", phase="prepass")
            remaining = [image_name for image_name in selected if image_name not in cached_records]
            if torch.cuda.is_available() and torch.cuda.device_count() > 1 and remaining:
                _unload_inference_runtimes()
                devices = list(range(torch.cuda.device_count()))
                worker_count = min(len(devices), len(remaining))
                tasks = [
                    (image_name, str(_cache_path_for_image(image_name)))
                    for image_name in remaining
                ]
                buckets: List[List[Tuple[str, str]]] = [[] for _ in range(worker_count)]
                for idx, task in enumerate(tasks):
                    buckets[idx % worker_count].append(task)
                ctx = multiprocessing.get_context("spawn")
                mp_cancel = ctx.Event()
                progress_queue = ctx.Queue()
                workers = []
                prepass_payload_dict = prepass_payload.dict()
                for worker_idx in range(worker_count):
                    device_index = devices[worker_idx]
                    bucket = buckets[worker_idx]
                    if not bucket:
                        continue
                    proc = ctx.Process(
                        target=_calibration_prepass_worker,
                        args=(
                            device_index,
                            bucket,
                            payload.dataset_id,
                            labelmap,
                            glossary,
                            prepass_payload_dict,
                            mp_cancel,
                            progress_queue,
                        ),
                        daemon=True,
                    )
                    proc.start()
                    workers.append(proc)
                processed_local = 0
                while any(proc.is_alive() for proc in workers):
                    if job.cancel_event.is_set():
                        mp_cancel.set()
                    try:
                        inc = progress_queue.get(timeout=1.0)
                        if isinstance(inc, int):
                            processed_local += inc
                            processed = len(cached_records) + processed_local
                            progress = processed / total if total else 1.0
                            _calibration_update(job, processed=processed, progress=progress)
                    except queue.Empty:
                        pass
                    if mp_cancel.is_set():
                        break
                for proc in workers:
                    proc.join(timeout=5)
                if mp_cancel.is_set():
                    for proc in workers:
                        if proc.is_alive():
                            proc.terminate()
                    raise RuntimeError("cancelled")
            else:
                for image_name in remaining:
                    if job.cancel_event.is_set():
                        raise RuntimeError("cancelled")
                    img_path = None
                    dataset_root = _resolve_sam3_or_qwen_dataset(payload.dataset_id)
                    for split in ("val", "train"):
                        candidate = dataset_root / split / image_name
                        if candidate.exists():
                            img_path = candidate
                            break
                    if img_path is None:
                        continue
                    with Image.open(img_path) as img:
                        pil_img = img.convert("RGB")
                    image_token = _calibration_cache_image(pil_img, prepass_payload.sam_variant)
                    result = _agent_run_deep_prepass(
                        prepass_payload,
                        pil_img=pil_img,
                        image_token=image_token,
                        labelmap=labelmap,
                        glossary=glossary,
                        trace_writer=None,
                        trace_full_writer=None,
                        trace_readable=None,
                    )
                    detections = list(result.get("detections") or [])
                    warnings = list(result.get("warnings") or [])
                    record = {
                        "image": image_name,
                        "dataset_id": payload.dataset_id,
                        "detections": detections,
                        "warnings": warnings,
                    }
                    _write_cached_record(image_name, record)
                    cached_records[image_name] = record
                    processed += 1
                    progress = processed / total if total else 1.0
                    _calibration_update(job, processed=processed, progress=progress)

        with prepass_path.open("w", encoding="utf-8") as handle:
            for image_name in selected:
                record = cached_records.get(image_name) or _load_cached_record(image_name)
                if not record:
                    continue

        prepass_cache_meta.write_text(
            json.dumps(
                {
                    "dataset_id": payload.dataset_id,
                    "labelmap_hash": labelmap_hash,
                    "glossary_hash": glossary_hash,
                    "prepass_config": prepass_config,
                    "cached_images": len(list(image_cache_dir.glob("*.json"))),
                    "updated_at": time.time(),
                    "config_key": prepass_config_key,
                },
                indent=2,
            )
        )

        if job.cancel_event.is_set():
            raise RuntimeError("cancelled")

        features_path = output_dir / "ensemble_features.npz"
        labeled_path = output_dir / f"ensemble_features_iou{float(payload.label_iou or 0.9):.2f}.npz"
        model_prefix = output_dir / "ensemble_mlp"
        meta_path = Path(str(model_prefix) + ".meta.json")
        eval_path = output_dir / "ensemble_mlp.eval.json"

        root_dir = Path(__file__).resolve().parent

        def _run_step(phase: str, message: str, args: List[str]) -> None:
            _calibration_update(job, phase=phase, message=message)
            if job.cancel_event.is_set():
                raise RuntimeError("cancelled")
            subprocess.run(args, check=True)

        classifier_id = payload.classifier_id or ""
        features_key = _calibration_hash_payload(
            {
                "prepass_key": prepass_key,
                "classifier_id": classifier_id or "",
                "support_iou": float(payload.support_iou or 0.5),
                "min_crop_size": 4,
                "context_radius": float(payload.context_radius or 0.075),
                "features_version": CALIBRATION_FEATURES_VERSION,
            }
        )
        features_cache_dir = CALIBRATION_CACHE_ROOT / "features" / features_key
        features_cache_dir.mkdir(parents=True, exist_ok=True)
        features_cache_path = features_cache_dir / "ensemble_features.npz"
        features_cache_meta = features_cache_dir / "features.meta.json"
        cached_features = features_cache_path.exists()

        if cached_features:
            _calibration_update(job, phase="features", message="Using cached features…", progress=1.0)
            _calibration_safe_link(features_cache_path, features_path)
        else:
            _run_step(
                "features",
                "Building features…",
                [
                    sys.executable,
                    str(root_dir / "tools" / "build_ensemble_features.py"),
                    "--input",
                    str(prepass_path),
                    "--dataset",
                    payload.dataset_id,
                    "--output",
                    str(features_cache_path),
                    "--support-iou",
                    str(float(payload.support_iou or 0.5)),
                    "--min-crop-size",
                    "4",
                    "--context-radius",
                    str(float(payload.context_radius or 0.075)),
                    "--device",
                    "cuda",
                ]
                + (["--classifier-id", classifier_id] if classifier_id else []),
            )
            features_cache_meta.write_text(
                json.dumps(
                    {
                        "prepass_key": prepass_key,
                        "features_key": features_key,
                        "features_version": CALIBRATION_FEATURES_VERSION,
                    },
                    indent=2,
                )
            )
            _calibration_safe_link(features_cache_path, features_path)

        labeled_key = _calibration_hash_payload(
            {
                "features_key": features_key,
                "label_iou": float(payload.label_iou or 0.9),
            }
        )
        labeled_cache_dir = CALIBRATION_CACHE_ROOT / "labeled" / labeled_key
        labeled_cache_dir.mkdir(parents=True, exist_ok=True)
        labeled_cache_path = labeled_cache_dir / f"ensemble_features_iou{float(payload.label_iou or 0.9):.2f}.npz"
        labeled_cache_meta = labeled_cache_dir / "labeled.meta.json"
        cached_labeled = labeled_cache_path.exists()

        if cached_labeled:
            _calibration_update(job, phase="labeling", message="Using cached labels…", progress=1.0)
            _calibration_safe_link(labeled_cache_path, labeled_path)
        else:
            _run_step(
                "labeling",
                "Labeling candidates…",
                [
                    sys.executable,
                    str(root_dir / "tools" / "label_candidates_iou90.py"),
                    "--input",
                    str(features_path),
                    "--dataset",
                    payload.dataset_id,
                    "--output",
                    str(labeled_cache_path),
                    "--iou",
                    str(float(payload.label_iou or 0.9)),
                ],
            )
            labeled_cache_meta.write_text(
                json.dumps(
                    {
                        "features_key": features_key,
                        "labeled_key": labeled_key,
                        "label_iou": float(payload.label_iou or 0.9),
                    },
                    indent=2,
                )
            )
            _calibration_safe_link(labeled_cache_path, labeled_path)
        _run_step(
            "train",
            "Training MLP…",
            [
                sys.executable,
                str(root_dir / "tools" / "train_ensemble_mlp.py"),
                "--input",
                str(labeled_path),
                "--output",
                str(model_prefix),
                "--hidden",
                str(payload.model_hidden or "256,128"),
                "--dropout",
                str(float(payload.model_dropout or 0.1)),
                "--epochs",
                str(int(payload.model_epochs or 20)),
                "--lr",
                str(float(payload.model_lr or 1e-3)),
                "--weight-decay",
                str(float(payload.model_weight_decay or 1e-4)),
                "--seed",
                str(int(payload.model_seed or 42)),
                "--device",
                "cuda",
            ],
        )
        optimize_metric = (payload.optimize_metric or "f1").strip().lower()
        if optimize_metric not in {"f1", "recall"}:
            optimize_metric = "f1"
        steps_val = int(payload.threshold_steps or 200)
        steps_val = max(20, min(1000, steps_val))
        calibrate_cmd = [
            sys.executable,
            str(root_dir / "tools" / "calibrate_ensemble_threshold.py"),
            "--model",
            str(Path(str(model_prefix) + ".pt")),
            "--data",
            str(labeled_path),
            "--meta",
            str(meta_path),
            "--target-fp-ratio",
            str(float(payload.base_fp_ratio or 0.1)),
            "--min-recall",
            str(float(payload.recall_floor or 0.6)),
            "--steps",
            str(steps_val),
            "--optimize",
            optimize_metric,
        ]
        if payload.per_class_thresholds is not False:
            calibrate_cmd.append("--per-class")
        _run_step("calibrate", "Calibrating thresholds…", calibrate_cmd)
        _run_step(
            "relax",
            "Relaxing thresholds…",
            [
                sys.executable,
                str(root_dir / "tools" / "relax_ensemble_thresholds.py"),
                "--model",
                str(Path(str(model_prefix) + ".pt")),
                "--data",
                str(labeled_path),
                "--meta",
                str(meta_path),
                "--fp-ratio-cap",
                str(float(payload.relax_fp_ratio or 0.2)),
            ],
        )
        _calibration_update(job, phase="eval", message="Evaluating model…")
        if job.cancel_event.is_set():
            raise RuntimeError("cancelled")
        iou_grid = payload.eval_iou_grid or "0.5,0.6,0.7,0.75,0.8,0.85,0.9"
        dedupe_grid = payload.dedupe_iou_grid or iou_grid
        eval_cmd = [
            sys.executable,
            str(root_dir / "tools" / "eval_ensemble_mlp_dedupe.py"),
            "--model",
            str(Path(str(model_prefix) + ".pt")),
            "--meta",
            str(meta_path),
            "--data",
            str(labeled_path),
            "--dataset",
            payload.dataset_id,
            "--eval-iou",
            str(float(payload.eval_iou or 0.5)),
            "--eval-iou-grid",
            iou_grid,
            "--dedupe-iou",
            str(float(payload.dedupe_iou or 0.1)),
            "--dedupe-iou-grid",
            dedupe_grid,
            "--scoreless-iou",
            str(float(payload.scoreless_iou or 0.0)),
            "--use-val-split",
        ]
        eval_run = subprocess.run(eval_cmd, check=True, capture_output=True, text=True)
        eval_text = eval_run.stdout.strip()
        if eval_text:
            eval_path.write_text(eval_text)

        metrics = {}
        try:
            metrics = json.loads(eval_path.read_text())
        except Exception:
            metrics = {}

        job.result = {
            "output_dir": str(output_dir),
            "prepass_jsonl": str(prepass_path),
            "features": str(features_path),
            "labeled": str(labeled_path),
            "model": str(Path(str(model_prefix) + ".pt")),
            "meta": str(meta_path),
            "eval": str(eval_path),
            "metrics": metrics,
        }
        _calibration_update(job, status="completed", message="Done", phase="completed", progress=1.0)
    except Exception as exc:  # noqa: BLE001
        if isinstance(exc, RuntimeError) and str(exc) == "cancelled":
            _calibration_update(job, status="cancelled", message="Cancelled", phase="cancelled")
        else:
            logger.exception("Calibration job %s failed", job.job_id)
            _calibration_update(job, status="failed", message="Failed", phase="failed", error=str(exc))
    finally:
        job.updated_at = time.time()


def _start_calibration_job(payload: CalibrationRequest) -> CalibrationJob:
    job_id = f"cal_{uuid.uuid4().hex[:8]}"
    job = CalibrationJob(job_id=job_id)
    with CALIBRATION_JOBS_LOCK:
        CALIBRATION_JOBS[job.job_id] = job
    thread = threading.Thread(target=_run_calibration_job, args=(job, payload), daemon=True)
    thread.start()
    return job


def _cancel_calibration_job(job_id: str) -> CalibrationJob:
    with CALIBRATION_JOBS_LOCK:
        job = CALIBRATION_JOBS.get(job_id)
    if not job:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="calibration_job_not_found")
    if job.status in {"completed", "failed", "cancelled"}:
        return job
    job.cancel_event.set()
    job.status = "cancelled"
    job.message = "Cancelled"
    job.phase = "cancelled"
    job.updated_at = time.time()
    return job
def _run_prompt_helper_job(job: PromptHelperJob, payload: PromptHelperRequest) -> None:
    with PROMPT_HELPER_JOBS_LOCK:
        PROMPT_HELPER_JOBS[job.job_id] = job
    job.status = "running"
    job.message = "Loading dataset…"
    job.request = payload.dict()
    job.updated_at = time.time()
    try:
        dataset_root = _resolve_sam3_or_qwen_dataset(payload.dataset_id)
        coco, gt_by_image_cat, images = _load_coco_index(dataset_root)
        categories = coco.get("categories") or []
        cat_to_images: Dict[int, set[int]] = {}
        for ann in coco.get("annotations", []):
            try:
                cat_id = int(ann["category_id"])
                img_id = int(ann["image_id"])
            except Exception:
                continue
            cat_to_images.setdefault(cat_id, set()).add(img_id)
        prompts_map: Dict[int, List[str]] = {}
        if payload.prompts_by_class:
            for k, vals in payload.prompts_by_class.items():
                try:
                    cid = int(k)
                except Exception:
                    continue
                cleaned = [v.strip() for v in vals if isinstance(v, str) and v.strip()]
                if cleaned:
                    prompts_map[cid] = cleaned
        results: List[Dict[str, Any]] = []
        total_classes = len(categories) or 1
        image_cache: Dict[int, Image.Image] = {}
        # Precompute total steps for progress: each prompt * each sampled image.
        total_steps = 0
        for idx, cat in enumerate(categories):
            cat_id = int(cat.get("id", idx))
            prompts = prompts_map.get(cat_id)
            if not prompts:
                prompts = _generate_prompt_variants_for_class(
                    str(cat.get("name", f"class_{cat_id}")),
                    payload.max_synonyms,
                    payload.use_qwen,
                )
            sample_ids = _sample_images_for_category(
                cat_id,
                list(cat_to_images.get(cat_id, set())),
                payload.sample_per_class,
                payload.seed,
            )
            total_steps += len(prompts) * max(1, len(sample_ids))
        job.total_steps = total_steps
        job.completed_steps = 0
        for idx, cat in enumerate(categories):
            cat_id = int(cat.get("id", idx))
            class_name = str(cat.get("name", f"class_{cat_id}"))
            job.message = f"Evaluating {class_name} ({idx + 1}/{total_classes})…"
            job.progress = (idx) / total_classes
            job.updated_at = time.time()
            candidates = prompts_map.get(cat_id)
            if not candidates:
                candidates = _generate_prompt_variants_for_class(
                    class_name,
                    payload.max_synonyms,
                    payload.use_qwen,
                )
            sampled_images = _sample_images_for_category(
                cat_id,
                list(cat_to_images.get(cat_id, set())),
                payload.sample_per_class,
                payload.seed,
            )
            candidate_results: List[Dict[str, Any]] = []
            for prompt in candidates:
                step_label = f"{class_name}: '{prompt}'"
                try:
                    job.logs.append({"ts": time.time(), "msg": f"Running {step_label} on {len(sampled_images)} images"})
                    if len(job.logs) > MAX_JOB_LOGS:
                        job.logs[:] = job.logs[-MAX_JOB_LOGS:]
                except Exception:
                    pass
                metrics = _evaluate_prompt_for_class(
                    prompt,
                    cat_id=cat_id,
                    image_ids=sampled_images,
                    gt_by_image_cat=gt_by_image_cat,
                    images=images,
                    score_threshold=payload.score_threshold,
                    max_dets=payload.max_dets,
                    iou_threshold=payload.iou_threshold,
                    image_cache=image_cache,
                )
                candidate_results.append(metrics)
                job.completed_steps += max(1, len(sampled_images))
                if job.total_steps:
                    job.progress = min(1.0, job.completed_steps / job.total_steps)
                job.updated_at = time.time()
            candidate_results.sort(key=lambda m: (m.get("score", 0.0), m.get("recall", 0.0), m.get("precision", 0.0)), reverse=True)
            results.append(
                {
                    "class_id": cat_id,
                    "class_name": class_name,
                    "images_sampled": len(sampled_images),
                    "candidates": candidate_results,
                }
            )
            job.progress = (idx + 1) / total_classes
            job.updated_at = time.time()
        job.status = "completed"
        job.message = "Done"
        job.result = {
            "classes": results,
            "config": payload.dict(),
            "dataset_id": payload.dataset_id,
        }
    except Exception as exc:  # noqa: BLE001
        logger.exception("Prompt helper job %s failed", job.job_id)
        job.status = "failed"
        job.error = str(exc)
        job.message = "Failed"
    finally:
        job.updated_at = time.time()


def _run_prompt_helper_search_job(job: PromptHelperJob, payload: PromptHelperSearchRequest) -> None:
    with PROMPT_HELPER_JOBS_LOCK:
        PROMPT_HELPER_JOBS[job.job_id] = job
    job.status = "running"
    job.message = "Loading dataset…"
    job.request = {"mode": "search", **payload.dict()}
    job.updated_at = time.time()
    try:
        dataset_root = _resolve_sam3_or_qwen_dataset(payload.dataset_id)
        coco, gt_by_image_cat, images = _load_coco_index(dataset_root)
        categories = coco.get("categories") or []
        target_class_id = payload.class_id
        if not payload.prompts_by_class:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="search_prompts_required")
        cat_to_images: Dict[int, set[int]] = {}
        for ann in coco.get("annotations", []):
            try:
                cat_id = int(ann["category_id"])
                img_id = int(ann["image_id"])
            except Exception:
                continue
            cat_to_images.setdefault(cat_id, set()).add(img_id)
        prompts_map: Dict[int, List[str]] = {}
        for k, vals in payload.prompts_by_class.items():
            try:
                cid = int(k)
            except Exception:
                continue
            if not isinstance(vals, (list, tuple)):
                continue
            cleaned = [v.strip() for v in vals if isinstance(v, str) and v.strip()]
            if cleaned:
                prompts_map[cid] = cleaned
        if not prompts_map:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="search_prompts_empty")
        all_img_ids = list(images.keys())
        image_cache: Dict[int, Image.Image] = {}
        if target_class_id is not None:
            categories = [c for c in categories if int(c.get("id", categories.index(c))) == target_class_id]
            if not categories:
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="search_class_not_found")

        total_steps = 0
        for idx, cat in enumerate(categories):
            cat_id = int(cat.get("id", idx))
            prompts = prompts_map.get(cat_id)
            if not prompts:
                continue
            pos_ids = _sample_images_for_category(
                cat_id,
                list(cat_to_images.get(cat_id, set())),
                payload.sample_per_class,
                payload.seed,
            )
            neg_ids = _sample_negative_images(
                cat_id,
                all_img_ids,
                cat_to_images,
                payload.negatives_per_class,
                payload.seed,
            )
            eval_count = len(set(pos_ids + neg_ids)) or 1
            total_steps += len(prompts) * eval_count
        job.total_steps = total_steps
        job.completed_steps = 0
        results: List[Dict[str, Any]] = []
        total_classes = len(categories) or 1
        for idx, cat in enumerate(categories):
            cat_id = int(cat.get("id", idx))
            class_name = str(cat.get("name", f"class_{cat_id}"))
            prompts = prompts_map.get(cat_id)
            if not prompts:
                continue
            job.message = f"Searching prompts for {class_name} ({idx + 1}/{total_classes})…"
            job.progress = idx / total_classes
            pos_ids = _sample_images_for_category(
                cat_id,
                list(cat_to_images.get(cat_id, set())),
                payload.sample_per_class,
                payload.seed,
            )
            neg_ids = _sample_negative_images(
                cat_id,
                all_img_ids,
                cat_to_images,
                payload.negatives_per_class,
                payload.seed,
            )
            eval_ids = list(dict.fromkeys([*pos_ids, *neg_ids]))
            candidate_results: List[Dict[str, Any]] = []
            for prompt in prompts:
                try:
                    job.logs.append(
                        {
                            "ts": time.time(),
                            "msg": f"Eval '{prompt}' on {len(eval_ids)} imgs (+{len(pos_ids)} pos / {len(neg_ids)} neg)",
                        }
                    )
                    if len(job.logs) > MAX_JOB_LOGS:
                        job.logs[:] = job.logs[-MAX_JOB_LOGS:]
                except Exception:
                    pass
                metrics = _evaluate_prompt_for_class(
                    prompt,
                    cat_id=cat_id,
                    image_ids=eval_ids,
                    gt_by_image_cat=gt_by_image_cat,
                    images=images,
                    score_threshold=payload.score_threshold,
                    max_dets=payload.max_dets,
                    iou_threshold=payload.iou_threshold,
                    image_cache=image_cache,
                )
                penalty = 1.0
                if payload.precision_floor > 0:
                    penalty = min(1.0, metrics["precision"] / max(payload.precision_floor, 1e-6))
                metrics["precision_penalty"] = penalty
                metrics["search_score"] = metrics["recall"] * (0.5 + 0.5 * metrics["det_rate"]) * penalty
                metrics["images_evaluated"] = len(eval_ids)
                metrics["positive_images"] = len(pos_ids)
                metrics["negative_images"] = len(neg_ids)
                candidate_results.append(metrics)
                job.completed_steps += max(1, len(eval_ids))
                if job.total_steps:
                    job.progress = min(1.0, job.completed_steps / job.total_steps)
                job.updated_at = time.time()
            candidate_results.sort(
                key=lambda m: (
                    m.get("search_score", 0.0),
                    m.get("recall", 0.0),
                    m.get("precision", 0.0),
                ),
                reverse=True,
            )
            best = candidate_results[0] if candidate_results else None
            results.append(
                {
                    "class_id": cat_id,
                    "class_name": class_name,
                    "positive_images": len(pos_ids),
                    "negative_images": len(neg_ids),
                    "best": best,
                    "candidates": candidate_results,
                }
            )
            job.progress = (idx + 1) / total_classes
            job.updated_at = time.time()
        job.status = "completed"
        job.message = "Done"
        job.result = {
            "classes": results,
            "config": payload.dict(),
            "dataset_id": payload.dataset_id,
            "mode": "search",
        }
    except Exception as exc:  # noqa: BLE001
        logger.exception("Prompt search job %s failed", job.job_id)
        job.status = "failed"
        job.error = str(exc)
        job.message = "Failed"
    finally:
        job.updated_at = time.time()


def _run_prompt_recipe_job(job: PromptHelperJob, payload: PromptRecipeRequest) -> None:
    with PROMPT_HELPER_JOBS_LOCK:
        PROMPT_HELPER_JOBS[job.job_id] = job
    job.status = "running"
    job.message = "Loading dataset…"
    job.request = {"mode": "recipe", **payload.dict()}
    job.updated_at = time.time()
    try:
        if not payload.prompts:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="recipe_prompts_required")
        dataset_root = _resolve_sam3_or_qwen_dataset(payload.dataset_id)
        coco, gt_by_image_cat, images = _load_coco_index(dataset_root)
        categories = coco.get("categories") or []
        cat_entry = next((c for c in categories if int(c.get("id", categories.index(c))) == payload.class_id), None)
        if not cat_entry:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="recipe_class_not_found")
        class_name = str(cat_entry.get("name", f"class_{payload.class_id}"))
        cat_to_images: Dict[int, set[int]] = {}
        for ann in coco.get("annotations", []):
            try:
                cat_id = int(ann["category_id"])
                img_id = int(ann["image_id"])
            except Exception:
                continue
            cat_to_images.setdefault(cat_id, set()).add(img_id)
        pos_ids = _sample_images_for_category(
            payload.class_id,
            list(cat_to_images.get(payload.class_id, set())),
            payload.sample_size,
            payload.seed,
        )
        all_img_ids = list(images.keys())
        neg_ids = _sample_negative_images(
            payload.class_id,
            all_img_ids,
            cat_to_images,
            payload.negatives,
            payload.seed,
        )
        eval_ids = list(dict.fromkeys([*pos_ids, *neg_ids]))
        image_cache: Dict[int, Image.Image] = {}
        gt_index_all, all_gt_keys_all, per_image_gt_all = _build_gt_index_for_class(gt_by_image_cat, payload.class_id)
        gt_index = {img_id: entries for img_id, entries in gt_index_all.items() if img_id in eval_ids}
        per_image_gt = {img_id: per_image_gt_all.get(img_id, 0) for img_id in eval_ids}
        all_gt_keys = set()
        for entries in gt_index.values():
            for key, _ in entries:
                all_gt_keys.add(key)
        if not eval_ids:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="recipe_no_images_sampled")
        if not all_gt_keys:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="recipe_no_gt_for_class")
        thresholds_cache: Dict[str, List[float]] = {}
        total_steps = 0
        for prompt_entry in payload.prompts:
            key = prompt_entry.prompt
            thresholds_cache[key] = _normalize_recipe_thresholds(
                prompt_entry.thresholds or payload.threshold_candidates,
                payload.score_threshold,
            )
            total_steps += len(thresholds_cache[key]) * max(1, len(eval_ids))
        job.total_steps = total_steps
        job.completed_steps = 0
        candidates: List[Dict[str, Any]] = []
        for idx, prompt_entry in enumerate(payload.prompts):
            thresholds = thresholds_cache.get(prompt_entry.prompt) or [payload.score_threshold]
            min_threshold = min(thresholds) if thresholds else payload.score_threshold
            detections = _collect_prompt_detections(
                prompt_entry.prompt,
                min_threshold,
                image_ids=eval_ids,
                images=images,
                image_cache=image_cache,
                max_dets=payload.max_dets,
            )
            for thr in thresholds:
                try:
                    job.logs.append(
                        {
                            "ts": time.time(),
                            "msg": f"Eval prompt {idx + 1}/{len(payload.prompts)} @ {thr:.2f} on {len(eval_ids)} images",
                        }
                    )
                    if len(job.logs) > MAX_JOB_LOGS:
                        job.logs[:] = job.logs[-MAX_JOB_LOGS:]
                except Exception:
                    pass
                metrics = _evaluate_prompt_candidate(
                    prompt_entry.prompt,
                    thr,
                    cat_id=payload.class_id,
                    image_ids=eval_ids,
                    gt_index=gt_index,
                    images=images,
                    iou_threshold=payload.iou_threshold,
                    max_dets=payload.max_dets,
                    image_cache=image_cache,
                    cached_detections=detections,
                )
                metrics["class_name"] = class_name
                metrics["class_id"] = payload.class_id
                metrics["image_count"] = len(eval_ids)
                candidates.append(metrics)
                job.completed_steps += len(eval_ids)
                if job.total_steps:
                    job.progress = min(1.0, job.completed_steps / job.total_steps)
                job.message = f"Evaluated {prompt_entry.prompt} ({job.completed_steps}/{job.total_steps} images)"
                job.updated_at = time.time()
        recipe, coverage_by_image = _build_prompt_recipe(
            candidates,
            all_gt_keys,
            per_image_gt,
            images,
            eval_ids,
            gt_index,
        )
        job.status = "completed"
        job.message = "Done"
        job.result = {
            "mode": "recipe",
            "dataset_id": payload.dataset_id,
            "class_id": payload.class_id,
            "class_name": class_name,
            "positive_images": len(pos_ids),
            "negative_images": len(neg_ids),
            "positive_image_ids": pos_ids,
            "negative_image_ids": neg_ids,
            "evaluated_image_ids": eval_ids,
            "gt_count": len(all_gt_keys),
            "config": payload.dict(),
            "candidates": [
                {
                    **{k: v for k, v in cand.items() if k not in {"matched_gt_keys", "matches_by_image"}},
                    "matched_gt": len(cand.get("matched_gt_keys") or []),
                }
                for cand in candidates
            ],
            "recipe": recipe,
            "coverage_by_image": coverage_by_image,
        }
    except Exception as exc:  # noqa: BLE001
        logger.exception("Prompt recipe job %s failed", job.job_id)
        job.status = "failed"
        job.error = str(exc)
        job.message = "Failed"
    finally:
        job.updated_at = time.time()


def _run_agent_mining_job(job: AgentMiningJob, payload: AgentMiningRequest) -> None:
    """
    SAM3 Recipe Mining (steps mode).

    Builds schema-v2 multi-step recipes using:
    - prompt bank (text prompts)
    - pretrained CLIP head (required) for filtering + cleanliness tuning

    Then evaluates the full steps pipeline on a deterministic sample of images.
    """
    with AGENT_MINING_JOBS_LOCK:
        AGENT_MINING_JOBS[job.job_id] = job
    job.status = "running"
    job.message = "Preparing image sample…"
    job.request = payload.dict()
    job.updated_at = time.time()
    start_ts = time.time()
    compute_estimate_info: Optional[Dict[str, Any]] = None

    def _log(msg: str) -> None:
        try:
            job.logs.append({"ts": time.time(), "msg": msg})
            if len(job.logs) > MAX_JOB_LOGS:
                job.logs[:] = job.logs[-MAX_JOB_LOGS:]
        except Exception:
            pass
        job.updated_at = time.time()

    def _cancelled() -> bool:
        return bool(job.cancel_event.is_set())

    try:
        _enforce_agent_mining_cache_limits(AGENT_MINING_DET_CACHE_ROOT, allow_when_running=False)

        dataset_root = _resolve_sam3_or_qwen_dataset(payload.dataset_id)
        _log(f"Dataset resolved at {dataset_root}")
        _log(
            "Request: "
            f"eval_images={payload.eval_image_count} "
            f"sample_seed={payload.split_seed}"
        )

        sample = _ensure_agent_mining_sample(
            payload.dataset_id,
            dataset_root,
            sample_size=payload.eval_image_count,
            seed=payload.split_seed,
        )
        eval_ids = [int(i) for i in (sample.get("sample_ids") or [])]
        if not eval_ids:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_mining_empty_sample")
        _log(f"Prepared sample with {len(eval_ids)} images (cached={sample.get('_cached', False)})")
        sample_hash = "unknown"
        try:
            sample_hash = hashlib.sha256(",".join(str(i) for i in eval_ids).encode("utf-8")).hexdigest()[:12]
        except Exception:
            sample_hash = "unknown"
        job.progress = 0.05

        coco, gt_by_image_cat, images = _load_coco_index(dataset_root)
        categories = coco.get("categories") or []
        _log(f"Loaded COCO with {len(categories)} categories")

        cat_filter: Optional[set[int]] = None
        if payload.classes:
            cat_filter = set()
            for cid in payload.classes:
                try:
                    cat_filter.add(int(cid))
                except Exception:
                    continue

        selected_categories: List[Tuple[int, str]] = []
        for idx, cat in enumerate(categories):
            try:
                cid = int(cat.get("id", idx))
            except Exception:
                cid = idx
            if cat_filter and cid not in cat_filter:
                continue
            selected_categories.append((cid, str(cat.get("name", f"class_{cid}"))))
        if not selected_categories:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_mining_no_classes_selected")

        _log(
            "Config: "
            + f"text_thr={payload.seed_threshold} visual_thr={payload.expand_threshold} "
            + f"cand_iou={payload.seed_dedupe_iou} out_iou={payload.dedupe_iou} "
            + f"mask_thr={payload.mask_threshold} max_results={payload.max_results} iou_eval={payload.iou_threshold} "
            + f"llm_prompts={payload.prompt_llm_max_prompts} workers_per_gpu={getattr(payload, 'max_workers_per_device', 1)} "
            + f"steps(max_steps={int(payload.steps_max_steps_per_recipe)}, candidates/step={int(payload.steps_max_visual_seeds_per_step)}) "
            + f"bg_guard={bool(payload.clip_head_background_guard)} "
            + f"bg_apply={getattr(payload, 'clip_head_background_apply', 'final')} "
            + f"bg_margin={getattr(payload, 'clip_head_background_margin', 0.0)} "
            + f"bg_penalty={getattr(payload, 'clip_head_background_penalty', 0.0)}"
        )

        # Pretrained CLIP head (LogReg) is required for Agent Mining (recipe mining).
        clip_head_path = _resolve_agent_clip_classifier_path(payload.clip_head_classifier_path)
        if clip_head_path is None:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_mining_clip_head_required")
        clip_head = _load_clip_head_from_classifier(clip_head_path)
        if not isinstance(clip_head, dict):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_mining_clip_head_required")
        head_encoder_type = str(clip_head.get("encoder_type") or "clip").lower().strip()
        if not head_encoder_type:
            head_encoder_type = "clip"
        prefilter_allowed = head_encoder_type == "clip"
        head_mode_bits = ""
        try:
            if bool(getattr(payload, "clip_head_auto_tune", True)):
                head_mode_bits = (
                    f"(auto-tune: target_precision={float(getattr(payload, 'clip_head_target_precision', 0.9)):.2f}; "
                    f"seed min_prob={payload.clip_head_min_prob} margin={payload.clip_head_margin})"
                )
            else:
                head_mode_bits = f"(fixed thresholds: min_prob={payload.clip_head_min_prob} margin={payload.clip_head_margin})"
        except Exception:
            head_mode_bits = ""
        clip_name = clip_head.get("clip_model") if isinstance(clip_head, dict) else None
        clip_name_str = str(clip_name).strip() if isinstance(clip_name, str) and clip_name.strip() else "unknown"
        classes_list = clip_head.get("classes") if isinstance(clip_head.get("classes"), list) else []
        _log(
            "Pretrained CLIP head enabled: "
            f"{clip_head_path.name} "
            f"(clip={clip_name_str}, classes={len(classes_list)}, mode={clip_head.get('proba_mode')}) "
            f"{head_mode_bits}"
        )
        if classes_list:
            preview_limit = 60
            preview_items = []
            for idx, label in enumerate(classes_list):
                preview_items.append(f"{idx}:{label}")
                if len(preview_items) >= preview_limit:
                    break
            preview = ", ".join(preview_items)
            suffix = f", … (+{len(classes_list) - preview_limit} more)" if len(classes_list) > preview_limit else ""
            _log(f"CLIP head classes (index:name): {preview}{suffix}")

        sample_images = int(len(eval_ids))
        class_count = int(len(selected_categories))
        per_image_units = int(payload.steps_max_steps_per_recipe) * (1 + int(payload.steps_max_visual_seeds_per_step))
        speed_factor = _estimate_steps_speed_factor(payload, allow_prefilter=prefilter_allowed)
        base_units_per_class = float(sample_images * per_image_units) * float(speed_factor)
        global_enabled = bool(getattr(payload, "steps_optimize_global", False)) and bool(clip_head)
        global_units_per_class = 0.0
        budgets: List[int] = []
        invalid_budgets = False
        if global_enabled:
            global_images, budgets, invalid_budgets = _estimate_agent_global_optimizer_image_evals(
                val_images=sample_images,
                eval_caps=list(getattr(payload, "steps_optimize_global_eval_caps", []) or []),
                keep_ratio=float(getattr(payload, "steps_optimize_global_keep_ratio", 0.5) or 0.5),
                rounds=int(getattr(payload, "steps_optimize_global_rounds", 1) or 1),
                max_trials=int(getattr(payload, "steps_optimize_global_max_trials", 1) or 1),
                mutations_per_round=int(getattr(payload, "steps_optimize_global_mutations_per_round", 1) or 1),
            )
            if not invalid_budgets:
                global_units_per_class = float(global_images * per_image_units) * float(speed_factor)
        total_units_per_class = float(base_units_per_class + global_units_per_class)
        total_units_all = float(total_units_per_class * class_count) if class_count > 0 else None
        compute_estimate_info = {
            "sample_images": sample_images,
            "class_count": class_count,
            "steps": int(payload.steps_max_steps_per_recipe),
            "seeds_per_step": int(payload.steps_max_visual_seeds_per_step),
            "per_image_units": int(per_image_units),
            "speed_factor": float(speed_factor),
            "base_units_per_class": float(base_units_per_class),
            "global_units_per_class": float(global_units_per_class),
            "total_units_per_class": float(total_units_per_class),
            "total_units_all_classes": float(total_units_all) if total_units_all is not None else None,
            "global_optimizer": {
                "enabled": bool(global_enabled),
                "eval_caps": budgets,
                "invalid_budgets": bool(invalid_budgets),
            },
        }
        _log(
            "Compute estimate: "
            f"sample_images={sample_images} steps={int(payload.steps_max_steps_per_recipe)} candidates/step={int(payload.steps_max_visual_seeds_per_step)} "
            f"speed_factor={float(speed_factor):.2f} base_units/class={base_units_per_class:.0f} "
            f"global_units/class={global_units_per_class:.0f} total_units/class={total_units_per_class:.0f}"
            + (f" total_units/all_classes={total_units_all:.0f}" if total_units_all is not None else "")
        )

        # Prompt mining (Qwen) per class.
        base_prompts_all: List[str] = []
        if payload.extra_prompts_by_class and isinstance(payload.extra_prompts_by_class, dict):
            raw_base = payload.extra_prompts_by_class.get("__base__")
            if isinstance(raw_base, list):
                base_prompts_all = [p for p in raw_base if isinstance(p, str) and p.strip()]
            elif isinstance(raw_base, str) and raw_base.strip():
                base_prompts_all = [raw_base.strip()]
        base_prompts_all = _sanitize_prompts(base_prompts_all)
        if base_prompts_all:
            _log(f"Base prompts (all classes): {', '.join(base_prompts_all)}")

        prefilter_cfg = _resolve_steps_prompt_prefilter_config(payload, allow_prefilter=prefilter_allowed)
        clip_model_for_prefilter: Optional[str] = None
        if isinstance(clip_head, dict):
            raw_model = clip_head.get("clip_model")
            if isinstance(raw_model, str) and raw_model.strip():
                clip_model_for_prefilter = raw_model.strip()
        if prefilter_cfg.get("enabled"):
            _log(
                "CLIP prompt prefilter enabled: "
                f"mode={prefilter_cfg.get('mode')} sample_size={prefilter_cfg.get('sample_size')} "
                f"keep_ratio={float(prefilter_cfg.get('keep_ratio') or 0.0):.2f}"
            )
        elif prefilter_cfg.get("requested") and not prefilter_allowed:
            _log(f"CLIP prompt prefilter disabled: head encoder_type={head_encoder_type}")

        bg_indices = _clip_head_background_indices(classes_list) if clip_head else []
        prompt_bg_drop_cfg = _resolve_steps_prompt_bg_drop_config(payload, allow_drop=bool(bg_indices))
        if prompt_bg_drop_cfg.get("enabled"):
            _log(
                "Prompt background drop enabled: "
                f"mode={prompt_bg_drop_cfg.get('mode')} min_checked={prompt_bg_drop_cfg.get('min_checked')} "
                f"drop_rate={float(prompt_bg_drop_cfg.get('drop_rate') or 0.0):.2f}"
            )
        elif prompt_bg_drop_cfg.get("requested") and prompt_bg_drop_cfg.get("disabled_reason") == "no_background_classes":
            _log("Prompt background drop disabled: no __bg_* classes in CLIP head.")

        prepared_prompts: Dict[int, List[str]] = {}
        prompt_prefilter_stats: Dict[int, Dict[str, Any]] = {}
        for cid, name in selected_categories:
            if _cancelled():
                break
            base = []
            if payload.text_prompts_by_class and cid in payload.text_prompts_by_class:
                base = [p for p in payload.text_prompts_by_class.get(cid) or [] if isinstance(p, str) and p.strip()]
            if not base:
                base = [name]
            user_extras: List[str] = []
            if payload.extra_prompts_by_class and isinstance(payload.extra_prompts_by_class, dict):
                raw_extra = payload.extra_prompts_by_class.get(name)
                if isinstance(raw_extra, list):
                    user_extras = [p for p in raw_extra if isinstance(p, str) and p.strip()]
                elif isinstance(raw_extra, str) and raw_extra.strip():
                    user_extras = [raw_extra.strip()]
            if user_extras:
                _log(f"User extra prompts for {name}: {', '.join(user_extras)}")
            base = _sanitize_prompts([*base, *user_extras]) or [name]
            extras: List[str] = []
            if payload.prompt_llm_max_prompts > 0:
                extras = _expand_prompts_with_prompt_llm(
                    name,
                    base,
                    payload.prompt_llm_max_prompts,
                    log_fn=_log,
                    max_new_tokens=payload.prompt_max_new_tokens,
                )
            merged: List[str] = []
            seen = set()
            for p in [*base, *extras, *base_prompts_all]:
                key = str(p).lower().strip()
                if not key or key in seen:
                    continue
                seen.add(key)
                merged.append(str(p))
            base_keep = _sanitize_prompts([*base, *base_prompts_all])
            prefilter_total = len(merged)
            if prefilter_cfg.get("enabled"):
                merged = _prefilter_prompts_with_clip(
                    merged,
                    keep_prompts=base_keep,
                    cat_id=int(cid),
                    class_name=str(name),
                    eval_ids=eval_ids,
                    images=images,
                    gt_by_image_cat=gt_by_image_cat,
                    clip_model_name=clip_model_for_prefilter,
                    sample_size=int(prefilter_cfg.get("sample_size") or 0),
                    keep_ratio=float(prefilter_cfg.get("keep_ratio") or 0.0),
                    seed=int(payload.split_seed) + int(cid),
                    log_fn=_log,
                )
            prompt_prefilter_stats[int(cid)] = {
                "enabled": bool(prefilter_cfg.get("enabled")),
                "mode": str(prefilter_cfg.get("mode") or "balanced"),
                "total": int(prefilter_total),
                "kept": int(len(merged)),
                "requested": bool(prefilter_cfg.get("requested")),
                "disabled_reason": prefilter_cfg.get("disabled_reason"),
            }
            if prefilter_cfg.get("enabled"):
                _log(
                    f"[steps] Prompt prefilter summary for {name}: kept {len(merged)}/{prefilter_total} "
                    f"(mode={prefilter_cfg.get('mode')})"
                )
            prepared_prompts[cid] = merged or base
            _log(f"Prompt list for {name}: {', '.join(prepared_prompts[cid])}")

        try:
            _unload_qwen_runtime()
            _log("Qwen prompt expansion runtime unloaded to free memory before SAM3 mining")
        except Exception:
            pass
        job.progress = max(job.progress, 0.07)

        eval_set = set(eval_ids)
        class_entries: List[Dict[str, Any]] = []
        total_classes = len(selected_categories)
        _log(f"Preparing {total_classes} class(es)…")

        for class_idx, (cid, name) in enumerate(selected_categories, start=1):
            if _cancelled():
                break
            job.message = f"Preparing {name} ({class_idx}/{total_classes})"
            job.updated_at = time.time()

            eval_gt = 0
            for img_id, cat_map in gt_by_image_cat.items():
                bboxes = cat_map.get(cid)
                if not bboxes:
                    continue
                if img_id in eval_set:
                    eval_gt += len(bboxes)

            prompts_for_class = prepared_prompts.get(cid) or [name]
            head_target_index: Optional[int] = None
            if clip_head:
                head_target_index = _find_clip_head_target_index(classes_list, name)
                if head_target_index is None:
                    _log(f"CLIP head mapping: class '{name}' not found; skipping head filter for this class.")
                else:
                    mapped_label = (
                        str(classes_list[head_target_index]) if 0 <= head_target_index < len(classes_list) else "unknown"
                    )
                    _log(f"CLIP head mapping: class '{name}' -> idx {head_target_index} (head='{mapped_label}')")
            class_entries.append(
                {
                    "id": int(cid),
                    "name": name,
                    "eval_gt": eval_gt,
                    "text_prompts": prompts_for_class,
                    "clip_head_target_index": head_target_index,
                }
            )

            job.progress = max(job.progress, 0.07 + 0.03 * (class_idx / max(1, total_classes)))
            job.updated_at = time.time()

        eval_payload = payload
        summaries: Dict[int, Dict[str, Any]] = {}
        recipes_by_class: Dict[int, Dict[str, Any]] = {}
        if _cancelled():
            summaries = {}
        else:
            job.message = "Evaluating recipes on sample…"
            job.updated_at = time.time()
            job.progress = max(job.progress, 0.1)
            eval_log_every = 5 if len(eval_ids) <= 50 else 50

            def _on_eval_progress(done: int, total: int) -> None:
                if total <= 0:
                    return
                if done != total and (eval_log_every <= 0 or done % eval_log_every != 0):
                    return
                frac = max(0.0, min(1.0, float(done) / float(total)))
                job.progress = max(job.progress, 0.1 + 0.9 * frac)
                job.message = f"Evaluating recipes: {done}/{total} sample images"
                job.updated_at = time.time()

            eval_workers: Optional[List[_Sam3GreedyEvalWorker]] = None
            try:
                eval_workers = _build_sam3_greedy_eval_workers(eval_payload, log_fn=_log)
                total_classes = len(class_entries)
                for class_idx, entry in enumerate(class_entries, start=1):
                    if _cancelled():
                        break
                    cid = int(entry.get("id"))
                    name = str(entry.get("name") or f"class_{cid}")
                    prompts_for_class = entry.get("text_prompts") or [name]
                    head_target_index = entry.get("clip_head_target_index")

                    class_base = 0.1 + 0.9 * ((class_idx - 1) / max(1, total_classes))
                    class_span = 0.9 * (1.0 / max(1, total_classes))
                    seed_span = class_span * 0.45
                    tune_span = class_span * 0.55

                    def _mk_phase_cb(base: float, span: float, label: str) -> Callable[[int, int], None]:
                        def _cb(done: int, total: int) -> None:
                            if total <= 0:
                                return
                            frac = max(0.0, min(1.0, float(done) / float(total)))
                            job.progress = max(job.progress, base + span * frac)
                            job.message = f"{label} {done}/{total} ({name})"
                            job.updated_at = time.time()

                        return _cb

                    job.message = f"[steps] Evaluating text candidates for {name} ({class_idx}/{total_classes})…"
                    job.updated_at = time.time()
                    seed_stats = _mine_seed_prompt_stats_image_first(
                        cat_id=cid,
                        prompts=prompts_for_class,
                        val_ids=eval_ids,
                        images=images,
                        gt_by_image_cat=gt_by_image_cat,
                        payload=eval_payload,
                        clip_head=clip_head,
                        clip_head_target_index=head_target_index,
                        clip_head_bg_indices=bg_indices,
                        prompt_bg_drop_cfg=prompt_bg_drop_cfg,
                        workers=eval_workers,
                        log_every=eval_log_every,
                        log_fn=_log,
                        cancel_event=job.cancel_event,
                        progress_callback=_mk_phase_cb(class_base, seed_span, "[steps] Candidate eval"),
                    )
                    prompt_bg_drop_summary: Optional[Dict[str, Any]] = None
                    if isinstance(prompt_bg_drop_cfg, dict) and (prompt_bg_drop_cfg.get("enabled") or prompt_bg_drop_cfg.get("requested")):
                        dropped = sum(1 for s in (seed_stats or []) if isinstance(s, dict) and s.get("bg_drop"))
                        prompt_bg_drop_summary = {
                            "enabled": bool(prompt_bg_drop_cfg.get("enabled")),
                            "mode": str(prompt_bg_drop_cfg.get("mode") or "balanced"),
                            "min_checked": int(prompt_bg_drop_cfg.get("min_checked") or 0),
                            "drop_rate": float(prompt_bg_drop_cfg.get("drop_rate") or 0.0),
                            "total": int(len(seed_stats or [])),
                            "dropped": int(dropped),
                            "requested": bool(prompt_bg_drop_cfg.get("requested")),
                            "disabled_reason": prompt_bg_drop_cfg.get("disabled_reason"),
                        }
                    target_prec = (
                        float(getattr(eval_payload, "clip_head_target_precision", 0.0) or 0.0)
                        if (clip_head is not None and head_target_index is not None)
                        else None
                    )
                    early_stop_cfg = _resolve_steps_early_stop_config(eval_payload, target_precision=target_prec)
                    if early_stop_cfg.get("enabled"):
                        _log(
                            "Early-stop enabled: "
                            f"mode={early_stop_cfg.get('mode')} min_steps={early_stop_cfg.get('min_steps')} "
                            f"window={early_stop_cfg.get('window')} min_increment={float(early_stop_cfg.get('min_increment') or 0.0):.3f} "
                            f"precision_margin={float(early_stop_cfg.get('precision_margin') or 0.0):.2f}"
                        )
                    selected, early_stop_info = _select_steps_from_seed_prompt_stats(
                        seed_stats,
                        max_steps=int(getattr(eval_payload, "steps_max_steps_per_recipe", 6) or 6),
                        target_precision=target_prec,
                        early_stop=early_stop_cfg,
                        log_fn=_log,
                    )
                    if early_stop_info.get("enabled"):
                        _log(
                            f"[steps] Early-stop summary for {name}: "
                            f"selected_steps={early_stop_info.get('selected_steps')}/{early_stop_info.get('max_steps')} "
                            f"mode={early_stop_info.get('mode')} reason={early_stop_info.get('reason')}"
                        )
                    refine_info: Optional[Dict[str, Any]] = None
                    if bool(getattr(eval_payload, "steps_refine_prompt_subset", False)):
                        selected, refine_info = _refine_steps_prompt_subset_seed_stage(
                            seed_stats,
                            selected,
                            max_steps=int(getattr(eval_payload, "steps_max_steps_per_recipe", 6) or 6),
                            target_precision=target_prec,
                            max_iters=int(getattr(eval_payload, "steps_refine_prompt_subset_max_iters", 6) or 6),
                            top_k=int(getattr(eval_payload, "steps_refine_prompt_subset_top_k", 6) or 6),
                            base_seed_threshold=float(eval_payload.seed_threshold),
                            log_fn=_log,
                        )
                    if isinstance(early_stop_info, dict):
                        early_stop_info["selected_steps_final"] = int(len(selected))
                    selected_prompts, step_list = _build_steps_recipe_step_list_from_selected_stats(
                        selected,
                        prompts_fallback=prompts_for_class,
                        payload=eval_payload,
                    )

                    tier1_info: Optional[Dict[str, Any]] = None
                    tier2_info: Optional[Dict[str, Any]] = None
                    global_info: Optional[Dict[str, Any]] = None
                    summary: Dict[str, Any]
                    if clip_head is not None and head_target_index is not None:
                        global_enabled = bool(getattr(eval_payload, "steps_optimize_global", False))
                        if global_enabled:
                            job.message = f"[steps] Global optimization for {name} ({class_idx}/{total_classes})…"
                            job.updated_at = time.time()
                            step_list, global_info = _tune_steps_global_optimizer_image_first(
                                cat_id=cid,
                                steps=step_list,
                                seed_stats=seed_stats,
                                val_ids=eval_ids,
                                images=images,
                                gt_by_image_cat=gt_by_image_cat,
                                payload=eval_payload,
                                clip_head=clip_head,
                                clip_head_target_index=int(head_target_index),
                                workers=eval_workers,
                                log_fn=_log,
                                cancel_event=job.cancel_event,
                            )
                        else:
                            if bool(getattr(eval_payload, "steps_optimize_tier1", False)):
                                job.message = f"[steps] Tier-1 grid search for {name} ({class_idx}/{total_classes})…"
                                job.updated_at = time.time()
                                step_list, tier1_info = _tune_steps_tier1_knobs_image_first(
                                    cat_id=cid,
                                    steps=step_list,
                                    val_ids=eval_ids,
                                    images=images,
                                    gt_by_image_cat=gt_by_image_cat,
                                    payload=eval_payload,
                                    clip_head=clip_head,
                                    clip_head_target_index=int(head_target_index),
                                    workers=eval_workers,
                                    log_fn=_log,
                                    cancel_event=job.cancel_event,
                                )
                            if bool(getattr(eval_payload, "steps_optimize_tier2", False)):
                                job.message = f"[steps] Tier-2 tuning for {name} ({class_idx}/{total_classes})…"
                                job.updated_at = time.time()
                                step_list, tier2_info = _tune_steps_tier2_knobs_image_first(
                                    cat_id=cid,
                                    steps=step_list,
                                    val_ids=eval_ids,
                                    images=images,
                                    gt_by_image_cat=gt_by_image_cat,
                                    payload=eval_payload,
                                    clip_head=clip_head,
                                    clip_head_target_index=int(head_target_index),
                                    workers=eval_workers,
                                    log_fn=_log,
                                    cancel_event=job.cancel_event,
                                )
                        job.message = f"[steps] Tuning CLIP head for {name} ({class_idx}/{total_classes})…"
                        job.updated_at = time.time()
                        summary = _tune_clip_head_for_selected_steps_image_first(
                            cat_id=cid,
                            class_name=name,
                            steps=step_list,
                            val_ids=eval_ids,
                            images=images,
                            gt_by_image_cat=gt_by_image_cat,
                            payload=eval_payload,
                            clip_head=clip_head,
                            clip_head_target_index=int(head_target_index),
                            workers=eval_workers,
                            log_every=eval_log_every,
                            log_fn=_log,
                            cancel_event=job.cancel_event,
                            progress_callback=_mk_phase_cb(class_base + seed_span, tune_span, "[steps] Final tune"),
                            export_hard_negatives=True,
                        )
                        if tier1_info and isinstance(summary, dict):
                            summary["tier1_tuning"] = tier1_info
                        if tier2_info and isinstance(summary, dict):
                            summary["tier2_tuning"] = tier2_info
                        if global_info and isinstance(summary, dict):
                            summary["global_optimizer"] = global_info
                        if refine_info and isinstance(summary, dict):
                            summary["prompt_subset_refinement"] = refine_info
                        if isinstance(summary, dict):
                            prefilter_summary = prompt_prefilter_stats.get(int(cid))
                            if isinstance(prefilter_summary, dict):
                                summary["prompt_prefilter"] = prefilter_summary
                            if isinstance(prompt_bg_drop_summary, dict):
                                summary["prompt_bg_drop"] = prompt_bg_drop_summary
                            if isinstance(early_stop_info, dict):
                                summary["early_stop"] = early_stop_info
                    else:
                        total_gt = int(entry.get("eval_gt") or 0)
                        summary = {
                            "gts": total_gt,
                            "matches": 0,
                            "fps": 0,
                            "duplicates": 0,
                            "preds": 0,
                            "precision": 0.0,
                            "recall": 0.0,
                            "coverage_rate": 0.0,
                            "det_rate": 0.0,
                        }
                        if refine_info:
                            summary["prompt_subset_refinement"] = refine_info
                        prefilter_summary = prompt_prefilter_stats.get(int(cid))
                        if isinstance(prefilter_summary, dict):
                            summary["prompt_prefilter"] = prefilter_summary
                        if isinstance(prompt_bg_drop_summary, dict):
                            summary["prompt_bg_drop"] = prompt_bg_drop_summary
                        if isinstance(early_stop_info, dict):
                            summary["early_stop"] = early_stop_info

                    summaries[cid] = summary
                    tuned_min_prob = None
                    tuned_margin = None
                    tuned_bg_margin = None
                    if isinstance(summary, dict):
                        tuned_min_prob = summary.get("clip_head_min_prob")
                        tuned_margin = summary.get("clip_head_margin")
                        tuned_bg_margin = summary.get("clip_head_background_margin")
                    try:
                        final_min_prob = float(tuned_min_prob) if tuned_min_prob is not None else float(payload.clip_head_min_prob)
                    except Exception:
                        final_min_prob = float(payload.clip_head_min_prob)
                    try:
                        final_margin = float(tuned_margin) if tuned_margin is not None else float(payload.clip_head_margin)
                    except Exception:
                        final_margin = float(payload.clip_head_margin)
                    try:
                        final_bg_margin = float(tuned_bg_margin) if tuned_bg_margin is not None else float(payload.clip_head_background_margin)
                    except Exception:
                        final_bg_margin = float(payload.clip_head_background_margin)
                    if step_list:
                        for step in step_list:
                            if not isinstance(step, dict):
                                continue
                            if "clip_seed" not in step:
                                step["clip_seed"] = {"min_prob": 0.0, "margin": 0.0}
                            if "clip_final" not in step:
                                step["clip_final"] = {"min_prob": float(final_min_prob), "margin": float(final_margin)}
                    tuned_expand = float(eval_payload.expand_threshold)
                    tuned_max_seeds = int(getattr(eval_payload, "steps_max_visual_seeds_per_step", 5) or 0)
                    tuned_seed_iou = float(eval_payload.seed_dedupe_iou)
                    tuned_out_iou = float(eval_payload.dedupe_iou)
                    tuned_max_results = int(eval_payload.max_results)
                    if step_list and isinstance(step_list[0], dict):
                        try:
                            if step_list[0].get("expand_threshold") is not None:
                                tuned_expand = float(step_list[0].get("expand_threshold"))
                        except Exception:
                            tuned_expand = float(eval_payload.expand_threshold)
                        try:
                            if step_list[0].get("max_visual_seeds") is not None:
                                tuned_max_seeds = int(step_list[0].get("max_visual_seeds"))
                        except Exception:
                            tuned_max_seeds = int(getattr(eval_payload, "steps_max_visual_seeds_per_step", 5) or 0)
                        try:
                            if step_list[0].get("seed_dedupe_iou") is not None:
                                tuned_seed_iou = float(step_list[0].get("seed_dedupe_iou"))
                        except Exception:
                            tuned_seed_iou = float(eval_payload.seed_dedupe_iou)
                        try:
                            if step_list[0].get("dedupe_iou") is not None:
                                tuned_out_iou = float(step_list[0].get("dedupe_iou"))
                        except Exception:
                            tuned_out_iou = float(eval_payload.dedupe_iou)
                        try:
                            if step_list[0].get("max_results") is not None:
                                tuned_max_results = int(step_list[0].get("max_results"))
                        except Exception:
                            tuned_max_results = int(eval_payload.max_results)
                    optimizer = {
                        "algorithm": "sam3_steps_v2",
                        "version": 1,
                        "created_at": float(time.time()),
                        "sample_seed": int(eval_payload.split_seed),
                        "sample_size": int(eval_payload.eval_image_count),
                        "sample_hash": str(sample_hash),
                        "sample_images": int(len(eval_ids)),
                        "target_precision": float(getattr(eval_payload, "clip_head_target_precision", 0.0) or 0.0),
                        "max_steps_per_recipe": int(getattr(eval_payload, "steps_max_steps_per_recipe", 6) or 6),
                        "seed_threshold": {
                            "base": float(eval_payload.seed_threshold),
                            "seed_eval_threshold": float(_compute_steps_seed_eval_threshold(eval_payload)),
                            "seed_eval_max_results": int(_compute_steps_seed_eval_max_results(eval_payload)),
                            "strategy": "curve_candidates",
                            "max_candidates_per_prompt": 6,
                        },
                        "global_optimizer": global_info
                        if isinstance(global_info, dict)
                        else {
                            "enabled": False,
                            "requested": bool(getattr(eval_payload, "steps_optimize_global", False)),
                            "eval_caps": list(getattr(eval_payload, "steps_optimize_global_eval_caps", []) or []),
                            "max_trials": int(getattr(eval_payload, "steps_optimize_global_max_trials", 0) or 0),
                            "keep_ratio": float(getattr(eval_payload, "steps_optimize_global_keep_ratio", 0.0) or 0.0),
                            "rounds": int(getattr(eval_payload, "steps_optimize_global_rounds", 0) or 0),
                            "mutations_per_round": int(getattr(eval_payload, "steps_optimize_global_mutations_per_round", 0) or 0),
                            "enable_ordering": bool(getattr(eval_payload, "steps_optimize_global_enable_ordering", False)),
                            "enable_max_results": bool(getattr(eval_payload, "steps_optimize_global_enable_max_results", False)),
                        },
                        "tier1": tier1_info
                        if isinstance(tier1_info, dict)
                        else {
                            "enabled": False,
                            "requested": bool(getattr(eval_payload, "steps_optimize_tier1", False)),
                            "eval_cap": int(getattr(eval_payload, "steps_optimize_tier1_eval_cap", 0) or 0),
                            "max_trials": int(getattr(eval_payload, "steps_optimize_tier1_max_trials", 0) or 0),
                        },
                        "tier2": tier2_info
                        if isinstance(tier2_info, dict)
                        else {
                            "enabled": False,
                            "requested": bool(getattr(eval_payload, "steps_optimize_tier2", False)),
                            "eval_cap": int(getattr(eval_payload, "steps_optimize_tier2_eval_cap", 0) or 0),
                            "max_trials": int(getattr(eval_payload, "steps_optimize_tier2_max_trials", 0) or 0),
                        },
                        "prompt_subset_refinement": refine_info
                        if isinstance(refine_info, dict)
                        else {
                            "enabled": False,
                            "requested": bool(getattr(eval_payload, "steps_refine_prompt_subset", False)),
                            "max_iters": int(getattr(eval_payload, "steps_refine_prompt_subset_max_iters", 0) or 0),
                            "top_k": int(getattr(eval_payload, "steps_refine_prompt_subset_top_k", 0) or 0),
                        },
                        "early_stop": {
                            **(early_stop_cfg or {}),
                            "requested": bool(getattr(eval_payload, "steps_early_stop", False)),
                        },
                        "prompt_prefilter": {
                            **(prefilter_cfg or {}),
                            "requested": bool(
                                prefilter_cfg.get("requested")
                                if isinstance(prefilter_cfg, dict)
                                else getattr(eval_payload, "steps_prompt_prefilter", False)
                            ),
                            "keep_base_prompts": True,
                        },
                    }
                    recipes_by_class[cid] = {
                        "schema_version": 2,
                        "mode": "sam3_steps",
                        "optimizer": optimizer,
                        "text_prompts": list(prompts_for_class),
                        "steps": step_list,
                        "params": {
                            "use_clip_fp_guard": False,
                            "use_negative_exemplars": False,
                            "negative_strength": 0.0,
                            "similarity_score": 0.0,
                            "seed_threshold": float(eval_payload.seed_threshold),
                            "expand_threshold": float(tuned_expand),
                            "max_visual_seeds": int(tuned_max_seeds),
                            "seed_dedupe_iou": float(tuned_seed_iou),
                            "dedupe_iou": float(tuned_out_iou),
                            "mask_threshold": float(eval_payload.mask_threshold),
                            "min_size": int(eval_payload.min_size),
                            "simplify_epsilon": float(eval_payload.simplify_epsilon),
                            "max_results": int(tuned_max_results),
                            "clip_head_background_guard": bool(getattr(eval_payload, "clip_head_background_guard", False)),
                            "clip_head_background_margin": float(final_bg_margin),
                            "clip_head_background_apply": str(getattr(eval_payload, "clip_head_background_apply", "final") or "final"),
                            "clip_head_background_penalty": float(getattr(eval_payload, "clip_head_background_penalty", 0.0) or 0.0),
                        },
                        "summary": {
                            **(summary or {}),
                            "seed_prompt_stats": [
                                {
                                    "prompt": s.get("prompt"),
                                    "matches": int(s.get("matches") or 0),
                                    "fps": int(s.get("fps") or 0),
                                    "precision": float(s.get("precision") or 0.0),
                                    "selected_seed_threshold": float(
                                        s.get("selected_seed_threshold")
                                        if s.get("selected_seed_threshold") is not None
                                        else (s.get("seed_threshold_recommended") if s.get("seed_threshold_recommended") is not None else eval_payload.seed_threshold)
                                    ),
                                    "selected_seed_threshold_point": s.get("selected_seed_threshold_point"),
                                    "seed_threshold_base": float(s.get("seed_threshold_base") or eval_payload.seed_threshold),
                                    "seed_threshold_recommended": float(
                                        s.get("seed_threshold_recommended") if s.get("seed_threshold_recommended") is not None else eval_payload.seed_threshold
                                    ),
                                    "seed_threshold_base_point": s.get("seed_threshold_base_point"),
                                    "seed_threshold_recommended_point": s.get("seed_threshold_recommended_point"),
                                    "seed_threshold_curve": s.get("seed_threshold_curve") or [],
                                }
                                for s in selected
                            ],
                        },
                    }
                job.progress = max(job.progress, 0.98)
            finally:
                if eval_workers:
                    for w in eval_workers:
                        try:
                            w.close()
                        except Exception:
                            continue

        results: List[Dict[str, Any]] = []
        for entry in class_entries:
            cid = int(entry.get("id"))
            name = str(entry.get("name") or f"class_{cid}")
            summary = summaries.get(cid)
            if summary is None:
                summary = {
                    "gts": int(entry.get("eval_gt") or 0),
                    "matches": 0,
                    "fps": 0,
                    "duplicates": 0,
                    "preds": 0,
                    "precision": 0.0,
                    "recall": 0.0,
                    "coverage_rate": 0.0,
                    "det_rate": 0.0,
                }
            recipe = recipes_by_class.get(cid) or {}
            if clip_head is not None and isinstance(recipe, dict):
                try:
                    tuned_min_prob = summary.get("clip_head_min_prob")
                    tuned_margin = summary.get("clip_head_margin")
                    if tuned_min_prob is not None or tuned_margin is not None:
                        recipe["_clip_head_classifier_path"] = str(clip_head_path)
                    recipe["clip_head"] = {
                        "artifact": "clip_head/head.npz",
                        "clip_model": clip_head.get("clip_model"),
                        "proba_mode": clip_head.get("proba_mode"),
                        "classes": clip_head.get("classes") if isinstance(clip_head.get("classes"), list) else [],
                        "min_prob": float(tuned_min_prob) if tuned_min_prob is not None else float(payload.clip_head_min_prob),
                        "margin": float(tuned_margin) if tuned_margin is not None else float(payload.clip_head_margin),
                        "background_margin": float(tuned_bg_margin)
                        if tuned_bg_margin is not None
                        else float(getattr(payload, "clip_head_background_margin", 0.0) or 0.0),
                        "auto_tuned": bool(getattr(payload, "clip_head_auto_tune", True)),
                        "target_precision": float(getattr(payload, "clip_head_target_precision", 0.9)),
                    }
                except Exception:
                    pass

            results.append(
                {
                    "id": cid,
                    "name": name,
                    "eval_gt": int(entry.get("eval_gt") or 0),
                    "recipe": recipe,
                }
            )

        job.result = {
            "dataset_id": payload.dataset_id,
            "sample": {
                "count": len(eval_ids),
                "seed": payload.split_seed,
            },
            "compute_estimate": compute_estimate_info,
            "classes": results,
            "config": eval_payload.dict(),
            "note": "Agent mining completed (steps mode).",
        }
        if compute_estimate_info and total_units_per_class:
            runtime_sec = max(0.0, float(time.time() - start_ts))
            total_units = compute_estimate_info.get("total_units_all_classes") or compute_estimate_info.get("total_units_per_class")
            try:
                total_units_val = float(total_units) if total_units is not None else 0.0
            except Exception:
                total_units_val = 0.0
            sec_per_unit = runtime_sec / total_units_val if total_units_val > 0 else None
            compute_estimate_info["runtime_sec"] = runtime_sec
            compute_estimate_info["sec_per_unit"] = sec_per_unit
            _log(
                "Compute estimate calibration: "
                f"runtime_sec={runtime_sec:.1f} units={total_units_val:.0f} "
                + (f"sec_per_unit={sec_per_unit:.6f}" if sec_per_unit is not None else "sec_per_unit=n/a")
            )
        if _cancelled():
            job.status = "cancelled"
            job.message = "Cancelled"
        else:
            job.status = "completed"
            job.message = "Done"
            job.progress = 1.0
    except Exception as exc:  # noqa: BLE001
        logger.exception("Agent mining job %s failed", job.job_id)
        job.status = "failed"
        job.error = str(exc)
        job.message = "Failed"
    finally:
        job.updated_at = time.time()
def _start_prompt_helper_job(payload: PromptHelperRequest) -> PromptHelperJob:
    job_id = f"ph_{uuid.uuid4().hex[:8]}"
    job = PromptHelperJob(job_id=job_id)
    with PROMPT_HELPER_JOBS_LOCK:
        PROMPT_HELPER_JOBS[job.job_id] = job
    thread = threading.Thread(target=_run_prompt_helper_job, args=(job, payload), daemon=True)
    thread.start()
    return job


def _start_agent_mining_job(payload: AgentMiningRequest) -> AgentMiningJob:
    clip_head_path = _resolve_agent_clip_classifier_path(payload.clip_head_classifier_path)
    if clip_head_path is None:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_mining_clip_head_required")
    # Validate early so we fail fast (no background job created).
    _load_clip_head_from_classifier(clip_head_path)
    job_id = f"am_{uuid.uuid4().hex[:8]}"
    job = AgentMiningJob(job_id=job_id)
    with AGENT_MINING_JOBS_LOCK:
        AGENT_MINING_JOBS[job.job_id] = job
    thread = threading.Thread(target=_run_agent_mining_job, args=(job, payload), daemon=True)
    thread.start()
    return job


def _cancel_agent_mining_job(job_id: str) -> AgentMiningJob:
    with AGENT_MINING_JOBS_LOCK:
        job = AGENT_MINING_JOBS.get(job_id)
    if not job:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="agent_mining_job_not_found")
    if job.status in {"completed", "failed", "cancelled"}:
        return job
    job.cancel_event.set()
    job.status = "cancelled"
    job.message = "Cancelled"
    job.updated_at = time.time()
    return job


def _start_prompt_helper_search_job(payload: PromptHelperSearchRequest) -> PromptHelperJob:
    job_id = f"phs_{uuid.uuid4().hex[:8]}"
    job = PromptHelperJob(job_id=job_id)
    with PROMPT_HELPER_JOBS_LOCK:
        PROMPT_HELPER_JOBS[job.job_id] = job
    thread = threading.Thread(target=_run_prompt_helper_search_job, args=(job, payload), daemon=True)
    thread.start()
    return job


def _start_prompt_recipe_job(payload: PromptRecipeRequest) -> PromptHelperJob:
    job_id = f"phr_{uuid.uuid4().hex[:8]}"
    job = PromptHelperJob(job_id=job_id)
    with PROMPT_HELPER_JOBS_LOCK:
        PROMPT_HELPER_JOBS[job.job_id] = job
    thread = threading.Thread(target=_run_prompt_recipe_job, args=(job, payload), daemon=True)
    thread.start()
    return job


@app.post("/agent_mining/jobs")
def start_agent_mining_job(payload: AgentMiningRequest):
    job = _start_agent_mining_job(payload)
    return _serialize_agent_mining_job(job)


@app.get("/agent_mining/jobs")
def list_agent_mining_jobs():
    _prune_job_registry(AGENT_MINING_JOBS, AGENT_MINING_JOBS_LOCK)
    with AGENT_MINING_JOBS_LOCK:
        jobs = list(AGENT_MINING_JOBS.values())
    jobs.sort(key=lambda j: j.created_at, reverse=True)
    return [_serialize_agent_mining_job(j) for j in jobs]


@app.get("/agent_mining/jobs/{job_id}")
def get_agent_mining_job(job_id: str):
    with AGENT_MINING_JOBS_LOCK:
        job = AGENT_MINING_JOBS.get(job_id)
    if not job:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="agent_mining_job_not_found")
    return _serialize_agent_mining_job(job)


@app.post("/agent_mining/jobs/{job_id}/cancel")
def cancel_agent_mining_job(job_id: str):
    job = _cancel_agent_mining_job(job_id)
    return _serialize_agent_mining_job(job)


@app.get("/agent_mining/results/latest")
def get_latest_agent_mining_result():
    with AGENT_MINING_JOBS_LOCK:
        jobs = [j for j in AGENT_MINING_JOBS.values() if j.status in {"running", "completed", "failed", "cancelled"}]
    if not jobs:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="agent_mining_result_not_found")
    jobs.sort(key=lambda j: j.updated_at, reverse=True)
    return _serialize_agent_mining_job(jobs[0])


@app.get("/agent_mining/cache_size")
def agent_mining_cache_size():
    cache_root = AGENT_MINING_DET_CACHE_ROOT
    # Light touch: enforce TTL/size only when no active job to avoid surprises.
    _enforce_agent_mining_cache_limits(cache_root, allow_when_running=False)
    total = 0
    files = 0
    try:
        for p in cache_root.rglob("*"):
            try:
                if p.is_file():
                    total += p.stat().st_size
                    files += 1
            except Exception:
                continue
    except Exception:
        total = 0
    return {
        "bytes": total,
        "files": files,
        "max_bytes": AGENT_MINING_CACHE_MAX_BYTES,
        "ttl_hours": AGENT_MINING_CACHE_TTL_HOURS,
    }


@app.post("/agent_mining/cache/purge")
def agent_mining_cache_purge():
    cache_root = AGENT_MINING_DET_CACHE_ROOT
    if _agent_cache_running_jobs():
        raise HTTPException(status_code=HTTP_409_CONFLICT, detail="agent_cache_purge_blocked_active_jobs")
    if not cache_root.exists():
        return {"status": "ok", "deleted_bytes": 0, "deleted_files": 0}
    deleted = 0
    deleted_files = 0
    paths = sorted(cache_root.rglob("*"), key=lambda x: len(x.parts), reverse=True)
    for p in paths:
        try:
            if p.is_file():
                deleted += p.stat().st_size
                deleted_files += 1
                p.unlink()
            elif p.is_dir():
                p.rmdir()
        except Exception:
            continue
    return {"status": "ok", "deleted_bytes": deleted, "deleted_files": deleted_files}


class AgentApplyImageRequest(BaseModel):
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    image_name: Optional[str] = None
    sam_variant: Optional[str] = None
    recipe: Dict[str, Any]
    mask_threshold: float = Field(0.5, ge=0.0, le=1.0)
    min_size: int = Field(0, ge=0, le=10_000)
    simplify_epsilon: float = Field(0.0, ge=0.0, le=1_000.0)
    max_results: int = Field(1000, ge=1, le=5000)
    override_class_id: Optional[int] = Field(None, ge=0)
    override_class_name: Optional[str] = None
    clip_head_min_prob_override: Optional[float] = Field(
        None,
        ge=0.0,
        le=1.0,
        description=(
            "Optional extra CLIP-head probability threshold applied in addition to the recipe's baked-in head thresholds. "
            "Effective min_prob is max(recipe_min_prob, clip_head_min_prob_override)."
        ),
    )
    clip_head_margin_override: Optional[float] = Field(
        None,
        ge=0.0,
        le=1.0,
        description=(
            "Optional extra CLIP-head margin threshold applied in addition to the recipe's baked-in head thresholds. "
            "Effective margin is max(recipe_margin, clip_head_margin_override)."
        ),
    )
    extra_clip_classifier_path: Optional[str] = Field(
        None,
        description=(
            "Optional extra CLIP classifier head (trained via the CLIP tab) to apply after the recipe runs. "
            "Useful for adding a classifier-based filter to crop-bank recipes."
        ),
    )
    extra_clip_min_prob: Optional[float] = Field(
        None,
        ge=0.0,
        le=1.0,
        description=(
            "Minimum probability for the step output class when using extra_clip_classifier_path. "
            "This filter is applied in addition to any CLIP filtering already baked into the recipe."
        ),
    )
    extra_clip_margin: Optional[float] = Field(
        None,
        ge=0.0,
        le=1.0,
        description=(
            "Optional margin for the step output class when using extra_clip_classifier_path "
            "(p(target) - max(p(other)) must be >= margin)."
        ),
    )

    @root_validator(skip_on_failure=True)
    def _ensure_agent_apply_image_payload(cls, values):  # noqa: N805
        if not values.get("image_base64") and not values.get("image_token"):
            raise ValueError("image_payload_missing")
        if not isinstance(values.get("recipe"), dict) or not values.get("recipe"):
            raise ValueError("recipe_required")
        return values


class AgentApplyChainStep(BaseModel):
    enabled: bool = True
    recipe_id: Optional[str] = None
    recipe: Optional[Dict[str, Any]] = None
    override_class_id: Optional[int] = Field(None, ge=0)
    override_class_name: Optional[str] = None
    dedupe_group: Optional[str] = None
    participate_cross_class_dedupe: bool = True
    clip_head_min_prob_override: Optional[float] = Field(
        None,
        ge=0.0,
        le=1.0,
        description=(
            "Optional extra CLIP-head probability threshold applied in addition to the recipe's baked-in head thresholds. "
            "Effective min_prob is max(recipe_min_prob, clip_head_min_prob_override)."
        ),
    )
    clip_head_margin_override: Optional[float] = Field(
        None,
        ge=0.0,
        le=1.0,
        description=(
            "Optional extra CLIP-head margin threshold applied in addition to the recipe's baked-in head thresholds. "
            "Effective margin is max(recipe_margin, clip_head_margin_override)."
        ),
    )
    extra_clip_classifier_path: Optional[str] = Field(
        None,
        description=(
            "Optional extra CLIP classifier head (trained via the CLIP tab) to apply after the recipe runs. "
            "Useful for adding a classifier-based filter to crop-bank recipes."
        ),
    )
    extra_clip_min_prob: Optional[float] = Field(
        None,
        ge=0.0,
        le=1.0,
        description=(
            "Minimum probability for the step output class when using extra_clip_classifier_path. "
            "This filter is applied in addition to any CLIP filtering already baked into the recipe."
        ),
    )
    extra_clip_margin: Optional[float] = Field(
        None,
        ge=0.0,
        le=1.0,
        description=(
            "Optional margin for the step output class when using extra_clip_classifier_path "
            "(p(target) - max(p(other)) must be >= margin)."
        ),
    )

    @root_validator(skip_on_failure=True)
    def _ensure_chain_step(cls, values):  # noqa: N805
        recipe_id = values.get("recipe_id")
        recipe_obj = values.get("recipe")
        if not recipe_id and not (isinstance(recipe_obj, dict) and recipe_obj):
            raise ValueError("recipe_required")
        return values


class AgentCascadeDedupeConfig(BaseModel):
    per_class_iou: float = Field(0.5, ge=0.0, le=1.0)
    cross_class_enabled: bool = False
    cross_class_iou: float = Field(0.5, ge=0.0, le=1.0)
    cross_class_scope: Literal["groups", "global"] = "groups"
    confidence: Literal["sam_score", "clip_head_prob", "clip_head_margin"] = "sam_score"
    clip_head_recipe_id: Optional[str] = None


class AgentApplyImageChainRequest(BaseModel):
    image_base64: Optional[str] = None
    image_token: Optional[str] = None
    image_name: Optional[str] = None
    sam_variant: Optional[str] = None
    steps: List[AgentApplyChainStep]
    dedupe: AgentCascadeDedupeConfig = Field(default_factory=AgentCascadeDedupeConfig)
    mask_threshold: float = Field(0.5, ge=0.0, le=1.0)
    min_size: int = Field(0, ge=0, le=10_000)
    simplify_epsilon: float = Field(0.0, ge=0.0, le=1_000.0)
    max_results: int = Field(1000, ge=1, le=5000)

    @root_validator(skip_on_failure=True)
    def _ensure_agent_apply_chain_payload(cls, values):  # noqa: N805
        if not values.get("image_base64") and not values.get("image_token"):
            raise ValueError("image_payload_missing")
        steps = values.get("steps") or []
        if not isinstance(steps, list) or not steps:
            raise ValueError("steps_required")
        enabled_steps: List[Any] = []
        for step in steps:
            if isinstance(step, dict):
                if step.get("enabled", True):
                    enabled_steps.append(step)
                continue
            if bool(getattr(step, "enabled", True)):
                enabled_steps.append(step)
        if not enabled_steps:
            raise ValueError("steps_required")
        return values


@app.post("/agent_mining/apply_image", response_model=Sam3TextPromptResponse)
def agent_mining_apply_image(payload: AgentApplyImageRequest):
    variant = _default_variant(payload.sam_variant or "sam3")
    if variant != "sam3":
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_apply_requires_sam3")
    pil_img, _, token = resolve_image_payload(payload.image_base64, payload.image_token, variant)
    warnings: List[str] = []
    recipe_meta = payload.recipe or {}

    class_id_val = recipe_meta.get("class_id")
    class_name_val = recipe_meta.get("class_name")
    if payload.override_class_id is not None or payload.override_class_name:
        warnings.append("class_override_used")
        if payload.override_class_id is not None:
            class_id_val = payload.override_class_id
        if payload.override_class_name:
            class_name_val = payload.override_class_name
    class_id_int: Optional[int] = None
    if class_id_val is not None:
        try:
            class_id_int = int(class_id_val)
        except Exception:
            class_id_int = None

    # Reuse the existing apply implementation by staging the in-memory image to a temp file.
    staging_dir = Path(tempfile.mkdtemp(prefix="agent_apply_image_"))
    try:
        img_path = staging_dir / "image.png"
        try:
            pil_img.save(img_path, format="PNG")
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"agent_apply_image_encode_failed:{exc}") from exc
        dets = _apply_agent_recipe_to_image(
            payload.recipe,
            image={"path": str(img_path)},
            dataset_id="image_payload",
            images={},
            mask_threshold=payload.mask_threshold,
            min_size=payload.min_size,
            simplify_epsilon=payload.simplify_epsilon,
            max_results=payload.max_results,
            class_id=class_id_int,
            class_name=str(class_name_val) if class_name_val is not None else None,
            clip_head_min_prob_override=payload.clip_head_min_prob_override,
            clip_head_margin_override=payload.clip_head_margin_override,
            extra_clip_classifier_path=payload.extra_clip_classifier_path,
            extra_clip_min_prob=payload.extra_clip_min_prob,
            extra_clip_margin=payload.extra_clip_margin,
            warnings=warnings,
        )
    finally:
        shutil.rmtree(staging_dir, ignore_errors=True)
    return Sam3TextPromptResponse(detections=dets, warnings=warnings, image_token=token)


@app.post("/agent_mining/apply_image_chain", response_model=Sam3TextPromptResponse)
def agent_mining_apply_image_chain(payload: AgentApplyImageChainRequest):
    variant = _default_variant(payload.sam_variant or "sam3")
    if variant != "sam3":
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_apply_requires_sam3")
    pil_img, _, token = resolve_image_payload(payload.image_base64, payload.image_token, variant)
    warnings: List[str] = []

    enabled_steps = [s for s in payload.steps if s.enabled]
    if not enabled_steps:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="steps_required")

    staging_dir = Path(tempfile.mkdtemp(prefix="agent_apply_chain_"))
    try:
        img_path = staging_dir / "image.png"
        try:
            pil_img.save(img_path, format="PNG")
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"agent_apply_image_encode_failed:{exc}") from exc

        all_dets: List[QwenDetection] = []
        det_meta: Dict[int, Dict[str, Any]] = {}

        for idx, step in enumerate(enabled_steps):
            recipe_obj: Optional[Dict[str, Any]] = None
            if isinstance(step.recipe, dict) and step.recipe:
                recipe_obj = step.recipe
            elif step.recipe_id:
                recipe_obj = _load_agent_recipe_json_only(step.recipe_id)
            if not isinstance(recipe_obj, dict) or not recipe_obj:
                continue

            class_id_val = recipe_obj.get("class_id")
            class_name_val = recipe_obj.get("class_name")
            if step.override_class_id is not None or step.override_class_name:
                warnings.append("class_override_used")
                if step.override_class_id is not None:
                    class_id_val = step.override_class_id
                if step.override_class_name:
                    class_name_val = step.override_class_name

            class_id_int: Optional[int] = None
            if class_id_val is not None:
                try:
                    class_id_int = int(class_id_val)
                except Exception:
                    class_id_int = None
            class_name_str = str(class_name_val) if class_name_val is not None else None

            dets = _apply_agent_recipe_to_image(
                recipe_obj,
                image={"path": str(img_path)},
                dataset_id="image_payload",
                images={},
                mask_threshold=payload.mask_threshold,
                min_size=payload.min_size,
                simplify_epsilon=payload.simplify_epsilon,
                max_results=payload.max_results,
                class_id=class_id_int,
                class_name=class_name_str,
                clip_head_min_prob_override=step.clip_head_min_prob_override,
                clip_head_margin_override=step.clip_head_margin_override,
                extra_clip_classifier_path=step.extra_clip_classifier_path,
                extra_clip_min_prob=step.extra_clip_min_prob,
                extra_clip_margin=step.extra_clip_margin,
                warnings=warnings,
            )
            group_val = (step.dedupe_group or "").strip() or "default"
            cross_val = bool(step.participate_cross_class_dedupe)
            for det in dets:
                det_meta[id(det)] = {
                    "step_index": idx,
                    "dedupe_group": group_val,
                    "cross_class": cross_val,
                }
            all_dets.extend(dets)

        if not all_dets:
            return Sam3TextPromptResponse(detections=[], warnings=warnings, image_token=token)

        original_scores: Dict[int, Optional[float]] = {}
        confidence_mode = payload.dedupe.confidence
        if confidence_mode in {"clip_head_prob", "clip_head_margin"}:
            head_recipe_id = (payload.dedupe.clip_head_recipe_id or "").strip() or None
            if head_recipe_id is None:
                for step in enabled_steps:
                    rid = None
                    if step.recipe_id:
                        rid = step.recipe_id
                    elif isinstance(step.recipe, dict):
                        rid = step.recipe.get("id")
                    if not rid:
                        continue
                    candidate = (AGENT_MINING_RECIPES_ROOT / str(rid) / "clip_head" / "head.npz").resolve()
                    if _path_is_within_root(candidate, AGENT_MINING_RECIPES_ROOT.resolve()) and candidate.exists():
                        head_recipe_id = str(rid)
                        break

            clip_head: Optional[Dict[str, Any]] = None
            if head_recipe_id:
                try:
                    head_recipe = _load_agent_recipe_json_only(head_recipe_id)
                except HTTPException:
                    head_recipe = None
                fallback_meta: Optional[Dict[str, Any]] = None
                if isinstance(head_recipe, dict):
                    recipe_block = head_recipe.get("recipe")
                    if isinstance(recipe_block, dict) and isinstance(recipe_block.get("clip_head"), dict):
                        fallback_meta = recipe_block.get("clip_head")
                    elif isinstance(head_recipe.get("clip_head"), dict):
                        fallback_meta = head_recipe.get("clip_head")
                recipe_root = (AGENT_MINING_RECIPES_ROOT / str(head_recipe_id)).resolve()
                if _path_is_within_root(recipe_root, AGENT_MINING_RECIPES_ROOT.resolve()):
                    clip_head = _load_clip_head_artifacts(recipe_dir=recipe_root, fallback_meta=fallback_meta)

            clip_scores = (
                _score_detections_with_clip_head(
                    all_dets,
                    pil_img=pil_img,
                    clip_head=clip_head,
                    score_mode=confidence_mode,  # type: ignore[arg-type]
                )
                if clip_head
                else None
            )
            if clip_scores is None:
                warnings.append("clip_head_unavailable")
            else:
                if all_dets and not clip_scores:
                    warnings.append("clip_head_no_scores")
                for det in all_dets:
                    det_id = id(det)
                    original_scores[det_id] = det.score
                    if det_id in clip_scores:
                        det.score = float(clip_scores[det_id])

        # Always dedupe within each output class first.
        per_class_iou = float(payload.dedupe.per_class_iou)
        by_class: Dict[str, List[QwenDetection]] = {}
        for det in all_dets:
            class_name_key = str(det.class_name or "").strip()
            if class_name_key:
                key = f"name:{class_name_key}"
            elif det.class_id is not None:
                key = f"id:{int(det.class_id)}"
            else:
                key = "unknown"
            by_class.setdefault(key, []).append(det)
        deduped: List[QwenDetection] = []
        for group in by_class.values():
            deduped.extend(
                _dedupe_qwen_detections_iou(group, img_w=pil_img.width, img_h=pil_img.height, iou_thresh=per_class_iou)
            )
        final = deduped

        # Optional cross-class dedupe across steps (grouped or global).
        if payload.dedupe.cross_class_enabled:
            cross_iou = float(payload.dedupe.cross_class_iou)
            scope = payload.dedupe.cross_class_scope
            group_map: Dict[str, List[QwenDetection]] = {}
            for det in final:
                meta = det_meta.get(id(det)) or {}
                if not meta.get("cross_class", True):
                    continue
                group_key = "global" if scope == "global" else str(meta.get("dedupe_group") or "default")
                group_map.setdefault(group_key, []).append(det)

            kept_ids: set[int] = set()
            for group in group_map.values():
                kept = _dedupe_qwen_detections_iou(group, img_w=pil_img.width, img_h=pil_img.height, iou_thresh=cross_iou)
                kept_ids.update(id(d) for d in kept)

            if group_map:
                final = [
                    det
                    for det in final
                    if (id(det) in kept_ids) or not (det_meta.get(id(det)) or {}).get("cross_class", True)
                ]

        if original_scores:
            for det in all_dets:
                det_id = id(det)
                if det_id in original_scores:
                    det.score = original_scores[det_id]

        return Sam3TextPromptResponse(detections=final, warnings=warnings, image_token=token)
    finally:
        shutil.rmtree(staging_dir, ignore_errors=True)


class AgentRecipeExportRequest(BaseModel):
    dataset_id: str
    class_id: Optional[int] = None
    class_name: Optional[str] = None
    label: str = Field(..., min_length=1, max_length=128)
    recipe: Dict[str, Any]


class AgentCascadeSaveRequest(BaseModel):
    label: str = Field(..., min_length=1, max_length=128)
    steps: List[AgentApplyChainStep]
    dedupe: AgentCascadeDedupeConfig = Field(default_factory=AgentCascadeDedupeConfig)


@app.post("/agent_mining/recipes", response_model=Dict[str, Any])
def agent_mining_save_recipe(payload: AgentRecipeExportRequest):
    recipe = _persist_agent_recipe(
        payload.dataset_id,
        payload.class_id,
        payload.class_name,
        payload.label,
        payload.recipe,
    )
    return recipe


@app.get("/agent_mining/recipes", response_model=List[Dict[str, Any]])
def agent_mining_list_recipes(dataset_id: Optional[str] = None):
    return _list_agent_recipes(dataset_id)


@app.get("/agent_mining/recipes/{recipe_id}", response_model=Dict[str, Any])
def agent_mining_get_recipe(recipe_id: str):
    return _load_agent_recipe(recipe_id)


@app.get("/agent_mining/recipes/{recipe_id}/export")
def agent_mining_export_recipe(recipe_id: str):
    recipe = _load_agent_recipe(recipe_id)
    zip_path = _ensure_recipe_zip(recipe)
    filename = f"{recipe_id}.zip"
    headers = {"Content-Disposition": f'attachment; filename="{filename}"'}
    try:
        stream = zip_path.open("rb")
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"agent_recipe_export_failed:{exc}") from exc
    return StreamingResponse(stream, media_type="application/zip", headers=headers)


@app.post("/agent_mining/recipes/import", response_model=Dict[str, Any])
async def agent_mining_import_recipe(file: UploadFile = File(...)):
    staging_dir = Path(tempfile.mkdtemp(prefix="agent_recipe_import_", dir=str(AGENT_MINING_RECIPES_ROOT)))
    zip_path = staging_dir / "payload.zip"
    data: Dict[str, Any] = {}
    crops: Dict[str, bytes] = {}
    clip_head_files: Dict[str, bytes] = {}
    try:
        await _write_upload_file(
            file,
            zip_path,
            max_bytes=AGENT_RECIPE_MAX_BYTES,
            quota_root=staging_dir,
            quota_limit=AGENT_RECIPE_MAX_BYTES,
            allow_overwrite=True,
        )
        if not zipfile.is_zipfile(zip_path):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_import_zip_only")
        with zipfile.ZipFile(zip_path) as zf:
            names = zf.namelist()
            json_name = None
            for name in names:
                if name.lower().endswith(".json"):
                    json_name = name
                    break
            if not json_name:
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_import_no_json")
            json_info = zf.getinfo(json_name)
            if json_info.file_size > AGENT_RECIPE_MAX_JSON_BYTES:
                raise HTTPException(status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="agent_recipe_import_json_too_large")
            json_path = Path(json_name)
            if json_path.is_absolute() or ".." in json_path.parts:
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_import_invalid_path")
            with zf.open(json_name) as jf:
                data = json.load(jf)

            total_bytes = 0
            crop_count = 0
            clip_head_bytes = 0
            for name in names:
                info = zf.getinfo(name)
                arc_path = Path(name)
                if arc_path.is_dir():
                    continue
                if arc_path.is_absolute() or ".." in arc_path.parts:
                    raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_import_invalid_path")
                if len(arc_path.parts) < 2 or arc_path.parts[0] != "crops":
                    # Non-crop artifacts we support (portable CLIP head).
                    if len(arc_path.parts) == 2 and arc_path.parts[0] == "clip_head" and arc_path.name in {"head.npz", "meta.json"}:
                        clip_head_bytes += info.file_size
                        if clip_head_bytes > AGENT_RECIPE_MAX_CLIP_HEAD_BYTES:
                            raise HTTPException(
                                status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="agent_recipe_import_clip_head_too_large"
                            )
                        clip_head_files[f"clip_head/{arc_path.name}"] = zf.read(name)
                    continue
                if arc_path.suffix.lower() != ".png":
                    continue
                crop_count += 1
                total_bytes += info.file_size
                if crop_count > AGENT_RECIPE_MAX_CROPS or total_bytes > AGENT_RECIPE_MAX_CROP_BYTES:
                    raise HTTPException(status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="agent_recipe_import_crops_too_large")
                crops[f"crops/{arc_path.name}"] = zf.read(name)
    except HTTPException:
        raise
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"agent_recipe_import_failed:{exc}") from exc
    finally:
        shutil.rmtree(staging_dir, ignore_errors=True)
    if not isinstance(data, dict) or not data:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_recipe_import_no_json")
    dataset_id = data.get("dataset_id") or (data.get("recipe") or {}).get("dataset_id") or ""
    label = data.get("label") or (data.get("recipe") or {}).get("label") or "imported_recipe"
    class_id = data.get("class_id")
    class_name = data.get("class_name")
    meta_overrides = {
        "dataset_signature": data.get("dataset_signature"),
        "labelmap_hash": data.get("labelmap_hash"),
        "labelmap": data.get("labelmap"),
    }
    persisted = _persist_agent_recipe(
        dataset_id,
        class_id,
        class_name,
        label,
        data,
        crop_overrides=crops,
        clip_head_overrides=clip_head_files,
        meta_overrides=meta_overrides,
    )
    return persisted


@app.delete("/agent_mining/recipes/{recipe_id}")
def agent_mining_delete_recipe(recipe_id: str):
    _delete_agent_recipe(recipe_id)
    return {"id": recipe_id, "deleted": True}


@app.post("/agent_mining/cascades", response_model=Dict[str, Any])
def agent_mining_save_cascade(payload: AgentCascadeSaveRequest):
    cascade_payload = {
        "steps": [s.dict() for s in payload.steps],
        "dedupe": payload.dedupe.dict(),
    }
    return _persist_agent_cascade(payload.label, cascade_payload)


@app.get("/agent_mining/cascades", response_model=List[Dict[str, Any]])
def agent_mining_list_cascades():
    return _list_agent_cascades()


@app.get("/agent_mining/cascades/{cascade_id}", response_model=Dict[str, Any])
def agent_mining_get_cascade(cascade_id: str):
    return _load_agent_cascade(cascade_id)


@app.delete("/agent_mining/cascades/{cascade_id}")
def agent_mining_delete_cascade(cascade_id: str):
    _delete_agent_cascade(cascade_id)
    return {"id": cascade_id, "deleted": True}


@app.get("/agent_mining/cascades/{cascade_id}/export")
def agent_mining_export_cascade(cascade_id: str):
    cascade = _load_agent_cascade(cascade_id)
    zip_path = _ensure_cascade_zip(cascade)
    filename = f"{cascade_id}.zip"
    headers = {"Content-Disposition": f'attachment; filename="{filename}"'}
    try:
        stream = zip_path.open("rb")
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"agent_cascade_export_failed:{exc}") from exc
    return StreamingResponse(stream, media_type="application/zip", headers=headers)


@app.post("/agent_mining/cascades/import", response_model=Dict[str, Any])
async def agent_mining_import_cascade(file: UploadFile = File(...)):
    staging_dir = Path(tempfile.mkdtemp(prefix="agent_cascade_import_", dir=str(AGENT_MINING_CASCADES_ROOT)))
    zip_path = staging_dir / "payload.zip"
    try:
        await _write_upload_file(
            file,
            zip_path,
            max_bytes=AGENT_CASCADE_MAX_BYTES,
            quota_root=staging_dir,
            quota_limit=AGENT_CASCADE_MAX_BYTES,
            allow_overwrite=True,
        )
        if not zipfile.is_zipfile(zip_path):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_import_zip_only")
        with zipfile.ZipFile(zip_path) as zf:
            names = zf.namelist()
            cascade_name = None
            for name in names:
                if Path(name).name.lower() == "cascade.json":
                    cascade_name = name
                    break
            if not cascade_name:
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_import_no_json")
            info = zf.getinfo(cascade_name)
            if info.file_size > AGENT_CASCADE_MAX_JSON_BYTES:
                raise HTTPException(status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="agent_cascade_import_json_too_large")
            cascade_path = Path(cascade_name)
            if cascade_path.is_absolute() or ".." in cascade_path.parts:
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_import_invalid_path")
            with zf.open(cascade_name) as jf:
                cascade_data = json.load(jf)
            if not isinstance(cascade_data, dict):
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_invalid_schema")

            recipe_zip_names: List[str] = []
            classifier_file_names: List[str] = []
            for name in names:
                arc_path = Path(name)
                if arc_path.is_dir():
                    continue
                if arc_path.is_absolute() or ".." in arc_path.parts:
                    raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_import_invalid_path")
                if len(arc_path.parts) >= 2 and arc_path.parts[0] == "recipes" and arc_path.suffix.lower() == ".zip":
                    recipe_zip_names.append(name)
                if len(arc_path.parts) >= 2 and arc_path.parts[0] == "classifiers":
                    if arc_path.name.endswith(".meta.pkl") or arc_path.suffix.lower() in CLASSIFIER_ALLOWED_EXTS:
                        classifier_file_names.append(name)

            classifier_map: Dict[str, str] = {}
            if classifier_file_names:
                allowed_root = (UPLOAD_ROOT / "classifiers").resolve()
                import_tag = f"cascade_{uuid.uuid4().hex[:8]}"
                import_root = (allowed_root / "imports" / import_tag).resolve()
                if not _path_is_within_root(import_root, allowed_root):
                    raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_import_invalid_path")
                import_root.mkdir(parents=True, exist_ok=True)
                for name in classifier_file_names:
                    arc_path = Path(name)
                    rel_inside = Path(*arc_path.parts[1:])
                    # Keep the directory structure from the zip under imports/<tag>/...
                    dest_path = (import_root / rel_inside).resolve()
                    if not _path_is_within_root(dest_path, allowed_root):
                        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_import_invalid_path")
                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                    try:
                        blob = zf.read(name)
                    except Exception:
                        continue
                    try:
                        dest_path.write_bytes(blob)
                    except Exception:
                        continue
                    if not arc_path.name.endswith(".meta.pkl") and arc_path.suffix.lower() in CLASSIFIER_ALLOWED_EXTS:
                        orig_rel = str(rel_inside.as_posix())
                        new_rel = str((Path("imports") / import_tag / rel_inside).as_posix())
                        classifier_map[orig_rel] = new_rel

            id_map: Dict[str, str] = {}
            for name in recipe_zip_names:
                src_id = None
                try:
                    arc_path = Path(name)
                    src_id = arc_path.stem
                except Exception:
                    src_id = None
                old_id, persisted = _import_agent_recipe_zip_bytes(zf.read(name))
                new_id = persisted.get("id")
                if isinstance(new_id, str) and new_id:
                    if old_id:
                        id_map[str(old_id)] = str(new_id)
                    if src_id:
                        id_map[str(src_id)] = str(new_id)

            label = cascade_data.get("label") or "imported_cascade"
            steps_raw = cascade_data.get("steps")
            if not isinstance(steps_raw, list) or not steps_raw:
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_invalid_schema")
            dedupe_raw = cascade_data.get("dedupe") if isinstance(cascade_data.get("dedupe"), dict) else {}

            steps_out: List[Dict[str, Any]] = []
            for raw in steps_raw:
                if not isinstance(raw, dict):
                    raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_invalid_schema")
                rid = raw.get("recipe_id")
                if not rid and isinstance(raw.get("recipe"), dict):
                    rid = raw["recipe"].get("id")
                if not isinstance(rid, str) or not rid.strip():
                    raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_invalid_schema")
                rid = rid.strip()
                mapped = id_map.get(rid)
                if not mapped:
                    raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="agent_cascade_import_missing_recipe")
                extra_classifier_ref = None
                if isinstance(raw.get("extra_clip_classifier_path"), str) and raw.get("extra_clip_classifier_path").strip():
                    extra_classifier_ref = str(raw.get("extra_clip_classifier_path")).strip()
                    if classifier_map:
                        mapped_classifier = classifier_map.get(extra_classifier_ref)
                        if not mapped_classifier:
                            raise HTTPException(
                                status_code=HTTP_400_BAD_REQUEST,
                                detail="agent_cascade_import_missing_classifier",
                            )
                        extra_classifier_ref = mapped_classifier
                steps_out.append(
                    {
                        "enabled": bool(raw.get("enabled", True)),
                        "recipe_id": mapped,
                        "override_class_id": raw.get("override_class_id"),
                        "override_class_name": raw.get("override_class_name"),
                        "dedupe_group": raw.get("dedupe_group"),
                        "participate_cross_class_dedupe": bool(raw.get("participate_cross_class_dedupe", True)),
                        "clip_head_min_prob_override": raw.get("clip_head_min_prob_override"),
                        "clip_head_margin_override": raw.get("clip_head_margin_override"),
                        "extra_clip_classifier_path": extra_classifier_ref,
                        "extra_clip_min_prob": raw.get("extra_clip_min_prob"),
                        "extra_clip_margin": raw.get("extra_clip_margin"),
                    }
                )

            if isinstance(dedupe_raw.get("clip_head_recipe_id"), str) and dedupe_raw.get("clip_head_recipe_id"):
                mapped_head = id_map.get(str(dedupe_raw.get("clip_head_recipe_id")))
                if mapped_head:
                    dedupe_raw["clip_head_recipe_id"] = mapped_head

            return _persist_agent_cascade(str(label), {"steps": steps_out, "dedupe": dedupe_raw})
    except HTTPException:
        raise
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"agent_cascade_import_failed:{exc}") from exc
    finally:
        shutil.rmtree(staging_dir, ignore_errors=True)


@app.post("/sam3/prompt_helper/suggest")
def prompt_helper_suggest(payload: PromptHelperSuggestRequest):
    return _suggest_prompts_for_dataset(payload)


@app.post("/sam3/prompt_helper/expand")
def prompt_helper_expand(payload: PromptRecipeExpandRequest):
    dataset_root = _resolve_sam3_or_qwen_dataset(payload.dataset_id)
    coco, _, _ = _load_coco_index(dataset_root)
    categories = coco.get("categories") or []
    cat_entry = next((c for c in categories if int(c.get("id", categories.index(c))) == payload.class_id), None)
    if not cat_entry:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="recipe_class_not_found")
    class_name = str(cat_entry.get("name", f"class_{payload.class_id}"))
    base_prompts = [p.strip() for p in payload.base_prompts if isinstance(p, str) and p.strip()]
    new_prompts = _expand_prompts_with_prompt_llm(
        class_name,
        base_prompts,
        payload.max_new,
        max_new_tokens=payload.max_new_tokens if hasattr(payload, "max_new_tokens") else 128,
    )
    combined: List[str] = []
    seen = set()
    for prompt in [*base_prompts, *new_prompts]:
        low = prompt.lower()
        if low in seen:
            continue
        seen.add(low)
        combined.append(prompt)
    return {
        "class_id": payload.class_id,
        "class_name": class_name,
        "base_prompts": base_prompts,
        "new_prompts": new_prompts,
        "combined": combined,
    }


def _list_prompt_helper_presets() -> List[Dict[str, Any]]:
    presets: List[Dict[str, Any]] = []
    for path in PROMPT_HELPER_PRESET_ROOT.glob("*.json"):
        try:
            with path.open("r", encoding="utf-8") as handle:
                data = json.load(handle)
            presets.append(data)
        except Exception:
            continue
    presets.sort(key=lambda p: p.get("created_at", 0), reverse=True)
    return presets


def _load_prompt_helper_preset(preset_id: str) -> Dict[str, Any]:
    path = (PROMPT_HELPER_PRESET_ROOT / f"{preset_id}.json").resolve()
    if not str(path).startswith(str(PROMPT_HELPER_PRESET_ROOT.resolve())) or not path.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="prompt_helper_preset_not_found")
    try:
        with path.open("r", encoding="utf-8") as handle:
            return json.load(handle)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"prompt_helper_preset_load_failed:{exc}") from exc


def _save_prompt_helper_preset(label: str, dataset_id: str, prompts_by_class: Dict[int, List[str]]) -> Dict[str, Any]:
    preset_id = f"phset_{uuid.uuid4().hex[:8]}"
    created_at = time.time()
    payload = {
        "id": preset_id,
        "label": label or preset_id,
        "dataset_id": dataset_id,
        "created_at": created_at,
        "prompts_by_class": prompts_by_class,
    }
    path = (PROMPT_HELPER_PRESET_ROOT / f"{preset_id}.json").resolve()
    if not str(path).startswith(str(PROMPT_HELPER_PRESET_ROOT.resolve())):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="prompt_helper_preset_path_invalid")
    with path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle, ensure_ascii=False, indent=2)
    return payload


@app.post("/sam3/prompt_helper/jobs")
def start_prompt_helper_job(payload: PromptHelperRequest):
    job = _start_prompt_helper_job(payload)
    return _serialize_prompt_helper_job(job)


@app.post("/sam3/prompt_helper/search")
def start_prompt_helper_search(payload: PromptHelperSearchRequest):
    job = _start_prompt_helper_search_job(payload)
    return _serialize_prompt_helper_job(job)


@app.post("/sam3/prompt_helper/recipe")
def start_prompt_helper_recipe(payload: PromptRecipeRequest):
    job = _start_prompt_recipe_job(payload)
    return _serialize_prompt_helper_job(job)

@app.get("/sam3/prompt_helper/presets")
def list_prompt_helper_presets():
    return _list_prompt_helper_presets()


@app.get("/sam3/prompt_helper/presets/{preset_id}")
def get_prompt_helper_preset(preset_id: str):
    return _load_prompt_helper_preset(preset_id)


@app.post("/sam3/prompt_helper/presets")
def create_prompt_helper_preset(
    dataset_id: str = Form(...),
    label: str = Form(""),
    prompts_json: str = Form(...),
):
    try:
        prompts_by_class = json.loads(prompts_json)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"invalid_prompts:{exc}") from exc
    if not isinstance(prompts_by_class, dict):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="prompts_must_be_object")
    normalized: Dict[int, List[str]] = {}
    for key, vals in prompts_by_class.items():
        try:
            cid = int(key)
        except Exception:
            continue
        if not isinstance(vals, (list, tuple)):
            continue
        cleaned = [str(v).strip() for v in vals if isinstance(v, str) and str(v).strip()]
        if cleaned:
            normalized[cid] = cleaned
    if not normalized:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="no_prompts_provided")
    preset = _save_prompt_helper_preset(label, dataset_id, normalized)
    return preset


@app.get("/sam3/prompt_helper/jobs")
def list_prompt_helper_jobs():
    _prune_job_registry(PROMPT_HELPER_JOBS, PROMPT_HELPER_JOBS_LOCK)
    with PROMPT_HELPER_JOBS_LOCK:
        jobs = list(PROMPT_HELPER_JOBS.values())
    jobs.sort(key=lambda j: j.created_at, reverse=True)
    return [_serialize_prompt_helper_job(j) for j in jobs]


@app.get("/sam3/prompt_helper/jobs/{job_id}")
def get_prompt_helper_job(job_id: str):
    with PROMPT_HELPER_JOBS_LOCK:
        job = PROMPT_HELPER_JOBS.get(job_id)
    if not job:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="prompt_helper_job_not_found")
    return _serialize_prompt_helper_job(job)


@app.post("/segmentation/build/jobs")
def start_segmentation_build_job(request: SegmentationBuildRequest):
    job = _start_segmentation_build_job(request)
    return _serialize_seg_job(job)


@app.get("/segmentation/build/jobs")
def list_segmentation_build_jobs():
    _prune_job_registry(SEGMENTATION_BUILD_JOBS, SEGMENTATION_BUILD_JOBS_LOCK)
    with SEGMENTATION_BUILD_JOBS_LOCK:
        jobs = list(SEGMENTATION_BUILD_JOBS.values())
    jobs.sort(key=lambda j: j.created_at, reverse=True)
    return [_serialize_seg_job(job) for job in jobs]


@app.get("/segmentation/build/jobs/{job_id}")
def get_segmentation_build_job(job_id: str):
    with SEGMENTATION_BUILD_JOBS_LOCK:
        job = SEGMENTATION_BUILD_JOBS.get(job_id)
    if not job:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="segmentation_job_not_found")
    return _serialize_seg_job(job)


def _collect_labels_from_qwen_jsonl(jsonl_path: Path) -> List[str]:
    labels: set[str] = set()
    if not jsonl_path.exists():
        return []
    try:
        with jsonl_path.open("r", encoding="utf-8") as handle:
            for line in handle:
                line = line.strip()
                if not line:
                    continue
                try:
                    payload = json.loads(line)
                except Exception:
                    continue
                detections = _extract_qwen_detections_from_payload(payload)
                for det in detections:
                    label = str(det.get("label", "")).strip()
                    if label:
                        labels.add(label)
    except Exception as exc:  # noqa: BLE001
        logger.warning("Failed to scan labels from %s: %s", jsonl_path, exc)
    return sorted(labels)


def _extract_qwen_detections_from_payload(payload: Dict[str, Any]) -> List[Dict[str, Any]]:
    if not isinstance(payload, dict):
        return []
    detections = payload.get("detections")
    if isinstance(detections, list):
        return [det for det in detections if isinstance(det, dict)]
    conversations = payload.get("conversations")
    if not isinstance(conversations, list):
        return []
    for message in reversed(conversations):
        if not isinstance(message, dict):
            continue
        if message.get("from") != "gpt":
            continue
        value = message.get("value")
        if not isinstance(value, str):
            continue
        try:
            parsed = json.loads(value)
        except Exception:
            continue
        detections = parsed.get("detections")
        if isinstance(detections, list):
            return [det for det in detections if isinstance(det, dict)]
    return []


def _load_qwen_labelmap(dataset_root: Path) -> List[str]:
    meta = _load_qwen_dataset_metadata(dataset_root) or {}
    classes = [str(cls).strip() for cls in meta.get("classes", []) if str(cls).strip()]
    if classes:
        return classes
    labels = set()
    for split in ("train", "val"):
        labels.update(_collect_labels_from_qwen_jsonl(dataset_root / split / "annotations.jsonl"))
    return sorted(labels)


def _discover_yolo_labelmap(dataset_root: Path) -> List[str]:
    for name in ("labelmap.txt", "classes.txt", "labels.txt"):
        candidate = dataset_root / name
        classes = _load_labelmap_file(candidate)
        if classes:
            return classes
    return []


def _coco_info_block(dataset_id: str) -> Dict[str, Any]:
    """Minimal COCO info section to keep pycocotools happy."""
    return {
        "description": f"{dataset_id} generated by tator",
        "version": "1.0",
        "year": int(time.strftime("%Y", time.gmtime())),
        "contributor": "tator",
        "date_created": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
    }


def _write_coco_annotations(
    output_path: Path,
    *,
    dataset_id: str,
    categories: List[Dict[str, Any]],
    images: List[Dict[str, Any]],
    annotations: List[Dict[str, Any]],
) -> None:
    payload = {
        "info": _coco_info_block(dataset_id),
        "licenses": [],
        "images": images,
        "annotations": annotations,
        "categories": categories,
    }
    with output_path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle)


def _ensure_coco_info_fields(path: Path, dataset_id: str, categories: List[Dict[str, Any]]) -> str:
    """Backfill missing COCO 'info'/'licenses' for older conversions."""
    try:
        with path.open("r", encoding="utf-8") as handle:
            data = json.load(handle)
    except Exception as exc:  # noqa: BLE001
        logger.warning("Failed to load COCO file %s to backfill info: %s", path, exc)
        return str(path)
    if not isinstance(data, dict):
        return str(path)
    modified = False
    if "info" not in data or not isinstance(data["info"], dict):
        data["info"] = _coco_info_block(dataset_id)
        modified = True
    if "licenses" not in data or not isinstance(data["licenses"], list):
        data["licenses"] = []
        modified = True
    if categories and (not isinstance(data.get("categories"), list) or not data["categories"]):
        data["categories"] = categories
        modified = True
    if modified:
        try:
            with path.open("w", encoding="utf-8") as handle:
                json.dump(data, handle)
        except Exception as exc:  # noqa: BLE001
            logger.warning("Failed to rewrite COCO file %s: %s", path, exc)
    return str(path)


def _ensure_coco_supercategory(path: Path, default: str = "object") -> bool:
    """Ensure every COCO category has a supercategory (RF-DETR expects it)."""
    try:
        with path.open("r", encoding="utf-8") as handle:
            data = json.load(handle)
    except Exception:
        return False
    if not isinstance(data, dict):
        return False
    categories = data.get("categories")
    if not isinstance(categories, list):
        return False
    modified = False
    for category in categories:
        if not isinstance(category, dict):
            continue
        if "supercategory" not in category:
            category["supercategory"] = default
            modified = True
    if modified:
        try:
            with path.open("w", encoding="utf-8") as handle:
                json.dump(data, handle)
        except Exception:
            return False
    return modified


def _rfdetr_remap_coco_ids(src_path: Path, dest_path: Path) -> None:
    """Create a 0-based COCO category id mapping for RF-DETR."""
    data = json.loads(src_path.read_text())
    categories = data.get("categories", [])
    annotations = data.get("annotations", [])
    if not isinstance(categories, list) or not isinstance(annotations, list):
        raise RuntimeError("rfdetr_coco_invalid")
    ordered = [c for c in categories if isinstance(c, dict) and "id" in c and "name" in c]
    ordered.sort(key=lambda c: int(c.get("id", 0)))
    mapping = {int(cat["id"]): idx for idx, cat in enumerate(ordered)}
    new_categories = []
    for idx, cat in enumerate(ordered):
        new_categories.append(
            {
                "id": idx,
                "name": str(cat.get("name")),
                "supercategory": cat.get("supercategory") or "object",
            }
        )
    new_annotations = []
    for ann in annotations:
        if not isinstance(ann, dict):
            continue
        try:
            cat_id = int(ann.get("category_id"))
        except Exception:
            continue
        if cat_id not in mapping:
            continue
        ann = dict(ann)
        ann["category_id"] = mapping[cat_id]
        new_annotations.append(ann)
    data["categories"] = new_categories
    data["annotations"] = new_annotations
    dest_path.parent.mkdir(parents=True, exist_ok=True)
    dest_path.write_text(json.dumps(data))


def _rfdetr_prepare_dataset(dataset_root: Path, run_dir: Path, coco_train: str, coco_val: str) -> Path:
    """Prepare a RF-DETR-compatible dataset layout with 0-based category ids."""
    dataset_dir = run_dir / "dataset"
    dataset_dir.mkdir(parents=True, exist_ok=True)
    train_src = dataset_root / "train"
    valid_src = dataset_root / "valid"
    val_src = dataset_root / "val"
    test_src = dataset_root / "test"

    if not train_src.exists():
        train_src = dataset_root
    if not valid_src.exists():
        valid_src = val_src if val_src.exists() else train_src
    if not test_src.exists():
        test_src = valid_src if valid_src.exists() else train_src

    def _link_split(name: str, source: Path) -> None:
        dest = dataset_dir / name
        if dest.exists() or dest.is_symlink():
            return
        try:
            dest.symlink_to(source, target_is_directory=True)
        except Exception:
            shutil.copytree(source, dest)

    _link_split("train", train_src)
    _link_split("valid", valid_src)
    _link_split("test", test_src)

    train_dest = dataset_dir / "train" / "_annotations.coco.json"
    val_dest = dataset_dir / "valid" / "_annotations.coco.json"
    test_dest = dataset_dir / "test" / "_annotations.coco.json"
    _rfdetr_remap_coco_ids(Path(coco_train), train_dest)
    _rfdetr_remap_coco_ids(Path(coco_val), val_dest)
    _rfdetr_remap_coco_ids(Path(coco_val), test_dest)
    return dataset_dir


def _normalize_device_list(devices: Optional[List[Any]]) -> List[int]:
    if not devices:
        return []
    cleaned: List[int] = []
    for value in devices:
        try:
            cleaned.append(int(value))
        except Exception:
            continue
    return [value for value in cleaned if value >= 0]


def _parse_device_ids_string(raw: Optional[str]) -> Optional[List[int]]:
    if raw is None:
        return None
    text = str(raw).strip()
    if not text:
        return []
    parts = [part.strip() for part in text.split(",") if part.strip()]
    if not parts:
        return []
    ids: List[int] = []
    for part in parts:
        if not part.isdigit():
            raise ValueError(f"invalid_device_token:{part}")
        ids.append(int(part))
    return ids


def _validate_cuda_device_ids(device_ids: Sequence[int]) -> None:
    if not device_ids:
        return
    if not torch.cuda.is_available():
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="qwen_devices_unavailable")
    max_id = torch.cuda.device_count() - 1
    invalid = [device for device in device_ids if device < 0 or device > max_id]
    if invalid:
        raise HTTPException(
            status_code=HTTP_400_BAD_REQUEST,
            detail=f"qwen_invalid_devices:available=0-{max_id}",
        )


def _find_free_port() -> int:
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.bind(("", 0))
        return int(sock.getsockname()[1])


def _rfdetr_ddp_worker(
    rank: int,
    world_size: int,
    variant_id: str,
    model_kwargs: Dict[str, Any],
    train_kwargs: Dict[str, Any],
    aug_policy: Optional[Dict[str, Any]],
    dist_url: str,
) -> None:
    os.environ["RANK"] = str(rank)
    os.environ["WORLD_SIZE"] = str(world_size)
    os.environ["LOCAL_RANK"] = str(rank)
    if dist_url.startswith("tcp://"):
        try:
            host_port = dist_url.replace("tcp://", "")
            host, port = host_port.split(":", 1)
            os.environ["MASTER_ADDR"] = host
            os.environ["MASTER_PORT"] = port
        except Exception:
            pass
    try:
        from rfdetr import (
            RFDETRBase,
            RFDETRLarge,
            RFDETRNano,
            RFDETRSmall,
            RFDETRMedium,
            RFDETRSegPreview,
        )
    except Exception as exc:  # noqa: BLE001
        raise RuntimeError(f"rfdetr_import_failed:{exc}") from exc
    model_cls_map = {
        "rfdetr-nano": RFDETRNano,
        "rfdetr-small": RFDETRSmall,
        "rfdetr-medium": RFDETRMedium,
        "rfdetr-base": RFDETRBase,
        "rfdetr-large": RFDETRLarge,
        "rfdetr-seg-preview": RFDETRSegPreview,
    }
    model_cls = model_cls_map.get(variant_id)
    if not model_cls:
        raise RuntimeError("rfdetr_variant_unknown")
    model_kwargs = dict(model_kwargs)
    model_kwargs["device"] = "cuda" if torch.cuda.is_available() else model_kwargs.get("device", "cpu")
    train_kwargs = dict(train_kwargs)
    train_kwargs["device"] = "cuda" if torch.cuda.is_available() else train_kwargs.get("device", "cpu")
    train_kwargs["world_size"] = world_size
    train_kwargs["dist_url"] = dist_url
    rf_detr = model_cls(**model_kwargs)
    restore = _rfdetr_install_augmentations(_rfdetr_normalize_aug_policy(aug_policy))
    try:
        rf_detr.train(**train_kwargs)
    finally:
        _rfdetr_restore_augmentations(restore)


def _convert_yolo_dataset_to_coco(dataset_root: Path) -> Dict[str, Any]:
    dataset_root = dataset_root.resolve()
    train_images = dataset_root / "train" / "images"
    train_labels = dataset_root / "train" / "labels"
    val_images = dataset_root / "val" / "images"
    val_labels = dataset_root / "val" / "labels"
    for path in (train_images, train_labels):
        if not path.exists():
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="sam3_yolo_split_missing")
    has_val_images = val_images.exists()
    has_val_labels = val_labels.exists()
    if has_val_images != has_val_labels:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="sam3_yolo_val_split_incomplete")
    if not has_val_images:
        # Allow train-only YOLO zips. We create an empty val split so downstream
        # code paths (COCO conversion, random split, etc.) have a consistent layout.
        val_images.mkdir(parents=True, exist_ok=True)
        val_labels.mkdir(parents=True, exist_ok=True)

    labelmap = _discover_yolo_labelmap(dataset_root)
    if not labelmap:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="sam3_labelmap_missing")
    label_to_id = {label: idx + 1 for idx, label in enumerate(labelmap)}
    categories = [{"id": cid, "name": name, "supercategory": "object"} for name, cid in label_to_id.items()]
    signature = _compute_dir_signature(dataset_root)
    existing_meta = _load_sam3_dataset_metadata(dataset_root)
    # Preserve previously recorded metadata and infer dataset type (bbox vs seg) so downstream
    # training can enable masks when appropriate.
    dataset_type = (existing_meta or {}).get("type", "bbox")
    dataset_label = (existing_meta or {}).get("label", dataset_root.name)
    dataset_source = (existing_meta or {}).get("source", "yolo")
    def _coco_has_invalid_image_refs(path: Path) -> bool:
        try:
            with path.open("r", encoding="utf-8") as handle:
                data = json.load(handle)
        except Exception:
            return True
        if not isinstance(data, dict):
            return True
        images = data.get("images")
        anns = data.get("annotations")
        if not isinstance(images, list) or not isinstance(anns, list):
            return True
        image_ids = set()
        for img in images:
            if not isinstance(img, dict):
                continue
            try:
                image_ids.add(int(img.get("id")))
            except Exception:
                continue
        for ann in anns:
            if not isinstance(ann, dict):
                continue
            try:
                img_id = int(ann.get("image_id"))
            except Exception:
                continue
            if img_id not in image_ids:
                return True
        return False

    def _coco_missing_segmentation(path: Path) -> bool:
        try:
            with path.open("r", encoding="utf-8") as handle:
                data = json.load(handle)
        except Exception:
            return True
        if not isinstance(data, dict):
            return True
        anns = data.get("annotations")
        if not isinstance(anns, list):
            return True
        for ann in anns:
            if not isinstance(ann, dict):
                continue
            seg = ann.get("segmentation")
            if seg is not None and seg != []:
                return False
        return True

    if (
        existing_meta
        and existing_meta.get("signature") == signature
        and existing_meta.get("coco_train_json")
        and existing_meta.get("coco_val_json")
    ):
        coco_train_path = Path(existing_meta["coco_train_json"])
        coco_val_path = Path(existing_meta["coco_val_json"])
        rebuild = _coco_has_invalid_image_refs(coco_train_path) or _coco_has_invalid_image_refs(coco_val_path)
        if dataset_type == "seg":
            rebuild = rebuild or _coco_missing_segmentation(coco_train_path) or _coco_missing_segmentation(coco_val_path)
        if not rebuild:
            # Backfill missing COCO info if this dataset was converted before we added it.
            _ensure_coco_info_fields(coco_train_path, dataset_root.name, categories)
            _ensure_coco_info_fields(coco_val_path, dataset_root.name, categories)
            return existing_meta

    # Rebuild/conversion path: infer bbox vs seg directly from labels.
    dataset_type = "bbox"

    image_id_counter = 1
    annotation_id = 1
    image_exts = {".jpg", ".jpeg", ".png", ".bmp", ".webp", ".tif", ".tiff"}

    def _image_path_for_label(labels_dir: Path, images_dir: Path, label_file: Path) -> Optional[Path]:
        stem = label_file.stem
        try:
            rel_label = label_file.relative_to(labels_dir)
        except Exception:
            rel_label = Path(label_file.name)
        # Prefer mirrored subdirectory structure when present.
        for ext in image_exts:
            candidate = images_dir / rel_label.with_suffix(ext)
            if candidate.exists():
                return candidate
        for ext in image_exts:
            candidate = images_dir / f"{stem}{ext}"
            if candidate.exists():
                return candidate
        for candidate in images_dir.rglob(f"{stem}.*"):
            if candidate.suffix.lower() in image_exts:
                return candidate
        return None

    def _convert_split(split_images: Path, split_labels: Path, split_name: str) -> str:
        nonlocal image_id_counter, annotation_id, dataset_type
        images: List[Dict[str, Any]] = []
        annotations: List[Dict[str, Any]] = []
        images_lookup: Dict[str, int] = {}
        image_sizes: Dict[str, Tuple[int, int]] = {}

        def _clamp01(val: float) -> float:
            return max(0.0, min(1.0, val))

        def _bbox_xyxy_from_cxcywh(cx: float, cy: float, w: float, h: float) -> Optional[Tuple[float, float, float, float]]:
            if w <= 0 or h <= 0:
                return None
            x1 = cx - w / 2.0
            y1 = cy - h / 2.0
            x2 = cx + w / 2.0
            y2 = cy + h / 2.0
            x1 = _clamp01(x1)
            y1 = _clamp01(y1)
            x2 = _clamp01(x2)
            y2 = _clamp01(y2)
            if x2 <= x1 or y2 <= y1:
                return None
            return (x1, y1, x2, y2)

        def _bbox_xyxy_from_polygon(coords: List[float]) -> Optional[Tuple[float, float, float, float]]:
            if len(coords) < 6 or len(coords) % 2 != 0:
                return None
            xs = coords[0::2]
            ys = coords[1::2]
            if not xs or not ys:
                return None
            min_x = _clamp01(min(xs))
            max_x = _clamp01(max(xs))
            min_y = _clamp01(min(ys))
            max_y = _clamp01(max(ys))
            if max_x <= min_x or max_y <= min_y:
                return None
            return (min_x, min_y, max_x, max_y)

        def _bbox_iou(a: Tuple[float, float, float, float], b: Tuple[float, float, float, float]) -> float:
            ax1, ay1, ax2, ay2 = a
            bx1, by1, bx2, by2 = b
            inter_w = max(0.0, min(ax2, bx2) - max(ax1, bx1))
            inter_h = max(0.0, min(ay2, by2) - max(ay1, by1))
            inter_area = inter_w * inter_h
            if inter_area <= 0:
                return 0.0
            area_a = max(0.0, ax2 - ax1) * max(0.0, ay2 - ay1)
            area_b = max(0.0, bx2 - bx1) * max(0.0, by2 - by1)
            denom = area_a + area_b - inter_area
            return inter_area / denom if denom > 0 else 0.0

        def _polygon_to_coco_segmentation(coords: List[float], width: int, height: int) -> Optional[List[List[float]]]:
            if len(coords) < 6 or len(coords) % 2 != 0:
                return None
            out: List[float] = []
            for idx in range(0, len(coords), 2):
                x = _clamp01(coords[idx]) * width
                y = _clamp01(coords[idx + 1]) * height
                out.extend([x, y])
            if len(out) < 6:
                return None
            return [out]

        for label_file in sorted(split_labels.rglob("*.txt")):
            image_path = _image_path_for_label(split_labels, split_images, label_file)
            if image_path is None:
                logger.warning("No matching image for label file %s", label_file)
                continue
            image_rel = str(image_path.relative_to(split_images.parent))
            if image_rel not in images_lookup:
                try:
                    with Image.open(image_path) as im:
                        width, height = im.size
                except Exception as exc:  # noqa: BLE001
                    logger.warning("Failed to read image %s: %s", image_path, exc)
                    continue
                images_lookup[image_rel] = image_id_counter
                image_sizes[image_rel] = (width, height)
                images.append(
                    {
                        "id": image_id_counter,
                        "file_name": image_rel,
                        "width": width,
                        "height": height,
                    }
                )
                image_id_counter += 1
            image_id = images_lookup[image_rel]
            width, height = image_sizes.get(image_rel, (None, None))
            try:
                with label_file.open("r", encoding="utf-8") as handle:
                    lines = [ln.strip() for ln in handle if ln.strip()]
            except Exception as exc:  # noqa: BLE001
                logger.warning("Failed to read YOLO labels from %s: %s", label_file, exc)
                continue
            for line in lines:
                parts = line.split()
                if len(parts) < 5:
                    continue
                try:
                    class_idx = int(float(parts[0]))
                except (TypeError, ValueError):
                    continue
                if class_idx < 0 or class_idx >= len(labelmap):
                    continue
                if width is None or height is None:
                    continue
                raw_vals = []
                for token in parts[1:]:
                    try:
                        raw_vals.append(float(token))
                    except (TypeError, ValueError):
                        raw_vals = []
                        break
                if not raw_vals:
                    continue
                bbox_xyxy: Optional[Tuple[float, float, float, float]] = None
                segmentation: Optional[List[List[float]]] = None
                if len(raw_vals) == 4:
                    cx, cy, w, h = raw_vals
                    bbox_xyxy = _bbox_xyxy_from_cxcywh(cx, cy, w, h)
                else:
                    # Support YOLO-seg polygon-only format (YOLOv8) and bbox+polygon format.
                    poly_only = raw_vals if len(raw_vals) >= 6 and len(raw_vals) % 2 == 0 else None
                    bbox_plus_poly = None
                    if len(raw_vals) > 4 and (len(raw_vals) - 4) >= 6 and (len(raw_vals) - 4) % 2 == 0:
                        bbox_plus_poly = (raw_vals[:4], raw_vals[4:])
                    chosen_poly = None
                    if poly_only is not None and bbox_plus_poly is not None:
                        bbox_fields, poly_fields = bbox_plus_poly
                        bbox_from_fields = _bbox_xyxy_from_cxcywh(*bbox_fields)
                        bbox_from_poly = _bbox_xyxy_from_polygon(poly_fields)
                        if bbox_from_fields is not None and bbox_from_poly is not None and _bbox_iou(bbox_from_fields, bbox_from_poly) >= 0.9:
                            chosen_poly = poly_fields
                            bbox_xyxy = bbox_from_poly
                        else:
                            chosen_poly = poly_only
                            bbox_xyxy = _bbox_xyxy_from_polygon(poly_only)
                    elif bbox_plus_poly is not None:
                        _, poly_fields = bbox_plus_poly
                        chosen_poly = poly_fields
                        bbox_xyxy = _bbox_xyxy_from_polygon(poly_fields)
                    elif poly_only is not None:
                        chosen_poly = poly_only
                        bbox_xyxy = _bbox_xyxy_from_polygon(poly_only)
                    else:
                        # Unknown extra fields; treat first four as bbox and ignore remainder.
                        bbox_xyxy = _bbox_xyxy_from_cxcywh(*raw_vals[:4])
                    if chosen_poly is not None and bbox_xyxy is not None:
                        segmentation = _polygon_to_coco_segmentation(chosen_poly, int(width), int(height))
                        dataset_type = "seg"

                if bbox_xyxy is None:
                    continue
                x1_n, y1_n, x2_n, y2_n = bbox_xyxy
                x1 = x1_n * width
                y1 = y1_n * height
                abs_w = (x2_n - x1_n) * width
                abs_h = (y2_n - y1_n) * height
                if abs_w <= 0 or abs_h <= 0:
                    continue
                ann = {
                    "id": annotation_id,
                    "image_id": image_id,
                    "category_id": class_idx + 1,
                    "bbox": [x1, y1, abs_w, abs_h],
                    "area": abs_w * abs_h,
                    "iscrowd": 0,
                }
                if segmentation is not None:
                    ann["segmentation"] = segmentation
                annotations.append(ann)
                annotation_id += 1
        output_path = dataset_root / split_name / "_annotations.coco.json"
        try:
            _write_coco_annotations(
                output_path,
                dataset_id=dataset_root.name,
                categories=categories,
                images=images,
                annotations=annotations,
            )
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"sam3_coco_write_failed:{exc}") from exc
        return str(output_path)

    coco_train = _convert_split(train_images, train_labels, "train")
    coco_val = _convert_split(val_images, val_labels, "val")
    sam3_meta = {
        "id": dataset_root.name,
        "label": dataset_label,
        "source": dataset_source,
        "type": dataset_type,
        "dataset_root": str(dataset_root),
        "signature": signature,
        "classes": labelmap,
        "context": "",
        "image_count": None,
        "train_count": None,
        "val_count": None,
        "coco_train_json": coco_train,
        "coco_val_json": coco_val,
        "converted_at": time.time(),
    }
    _persist_sam3_dataset_metadata(dataset_root, sam3_meta)
    return sam3_meta


def _convert_qwen_dataset_to_coco(dataset_root: Path) -> Dict[str, Any]:
    dataset_root = dataset_root.resolve()
    metadata = _load_qwen_dataset_metadata(dataset_root) or {}
    metadata, signature = _ensure_qwen_dataset_signature(dataset_root, metadata)
    if "type" not in metadata:
        metadata["type"] = "bbox"
        _persist_qwen_dataset_metadata(dataset_root, metadata)
    dataset_id = metadata.get("id") or dataset_root.name
    labelmap = _load_qwen_labelmap(dataset_root)
    if not labelmap:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="sam3_labelmap_missing")
    label_to_id = {label: idx + 1 for idx, label in enumerate(labelmap)}
    categories = [{"id": cid, "name": name, "supercategory": "object"} for name, cid in label_to_id.items()]
    existing_meta = _load_sam3_dataset_metadata(dataset_root)
    if (
        existing_meta
        and existing_meta.get("signature") == signature
        and existing_meta.get("coco_train_json")
        and existing_meta.get("coco_val_json")
    ):
        _ensure_coco_info_fields(Path(existing_meta["coco_train_json"]), dataset_id, categories)
        _ensure_coco_info_fields(Path(existing_meta["coco_val_json"]), dataset_id, categories)
        return existing_meta

    annotation_id = 1
    images_lookup: Dict[str, int] = {}
    image_sizes: Dict[str, Tuple[int, int]] = {}

    def _convert_split(split: str) -> str:
        nonlocal annotation_id
        jsonl_path = dataset_root / split / "annotations.jsonl"
        if not jsonl_path.exists():
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"sam3_annotations_missing:{split}")
        images: List[Dict[str, Any]] = []
        annotations: List[Dict[str, Any]] = []
        try:
            with jsonl_path.open("r", encoding="utf-8") as handle:
                for line in handle:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        payload = json.loads(line)
                    except Exception:
                        continue
                    image_rel = payload.get("image")
                    if not isinstance(image_rel, str):
                        continue
                    if image_rel not in images_lookup:
                        image_path = dataset_root / split / image_rel
                        if not image_path.exists():
                            logger.warning("Missing image referenced in %s: %s", jsonl_path, image_path)
                            continue
                        try:
                            with Image.open(image_path) as im:
                                width, height = im.size
                        except Exception as exc:  # noqa: BLE001
                            logger.warning("Failed to read image %s: %s", image_path, exc)
                            continue
                        images_lookup[image_rel] = len(images_lookup) + 1
                        image_sizes[image_rel] = (width, height)
                        images.append(
                            {
                                "id": images_lookup[image_rel],
                                "file_name": image_rel,
                                "width": width,
                                "height": height,
                            }
                        )
                    image_id = images_lookup[image_rel]
                    width, height = image_sizes.get(image_rel, (None, None))
                    detections = _extract_qwen_detections_from_payload(payload)
                    for det in detections:
                        label = str(det.get("label", "")).strip()
                        if not label or label not in label_to_id:
                            continue
                        bbox = det.get("bbox")
                        if isinstance(bbox, (list, tuple)) and len(bbox) >= 4:
                            try:
                                x1 = float(bbox[0])
                                y1 = float(bbox[1])
                                x2 = float(bbox[2])
                                y2 = float(bbox[3])
                            except (TypeError, ValueError):
                                continue
                            if width is not None and height is not None:
                                x1 = max(0.0, min(x1, width))
                                x2 = max(0.0, min(x2, width))
                                y1 = max(0.0, min(y1, height))
                                y2 = max(0.0, min(y2, height))
                            w = max(0.0, x2 - x1)
                            h = max(0.0, y2 - y1)
                            if w <= 0 or h <= 0:
                                continue
                            coco_bbox = [x1, y1, w, h]
                        else:
                            point = det.get("point")
                            if not (isinstance(point, (list, tuple)) and len(point) >= 2):
                                continue
                            try:
                                cx = float(point[0])
                                cy = float(point[1])
                            except (TypeError, ValueError):
                                continue
                            # Convert point to a tiny box to retain the signal.
                            size = 2.0
                            x1 = cx - size / 2.0
                            y1 = cy - size / 2.0
                            coco_bbox = [x1, y1, size, size]
                        area = coco_bbox[2] * coco_bbox[3]
                        if area <= 0:
                            continue
                        annotations.append(
                            {
                                "id": annotation_id,
                                "image_id": image_id,
                                "category_id": label_to_id[label],
                                "bbox": coco_bbox,
                                "area": area,
                                "iscrowd": 0,
                            }
                        )
                        annotation_id += 1
        except Exception as exc:  # noqa: BLE001
            logger.exception("Failed to convert %s to COCO: %s", jsonl_path, exc)
            raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"sam3_coco_conversion_failed:{split}")
        output_path = dataset_root / split / "_annotations.coco.json"
        try:
            _write_coco_annotations(
                output_path,
                dataset_id=dataset_id,
                categories=categories,
                images=images,
                annotations=annotations,
            )
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"sam3_coco_write_failed:{exc}") from exc
        return str(output_path)

    coco_train = _convert_split("train")
    coco_val = _convert_split("val")
    sam3_meta = {
        "id": metadata.get("id") or dataset_root.name,
        "label": metadata.get("label") or metadata.get("id") or dataset_root.name,
        "source": "qwen",
        "type": metadata.get("type", "bbox"),
        "dataset_root": str(dataset_root),
        "signature": signature,
        "classes": labelmap,
        "context": metadata.get("context", ""),
        "image_count": metadata.get("image_count"),
        "train_count": metadata.get("train_count"),
        "val_count": metadata.get("val_count"),
        "coco_train_json": coco_train,
        "coco_val_json": coco_val,
        "converted_at": time.time(),
    }
    _persist_sam3_dataset_metadata(dataset_root, sam3_meta)
    return sam3_meta


def _convert_coco_dataset_to_yolo(dataset_root: Path) -> Dict[str, Any]:
    dataset_root = dataset_root.resolve()
    ann_paths: List[Tuple[str, Path, Path]] = []
    for split in ("train", "val"):
        ann_path = dataset_root / split / "_annotations.coco.json"
        if not ann_path.exists():
            continue
        images_dir = ann_path.parent / "images"
        if not images_dir.exists():
            images_dir = ann_path.parent
        ann_paths.append((split, ann_path, images_dir))
    if not ann_paths:
        ann_path, images_dir = _find_coco_split(dataset_root)
        ann_paths = [("train", ann_path, images_dir)]

    category_map: Dict[int, str] = {}
    has_segmentation = False
    for _, ann_path, _ in ann_paths:
        try:
            data = json.loads(ann_path.read_text(encoding="utf-8"))
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"coco_load_failed:{exc}") from exc
        for cat in data.get("categories", []) or []:
            try:
                cid = int(cat.get("id"))
            except Exception:
                continue
            name = str(cat.get("name") or f"class_{cid}")
            category_map.setdefault(cid, name)
        if not category_map:
            for ann in data.get("annotations", []) or []:
                try:
                    cid = int(ann.get("category_id"))
                except Exception:
                    continue
                category_map.setdefault(cid, f"class_{cid}")
        if not has_segmentation:
            for ann in data.get("annotations", []) or []:
                seg = ann.get("segmentation")
                if isinstance(seg, list) and any(isinstance(poly, list) and len(poly) >= 6 for poly in seg):
                    has_segmentation = True
                    break

    if not category_map:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="coco_categories_missing")

    sorted_ids = sorted(category_map.keys())
    labelmap = [category_map[cid] for cid in sorted_ids]
    labelmap_path = dataset_root / "labelmap.txt"
    labelmap_path.write_text("\n".join(labelmap) + "\n", encoding="utf-8")
    cat_id_to_idx = {cid: idx for idx, cid in enumerate(sorted_ids)}

    def _resolve_image_path(file_name: str, images_dir: Path, split_name: str) -> Optional[Path]:
        if not file_name:
            return None
        rel_path = Path(file_name)
        candidates: List[Path] = []
        if rel_path.is_absolute():
            candidates.append(rel_path)
        candidates.append(images_dir / rel_path)
        candidates.append(images_dir / rel_path.name)
        candidates.append(dataset_root / rel_path)
        candidates.append(dataset_root / split_name / "images" / rel_path.name)
        for cand in candidates:
            if cand.exists():
                return cand
        return None

    def _label_relpath_for_image(file_name: str) -> Path:
        rel_path = Path(file_name)
        if rel_path.is_absolute():
            rel_path = Path(rel_path.name)
        if "images" in rel_path.parts:
            idx = rel_path.parts.index("images")
            rel_path = Path(*rel_path.parts[idx + 1 :])
        return rel_path.with_suffix(".txt")

    dataset_type = "seg" if has_segmentation else "bbox"
    for split_name, ann_path, images_dir in ann_paths:
        labels_dir = dataset_root / split_name / "labels"
        labels_dir.mkdir(parents=True, exist_ok=True)
        try:
            data = json.loads(ann_path.read_text(encoding="utf-8"))
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"coco_load_failed:{exc}") from exc
        images = data.get("images", []) or []
        annotations = data.get("annotations", []) or []
        ann_by_image: Dict[int, List[Dict[str, Any]]] = {}
        for ann in annotations:
            try:
                img_id = int(ann.get("image_id"))
            except Exception:
                continue
            ann_by_image.setdefault(img_id, []).append(ann)
        for img in images:
            try:
                img_id = int(img.get("id"))
            except Exception:
                continue
            file_name = str(img.get("file_name") or "")
            img_path = _resolve_image_path(file_name, images_dir, split_name)
            if img_path is None:
                logger.warning("COCO->YOLO: missing image for %s in %s", file_name, dataset_root)
                continue
            width = img.get("width")
            height = img.get("height")
            if not width or not height:
                try:
                    with Image.open(img_path) as im:
                        width, height = im.size
                except Exception as exc:  # noqa: BLE001
                    logger.warning("COCO->YOLO: failed to read image size for %s: %s", img_path, exc)
                    continue
            label_rel = _label_relpath_for_image(file_name)
            label_path = labels_dir / label_rel
            label_path.parent.mkdir(parents=True, exist_ok=True)
            lines: List[str] = []
            for ann in ann_by_image.get(img_id, []):
                try:
                    cat_id = int(ann.get("category_id"))
                except Exception:
                    continue
                if cat_id not in cat_id_to_idx:
                    continue
                class_idx = cat_id_to_idx[cat_id]
                bbox = ann.get("bbox") or []
                if len(bbox) < 4:
                    continue
                x, y, w, h = map(float, bbox[:4])
                if w <= 0 or h <= 0:
                    continue
                cx = (x + w / 2.0) / float(width)
                cy = (y + h / 2.0) / float(height)
                bw = w / float(width)
                bh = h / float(height)
                if dataset_type == "seg":
                    seg = ann.get("segmentation")
                    poly = None
                    if isinstance(seg, list):
                        for candidate in seg:
                            if isinstance(candidate, list) and len(candidate) >= 6:
                                poly = candidate
                                break
                    if poly is not None:
                        coords: List[str] = []
                        for idx in range(0, len(poly), 2):
                            px = float(poly[idx]) / float(width)
                            py = float(poly[idx + 1]) / float(height)
                            coords.append(f"{max(0.0, min(1.0, px)):.6f}")
                            coords.append(f"{max(0.0, min(1.0, py)):.6f}")
                        if len(coords) >= 6:
                            lines.append(f"{class_idx} " + " ".join(coords))
                            continue
                lines.append(f"{class_idx} {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}")
            if lines:
                label_path.write_text("\n".join(lines) + "\n", encoding="utf-8")

    meta = _load_sam3_dataset_metadata(dataset_root) or {}
    meta.setdefault("id", dataset_root.name)
    meta.setdefault("label", dataset_root.name)
    meta.setdefault("source", meta.get("source") or "coco")
    meta["classes"] = labelmap
    meta["type"] = dataset_type
    meta["dataset_root"] = str(dataset_root)
    meta["signature"] = _compute_dir_signature(dataset_root)
    meta["yolo_converted_at"] = time.time()
    _persist_sam3_dataset_metadata(dataset_root, meta)
    return meta


def _list_sam3_datasets() -> List[Dict[str, Any]]:
    return _list_all_datasets()


def _resolve_sam3_dataset_meta(dataset_id: str) -> Dict[str, Any]:
    dataset_root = _resolve_sam3_or_qwen_dataset(dataset_id)
    annotations_path = dataset_root / "train" / "annotations.jsonl"
    train_images = dataset_root / "train" / "images"
    train_labels = dataset_root / "train" / "labels"
    if annotations_path.exists():
        meta = _convert_qwen_dataset_to_coco(dataset_root)
    elif train_images.exists() and train_labels.exists():
        meta = _convert_yolo_dataset_to_coco(dataset_root)
    else:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="sam3_dataset_type_unsupported")
    meta["dataset_root"] = str(dataset_root)
    return meta


def _prepare_sam3_training_split(
    dataset_root: Path,
    meta: Dict[str, Any],
    job_id: str,
    *,
    random_split: bool,
    val_percent: float,
    split_seed: int,
    train_limit: Optional[int] = None,
    val_limit: Optional[int] = None,
    log_messages: Optional[List[str]] = None,
) -> Dict[str, Any]:
    if not random_split:
        return meta
    coco_train_path = Path(meta.get("coco_train_json", ""))
    coco_val_path = Path(meta.get("coco_val_json", ""))
    if not coco_train_path.exists() or not coco_val_path.exists():
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="sam3_coco_split_missing")
    try:
        with coco_train_path.open("r", encoding="utf-8") as handle:
            coco_train = json.load(handle)
        with coco_val_path.open("r", encoding="utf-8") as handle:
            coco_val = json.load(handle)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"sam3_coco_load_failed:{exc}") from exc
    categories = coco_train.get("categories") or coco_val.get("categories") or []
    if not categories and meta.get("classes"):
        categories = [{"id": idx + 1, "name": name} for idx, name in enumerate(meta.get("classes", []))]
    if not categories:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="sam3_categories_missing")
    images: Dict[int, Dict[str, Any]] = {}
    ann_by_image: Dict[int, List[Dict[str, Any]]] = {}
    for coco_blob in (coco_train, coco_val):
        for img in coco_blob.get("images", []):
            try:
                img_id = int(img["id"])
            except Exception:
                continue
            images[img_id] = {**img, "id": img_id, "file_name": str(img.get("file_name", ""))}
        for ann in coco_blob.get("annotations", []):
            try:
                img_id = int(ann["image_id"])
            except Exception:
                continue
            ann_by_image.setdefault(img_id, []).append(ann)
    image_ids = list(images.keys())
    if not image_ids:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="sam3_training_no_images")
    rnd = random.Random(split_seed)
    rnd.shuffle(image_ids)
    total = len(image_ids)
    vp = max(0.0, min(float(val_percent), 0.9))
    val_count = int(total * vp)
    if val_count <= 0 and total > 1:
        val_count = 1
    if val_limit is not None and val_limit > 0:
        val_count = min(val_limit, val_count if val_count > 0 else val_limit, total - 1 if total > 1 else total)
    val_ids = image_ids[:val_count]
    train_ids = image_ids[val_count:]
    if train_limit is not None and train_limit > 0:
        train_ids = train_ids[:train_limit]
    if not train_ids or not val_ids:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="sam3_training_split_empty")
    split_root = (SAM3_JOB_ROOT / "splits" / job_id).resolve()
    split_root.parent.mkdir(parents=True, exist_ok=True)
    if split_root.exists():
        shutil.rmtree(split_root, ignore_errors=True)
    (split_root / "train" / "images").mkdir(parents=True, exist_ok=True)
    (split_root / "val" / "images").mkdir(parents=True, exist_ok=True)

    def _find_image_source(file_name: str) -> Optional[Path]:
        rel_path = Path(file_name)
        candidates = [
            dataset_root / rel_path,
            dataset_root / "train" / rel_path,
            dataset_root / "val" / rel_path,
            dataset_root / "train" / "images" / rel_path,
            dataset_root / "val" / "images" / rel_path,
        ]
        for cand in candidates:
            if cand.exists():
                return cand
        if rel_path.is_absolute() and rel_path.exists():
            return rel_path
        return None

    def _write_split(target_ids: List[int], split_name: str) -> Tuple[str, int]:
        images_out: List[Dict[str, Any]] = []
        anns_out: List[Dict[str, Any]] = []
        for img_id in target_ids:
            info = images.get(img_id)
            if not info:
                continue
            file_name = info.get("file_name")
            if not file_name:
                continue
            src_path = _find_image_source(file_name)
            if src_path is None:
                continue
            rel_name = _normalise_relative_path(file_name)
            if rel_name.is_absolute():
                rel_name = Path(rel_name.name)
            dst_path = split_root / split_name / rel_name
            _link_or_copy_file(src_path, dst_path)
            info_out = dict(info)
            info_out["file_name"] = rel_name.as_posix()
            images_out.append(info_out)
            anns_out.extend(ann_by_image.get(img_id, []))
        ann_path = split_root / split_name / "_annotations.coco.json"
        _write_coco_annotations(
            ann_path,
            dataset_id=meta.get("id") or dataset_root.name,
            categories=categories,
            images=images_out,
            annotations=anns_out,
        )
        return str(ann_path), len(images_out)

    coco_train_new, train_count = _write_split(train_ids, "train")
    coco_val_new, val_count = _write_split(val_ids, "val")
    new_meta = {
        **meta,
        "dataset_root": str(split_root),
        "coco_train_json": coco_train_new,
        "coco_val_json": coco_val_new,
        "train_count": train_count,
        "val_count": val_count,
        "image_count": train_count + val_count,
        "signature": _compute_dir_signature(split_root),
        "source": meta.get("source", "resplit"),
    }
    _persist_sam3_dataset_metadata(split_root, new_meta)
    summary = (
        f"SAM3 split: {train_count} train / {val_count} val "
        f"(seed={split_seed}, val_percent={vp:.2f}, src={dataset_root}) -> {split_root}"
    )
    logger.info(summary)
    if log_messages is not None:
        log_messages.append(summary)
    return new_meta


def _plan_segmentation_build(request: SegmentationBuildRequest) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    dataset_root = _resolve_sam3_or_qwen_dataset(request.source_dataset_id)
    source_meta = _load_qwen_dataset_metadata(dataset_root) or _load_sam3_dataset_metadata(dataset_root)
    if not source_meta:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="segmentation_source_metadata_missing")
    dataset_type = source_meta.get("type", "bbox")
    if dataset_type != "bbox":
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="segmentation_builder_requires_bbox")
    source_id = source_meta.get("id") or dataset_root.name
    suggested_name = f"{source_id}_seg"
    output_id = _safe_run_name(request.output_name, suggested_name)
    output_root = (SAM3_DATASET_ROOT / output_id).resolve()
    if not str(output_root).startswith(str(SAM3_DATASET_ROOT.resolve())):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="segmentation_output_path_invalid")
    if output_root.exists():
        raise HTTPException(status_code=HTTP_409_CONFLICT, detail="segmentation_output_exists")

    classes = source_meta.get("classes") or []
    context = source_meta.get("context") or source_meta.get("dataset_context") or ""
    source_signature = source_meta.get("signature") or _compute_dir_signature(dataset_root)
    planned_meta = {
        "id": output_id,
        "label": source_meta.get("label") or source_id,
        "type": "seg",
        "source": "segmentation_builder",
        "source_dataset_id": source_id,
        "source_dataset_root": str(dataset_root),
        "source_signature": source_signature,
        "generator_variant": request.sam_variant,
        "output_format": request.output_format,
        "classes": classes,
        "context": context,
        "created_at": time.time(),
    }
    planned_layout = {
        "dataset_root": str(output_root),
        "images_dir": str(output_root / "images"),
        "labels_dir": str(output_root / "labels"),
        "metadata_path": str(output_root / SAM3_DATASET_META_NAME),
        "log_dir": str(SEG_BUILDER_ROOT / "logs" / output_id),
    }
    return planned_meta, planned_layout


def _start_segmentation_build_job(request: SegmentationBuildRequest) -> SegmentationBuildJob:
    planned_meta, planned_layout = _plan_segmentation_build(request)
    job_id = str(uuid.uuid4())
    job = SegmentationBuildJob(
        job_id=job_id,
        status="queued",
        message="Queued",
        progress=0.0,
        config={
            "source_dataset_id": request.source_dataset_id,
            "sam_variant": request.sam_variant,
            "output_format": request.output_format,
            "planned_metadata": planned_meta,
            "planned_layout": planned_layout,
        },
    )
    with SEGMENTATION_BUILD_JOBS_LOCK:
        SEGMENTATION_BUILD_JOBS[job_id] = job

    def worker() -> None:
        try:
            _seg_job_update(job, status="running", progress=0.02, message="Preparing segmentation build…", error=None)
            source_meta = _resolve_sam3_dataset_meta(request.source_dataset_id)
            classes = source_meta.get("classes") or []
            if not classes:
                # Try to load from labelmap.txt directly.
                try:
                    labelmap_file = _resolve_sam3_or_qwen_dataset(request.source_dataset_id) / "labelmap.txt"
                    classes = _load_labelmap_file(labelmap_file)
                except Exception:
                    classes = []
            if not classes:
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="segmentation_builder_no_classes")
            dataset_root = Path(source_meta.get("dataset_root") or _resolve_sam3_or_qwen_dataset(request.source_dataset_id))
            labelmap_file = dataset_root / "labelmap.txt"
            if not labelmap_file.exists() and classes:
                # Backfill labelmap file if missing.
                try:
                    labelmap_file.write_text("\n".join(classes), encoding="utf-8")
                except Exception:
                    pass
            output_root = Path(planned_layout["dataset_root"]).resolve()
            train_out = output_root / "train"
            val_out = output_root / "val"
            (train_out / "images").mkdir(parents=True, exist_ok=True)
            (train_out / "labels").mkdir(parents=True, exist_ok=True)
            (val_out / "images").mkdir(parents=True, exist_ok=True)
            (val_out / "labels").mkdir(parents=True, exist_ok=True)
            # Copy/link labelmap.
            if labelmap_file.exists():
                shutil.copy2(labelmap_file, output_root / "labelmap.txt")
            splits = []
            image_exts = {".jpg", ".jpeg", ".png", ".bmp", ".webp", ".tif", ".tiff"}

            def _find_image_for_label(labels_dir: Path, images_dir: Path, label_file: Path) -> Optional[Tuple[Path, Path]]:
                stem = label_file.stem
                for ext in image_exts:
                    candidate = images_dir / f"{stem}{ext}"
                    if candidate.exists():
                        try:
                            rel = candidate.relative_to(images_dir)
                        except Exception:
                            rel = Path(candidate.name)
                        return candidate, rel
                for candidate in images_dir.rglob(f"{stem}.*"):
                    if candidate.suffix.lower() in image_exts:
                        try:
                            rel = candidate.relative_to(images_dir)
                        except Exception:
                            rel = Path(candidate.name)
                        return candidate, rel
                return None

            image_uid = 1
            for split in ("train", "val"):
                images_dir = dataset_root / split / "images"
                labels_dir = dataset_root / split / "labels"
                if not images_dir.exists() or not labels_dir.exists():
                    continue
                entries = []
                for label_file in sorted(labels_dir.rglob("*.txt")):
                    match = _find_image_for_label(labels_dir, images_dir, label_file)
                    if match is None:
                        continue
                    image_path, rel_path = match
                    boxes = []
                    try:
                        with label_file.open("r", encoding="utf-8") as handle:
                            lines = [ln.strip() for ln in handle if ln.strip()]
                    except Exception:
                        continue
                    for ln in lines:
                        parts = ln.split()
                        if len(parts) < 5:
                            continue
                        try:
                            cls_idx = int(float(parts[0]))
                            cx = float(parts[1])
                            cy = float(parts[2])
                            w = float(parts[3])
                            h = float(parts[4])
                        except (TypeError, ValueError):
                            continue
                        if classes and (cls_idx < 0 or cls_idx >= len(classes)):
                            continue
                        boxes.append({"class_idx": cls_idx, "bbox": (cx, cy, w, h)})
                    entries.append(
                        {
                            "label_file": label_file,
                            "image_path": image_path,
                            "rel_path": rel_path,
                            "boxes": boxes,
                            "split": split,
                            "image_id": image_uid,
                        }
                    )
                    image_uid += 1
                splits.append((split, entries))
            total_images = sum(len(e) for _, e in splits)
            if total_images == 0:
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="segmentation_builder_no_images")
            _seg_job_log(job, f"Queued {total_images} images for conversion using {request.sam_variant.upper()}")
            for split_name, entries in splits:
                _seg_job_log(job, f"{split_name}: {len(entries)} images")
            base_devices = _resolve_sam3_mining_devices() if request.sam_variant == "sam3" else _resolve_sam1_devices()
            expanded_devices: List[torch.device] = []
            per_dev = max(1, int(os.environ.get("SEG_BUILDER_WORKERS_PER_DEVICE", "1")))
            max_total_env = os.environ.get("SEG_BUILDER_MAX_WORKERS")
            max_total = None
            try:
                if max_total_env is not None:
                    max_total = max(1, int(max_total_env))
            except Exception:
                max_total = None
            for dev in base_devices:
                for _ in range(per_dev):
                    expanded_devices.append(dev)
            if max_total is not None:
                expanded_devices = expanded_devices[:max_total]
            if not expanded_devices:
                raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail="segmentation_builder_no_devices")
            mining_pool = None
            sam1_workers: List[_Sam1SegWorker] = []
            try:
                if request.sam_variant == "sam3":
                    mining_pool = _Sam3MiningPool(expanded_devices)
                else:
                    for dev in expanded_devices:
                        sam1_workers.append(_Sam1SegWorker(dev))
            except Exception as exc:  # noqa: BLE001
                raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=str(exc)) from exc
            processed = 0
            simplify_eps = float(request.simplify_epsilon)
            mask_threshold = float(request.mask_threshold)
            min_threshold = float(request.score_threshold)
            max_results = int(max(1, request.max_results))
            min_area = float(request.min_size)
            progress_lock = threading.Lock()

            def _link_or_copy(src: Path, dst: Path) -> None:
                dst.parent.mkdir(parents=True, exist_ok=True)
                if dst.exists():
                    return
                try:
                    os.link(src, dst)
                except Exception:
                    shutil.copy2(src, dst)

            def _process_entry(entry: Dict[str, Any], worker: Any) -> None:
                nonlocal processed
                if job.cancel_event.is_set():
                    return
                image_path: Path = entry["image_path"]
                rel_path: Path = entry["rel_path"]
                split: str = entry["split"]
                boxes: List[Dict[str, Any]] = entry.get("boxes") or []
                tasks: List[Dict[str, Any]] = []
                try:
                    with Image.open(image_path) as im:
                        pil_img = im.convert("RGB")
                        width, height = pil_img.size
                        for idx, box in enumerate(boxes):
                            cx, cy, bw, bh = box["bbox"]
                            x1, y1, x2, y2 = _yolo_to_xyxy(width, height, (cx, cy, bw, bh))
                            tasks.append(
                                {
                                    "id": f"{rel_path}:{idx}",
                                    "type": "visual",
                                    "bbox": [x1, y1, x2 - x1, y2 - y1],
                                    "class_idx": box["class_idx"],
                                    "fallback_poly": [(x1, y1), (x2, y1), (x2, y2), (x1, y2)],
                                }
                            )
                        if not tasks:
                            outputs = {}
                        else:
                            outputs = worker.process_image(
                                image_id=entry.get("image_id", 0),
                                pil_img=pil_img,
                                tasks=tasks,
                                min_threshold=min_threshold,
                                mask_threshold=mask_threshold,
                                max_results=max_results,
                                min_size=min_area,
                                simplify=simplify_eps,
                                return_masks=True,
                            ) or {}
                            label_lines = []
                            for task in tasks:
                                task_id = task["id"]
                                class_idx = task["class_idx"]
                                fallback = task["fallback_poly"]
                                dets = outputs.get(task_id) or []
                                best = None
                                if dets:
                                    best = max(dets, key=lambda d: d.get("score") or 0.0)
                                mask_arr = None
                                best_score = best.get("score") if best else None
                                if best:
                                    mask_arr = best.get("mask_array")
                                    if mask_arr is None and best.get("mask"):
                                        mask_arr = decode_binary_mask(best.get("mask"))
                                polygon = mask_to_polygon(mask_arr, simplify_eps) if mask_arr is not None else []
                                if best_score is None or best_score < min_threshold:
                                    polygon = []
                                if len(polygon) < 3:
                                    polygon = fallback
                                coords: List[float] = []
                                for x, y in polygon:
                                    coords.extend(
                                        [
                                            max(0.0, min(1.0, x / width)),
                                            max(0.0, min(1.0, y / height)),
                                        ]
                                    )
                                if len(coords) >= 6:
                                    label_lines.append(f"{class_idx} " + " ".join(f"{v:.6f}" for v in coords))
                        dest_labels = (train_out if split == "train" else val_out) / "labels" / f"{rel_path.stem}.txt"
                        dest_images = (train_out if split == "train" else val_out) / "images" / rel_path
                        dest_labels.parent.mkdir(parents=True, exist_ok=True)
                        dest_images.parent.mkdir(parents=True, exist_ok=True)
                        _link_or_copy(image_path, dest_images)
                        dest_labels.write_text("\n".join(label_lines), encoding="utf-8")
                finally:
                    with progress_lock:
                        processed += 1
                        progress_val = min(1.0, 0.05 + 0.9 * (processed / max(total_images, 1)))
                    _seg_job_update(
                        job,
                        progress=progress_val,
                        message=f"Processed {processed}/{total_images} images ({progress_val*100:.1f}%)",
                        log_message=False,
                    )

            # Dispatch over workers
            try:
                workers_list = mining_pool.workers if mining_pool is not None else sam1_workers
                if not workers_list:
                    raise RuntimeError("segmentation_builder_no_workers")
                with ThreadPoolExecutor(max_workers=max(1, len(workers_list))) as executor:
                    futures = []
                    task_idx = 0
                    for _, entries in splits:
                        for entry in entries:
                            worker = workers_list[task_idx % len(workers_list)]
                            futures.append(executor.submit(_process_entry, entry, worker))
                            task_idx += 1
                    for fut in as_completed(futures):
                        if job.cancel_event.is_set():
                            break
                        try:
                            fut.result()
                        except Exception as exc:  # noqa: BLE001
                            logger.warning("Segmentation build worker failed: %s", exc)
            finally:
                try:
                    if mining_pool is not None:
                        mining_pool.close()
                except Exception:
                    pass
                if sam1_workers:
                    for worker in sam1_workers:
                        try:
                            worker.close()
                        except Exception:
                            pass
            if job.cancel_event.is_set():
                _seg_job_update(job, status="cancelled", message="Cancelled", progress=job.progress)
                return
            _seg_job_log(job, "Converting output to COCO…")
            try:
                coco_meta = _convert_yolo_dataset_to_coco(output_root)
            except Exception as exc:  # noqa: BLE001
                _seg_job_update(job, status="failed", message="COCO conversion failed", error=str(exc))
                return
            result_meta = _load_sam3_dataset_metadata(output_root) or coco_meta or planned_meta
            _seg_job_update(
                job,
                status="completed",
                progress=1.0,
                message="Segmentation build complete.",
                result={
                    "planned_metadata": planned_meta,
                    "output_dataset_id": result_meta.get("id") if isinstance(result_meta, dict) else planned_meta.get("id"),
                    "output_root": str(output_root),
                    "classes": classes,
                    "train_count": len(next((e for s, e in splits if s == "train"), [])),
                    "val_count": len(next((e for s, e in splits if s == "val"), [])),
                },
            )
        except HTTPException as exc:
            _seg_job_update(job, status="failed", message=str(exc.detail), error=str(exc.detail))
        except Exception as exc:  # noqa: BLE001
            _seg_job_update(job, status="failed", message=str(exc), error=str(exc))

    threading.Thread(target=worker, daemon=True, name=f"seg-build-{job_id[:8]}").start()
    return job


def _safe_run_name(desired: Optional[str], fallback: str) -> str:
    name = desired or fallback
    return re.sub(r"[^A-Za-z0-9._-]", "_", name).strip("_") or fallback


def _latest_checkpoint_in_dir(checkpoint_dir: Path) -> Optional[str]:
    if not checkpoint_dir.exists():
        return None
    candidates = sorted(
        checkpoint_dir.glob("*.pt"),
        key=lambda path: path.stat().st_mtime,
        reverse=True,
    )
    if candidates:
        return str(candidates[0])
    return None


def _save_sam3_config(cfg: OmegaConf, job_id: str) -> Tuple[str, Path]:
    SAM3_GENERATED_CONFIG_DIR.mkdir(parents=True, exist_ok=True)
    config_file = SAM3_GENERATED_CONFIG_DIR / f"{job_id}.yaml"
    yaml_text = OmegaConf.to_yaml(cfg)
    config_file.write_text("# @package _global_\n" + yaml_text, encoding="utf-8")
    return f"configs/generated/{config_file.name}", config_file


def _start_sam3_training_worker(
    job: Sam3TrainingJob,
    cfg: OmegaConf,
    num_gpus: int,
    *,
    val_score_thresh: Optional[float] = None,
    val_max_dets: Optional[int] = None,
) -> None:
    def worker():
        proc: Optional[subprocess.Popen] = None
        tail_logs: deque[str] = deque(maxlen=50)
        max_epochs = max(1, int(getattr(cfg.trainer, "max_epochs", 1) or 1))
        # Attempt to track steps per epoch from the config if present
        steps_per_epoch = None
        try:
            steps_per_epoch = int(cfg.scratch.target_epoch_size) if getattr(cfg.scratch, "target_epoch_size", None) else None
        except Exception:
            steps_per_epoch = None
        try:
            _prepare_for_training()
            _sam3_job_update(job, status="running", progress=0.05, message="Preparing SAM3 training job ...")
            config_name, config_file = _save_sam3_config(cfg, job.job_id)
            script_path = SAM3_PACKAGE_ROOT / "train" / "train.py"
            cmd = [sys.executable, str(script_path), "-c", config_name, "--use-cluster", "0"]
            if num_gpus is not None:
                cmd.extend(["--num-gpus", str(num_gpus)])
            env = os.environ.copy()
            existing_py = env.get("PYTHONPATH", "")
            py_root = f"{SAM3_VENDOR_ROOT}:{SAM3_REPO_ROOT}"
            env["PYTHONPATH"] = f"{py_root}:{existing_py}" if existing_py else py_root
            env.setdefault("CUDA_LAUNCH_BLOCKING", "1")
            env.setdefault("TORCH_SHOW_CPP_STACKTRACES", "1")
            env.setdefault("NCCL_DEBUG", "INFO")
            # Enable runtime monkeypatches (loaded via sitecustomize.py) to keep vendor tree untouched.
            env.setdefault("SAM3_MONKEYPATCH", "1")
            if val_score_thresh is not None:
                try:
                    env["SAM3_VAL_SCORE_THRESH"] = str(float(val_score_thresh))
                except Exception:
                    pass
            if val_max_dets is not None:
                try:
                    env["SAM3_VAL_MAX_DETS"] = str(int(val_max_dets))
                except Exception:
                    pass
            proc = subprocess.Popen(
                cmd,
                cwd=str(SAM3_VENDOR_ROOT),
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                env=env,
            )
            job.process = proc
            _sam3_job_log(job, f"Spawned {' '.join(cmd)}")
            while True:
                if proc.stdout is None:
                    break
                line = proc.stdout.readline()
                if line == "" and proc.poll() is not None:
                    break
                if not line:
                    continue
                if job.cancel_event.is_set() and proc.poll() is None:
                    proc.terminate()
                    _sam3_job_update(job, status="cancelling", message="Cancellation requested ...")
                    continue
                cleaned = line.rstrip("\n")
                tail_logs.append(cleaned)
                _sam3_job_log(job, cleaned)
                if "sam3-balance" in cleaned.lower() or cleaned.startswith("[sam3-balance]"):
                    job.result = job.result or {}
                    job.result["balance_info"] = cleaned
                try:
                    match = re.search(r"Train Epoch:\s*\[(\d+)\]\[\s*(\d+)\s*/\s*(\d+)\]", cleaned)
                    val_match = re.search(r"Val Epoch:\s*\[(\d+)\]\[\s*(\d+)\s*/\s*(\d+)\]", cleaned)
                    if val_match:
                        val_epoch_idx = int(val_match.group(1))
                        val_step_idx = int(val_match.group(2))
                        val_total_steps = max(1, int(val_match.group(3)))
                        val_frac = max(0.0, min(1.0, val_step_idx / val_total_steps))
                        prog_val = max(job.progress or 0.0, min(0.99, 0.9 + 0.1 * val_frac))
                        _sam3_job_update(
                            job,
                            progress=prog_val,
                            message=f"Validation running ({val_step_idx}/{val_total_steps})",
                            log_message=False,
                        )
                        _sam3_job_append_metric(
                            job,
                            {
                                "phase": "val",
                                "val_step": val_step_idx,
                                "val_total": val_total_steps,
                                "epoch": val_epoch_idx + 1,
                                "total_epochs": max_epochs,
                            },
                        )
                    if match:
                        epoch_idx = int(match.group(1))
                        step_idx = int(match.group(2))
                        total_steps = max(1, int(match.group(3)))
                        # Prefer log-reported total steps; fall back to config target if present
                        steps_in_epoch = total_steps or steps_per_epoch or total_steps
                        frac_epoch = (step_idx / steps_in_epoch) if steps_in_epoch else 0.0
                        frac = (epoch_idx + frac_epoch) / max_epochs
                        prog_val = max(0.05, min(0.99, frac))
                        _sam3_job_update(job, progress=prog_val, log_message=False)
                    loss_match = re.search(
                        r"Losses\/train_all_loss:\s*(?:(?:last|batch)=)?([0-9.+-eE]+)(?:.*?(?:avg\d*=?\s*([0-9.+-eE]+)|\(\s*([0-9.+-eE]+)\s*\)))?",
                        cleaned,
                    )
                    if loss_match and match:
                        instant = float(loss_match.group(1))
                        avg_loss = None
                        if loss_match.group(2):
                            avg_loss = float(loss_match.group(2))
                        elif loss_match.group(3):
                            avg_loss = float(loss_match.group(3))
                        total_steps = max(1, int(match.group(3)))
                        steps_in_epoch = total_steps or steps_per_epoch or total_steps
                        global_step = epoch_idx * steps_in_epoch + step_idx
                        metric_payload = {
                            "phase": "train",
                            "train_loss_batch": instant,
                            "train_loss_avg10": avg_loss,
                            "batch": step_idx,
                            "batches_per_epoch": steps_in_epoch,
                            "epoch": epoch_idx + 1,
                            "total_epochs": max_epochs,
                            "step": global_step,
                            "timestamp": time.time(),
                        }
                        _sam3_job_append_metric(job, metric_payload)
                    if "Meters:" in cleaned and "coco_eval_bbox_AP" in cleaned:
                        try:
                            # Extract key/value pairs like '...': np.float64(0.123)
                            pairs = re.findall(r"'([^']+)':\s*np\.float64\(([0-9.eE+-]+)\)", cleaned)
                            meter_map = {k: float(v) for k, v in pairs}
                            epoch_meta = re.search(r"'Trainer/epoch':\s*([0-9]+)", cleaned)
                            epoch_val = int(epoch_meta.group(1)) + 1 if epoch_meta else None
                            val_payload: Dict[str, Any] = {
                                "phase": "val",
                                "timestamp": time.time(),
                            }
                            if epoch_val is not None:
                                val_payload["epoch"] = epoch_val
                            # Pick the first coco_eval_bbox_* metrics if present.
                            for key, field in [
                                ("coco_eval_bbox_AP", "coco_ap"),
                                ("coco_eval_bbox_AP_50", "coco_ap50"),
                                ("coco_eval_bbox_AP_75", "coco_ap75"),
                                ("coco_eval_bbox_AR_maxDets@10", "coco_ar10"),
                                ("coco_eval_bbox_AR_maxDets@100", "coco_ar100"),
                            ]:
                                for meter_key, meter_val in meter_map.items():
                                    if meter_key.endswith(key):
                                        val_payload[field] = meter_val
                                        break
                            _sam3_job_append_metric(job, val_payload)
                        except Exception:
                            pass
                except Exception:
                    pass
                _sam3_job_update(job, message=cleaned[-200:], log_message=False)
            retcode = proc.wait() if proc else 1
            if job.cancel_event.is_set():
                _sam3_job_update(job, status="cancelled", message="Training cancelled")
                return
            if retcode != 0:
                sig_note = ""
                if retcode < 0:
                    sig_num = -retcode
                    try:
                        sig_name = signal.Signals(sig_num).name
                    except Exception:
                        sig_name = f"SIG{sig_num}"
                    sig_desc = signal.strsignal(sig_num) or sig_name
                    sig_note = f" (signal {sig_num}: {sig_desc})"
                tail_text = "\n".join(tail_logs)
                _sam3_job_update(
                    job,
                    status="failed",
                    message=f"Training failed (exit {retcode}{sig_note})",
                    error=f"exit_code:{retcode}{sig_note}\nlast_logs:\n{tail_text}",
                )
                return
            log_dir = Path(cfg.paths.experiment_log_dir)
            checkpoint_dir = log_dir / "checkpoints"
            latest_ckpt = _latest_checkpoint_in_dir(checkpoint_dir)
            seg_head = bool(getattr(cfg.scratch, "enable_segmentation_head", getattr(cfg.scratch, "enable_segmentation", True)))
            load_seg = bool(getattr(cfg.scratch, "load_segmentation", seg_head))
            result_payload = {
                "experiment_log_dir": str(log_dir),
                "checkpoint": latest_ckpt,
                "config_path": str(config_file),
                "enable_segmentation": seg_head,
                "enable_segmentation_head": seg_head,
                "load_segmentation": load_seg,
            }
            _sam3_job_update(job, status="succeeded", message="Training complete", progress=1.0, result=result_payload)
        except Exception as exc:  # noqa: BLE001
            _sam3_job_update(job, status="failed", message="Training crashed", error=str(exc))
        finally:
            if proc and proc.poll() is None:
                try:
                    proc.terminate()
                except Exception:
                    pass
            _finalize_training_environment()

    thread = threading.Thread(target=worker, name=f"sam3-train-{job.job_id}", daemon=True)
    thread.start()


def _start_yolo_training_worker(job: YoloTrainingJob) -> None:
    def worker() -> None:
        run_dir = _yolo_run_dir(job.job_id, create=True)
        config = dict(job.config or {})
        dataset_info = config.get("dataset") or {}
        task = str(dataset_info.get("task") or config.get("task") or "detect").lower()
        if job.cancel_event.is_set():
            _yolo_job_update(job, status="cancelled", message="Cancelled before start", progress=0.0)
            _yolo_write_run_meta(run_dir, {"job_id": job.job_id, "status": job.status, "message": job.message, "config": job.config})
            return
        if not dataset_info.get("yolo_ready"):
            _yolo_job_update(job, status="failed", message="Dataset is not YOLO-ready", error="yolo_not_ready")
            _yolo_write_run_meta(run_dir, {"job_id": job.job_id, "status": job.status, "message": job.message, "config": job.config})
            return
        try:
            _prepare_for_training()
            from ultralytics import YOLO  # type: ignore
        except Exception as exc:  # noqa: BLE001
            _yolo_job_update(job, status="failed", message="Ultralytics not installed", error=str(exc))
            _yolo_write_run_meta(run_dir, {"job_id": job.job_id, "status": job.status, "message": job.message, "config": job.config})
            return
        _yolo_job_update(job, status="running", message="Starting YOLOv8 training", progress=0.0)
        _yolo_job_log(job, "Preparing dataset + data.yaml")
        dataset_root = Path(dataset_info.get("prepared_root") or dataset_info.get("dataset_root") or "")
        data_yaml = _yolo_write_data_yaml(run_dir, dataset_root, dataset_info.get("yolo_layout"), dataset_info.get("yolo_labelmap_path"))
        from_scratch = bool(config.get("from_scratch"))
        base_weights = config.get("base_weights")
        variant = config.get("variant") or ""
        if task == "segment" and _yolo_p2_scale(variant):
            _yolo_job_update(job, status="failed", message="P2 head is only supported for detection.", error="yolo_p2_segment")
            _yolo_write_run_meta(run_dir, {"job_id": job.job_id, "status": job.status, "message": job.message, "config": job.config})
            return
        _, model_source = _yolo_resolve_model_source(variant, task, from_scratch, base_weights)
        device_arg = _yolo_device_arg(config.get("devices"))
        train_kwargs = {
            "data": str(data_yaml),
            "task": task,
            "epochs": config.get("epochs"),
            "imgsz": config.get("img_size"),
            "batch": config.get("batch"),
            "workers": config.get("workers"),
            "seed": config.get("seed"),
            "device": device_arg,
            "project": str(run_dir),
            "name": "train",
            "exist_ok": True,
        }
        p2_scale = _yolo_p2_scale(variant)
        if p2_scale and model_source.endswith("yolov8-p2.yaml"):
            try:
                import ultralytics  # type: ignore
            except Exception as exc:  # noqa: BLE001
                _yolo_job_update(job, status="failed", message="Ultralytics not installed", error=str(exc))
                _yolo_write_run_meta(run_dir, {"job_id": job.job_id, "status": job.status, "message": job.message, "config": job.config})
                return
            base_cfg = Path(ultralytics.__file__).resolve().parent / "cfg" / "models" / "v8" / "yolov8-p2.yaml"
            cfg_payload = yaml.safe_load(base_cfg.read_text())
            cfg_payload["scale"] = p2_scale
            p2_cfg = run_dir / f"yolov8{p2_scale}-p2.yaml"
            p2_cfg.write_text(yaml.safe_dump(cfg_payload, sort_keys=False))
            model_source = str(p2_cfg)
            if base_weights:
                train_kwargs["pretrained"] = base_weights
            else:
                train_kwargs["pretrained"] = False
            _yolo_job_log(job, f"P2 variant: scale={p2_scale} (config={p2_cfg.name}, pretrained={train_kwargs.get('pretrained')})")
        _yolo_job_log(job, f"Model source: {model_source}")
        train_kwargs.update(_yolo_build_aug_args(config.get("augmentations")))
        train_kwargs = {k: v for k, v in train_kwargs.items() if v is not None}
        monitor_stop = threading.Event()
        monitor_thread = None
        try:
            model = YOLO(model_source)
            _yolo_job_log(job, "Training started")
            monitor_thread = threading.Thread(
                target=_yolo_monitor_training,
                args=(job, run_dir, int(config.get("epochs") or 0), monitor_stop),
                name=f"yolo-monitor-{job.job_id[:8]}",
                daemon=True,
            )
            monitor_thread.start()
            results = model.train(**train_kwargs)
            train_dir = run_dir / "train"
            best_path = train_dir / "weights" / "best.pt"
            if best_path.exists():
                shutil.copy2(best_path, run_dir / "best.pt")
            results_csv = train_dir / "results.csv"
            args_yaml = train_dir / "args.yaml"
            if results_csv.exists():
                shutil.copy2(results_csv, run_dir / "results.csv")
            if args_yaml.exists():
                shutil.copy2(args_yaml, run_dir / "args.yaml")
            metrics_series: List[Dict[str, Any]] = []
            series_path = run_dir / "metrics_series.json"
            if (run_dir / "results.csv").exists():
                metrics_series = _yolo_parse_results_csv(run_dir / "results.csv")
                if metrics_series:
                    try:
                        series_path.write_text(json.dumps(metrics_series, indent=2, sort_keys=True))
                        job.metrics = metrics_series
                    except Exception:
                        pass
            metrics_payload = {}
            try:
                metrics_payload = results.metrics if results else {}
            except Exception:
                metrics_payload = {}
            if metrics_payload:
                (run_dir / "metrics.json").write_text(json.dumps(metrics_payload, indent=2, sort_keys=True))
            _yolo_prune_run_dir(run_dir)
            result_payload = {
                "run_dir": str(run_dir),
                "best_path": str(run_dir / "best.pt") if (run_dir / "best.pt").exists() else None,
                "metrics_path": str(run_dir / "metrics.json") if (run_dir / "metrics.json").exists() else None,
                "metrics_series_path": str(series_path) if series_path.exists() else None,
            }
            _yolo_job_update(job, status="succeeded", message="Training complete", progress=1.0, result=result_payload)
        except Exception as exc:  # noqa: BLE001
            _yolo_job_update(job, status="failed", message="Training failed", error=str(exc))
        finally:
            monitor_stop.set()
            if monitor_thread:
                monitor_thread.join(timeout=2.0)
            _finalize_training_environment()
            _yolo_write_run_meta(
                run_dir,
                {"job_id": job.job_id, "status": job.status, "message": job.message, "config": job.config, "result": job.result},
            )

    thread = threading.Thread(target=worker, name=f"yolo-train-{job.job_id}", daemon=True)
    thread.start()


def _start_rfdetr_training_worker(job: RfDetrTrainingJob) -> None:
    def worker() -> None:
        run_dir = _rfdetr_run_dir(job.job_id, create=True)
        config = dict(job.config or {})
        dataset_info = config.get("dataset") or {}
        if job.cancel_event.is_set():
            _rfdetr_job_update(job, status="cancelled", message="Cancelled before start", progress=0.0)
            _rfdetr_write_run_meta(run_dir, {"job_id": job.job_id, "status": job.status, "message": job.message, "config": job.config})
            return
        try:
            _prepare_for_training()
            from rfdetr import (
                RFDETRBase,
                RFDETRLarge,
                RFDETRNano,
                RFDETRSmall,
                RFDETRMedium,
                RFDETRSegPreview,
            )
        except Exception as exc:  # noqa: BLE001
            _rfdetr_job_update(job, status="failed", message="RF-DETR not installed", error=str(exc))
            _rfdetr_write_run_meta(run_dir, {"job_id": job.job_id, "status": job.status, "message": job.message, "config": job.config})
            return
        try:
            task = str(dataset_info.get("task") or config.get("task") or "detect").lower()
            variant_info = _rfdetr_variant_info(task, config.get("variant"))
            variant_id = variant_info.get("id")
            variant_label = variant_info.get("label")
            model_cls_map = {
                "rfdetr-nano": RFDETRNano,
                "rfdetr-small": RFDETRSmall,
                "rfdetr-medium": RFDETRMedium,
                "rfdetr-base": RFDETRBase,
                "rfdetr-large": RFDETRLarge,
                "rfdetr-seg-preview": RFDETRSegPreview,
            }
            model_cls = model_cls_map.get(variant_id)
            if not model_cls:
                raise RuntimeError("rfdetr_variant_unknown")
            dataset_root = Path(dataset_info.get("dataset_root") or "")
            coco_train = dataset_info.get("coco_train_json")
            coco_val = dataset_info.get("coco_val_json") or coco_train
            if not coco_train or not coco_val:
                raise RuntimeError("rfdetr_coco_missing")
            labelmap = _rfdetr_load_labelmap(dataset_root, coco_train)
            if labelmap:
                (run_dir / "labelmap.txt").write_text("\n".join(labelmap) + "\n")
            model_kwargs: Dict[str, Any] = {}
            if config.get("resolution"):
                try:
                    model_kwargs["resolution"] = int(config.get("resolution"))
                except Exception:
                    pass
            model_kwargs["device"] = "cuda" if torch.cuda.is_available() else "cpu"
            if config.get("from_scratch"):
                model_kwargs["pretrain_weights"] = None
            elif config.get("pretrain_weights"):
                model_kwargs["pretrain_weights"] = config.get("pretrain_weights")
            if task == "segment":
                model_kwargs["segmentation_head"] = True
            _rfdetr_job_update(job, status="running", message=f"Starting RF-DETR training ({variant_label})", progress=0.0)
            _rfdetr_job_log(job, "Preparing dataset + COCO annotations")
            prepared_root = _rfdetr_prepare_dataset(dataset_root, run_dir, coco_train, coco_val)
            _rfdetr_job_log(job, f"RF-DETR dataset prepared at {prepared_root}")
            total_epochs = max(1, int(config.get("epochs") or 100))
            train_kwargs: Dict[str, Any] = {
                "dataset_dir": str(prepared_root),
                "dataset_file": "roboflow",
                "output_dir": str(run_dir),
                "epochs": total_epochs,
                "batch_size": config.get("batch"),
                "grad_accum_steps": config.get("grad_accum"),
                "num_workers": config.get("workers"),
                "seed": config.get("seed"),
                "use_ema": config.get("use_ema"),
                "early_stopping": config.get("early_stopping"),
                "early_stopping_patience": config.get("early_stopping_patience"),
                "run": config.get("run_name") or job.job_id[:8],
                "project": "rfdetr",
            }
            aug_policy = _rfdetr_normalize_aug_policy(config.get("augmentations"))
            multi_scale = config.get("multi_scale")
            if multi_scale is not None:
                train_kwargs["multi_scale"] = bool(multi_scale)
                train_kwargs["expanded_scales"] = bool(config.get("expanded_scales")) if multi_scale else False
            if task == "segment":
                train_kwargs["segmentation_head"] = True
            train_kwargs = {k: v for k, v in train_kwargs.items() if v is not None}
            device_ids = _normalize_device_list(config.get("devices"))
            if not device_ids and torch.cuda.is_available():
                device_ids = list(range(torch.cuda.device_count()))
            cuda_visible = ",".join(str(d) for d in device_ids) if device_ids else None
            prev_cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES")
            if cuda_visible:
                os.environ["CUDA_VISIBLE_DEVICES"] = cuda_visible
            train_kwargs["device"] = "cuda" if torch.cuda.is_available() else "cpu"
            use_distributed = torch.cuda.is_available() and len(device_ids) > 1
            _rfdetr_job_log(job, f"Model variant: {variant_id}")
            if use_distributed:
                dist_url = f"tcp://127.0.0.1:{_find_free_port()}"
                world_size = len(device_ids)
                _rfdetr_job_log(job, f"Multi-GPU enabled: devices={cuda_visible} world_size={world_size}")
                _rfdetr_job_log(job, f"Training started (epochs={total_epochs})")
                monitor_stop = threading.Event()
                monitor_thread = threading.Thread(
                    target=_rfdetr_monitor_training,
                    args=(job, run_dir, total_epochs, monitor_stop),
                    name=f"rfdetr-monitor-{job.job_id[:8]}",
                    daemon=True,
                )
                monitor_thread.start()
                prev_skip_clip = os.environ.get("TATOR_SKIP_CLIP_LOAD")
                os.environ["TATOR_SKIP_CLIP_LOAD"] = "1"
                import torch.multiprocessing as mp
                try:
                    mp.spawn(
                        _rfdetr_ddp_worker,
                        args=(world_size, variant_id, model_kwargs, train_kwargs, aug_policy, dist_url),
                        nprocs=world_size,
                        join=True,
                    )
                finally:
                    monitor_stop.set()
                    monitor_thread.join(timeout=2.0)
                    if prev_skip_clip is None:
                        os.environ.pop("TATOR_SKIP_CLIP_LOAD", None)
                    else:
                        os.environ["TATOR_SKIP_CLIP_LOAD"] = prev_skip_clip
            else:
                rf_detr = model_cls(**model_kwargs)
                restore = _rfdetr_install_augmentations(aug_policy)

                def on_fit_epoch_end(stats: Dict[str, Any]) -> None:
                    metric = _rfdetr_sanitize_metric(stats)
                    if metric:
                        _rfdetr_job_append_metric(job, metric)
                    epoch = metric.get("epoch") if metric else None
                    if epoch is not None:
                        try:
                            epoch_idx = int(epoch)
                            progress = max(0.0, min(0.99, epoch_idx / total_epochs))
                            _rfdetr_job_update(job, progress=progress, message=f"Epoch {epoch_idx}/{total_epochs}")
                        except Exception:
                            pass
                    if job.cancel_event.is_set():
                        try:
                            rf_detr.model.request_early_stop()
                        except Exception:
                            pass

                rf_detr.callbacks["on_fit_epoch_end"].append(on_fit_epoch_end)
                _rfdetr_job_log(job, f"Training started (epochs={total_epochs})")
                try:
                    rf_detr.train(**train_kwargs)
                finally:
                    _rfdetr_restore_augmentations(restore)
            if job.cancel_event.is_set():
                _rfdetr_job_update(job, status="cancelled", message="Training cancelled", progress=job.progress)
            else:
                _rfdetr_job_update(job, status="succeeded", message="Training complete", progress=1.0)
            metrics_series = job.metrics or []
            if not metrics_series:
                metrics_series = _rfdetr_parse_log_series(run_dir / "log.txt")
                if metrics_series:
                    job.metrics = metrics_series
            if metrics_series:
                try:
                    (run_dir / "metrics_series.json").write_text(json.dumps(metrics_series, indent=2, sort_keys=True))
                except Exception:
                    pass
            best_path = _rfdetr_best_checkpoint(run_dir)
            optimized_path = None
            if best_path:
                try:
                    export_kwargs = dict(model_kwargs)
                    export_kwargs["pretrain_weights"] = best_path
                    export_kwargs["device"] = "cuda" if torch.cuda.is_available() else "cpu"
                    export_model = model_cls(**export_kwargs)
                    export_model.optimize_for_inference()
                    optimized_path = run_dir / "checkpoint_best_optimized.pt"
                    torch.jit.save(export_model.model.inference_model, str(optimized_path))
                    _rfdetr_job_log(job, f"Optimized export saved: {optimized_path.name}")
                except Exception as exc:  # noqa: BLE001
                    _rfdetr_job_log(job, f"Optimized export failed: {exc}")
            result_payload = {
                "run_dir": str(run_dir),
                "best_path": best_path,
                "optimized_path": str(optimized_path) if optimized_path else None,
                "results_path": str(run_dir / "results.json") if (run_dir / "results.json").exists() else None,
                "metrics_series_path": str(run_dir / "metrics_series.json") if (run_dir / "metrics_series.json").exists() else None,
                "log_path": str(run_dir / "log.txt") if (run_dir / "log.txt").exists() else None,
            }
            _rfdetr_prune_run_dir(run_dir)
            job.result = result_payload
        except Exception as exc:  # noqa: BLE001
            _rfdetr_job_update(job, status="failed", message="Training failed", error=str(exc))
        finally:
            if "prev_cuda_visible" in locals():
                if prev_cuda_visible is None:
                    os.environ.pop("CUDA_VISIBLE_DEVICES", None)
                else:
                    os.environ["CUDA_VISIBLE_DEVICES"] = prev_cuda_visible
            _finalize_training_environment()
            _rfdetr_write_run_meta(
                run_dir,
                {
                    "job_id": job.job_id,
                    "status": job.status,
                    "message": job.message,
                    "config": job.config,
                    "result": job.result,
                    "created_at": job.created_at,
                    "updated_at": job.updated_at,
                },
            )

    thread = threading.Thread(target=worker, name=f"rfdetr-train-{job.job_id}", daemon=True)
    thread.start()


def _get_sam3_job(job_id: str) -> Sam3TrainingJob:
    with SAM3_TRAINING_JOBS_LOCK:
        job = SAM3_TRAINING_JOBS.get(job_id)
        if not job:
            raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="sam3_job_not_found")
        return job


def _get_yolo_job(job_id: str) -> YoloTrainingJob:
    with YOLO_TRAINING_JOBS_LOCK:
        job = YOLO_TRAINING_JOBS.get(job_id)
        if not job:
            raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="yolo_job_not_found")
        return job


def _get_rfdetr_job(job_id: str) -> RfDetrTrainingJob:
    with RFDETR_TRAINING_JOBS_LOCK:
        job = RFDETR_TRAINING_JOBS.get(job_id)
        if not job:
            raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="rfdetr_job_not_found")
        return job


async def _save_upload_file(
    upload: UploadFile,
    root: Path,
    *,
    max_bytes: Optional[int] = None,
    quota_root: Optional[Path] = None,
    quota_limit: Optional[int] = None,
) -> Path:
    rel_path = _normalise_relative_path(upload.filename)
    dest = (root / rel_path).resolve()
    if not str(dest).startswith(str(root.resolve())):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="invalid_relative_path")
    if dest.exists():
        raise HTTPException(status_code=HTTP_409_CONFLICT, detail="upload_exists")
    await _write_upload_file(
        upload,
        dest,
        max_bytes=max_bytes,
        quota_root=quota_root or root,
        quota_limit=quota_limit,
    )
    return dest


def _validate_upload_size(upload: UploadFile, *, max_bytes: int = BASE64_IMAGE_MAX_BYTES) -> None:
    if not max_bytes:
        return
    try:
        size = upload.size  # Starlette UploadFile may have size attr
    except Exception:
        size = None
    if size is not None and size > max_bytes:
        raise HTTPException(status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="upload_too_large")


def _validate_upload_extension(filename: str, allowed_exts: set[str], detail: str) -> None:
    suffix = Path(filename).suffix.lower()
    if allowed_exts and suffix not in allowed_exts:
        raise HTTPException(status_code=HTTP_415_UNSUPPORTED_MEDIA_TYPE, detail=detail)


async def _save_asset(
    upload: UploadFile,
    *,
    subdir: str,
    allowed_exts: Optional[set[str]] = None,
    max_bytes: Optional[int] = None,
    quota_bytes: Optional[int] = None,
) -> str:
    dest_dir = UPLOAD_ROOT / subdir
    dest_dir.mkdir(parents=True, exist_ok=True)
    rel_name = Path(upload.filename or f"asset_{uuid.uuid4().hex}").name
    dest_path = dest_dir / rel_name
    if allowed_exts:
        _validate_upload_extension(rel_name, allowed_exts, "upload_extension_not_allowed")
    if dest_path.exists():
        raise HTTPException(status_code=HTTP_409_CONFLICT, detail="upload_exists")
    _validate_upload_size(upload, max_bytes=max_bytes or ASSET_MAX_BYTES)
    await _write_upload_file(
        upload,
        dest_path,
        max_bytes=max_bytes or ASSET_MAX_BYTES,
        quota_root=dest_dir,
        quota_limit=quota_bytes or ASSET_UPLOAD_QUOTA_BYTES,
    )
    return str(dest_path.resolve())


async def _write_upload_file(
    upload: UploadFile,
    dest: Path,
    *,
    max_bytes: Optional[int] = None,
    quota_root: Optional[Path] = None,
    quota_limit: Optional[int] = None,
    allow_overwrite: bool = False,
) -> None:
    dest.parent.mkdir(parents=True, exist_ok=True)
    if dest.exists() and not allow_overwrite:
        raise HTTPException(status_code=HTTP_409_CONFLICT, detail="upload_exists")
    written = 0
    existing = _dir_size_bytes(quota_root) if quota_root and quota_limit else 0
    with dest.open("wb") as handle:
        while True:
            chunk = await upload.read(1024 * 1024)
            if not chunk:
                break
            written += len(chunk)
            if max_bytes and written > max_bytes:
                handle.close()
                try:
                    dest.unlink()
                except Exception:
                    pass
                raise HTTPException(status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="upload_too_large")
            if quota_root and quota_limit and existing + written > quota_limit:
                handle.close()
                try:
                    dest.unlink()
                except Exception:
                    pass
                raise HTTPException(status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="upload_quota_exceeded")
            handle.write(chunk)
    await upload.close()


def _artifacts_to_payload(artifacts: TrainingArtifacts) -> Dict[str, Any]:
    data = asdict(artifacts)
    return data


def _cleanup_job(job: ClipTrainingJob) -> None:
    if job.temp_dir and os.path.isdir(job.temp_dir):
        shutil.rmtree(job.temp_dir, ignore_errors=True)


def _publish_clip_training_artifacts(artifacts: TrainingArtifacts) -> TrainingArtifacts:
    classifiers_root = (UPLOAD_ROOT / "classifiers").resolve()
    labelmaps_root = (UPLOAD_ROOT / "labelmaps").resolve()
    classifiers_root.mkdir(parents=True, exist_ok=True)
    labelmaps_root.mkdir(parents=True, exist_ok=True)

    model_src = Path(artifacts.model_path).resolve()
    labelmap_src = Path(artifacts.labelmap_path).resolve()
    meta_src = Path(artifacts.meta_path).resolve()

    model_dst = classifiers_root / model_src.name
    labelmap_dst = labelmaps_root / labelmap_src.name
    meta_dst = classifiers_root / meta_src.name

    try:
        if model_src.exists():
            _link_or_copy_file(model_src, model_dst, overwrite=True)
            artifacts.model_path = str(model_dst)
    except Exception as exc:
        logger.warning("Failed to publish CLIP classifier %s: %s", model_src, exc)
    try:
        if meta_src.exists():
            _link_or_copy_file(meta_src, meta_dst, overwrite=True)
            artifacts.meta_path = str(meta_dst)
    except Exception as exc:
        logger.warning("Failed to publish CLIP meta %s: %s", meta_src, exc)
    try:
        if labelmap_src.exists():
            _link_or_copy_file(labelmap_src, labelmap_dst, overwrite=True)
            artifacts.labelmap_path = str(labelmap_dst)
    except Exception as exc:
        logger.warning("Failed to publish CLIP labelmap %s: %s", labelmap_src, exc)

    return artifacts


def _load_labelmap_file(path: Optional[Union[str, Path]], *, strict: bool = False) -> List[str]:
    if path is None:
        return []
    if isinstance(path, Path):
        path_str = str(path)
        lower = path.name.lower()
    else:
        path_str = str(path)
        lower = Path(path_str).name.lower()
    if not path_str.strip():
        return []
    try:
        if lower.endswith(".pkl"):
            data = joblib.load(path_str)
            if isinstance(data, list):
                return [str(item) for item in data]
            raise ValueError("labelmap_pickle_invalid")
        entries: List[str] = []
        with open(path_str, "r", encoding="utf-8") as handle:
            for line in handle:
                stripped = line.strip()
                if stripped:
                    entries.append(stripped)
        if not entries:
            raise ValueError("labelmap_empty")
        return entries
    except FileNotFoundError as exc:
        if strict:
            raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="labelmap_not_found") from exc
        return []
    except Exception as exc:  # noqa: BLE001
        if strict:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"labelmap_load_failed:{exc}") from exc
        logger.warning("Failed to load labelmap from %s: %s", path_str, exc)
        return []


def _current_active_payload() -> Dict[str, Any]:
    encoder_ready = _active_encoder_ready()
    encoder_error = clip_last_error
    return {
        "clip_model": clip_model_name,
        "classifier_path": active_classifier_path,
        "labelmap_path": active_labelmap_path,
        "clip_ready": encoder_ready,
        "clip_error": clip_last_error,
        "labelmap_entries": list(active_label_list),
        "encoder_type": active_encoder_type,
        "encoder_model": active_encoder_model,
        "encoder_ready": encoder_ready,
        "encoder_error": encoder_error,
        "logit_adjustment_inference": (
            bool(active_classifier_head.get("logit_adjustment_inference"))
            if isinstance(active_classifier_head, dict)
            else None
        ),
    }


def _active_encoder_ready() -> bool:
    if clf is None:
        return False
    if str(active_encoder_type or "").strip().lower() == "dinov3":
        return bool(dinov3_initialized and dinov3_model is not None and dinov3_processor is not None)
    return bool(clip_initialized and clip_model is not None and clip_preprocess is not None)


def _resolve_head_normalize_embeddings(head: Optional[Dict[str, Any]], *, default: bool = True) -> bool:
    if not head:
        return default
    raw = head.get("normalize_embeddings")
    if raw is None:
        raw = head.get("mlp_normalize_embeddings")
    if raw is None:
        return default
    if isinstance(raw, str):
        return raw.strip().lower() in {"1", "true", "yes", "on"}
    return bool(raw)


def _resolve_active_head_normalize_embeddings(
    meta_obj: Optional[Dict[str, Any]],
    clf_obj: Optional[object],
    *,
    default: bool = True,
) -> bool:
    if isinstance(clf_obj, dict):
        head_type = str(clf_obj.get("classifier_type") or clf_obj.get("head_type") or "").lower()
        if head_type == "mlp":
            return _resolve_head_normalize_embeddings(clf_obj, default=default)
    if isinstance(meta_obj, dict) and str(meta_obj.get("classifier_type") or "").lower() == "mlp":
        raw = meta_obj.get("mlp_normalize_embeddings")
        if raw is not None:
            return bool(raw)
    return default

def mask_to_bounding_box(mask: np.ndarray) -> tuple[int,int,int,int]:
    rows = np.any(mask, axis=1)
    cols = np.any(mask, axis=0)
    if not np.any(rows) or not np.any(cols):
        return (0,0,0,0)
    y_min,y_max = np.where(rows)[0][[0,-1]]
    x_min,x_max = np.where(cols)[0][[0,-1]]
    return (int(x_min), int(y_min), int(x_max), int(y_max))


def encode_binary_mask(mask: np.ndarray) -> Optional[Dict[str, Any]]:
    try:
        mask_arr = np.asarray(mask)
    except Exception:
        return None
    if mask_arr.ndim == 3 and mask_arr.shape[0] == 1:
        mask_arr = mask_arr[0]
    if mask_arr.ndim == 3 and mask_arr.shape[-1] == 1:
        mask_arr = mask_arr[..., 0]
    if mask_arr.ndim != 2:
        return None
    mask_bool = mask_arr.astype(bool)
    height, width = mask_bool.shape
    packed = np.packbits(mask_bool.astype(np.uint8), axis=None)
    try:
        packed_bytes = packed.tobytes()
    except Exception:
        return None
    if MASK_ENCODE_MAX_BYTES > 0 and len(packed_bytes) > MASK_ENCODE_MAX_BYTES:
        return None
    try:
        encoded = base64.b64encode(packed_bytes).decode("ascii")
    except Exception:
        return None
    return {"size": [int(height), int(width)], "counts": encoded}


def _prune_detections_for_response(dets: List[Any], warnings: Optional[List[str]] = None) -> List[Any]:
    """Clamp response payload size by limiting detection count and mask payloads."""
    if not dets:
        return dets
    limited: List[Any] = list(dets[: MAX_RESPONSE_DETECTIONS]) if MAX_RESPONSE_DETECTIONS > 0 else list(dets)
    if warnings is not None and MAX_RESPONSE_DETECTIONS > 0 and len(dets) > MAX_RESPONSE_DETECTIONS:
        warnings.append("detections_pruned")
    mask_budget = MAX_RESPONSE_MASKS if MAX_RESPONSE_MASKS > 0 else None
    masks_used = 0
    for det in limited:
        mask_val = getattr(det, "mask", None)
        if mask_val is not None and mask_budget is not None:
            if masks_used >= mask_budget:
                try:
                    det.mask = None  # type: ignore[attr-defined]
                except Exception:
                    pass
                else:
                    if warnings is not None:
                        warnings.append("masks_pruned")
            else:
                masks_used += 1
    return limited


def decode_binary_mask(payload: Dict[str, Any]) -> Optional[np.ndarray]:
    if not payload:
        return None
    counts = payload.get("counts")
    size = payload.get("size") or []
    if not counts or len(size) != 2:
        return None
    try:
        packed = np.frombuffer(base64.b64decode(counts), dtype=np.uint8)
        bits = np.unpackbits(packed)[: int(size[0]) * int(size[1])]
        return bits.reshape(int(size[0]), int(size[1]))
    except Exception:
        return None


def _rdp(points: np.ndarray, epsilon: float) -> np.ndarray:
    """Ramer–Douglas–Peucker simplification for 2D points."""
    if points.shape[0] < 3 or epsilon <= 0:
        return points

    def _perp_dist(pt, start, end):
        if np.allclose(start, end):
            return np.linalg.norm(pt - start)
        return np.abs(np.cross(end - start, start - pt)) / np.linalg.norm(end - start)

    start_pt = points[0]
    end_pt = points[-1]
    dmax = 0.0
    idx = 0
    for i in range(1, len(points) - 1):
        d = _perp_dist(points[i], start_pt, end_pt)
        if d > dmax:
            idx = i
            dmax = d
    if dmax > epsilon:
        rec1 = _rdp(points[: idx + 1], epsilon)
        rec2 = _rdp(points[idx:], epsilon)
        return np.concatenate((rec1[:-1], rec2), axis=0)
    return np.array([start_pt, end_pt])


def mask_to_polygon(mask: np.ndarray, simplify_epsilon: float) -> List[Tuple[float, float]]:
    """Extract a coarse polygon outline from a binary mask."""
    try:
        mask_arr = np.asarray(mask).astype(bool)
    except Exception:
        return []
    if mask_arr.ndim != 2 or not mask_arr.any():
        return []
    coords = np.argwhere(mask_arr)  # y, x
    if coords.shape[0] < 3:
        return []
    points = np.stack([coords[:, 1], coords[:, 0]], axis=1)  # x, y
    hull_pts = points
    if ConvexHull is not None:
        try:
            hull = ConvexHull(points)
            hull_pts = points[hull.vertices]
        except Exception:
            hull_pts = points
    if simplify_epsilon and simplify_epsilon > 0:
        hull_pts = _rdp(hull_pts, simplify_epsilon)
    # Ensure at least 3 points.
    if hull_pts.shape[0] < 3:
        # Fallback to simple bounding box.
        xs, ys = points[:, 0], points[:, 1]
        x1, x2 = xs.min(), xs.max()
        y1, y2 = ys.min(), ys.max()
        hull_pts = np.array([[x1, y1], [x2, y1], [x2, y2], [x1, y2]])
    return [(float(x), float(y)) for x, y in hull_pts]

def to_yolo(w: int, h: int, left: int, top: int, right: int, bottom: int) -> List[float]:
    w_abs = float(right - left)
    h_abs = float(bottom - top)
    cx_abs = left + w_abs/2
    cy_abs = top + h_abs/2
    cx = cx_abs / w
    cy = cy_abs / h
    ww = w_abs / w
    hh = h_abs / h
    return [cx, cy, ww, hh]


def yolo_to_corners(box: List[float], w: int, h: int) -> Tuple[int, int, int, int]:
    if len(box) < 4:
        return (0, 0, 0, 0)
    cx, cy, ww, hh = box[:4]
    w_abs = max(0.0, float(ww) * w)
    h_abs = max(0.0, float(hh) * h)
    cx_abs = float(cx) * w
    cy_abs = float(cy) * h
    left = int(round(cx_abs - w_abs / 2))
    top = int(round(cy_abs - h_abs / 2))
    right = int(round(cx_abs + w_abs / 2))
    bottom = int(round(cy_abs + h_abs / 2))
    left = max(0, min(w, left))
    top = max(0, min(h, top))
    right = max(left, min(w, right))
    bottom = max(top, min(h, bottom))
    return left, top, right, bottom

@app.post("/predict_base64", response_model=PredictResponse)
def predict_base64(payload: Base64Payload):
    # If CLIP/logreg not loaded, return error message in "prediction"
    if not _active_encoder_ready():
        return PredictResponse(prediction=str(ERROR_MESSAGE), uuid=None, error="clip_unavailable") # messy ... returning the error message int as str. Crap logic needs cleanup

    pil_img, _np_img, _token = _resolve_detector_image(payload.image_base64, payload.image_token)
    feats_np = _encode_pil_batch_for_active([pil_img])
    if feats_np is None or not isinstance(feats_np, np.ndarray) or feats_np.size == 0:
        return PredictResponse(prediction=str(ERROR_MESSAGE), uuid=None, error="clip_unavailable")
    bg_guard = bool(payload.background_guard) if payload.background_guard is not None else False
    details = _clip_auto_predict_details(feats_np, background_guard=bg_guard)
    return PredictResponse(
        prediction=str(details.get("label") or "unknown"),
        proba=details.get("proba"),
        second_label=details.get("second_label"),
        second_proba=details.get("second_proba"),
        margin=details.get("margin"),
        error=details.get("error"),
        uuid=payload.uuid,
    )


@app.get("/clip/backbones")
def list_clip_backbones():
    return {
        "available": SUPPORTED_CLIP_MODELS,
        "active": clip_model_name,
    }


def _list_clip_classifiers() -> List[Dict[str, Any]]:
    """List classifier heads available for CLIP filtering (typically trained via the CLIP training tab)."""
    root = (UPLOAD_ROOT / "classifiers").resolve()
    labelmaps_root = (UPLOAD_ROOT / "labelmaps").resolve()
    classifiers: List[Dict[str, Any]] = []
    if not root.exists():
        return classifiers

    for path in sorted(root.rglob("*")):
        if not path.is_file():
            continue
        if path.suffix.lower() not in CLASSIFIER_ALLOWED_EXTS:
            continue
        if path.name.endswith(".meta.pkl"):
            continue
        if not _path_is_within_root(path.resolve(), root):
            continue

        entry: Dict[str, Any] = {
            "filename": path.name,
            "path": str(path.resolve()),
            "rel_path": str(path.relative_to(root)),
        }

        meta_path = os.path.splitext(str(path))[0] + ".meta.pkl"
        if os.path.exists(meta_path):
            try:
                meta_obj = joblib.load(meta_path)
                if isinstance(meta_obj, dict):
                    entry["clip_model"] = meta_obj.get("clip_model")
                    entry["encoder_type"] = meta_obj.get("encoder_type") or "clip"
                    entry["encoder_model"] = meta_obj.get("encoder_model") or entry.get("clip_model")
                    entry["solver"] = meta_obj.get("solver")
                    entry["classifier_type"] = meta_obj.get("classifier_type")
                    entry["embedding_dim"] = meta_obj.get("embedding_dim")
                    entry["n_samples_train"] = meta_obj.get("n_samples_train")
                    entry["n_samples_test"] = meta_obj.get("n_samples_test")
                    labelmap_hint = meta_obj.get("labelmap_filename") or meta_obj.get("labelmap_path")
                    if labelmap_hint:
                        resolved = _resolve_clip_labelmap_path(labelmap_hint, root_hint="labelmaps")
                        if resolved is not None:
                            entry["labelmap_guess"] = str(resolved)
                            entry["labelmap_guess_rel"] = str(resolved.relative_to(labelmaps_root))
            except Exception:
                pass

        try:
            clf_obj = joblib.load(str(path))
            classes_raw = getattr(clf_obj, "classes_", None)
            if classes_raw is not None:
                entry["classes"] = [str(c) for c in list(classes_raw)]
                entry["n_classes"] = len(entry["classes"])
            elif isinstance(clf_obj, dict) and clf_obj.get("classes"):
                entry["classes"] = [str(c) for c in list(clf_obj.get("classes") or [])]
                entry["n_classes"] = len(entry["classes"])
                entry["classifier_type"] = clf_obj.get("classifier_type") or clf_obj.get("head_type")
            coef = getattr(clf_obj, "coef_", None)
            if coef is not None and hasattr(coef, "shape") and len(getattr(coef, "shape", [])) >= 2:
                entry["embedding_dim"] = int(coef.shape[1])
        except Exception as exc:  # noqa: BLE001
            entry["load_error"] = str(exc)
        if "encoder_type" not in entry:
            entry["encoder_type"] = "clip"
        if "encoder_model" not in entry:
            entry["encoder_model"] = entry.get("clip_model")

        try:
            entry["modified_at"] = path.stat().st_mtime
        except Exception:
            entry["modified_at"] = None
        try:
            stem = path.stem
            for ext in LABELMAP_ALLOWED_EXTS:
                candidate = (labelmaps_root / f"{stem}{ext}").resolve()
                if candidate.exists() and _path_is_within_root(candidate, labelmaps_root):
                    entry["labelmap_guess"] = str(candidate)
                    entry["labelmap_guess_rel"] = str(candidate.relative_to(labelmaps_root))
                    break
        except Exception:
            pass
        classifiers.append(entry)

    classifiers.sort(key=lambda c: (c.get("modified_at") or 0), reverse=True)
    return classifiers


@app.get("/clip/classifiers")
def list_clip_classifiers():
    return _list_clip_classifiers()


def _resolve_clip_labelmap_path(path_str: Optional[str], *, root_hint: Optional[str] = None) -> Optional[Path]:
    if not path_str:
        return None
    raw = str(path_str).strip()
    if not raw:
        return None
    roots: List[Path] = []
    labelmaps_root = (UPLOAD_ROOT / "labelmaps").resolve()
    classifiers_root = (UPLOAD_ROOT / "classifiers").resolve()
    if root_hint == "classifiers":
        roots = [classifiers_root]
    elif root_hint == "labelmaps":
        roots = [labelmaps_root]
    else:
        roots = [labelmaps_root, classifiers_root]
    for root in roots:
        try:
            candidate = (root / raw).resolve()
        except Exception:
            continue
        if not _path_is_within_root(candidate, root):
            continue
        if candidate.suffix.lower() not in LABELMAP_ALLOWED_EXTS:
            continue
        if candidate.exists() and candidate.is_file():
            return candidate
    return None


def _find_labelmap_for_classifier(classifier_path: Path) -> Optional[Path]:
    meta_path = Path(os.path.splitext(str(classifier_path))[0] + ".meta.pkl")
    if meta_path.exists():
        try:
            meta_obj = joblib.load(meta_path)
            if isinstance(meta_obj, dict):
                labelmap_hint = meta_obj.get("labelmap_filename") or meta_obj.get("labelmap_path")
                resolved = _resolve_clip_labelmap_path(labelmap_hint, root_hint="labelmaps")
                if resolved is not None:
                    return resolved
        except Exception:
            pass
    stem = classifier_path.stem
    roots = [
        (UPLOAD_ROOT / "labelmaps").resolve(),
        (UPLOAD_ROOT / "classifiers").resolve(),
    ]
    for root in roots:
        if not root.exists():
            continue
        for ext in LABELMAP_ALLOWED_EXTS:
            candidate = (root / f"{stem}{ext}").resolve()
            if candidate.exists() and candidate.is_file() and _path_is_within_root(candidate, root):
                return candidate
    return None


def _list_clip_labelmaps() -> List[Dict[str, Any]]:
    labelmaps_root = (UPLOAD_ROOT / "labelmaps").resolve()
    entries: List[Dict[str, Any]] = []
    root = labelmaps_root
    if not root.exists():
        return entries
    for path in sorted(root.rglob("*")):
        if not path.is_file():
            continue
        if path.suffix.lower() not in LABELMAP_ALLOWED_EXTS:
            continue
        if path.name.endswith(".meta.pkl"):
            continue
        try:
            stat = path.stat()
        except OSError:
            continue
        try:
            classes = _load_labelmap_file(path)
        except Exception:
            classes = []
        if not classes:
            continue
        entries.append(
            {
                "filename": path.name,
                "path": str(path.resolve()),
                "rel_path": str(path.relative_to(root)),
                "root": "labelmaps",
                "n_classes": len(classes),
                "modified_at": stat.st_mtime,
            }
        )
    entries.sort(key=lambda item: (item.get("modified_at") or 0), reverse=True)
    return entries


@app.get("/clip/labelmaps")
def list_clip_labelmaps():
    return _list_clip_labelmaps()


@app.get("/clip/classifiers/download")
def download_clip_classifier(rel_path: str = Query(...)):
    classifier_path = _resolve_agent_clip_classifier_path(rel_path)
    if classifier_path is None:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="classifier_not_found")
    stream = classifier_path.open("rb")
    headers = {"Content-Disposition": f'attachment; filename="{classifier_path.name}"'}
    return StreamingResponse(stream, media_type="application/octet-stream", headers=headers)


@app.get("/clip/classifiers/download_zip")
def download_clip_classifier_zip(rel_path: str = Query(...)):
    classifier_path = _resolve_agent_clip_classifier_path(rel_path)
    if classifier_path is None:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="classifier_not_found")
    buffer = io.BytesIO()
    meta_path = Path(os.path.splitext(str(classifier_path))[0] + ".meta.pkl")
    labelmap_path = _find_labelmap_for_classifier(classifier_path)
    with zipfile.ZipFile(buffer, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        zf.write(classifier_path, arcname=classifier_path.name)
        if meta_path.exists():
            zf.write(meta_path, arcname=meta_path.name)
        if labelmap_path is not None:
            zf.write(labelmap_path, arcname=labelmap_path.name)
    buffer.seek(0)
    filename = f"{classifier_path.stem}_clip_head.zip"
    headers = {"Content-Disposition": f'attachment; filename="{filename}"'}
    return StreamingResponse(buffer, media_type="application/zip", headers=headers)


@app.delete("/clip/classifiers")
def delete_clip_classifier(rel_path: str = Query(...)):
    classifier_path = _resolve_agent_clip_classifier_path(rel_path)
    if classifier_path is None:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="classifier_not_found")
    try:
        classifier_path.unlink()
    except FileNotFoundError:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="classifier_not_found")
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=str(exc))
    meta_path = Path(os.path.splitext(str(classifier_path))[0] + ".meta.pkl")
    try:
        if meta_path.exists():
            meta_path.unlink()
    except Exception:
        pass
    return {"status": "deleted", "rel_path": rel_path}


@app.post("/clip/classifiers/rename")
def rename_clip_classifier(
    rel_path: str = Form(...),
    new_name: str = Form(...),
):
    classifier_path = _resolve_agent_clip_classifier_path(rel_path)
    if classifier_path is None:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="classifier_not_found")
    raw = str(new_name or "").strip()
    if not raw:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="classifier_new_name_required")
    # Strip any directory components; only allow file names.
    raw_name = Path(raw).name
    if not raw_name:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="classifier_new_name_invalid")
    current_suffix = classifier_path.suffix
    target_name = raw_name
    if not Path(target_name).suffix:
        target_name = f"{target_name}{current_suffix}"
    target_suffix = Path(target_name).suffix.lower()
    if target_suffix not in CLASSIFIER_ALLOWED_EXTS:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="classifier_extension_not_allowed")

    classifiers_root = (UPLOAD_ROOT / "classifiers").resolve()
    parent = classifier_path.parent.resolve()
    if not _path_is_within_root(parent, classifiers_root):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="classifier_path_invalid")
    target_path = (parent / target_name).resolve()
    if not _path_is_within_root(target_path, classifiers_root):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="classifier_path_invalid")

    if target_path == classifier_path:
        return {"status": "unchanged", "rel_path": str(classifier_path.relative_to(classifiers_root))}

    if target_path.exists():
        stem = target_path.stem
        suffix = target_path.suffix
        for idx in range(1, 1000):
            candidate = (parent / f"{stem}_{idx}{suffix}").resolve()
            if not candidate.exists():
                target_path = candidate
                break
        else:
            raise HTTPException(status_code=HTTP_409_CONFLICT, detail="classifier_rename_conflict")

    try:
        classifier_path.rename(target_path)
    except FileNotFoundError:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="classifier_not_found")
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=str(exc))

    old_meta = Path(os.path.splitext(str(classifier_path))[0] + ".meta.pkl")
    new_meta = Path(os.path.splitext(str(target_path))[0] + ".meta.pkl")
    try:
        if old_meta.exists():
            if new_meta.exists():
                try:
                    new_meta.unlink()
                except Exception:
                    pass
            old_meta.replace(new_meta)
    except Exception:
        pass

    try:
        global active_classifier_path
        if active_classifier_path and Path(active_classifier_path).resolve() == classifier_path.resolve():
            active_classifier_path = str(target_path)
    except Exception:
        pass

    return {
        "status": "renamed",
        "old_rel_path": str(classifier_path.relative_to(classifiers_root)),
        "new_rel_path": str(target_path.relative_to(classifiers_root)),
        "old_path": str(classifier_path),
        "new_path": str(target_path),
        "new_name": target_path.name,
    }


@app.get("/clip/labelmaps/download")
def download_clip_labelmap(rel_path: str = Query(...), root: Optional[str] = Query(None)):
    labelmap_path = _resolve_clip_labelmap_path(rel_path, root_hint=root)
    if labelmap_path is None:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="labelmap_not_found")
    stream = labelmap_path.open("rb")
    headers = {"Content-Disposition": f'attachment; filename="{labelmap_path.name}"'}
    return StreamingResponse(stream, media_type="application/octet-stream", headers=headers)


@app.delete("/clip/labelmaps")
def delete_clip_labelmap(rel_path: str = Query(...), root: Optional[str] = Query(None)):
    labelmap_path = _resolve_clip_labelmap_path(rel_path, root_hint=root)
    if labelmap_path is None:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="labelmap_not_found")
    try:
        labelmap_path.unlink()
    except FileNotFoundError:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="labelmap_not_found")
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=str(exc))
    return {"status": "deleted", "rel_path": rel_path}


@app.post("/fs/upload_classifier")
async def upload_classifier(file: UploadFile = File(...)):
    if not file.filename:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="filename_required")
    saved_path = await _save_asset(
        file,
        subdir="classifiers",
        allowed_exts=CLASSIFIER_ALLOWED_EXTS,
        max_bytes=ASSET_MAX_BYTES,
        quota_bytes=ASSET_UPLOAD_QUOTA_BYTES,
    )
    return {"path": saved_path}


@app.post("/fs/upload_labelmap")
async def upload_labelmap(file: UploadFile = File(...)):
    if not file.filename:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="filename_required")
    saved_path = await _save_asset(
        file,
        subdir="labelmaps",
        allowed_exts=LABELMAP_ALLOWED_EXTS,
        max_bytes=ASSET_MAX_BYTES,
        quota_bytes=ASSET_UPLOAD_QUOTA_BYTES,
    )
    return {"path": saved_path}


def _validate_job_exists(job_id: str) -> ClipTrainingJob:
    with TRAINING_JOBS_LOCK:
        job = TRAINING_JOBS.get(job_id)
        if not job:
            raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="job_not_found")
        return job


def _start_training_worker(job: ClipTrainingJob, *, images_dir: str, labels_dir: str, labelmap_path: Optional[str],
                           clip_name: str, encoder_type: str, encoder_model: Optional[str],
                           output_dir: str, labelmap_dir: str, model_filename: str, labelmap_filename: str,
                           test_size: float, random_seed: int, batch_size: int, max_iter: int,
                           min_per_class: int, class_weight: str, effective_beta: float, C: float, device_override: Optional[str],
                           solver: str, classifier_type: str, mlp_hidden_sizes: str, mlp_dropout: float,
                           mlp_epochs: int, mlp_lr: float, mlp_weight_decay: float, mlp_label_smoothing: float,
                           mlp_loss_type: str, mlp_focal_gamma: float, mlp_focal_alpha: Optional[float],
                           mlp_sampler: str, mlp_mixup_alpha: float, mlp_normalize_embeddings: bool,
                           mlp_patience: int, mlp_activation: str, mlp_layer_norm: bool, mlp_hard_mining_epochs: int,
                           logit_adjustment_mode: str, logit_adjustment_inference: Optional[bool],
                           arcface_enabled: bool, arcface_margin: float, arcface_scale: float,
                           supcon_weight: float, supcon_temperature: float, supcon_projection_dim: int, supcon_projection_hidden: int,
                           embedding_center: bool, embedding_standardize: bool,
                           calibration_mode: str, calibration_max_iters: int, calibration_min_temp: float, calibration_max_temp: float,
                           reuse_embeddings: bool, hard_example_mining: bool,
                           hard_mining_misclassified_weight: float,
                           hard_mining_low_conf_weight: float,
                           hard_mining_low_conf_threshold: float,
                           hard_mining_margin_threshold: float,
                           convergence_tol: float,
                           bg_class_count: int,
                           cancel_event: threading.Event) -> None:

    def progress_cb(value: float, message: str) -> None:
        with TRAINING_JOBS_LOCK:
            if cancel_event.is_set() and job.status not in {"cancelled", "failed"}:
                _job_update(job, status="cancelling", message="Cancellation requested ...", progress=value)
                return
            _job_update(job, status="running", progress=value, message=message)

    def metrics_cb(metric: Dict[str, Any]) -> None:
        if not metric:
            return
        with TRAINING_JOBS_LOCK:
            _clip_job_append_metric(job, metric)

    def worker() -> None:
        try:
            _prepare_for_training()
            with TRAINING_JOBS_LOCK:
                if cancel_event.is_set():
                    _job_update(job, status="cancelled", progress=job.progress, message="Training cancelled before start.")
                    return
                _job_update(job, status="running", progress=0.01, message="Preparing training job ...")
            artifacts = train_clip_from_yolo(
                images_path=images_dir,
                labels_path=labels_dir,
                model_output=os.path.join(output_dir, model_filename),
                labelmap_output=os.path.join(labelmap_dir, labelmap_filename),
                clip_model=clip_name,
                encoder_type=encoder_type,
                encoder_model=encoder_model,
                input_labelmap=labelmap_path,
                test_size=test_size,
                random_seed=random_seed,
                batch_size=batch_size,
                max_iter=max_iter,
                min_per_class=min_per_class,
                class_weight=class_weight,
                effective_beta=effective_beta,
                C=C,
                solver=solver,
                classifier_type=classifier_type,
                mlp_hidden_sizes=mlp_hidden_sizes,
                mlp_dropout=mlp_dropout,
                mlp_epochs=mlp_epochs,
                mlp_lr=mlp_lr,
                mlp_weight_decay=mlp_weight_decay,
                mlp_label_smoothing=mlp_label_smoothing,
                mlp_loss_type=mlp_loss_type,
                mlp_focal_gamma=mlp_focal_gamma,
                mlp_focal_alpha=mlp_focal_alpha,
                mlp_sampler=mlp_sampler,
                mlp_mixup_alpha=mlp_mixup_alpha,
                mlp_normalize_embeddings=mlp_normalize_embeddings,
                mlp_patience=mlp_patience,
                mlp_activation=mlp_activation,
                mlp_layer_norm=mlp_layer_norm,
                mlp_hard_mining_epochs=mlp_hard_mining_epochs,
                logit_adjustment_mode=logit_adjustment_mode,
                logit_adjustment_inference=logit_adjustment_inference,
                arcface_enabled=arcface_enabled,
                arcface_margin=arcface_margin,
                arcface_scale=arcface_scale,
                supcon_weight=supcon_weight,
                supcon_temperature=supcon_temperature,
                supcon_projection_dim=supcon_projection_dim,
                supcon_projection_hidden=supcon_projection_hidden,
                embedding_center=embedding_center,
                embedding_standardize=embedding_standardize,
                calibration_mode=calibration_mode,
                calibration_max_iters=calibration_max_iters,
                calibration_min_temp=calibration_min_temp,
                calibration_max_temp=calibration_max_temp,
                reuse_embeddings=reuse_embeddings,
                hard_example_mining=hard_example_mining,
                hard_mining_misclassified_weight=hard_mining_misclassified_weight,
                hard_mining_low_conf_weight=hard_mining_low_conf_weight,
                hard_mining_low_conf_threshold=hard_mining_low_conf_threshold,
                hard_mining_margin_threshold=hard_mining_margin_threshold,
                convergence_tol=convergence_tol,
                bg_class_count=bg_class_count,
                device=device_override,
                progress_cb=progress_cb,
                metrics_cb=metrics_cb,
                should_cancel=cancel_event.is_set,
            )
            artifacts = _publish_clip_training_artifacts(artifacts)
            payload = _artifacts_to_payload(artifacts)
            with TRAINING_JOBS_LOCK:
                _job_update(job, status="succeeded", progress=1.0, message="Training completed.", artifacts=payload)
        except TrainingError as exc:
            with TRAINING_JOBS_LOCK:
                if str(exc) == "cancelled":
                    _job_update(job, status="cancelled", message="Training cancelled by user.")
                    logger.info("[clip-train %s] Training cancelled", job.job_id[:8])
                else:
                    _job_update(job, status="failed", message=str(exc), error=str(exc))
                    logger.warning("[clip-train %s] Training failed: %s", job.job_id[:8], exc)
        except Exception as exc:  # noqa: BLE001
            with TRAINING_JOBS_LOCK:
                _job_update(job, status="failed", message="Training crashed.", error=str(exc))
            logger.exception("[clip-train %s] Training crashed", job.job_id[:8])
        finally:
            _finalize_training_environment()
            _cleanup_job(job)

    threading.Thread(target=worker, name=f"clip-train-{job.job_id[:8]}", daemon=True).start()


def _ensure_directory(path: str) -> str:
    abs_path = os.path.abspath(path or ".")
    if not os.path.isdir(abs_path):
        try:
            Path(abs_path).mkdir(parents=True, exist_ok=True)
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"output_dir_missing:{abs_path}") from exc
    return abs_path


def _coerce_int(value: Any, fallback: int, *, minimum: Optional[int] = None) -> int:
    try:
        result = int(value)
    except (TypeError, ValueError):
        result = fallback
    if minimum is not None and result < minimum:
        result = minimum
    return result


def _coerce_float(value: Any, fallback: float, *, minimum: Optional[float] = None, maximum: Optional[float] = None) -> float:
    try:
        result = float(value)
    except (TypeError, ValueError):
        result = fallback
    if minimum is not None:
        result = max(minimum, result)
    if maximum is not None:
        result = min(maximum, result)
    return result


def _normalise_optional_path(value: Optional[str]) -> Optional[str]:
    if value is None:
        return None
    trimmed = value.strip()
    if not trimmed:
        return None
    return trimmed


def _parse_bool(value: Optional[str]) -> bool:
    if value is None:
        return False
    return str(value).strip().lower() in {"1", "true", "yes", "on"}


def _load_labelmap_simple(path: Optional[str]) -> List[str]:
    if not path:
        return []
    try:
        classes = _load_labelmap_file(Path(path))
        return classes
    except Exception:
        return []


def _validate_clip_dataset(inputs: Dict[str, str]) -> Dict[str, Any]:
    """
    Light validation of staged CLIP dataset to fail fast before launching a job.
    Expects keys: images_dir, labels_dir, optional labelmap_path.
    """
    images_dir = inputs.get("images_dir")
    labels_dir = inputs.get("labels_dir")
    labelmap_path = inputs.get("labelmap_path")
    if not images_dir or not labels_dir:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="clip_dataset_missing_paths")
    img_root = Path(images_dir)
    lbl_root = Path(labels_dir)
    if not img_root.is_dir() or not lbl_root.is_dir():
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="clip_dataset_missing_paths")
    valid_exts = {".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff", ".webp"}
    image_files: List[Path] = []
    for p in img_root.rglob("*"):
        if p.is_file() and p.suffix.lower() in valid_exts:
            image_files.append(p)
    if not image_files:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="clip_images_missing")
    label_files = [p for p in lbl_root.rglob("*") if p.is_file() and p.suffix.lower() == ".txt"]
    if not label_files:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="clip_labels_missing")
    labelmap = _load_labelmap_simple(labelmap_path)
    max_cid = -1
    box_count = 0
    for lf in label_files:
        try:
            for line in lf.read_text(encoding="utf-8").splitlines():
                parts = line.strip().split()
                if len(parts) < 5:
                    continue
                try:
                    cid = int(float(parts[0]))
                except Exception:
                    continue
                max_cid = max(max_cid, cid)
                box_count += 1
        except Exception:
            continue
    if box_count == 0:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="clip_labels_empty")
    if labelmap and max_cid >= len(labelmap):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="clip_labelmap_class_mismatch")
    return {
        "images": len(image_files),
        "labels": len(label_files),
        "boxes": box_count,
        "labelmap_classes": len(labelmap),
    }


@app.post("/clip/train")
async def start_clip_training(
    images: Optional[List[UploadFile]] = File(None),
    labels: Optional[List[UploadFile]] = File(None),
    labelmap: Optional[UploadFile] = File(None),
    clip_model_name: str = Form(DEFAULT_CLIP_MODEL),
    encoder_type: str = Form("clip"),
    encoder_model: Optional[str] = Form(None),
    output_dir: str = Form("."),
    model_filename: str = Form("my_logreg_model.pkl"),
    labelmap_filename: str = Form("my_label_list.pkl"),
    test_size: float = Form(0.2),
    random_seed: int = Form(42),
    batch_size: int = Form(64),
    max_iter: int = Form(1000),
    min_per_class: int = Form(2),
    class_weight: str = Form("balanced"),
    effective_beta: float = Form(0.9999),
    C: float = Form(1.0),
    device_override: Optional[str] = Form(None),
    images_path_native: Optional[str] = Form(None),
    labels_path_native: Optional[str] = Form(None),
    labelmap_path_native: Optional[str] = Form(None),
    solver: str = Form("saga"),
    classifier_type: str = Form("logreg"),
    mlp_hidden_sizes: str = Form("256"),
    mlp_dropout: float = Form(0.1),
    mlp_epochs: int = Form(50),
    mlp_lr: float = Form(1e-3),
    mlp_weight_decay: float = Form(1e-4),
    mlp_label_smoothing: float = Form(0.05),
    mlp_loss_type: str = Form("ce"),
    mlp_focal_gamma: float = Form(2.0),
    mlp_focal_alpha: float = Form(-1.0),
    mlp_sampler: str = Form("balanced"),
    mlp_mixup_alpha: float = Form(0.1),
    mlp_normalize_embeddings: Optional[str] = Form("true"),
    mlp_patience: int = Form(6),
    mlp_activation: str = Form("relu"),
    mlp_layer_norm: Optional[str] = Form("false"),
    mlp_hard_mining_epochs: int = Form(5),
    logit_adjustment_mode: str = Form("none"),
    logit_adjustment_inference: Optional[str] = Form(None),
    arcface_enabled: Optional[str] = Form("false"),
    arcface_margin: float = Form(0.2),
    arcface_scale: float = Form(30.0),
    supcon_weight: float = Form(0.0),
    supcon_temperature: float = Form(0.07),
    supcon_projection_dim: int = Form(128),
    supcon_projection_hidden: int = Form(0),
    embedding_center: Optional[str] = Form("false"),
    embedding_standardize: Optional[str] = Form("false"),
    calibration_mode: str = Form("none"),
    calibration_max_iters: int = Form(50),
    calibration_min_temp: float = Form(0.5),
    calibration_max_temp: float = Form(5.0),
    reuse_embeddings: Optional[str] = Form(None),
    hard_example_mining: Optional[str] = Form(None),
    hard_mis_weight: float = Form(3.0),
    hard_low_conf_weight: float = Form(2.0),
    hard_low_conf_threshold: float = Form(0.65),
    hard_margin_threshold: float = Form(0.15),
    convergence_tol: float = Form(1e-4),
    bg_class_count: int = Form(2),
    staged_temp_dir: Optional[str] = Form(None),
):
    images_path_native = _normalise_optional_path(images_path_native)
    labels_path_native = _normalise_optional_path(labels_path_native)
    labelmap_path_native = _normalise_optional_path(labelmap_path_native)

    encoder_type_norm = (encoder_type or "clip").strip().lower()
    if encoder_type_norm not in {"clip", "dinov3"}:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="clip_encoder_type_unsupported")
    encoder_model_name = (encoder_model or "").strip()
    if encoder_type_norm == "clip":
        if encoder_model_name:
            clip_model_name = encoder_model_name
        if clip_model_name not in SUPPORTED_CLIP_MODELS:
            SUPPORTED_CLIP_MODELS.append(clip_model_name)
    else:
        if not encoder_model_name:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="encoder_model_required")

    solver_name = (solver or "saga").strip().lower()
    if solver_name not in {"saga", "sag", "lbfgs", "liblinear", "newton-cg"}:
        solver_name = "saga"
    classifier_type_norm = (classifier_type or "logreg").strip().lower()
    if classifier_type_norm not in {"logreg", "mlp"}:
        classifier_type_norm = "logreg"
    reuse_embeddings_flag = _parse_bool(reuse_embeddings)
    hard_example_flag = _parse_bool(hard_example_mining)

    use_native_paths = bool(images_path_native and labels_path_native)
    if use_native_paths and (images or labels):
        logger.info("Ignoring uploaded files; using native dataset paths provided.")
    if reuse_embeddings_flag and not use_native_paths:
        logger.info("Embedding cache reuse requested but dataset is staged upload; disabling reuse for job %s", images_path_native or "<staged>")
        reuse_embeddings_flag = False

    if not use_native_paths:
        if not images:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="images_required")
        if not labels:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="labels_required")

    classifiers_dir = _ensure_directory(str((UPLOAD_ROOT / "classifiers").resolve()))
    labelmaps_dir = _ensure_directory(str((UPLOAD_ROOT / "labelmaps").resolve()))
    if output_dir and output_dir not in {".", classifiers_dir}:
        logger.info("Ignoring CLIP output_dir=%s; saving under %s", output_dir, classifiers_dir)

    temp_root: Optional[str] = None
    images_dir: Optional[str] = None
    labels_dir: Optional[str] = None

    if use_native_paths:
        images_dir = _ensure_directory(images_path_native)
        labels_dir = _ensure_directory(labels_path_native)
    else:
        temp_root = tempfile.mkdtemp(prefix="clip_train_")
        images_dir = os.path.join(temp_root, "images")
        labels_dir = os.path.join(temp_root, "labels")
        os.makedirs(images_dir, exist_ok=True)
        os.makedirs(labels_dir, exist_ok=True)

        for upload in images or []:
            await _save_upload_file(
                upload,
                Path(images_dir),
                max_bytes=CLIP_TRAIN_UPLOAD_MAX_BYTES,
                quota_root=Path(temp_root),
                quota_limit=CLIP_TRAIN_UPLOAD_QUOTA_BYTES,
            )

        for upload in labels or []:
            await _save_upload_file(
                upload,
                Path(labels_dir),
                max_bytes=CLIP_TRAIN_UPLOAD_MAX_BYTES,
                quota_root=Path(temp_root),
                quota_limit=CLIP_TRAIN_UPLOAD_QUOTA_BYTES,
            )

    labelmap_path = None
    if labelmap_path_native:
        labelmap_path = os.path.abspath(labelmap_path_native)
        if not os.path.isfile(labelmap_path):
            raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="labelmap_not_found")
    elif labelmap is not None:
        if temp_root is None:
            temp_root = tempfile.mkdtemp(prefix="clip_train_")
        labelmap_path = str(
            await _save_upload_file(
                labelmap,
                Path(temp_root),
                max_bytes=ASSET_MAX_BYTES,
                quota_root=Path(temp_root),
                quota_limit=CLIP_TRAIN_UPLOAD_QUOTA_BYTES,
            )
        )

    job_id = uuid.uuid4().hex
    if images_dir is None or labels_dir is None:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="dataset_paths_unresolved")
    # Fail fast on obviously invalid staged datasets.
    _validate_clip_dataset({"images_dir": images_dir, "labels_dir": labels_dir, "labelmap_path": labelmap_path})
    logger.info(
        "Starting training job %s (encoder=%s, model=%s, native_paths=%s)",
        job_id[:8],
        encoder_type_norm,
        clip_model_name,
        use_native_paths,
    )
    if staged_temp_dir:
        temp_root = os.path.abspath(staged_temp_dir)
    job = ClipTrainingJob(job_id=job_id, temp_dir=temp_root, images_dir=images_dir, labels_dir=labels_dir, labelmap_path=labelmap_path)
    job_message = "Job queued (native paths)" if use_native_paths else "Job queued (upload staging)"
    test_size_f = _coerce_float(test_size, 0.2, minimum=0.0, maximum=0.9)
    random_seed_i = _coerce_int(random_seed, 42)
    batch_size_i = _coerce_int(batch_size, 64, minimum=1)
    max_iter_i = _coerce_int(max_iter, 1000, minimum=1)
    min_per_class_i = _coerce_int(min_per_class, 2, minimum=1)
    class_weight_norm = (class_weight or "none").lower()
    if class_weight_norm not in {"balanced", "none", "effective"}:
        class_weight_norm = "none"
    C_f = _coerce_float(C, 1.0, minimum=0.0001)
    effective_beta_f = _coerce_float(effective_beta, 0.9999, minimum=0.5, maximum=0.99999)
    mlp_dropout_f = _coerce_float(mlp_dropout, 0.1, minimum=0.0, maximum=0.9)
    mlp_epochs_i = _coerce_int(mlp_epochs, 50, minimum=1)
    mlp_lr_f = _coerce_float(mlp_lr, 1e-3, minimum=1e-6)
    mlp_weight_decay_f = _coerce_float(mlp_weight_decay, 1e-4, minimum=0.0)
    mlp_label_smoothing_f = _coerce_float(mlp_label_smoothing, 0.05, minimum=0.0, maximum=0.3)
    mlp_loss_type_norm = (mlp_loss_type or "ce").strip().lower()
    if mlp_loss_type_norm not in {"ce", "focal"}:
        mlp_loss_type_norm = "ce"
    mlp_focal_gamma_f = _coerce_float(mlp_focal_gamma, 2.0, minimum=0.0)
    mlp_focal_alpha_f = _coerce_float(mlp_focal_alpha, -1.0)
    if mlp_focal_alpha_f < 0:
        mlp_focal_alpha_f = None
    mlp_sampler_norm = (mlp_sampler or "balanced").strip().lower()
    if mlp_sampler_norm not in {"balanced", "none", "shuffle"}:
        mlp_sampler_norm = "balanced"
    mlp_mixup_alpha_f = _coerce_float(mlp_mixup_alpha, 0.1, minimum=0.0)
    mlp_normalize_embeddings_flag = _parse_bool(mlp_normalize_embeddings)
    mlp_patience_i = _coerce_int(mlp_patience, 6, minimum=1)
    mlp_activation_norm = (mlp_activation or "relu").strip().lower()
    if mlp_activation_norm not in {"relu", "gelu"}:
        mlp_activation_norm = "relu"
    mlp_layer_norm_flag = _parse_bool(mlp_layer_norm)
    mlp_hard_mining_epochs_i = _coerce_int(mlp_hard_mining_epochs, 5, minimum=1)
    logit_adjustment_mode_norm = (logit_adjustment_mode or "none").strip().lower()
    if logit_adjustment_mode_norm not in {"none", "train", "infer", "both"}:
        logit_adjustment_mode_norm = "none"
    logit_adjustment_inference_flag = None
    if logit_adjustment_inference is not None:
        logit_adjustment_inference_flag = _parse_bool(logit_adjustment_inference)
    arcface_enabled_flag = _parse_bool(arcface_enabled)
    arcface_margin_f = _coerce_float(arcface_margin, 0.2, minimum=0.0)
    arcface_scale_f = _coerce_float(arcface_scale, 30.0, minimum=1.0)
    supcon_weight_f = _coerce_float(supcon_weight, 0.0, minimum=0.0)
    supcon_temperature_f = _coerce_float(supcon_temperature, 0.07, minimum=0.0001)
    supcon_projection_dim_i = _coerce_int(supcon_projection_dim, 128, minimum=0)
    supcon_projection_hidden_i = _coerce_int(supcon_projection_hidden, 0, minimum=0)
    embedding_center_flag = _parse_bool(embedding_center)
    embedding_standardize_flag = _parse_bool(embedding_standardize)
    calibration_mode_norm = (calibration_mode or "none").strip().lower()
    if calibration_mode_norm not in {"none", "temperature"}:
        calibration_mode_norm = "none"
    calibration_max_iters_i = _coerce_int(calibration_max_iters, 50, minimum=1)
    calibration_min_temp_f = _coerce_float(calibration_min_temp, 0.5, minimum=0.01)
    calibration_max_temp_f = _coerce_float(calibration_max_temp, 5.0, minimum=calibration_min_temp_f)
    device_override_clean = (device_override or None)
    hard_mis_weight_f = _coerce_float(hard_mis_weight, 3.0, minimum=1.0)
    hard_low_conf_weight_f = _coerce_float(hard_low_conf_weight, 2.0, minimum=1.0)
    hard_low_conf_threshold_f = _coerce_float(hard_low_conf_threshold, 0.65, minimum=0.0, maximum=0.9999)
    hard_margin_threshold_f = _coerce_float(hard_margin_threshold, 0.15, minimum=0.0)
    convergence_tol_f = _coerce_float(convergence_tol, 1e-4, minimum=1e-8)
    bg_class_count_i = _coerce_int(bg_class_count, 2, minimum=1)
    bg_class_count_i = max(1, min(10, bg_class_count_i))
    model_filename = Path(model_filename).name or "my_logreg_model.pkl"
    labelmap_filename = Path(labelmap_filename).name or "my_label_list.pkl"

    extras = [solver_name]
    extras.append(classifier_type_norm)
    if class_weight_norm and class_weight_norm != "none":
        extras.append(f"class_weight={class_weight_norm}")
    if reuse_embeddings_flag:
        extras.append("cache")
    if hard_example_flag:
        extras.append(f"hard({hard_mis_weight_f:.1f}/{hard_low_conf_weight_f:.1f})")
    extras.append(f"bg={bg_class_count_i}")
    job_message += f" [{', '.join(extras)}]"
    _job_log(job, job_message)

    with TRAINING_JOBS_LOCK:
        TRAINING_JOBS[job_id] = job

    _start_training_worker(
        job,
        images_dir=images_dir,
        labels_dir=labels_dir,
        labelmap_path=labelmap_path,
        clip_name=clip_model_name,
        encoder_type=encoder_type_norm,
        encoder_model=encoder_model_name or None,
        output_dir=classifiers_dir,
        labelmap_dir=labelmaps_dir,
        model_filename=model_filename,
        labelmap_filename=labelmap_filename,
        test_size=test_size_f,
        random_seed=random_seed_i,
        batch_size=batch_size_i,
        max_iter=max_iter_i,
        min_per_class=min_per_class_i,
        class_weight=class_weight_norm,
        effective_beta=effective_beta_f,
        C=C_f,
        device_override=device_override_clean,
        solver=solver_name,
        classifier_type=classifier_type_norm,
        mlp_hidden_sizes=str(mlp_hidden_sizes or "256"),
        mlp_dropout=mlp_dropout_f,
        mlp_epochs=mlp_epochs_i,
        mlp_lr=mlp_lr_f,
        mlp_weight_decay=mlp_weight_decay_f,
        mlp_label_smoothing=mlp_label_smoothing_f,
        mlp_loss_type=mlp_loss_type_norm,
        mlp_focal_gamma=mlp_focal_gamma_f,
        mlp_focal_alpha=mlp_focal_alpha_f,
        mlp_sampler=mlp_sampler_norm,
        mlp_mixup_alpha=mlp_mixup_alpha_f,
        mlp_normalize_embeddings=mlp_normalize_embeddings_flag,
        mlp_patience=mlp_patience_i,
        mlp_activation=mlp_activation_norm,
        mlp_layer_norm=mlp_layer_norm_flag,
        mlp_hard_mining_epochs=mlp_hard_mining_epochs_i,
        logit_adjustment_mode=logit_adjustment_mode_norm,
        logit_adjustment_inference=logit_adjustment_inference_flag,
        arcface_enabled=arcface_enabled_flag,
        arcface_margin=arcface_margin_f,
        arcface_scale=arcface_scale_f,
        supcon_weight=supcon_weight_f,
        supcon_temperature=supcon_temperature_f,
        supcon_projection_dim=supcon_projection_dim_i,
        supcon_projection_hidden=supcon_projection_hidden_i,
        embedding_center=embedding_center_flag,
        embedding_standardize=embedding_standardize_flag,
        calibration_mode=calibration_mode_norm,
        calibration_max_iters=calibration_max_iters_i,
        calibration_min_temp=calibration_min_temp_f,
        calibration_max_temp=calibration_max_temp_f,
        reuse_embeddings=reuse_embeddings_flag,
        hard_example_mining=hard_example_flag,
        hard_mining_misclassified_weight=hard_mis_weight_f,
        hard_mining_low_conf_weight=hard_low_conf_weight_f,
        hard_mining_low_conf_threshold=hard_low_conf_threshold_f,
        hard_mining_margin_threshold=hard_margin_threshold_f,
        convergence_tol=convergence_tol_f,
        bg_class_count=bg_class_count_i,
        cancel_event=job.cancel_event,
    )

    return {"job_id": job_id}


def _start_qwen_training_worker(job: QwenTrainingJob, config: QwenTrainingConfig) -> None:
    result_path = Path(config.result_path)

    def progress_cb(value: float, message: str) -> None:
        with QWEN_TRAINING_JOBS_LOCK:
            if job.cancel_event.is_set() and job.status not in {"cancelled", "failed"}:
                _qwen_job_update(job, status="cancelling", message="Cancelling ...", progress=value)
                return
            _qwen_job_update(job, status="running", message=message, progress=value)

    def metrics_cb(payload: Dict[str, Any]) -> None:
        if not payload:
            return
        with QWEN_TRAINING_JOBS_LOCK:
            _qwen_job_append_metric(job, payload)
            progress_val = payload.get("progress")
            progress = None
            if isinstance(progress_val, (int, float)):
                progress = max(0.0, min(float(progress_val), 0.999))
            message = _summarize_qwen_metric(payload)
            _qwen_job_update(job, status="running", message=message, progress=progress, log_message=False)

    def cancel_cb() -> bool:
        return job.cancel_event.is_set()

    def worker() -> None:
        try:
            _prepare_for_qwen_training()
            with QWEN_TRAINING_JOBS_LOCK:
                if job.cancel_event.is_set():
                    _qwen_job_update(job, status="cancelled", message="Cancelled before start.")
                    return
                _qwen_job_update(job, status="running", progress=0.01, message="Preparing Qwen training job ...")
            result = train_qwen_model(config, progress_cb=progress_cb, cancel_cb=cancel_cb, metrics_cb=metrics_cb)
            run_metadata = _persist_qwen_run_metadata(result_path, config, result)
            payload = {
                "checkpoints": result.checkpoints,
                "latest": result.latest_checkpoint,
                "epochs_ran": result.epochs_ran,
                "metadata": run_metadata,
            }
            with QWEN_TRAINING_JOBS_LOCK:
                _qwen_job_update(job, status="succeeded", progress=1.0, message="Training complete", result=payload)
        except QwenTrainingError as exc:
            with QWEN_TRAINING_JOBS_LOCK:
                status = "cancelled" if job.cancel_event.is_set() else "failed"
                _qwen_job_update(job, status=status, message=str(exc), error=str(exc))
        except Exception as exc:  # noqa: BLE001
            with QWEN_TRAINING_JOBS_LOCK:
                _qwen_job_update(job, status="failed", message="Unexpected error", error=str(exc))
        finally:
            _finalize_qwen_training_environment()

    thread = threading.Thread(target=worker, name=f"qwen-train-{job.job_id}", daemon=True)
    thread.start()



@app.get("/clip/train")
def list_training_jobs():
    _prune_job_registry(TRAINING_JOBS, TRAINING_JOBS_LOCK)
    with TRAINING_JOBS_LOCK:
        jobs = sorted(TRAINING_JOBS.values(), key=lambda job: job.created_at, reverse=True)
        return [{"job_id": job.job_id, "status": job.status, "created_at": job.created_at} for job in jobs]


@app.get("/clip/train/{job_id}")
def get_training_job(job_id: str):
    job = _validate_job_exists(job_id)
    return _serialize_job(job)


@app.post("/clip/train/{job_id}/cancel")
def cancel_training_job(job_id: str):
    job = _validate_job_exists(job_id)
    next_status = job.status
    with TRAINING_JOBS_LOCK:
        if job.status in {"succeeded", "failed", "cancelled"}:
            raise HTTPException(status_code=HTTP_428_PRECONDITION_REQUIRED, detail="job_not_cancellable")
        if job.cancel_event.is_set():
            return {"status": job.status}
        job.cancel_event.set()
        next_status = job.status if job.status not in {"running", "queued"} else "cancelling"
        _job_update(job, status=next_status, message="Cancellation requested ...")
    return {"status": next_status}


def _build_sam3_config(
    payload: Sam3TrainRequest,
    meta: Dict[str, Any],
    job_id: str,
    job_logs: Optional[List[str]] = None,
) -> Tuple[OmegaConf, int]:
    dataset_root = Path(meta.get("dataset_root") or SAM3_DATASET_ROOT)
    val_percent = payload.val_percent if payload.val_percent is not None else 0.3
    split_seed = int(payload.split_seed) if payload.split_seed is not None else 42
    random_split = payload.random_split if payload.random_split is not None else True
    train_limit = int(payload.train_limit) if payload.train_limit is not None and payload.train_limit > 0 else None
    val_limit = int(payload.val_limit) if payload.val_limit is not None and payload.val_limit > 0 else None
    meta = _prepare_sam3_training_split(
        dataset_root,
        meta,
        job_id,
        random_split=random_split,
        val_percent=val_percent,
        split_seed=split_seed,
        train_limit=train_limit,
        val_limit=val_limit,
        log_messages=job_logs,
    )
    cfg = OmegaConf.load(str(SAM3_CONFIG_TEMPLATE))
    if not hasattr(cfg.scratch, "enable_segmentation_head"):
        cfg.scratch.enable_segmentation_head = True
    if not hasattr(cfg.scratch, "load_segmentation"):
        cfg.scratch.load_segmentation = False
    train_ann = Path(meta["coco_train_json"]).resolve()
    val_ann = Path(meta["coco_val_json"]).resolve()
    cfg.paths.train_img_folder = str(train_ann.parent)
    cfg.paths.train_ann_file = str(train_ann)
    cfg.paths.val_img_folder = str(val_ann.parent)
    cfg.paths.val_ann_file = str(val_ann)
    run_name = _safe_run_name(payload.run_name, f"sam3_run_{job_id}")
    exp_dir = Path(payload.experiment_log_dir) if payload.experiment_log_dir else (SAM3_JOB_ROOT / run_name)
    if exp_dir.exists():
        raise HTTPException(status_code=HTTP_409_CONFLICT, detail="run_name_exists")
    cfg.paths.experiment_log_dir = str(exp_dir.resolve())
    cfg.paths.bpe_path = str(SAM3_BPE_PATH)
    cfg.launcher.experiment_log_dir = cfg.paths.experiment_log_dir
    cfg.launcher.gpus_per_node = max(1, int(payload.num_gpus or cfg.launcher.gpus_per_node or 1))
    cfg.trainer.max_epochs = int(payload.max_epochs) if payload.max_epochs is not None else cfg.trainer.max_epochs
    cfg.trainer.val_epoch_freq = int(payload.val_epoch_freq) if payload.val_epoch_freq is not None else cfg.trainer.val_epoch_freq
    cfg.scratch.target_epoch_size = int(payload.target_epoch_size) if payload.target_epoch_size is not None else cfg.scratch.target_epoch_size
    dataset_type = meta.get("type", "bbox")
    seg_head_requested = payload.enable_segmentation_head
    train_seg_requested = payload.train_segmentation
    default_seg = dataset_type == "seg"
    enable_seg_head = bool(seg_head_requested) if seg_head_requested is not None else (bool(cfg.scratch.enable_segmentation_head) or default_seg)
    train_segmentation = bool(train_seg_requested) if train_seg_requested is not None else (bool(cfg.scratch.load_segmentation) or default_seg)
    cfg.scratch.enable_segmentation_head = enable_seg_head or train_segmentation
    cfg.scratch.load_segmentation = train_segmentation
    # Keep legacy flag aligned with head presence so downstream activation sees the capability.
    cfg.scratch.enable_segmentation = cfg.scratch.enable_segmentation_head
    if payload.resolution is not None:
        cfg.scratch.resolution = int(payload.resolution)
    if payload.lr_scale is not None:
        cfg.scratch.lr_scale = float(payload.lr_scale)
    if payload.gradient_accumulation_steps is not None:
        cfg.scratch.gradient_accumulation_steps = int(payload.gradient_accumulation_steps)
    cfg.trainer.gradient_accumulation_steps = cfg.scratch.gradient_accumulation_steps
    if cfg.trainer.gradient_accumulation_steps and cfg.trainer.gradient_accumulation_steps > 1:
        try:
            train_collate = cfg.trainer.data.train.collate_fn
            train_collate._target_ = "sam3.train.data.collator.collate_fn_api_with_chunking"
            train_collate.num_chunks = int(cfg.trainer.gradient_accumulation_steps)
            train_collate._partial_ = True
            if not hasattr(train_collate, "repeats"):
                train_collate.repeats = cfg.scratch.hybrid_repeats
        except Exception:
            pass
    if payload.scheduler_warmup is not None:
        cfg.scratch.scheduler_warmup = int(payload.scheduler_warmup)
    if payload.scheduler_timescale is not None:
        cfg.scratch.scheduler_timescale = int(payload.scheduler_timescale)
    if payload.train_batch_size is not None:
        cfg.scratch.train_batch_size = int(payload.train_batch_size)
    if payload.val_batch_size is not None:
        cfg.scratch.val_batch_size = int(payload.val_batch_size)
    if payload.num_train_workers is not None:
        cfg.scratch.num_train_workers = int(payload.num_train_workers)
    if payload.num_val_workers is not None:
        cfg.scratch.num_val_workers = int(payload.num_val_workers)
    if payload.enable_inst_interactivity is not None:
        cfg.scratch.enable_inst_interactivity = bool(payload.enable_inst_interactivity)
    if payload.train_limit is not None:
        cfg.dataset.num_images = int(payload.train_limit)
    elif payload.target_epoch_size is not None:
        try:
            batches = max(1, int(payload.target_epoch_size))
            batch_size = int(payload.train_batch_size) if payload.train_batch_size is not None else int(cfg.scratch.train_batch_size)
            cfg.dataset.num_images = max(1, batches * batch_size)
        except Exception:
            pass
    if payload.val_limit is not None:
        try:
            val_limit = max(1, int(payload.val_limit))
            cfg.dataset.val_num_images = val_limit
            if hasattr(cfg, "trainer") and hasattr(cfg.trainer, "data") and hasattr(cfg.trainer.data, "val"):
                if hasattr(cfg.trainer.data.val, "dataset"):
                    cfg.trainer.data.val.dataset.limit_ids = val_limit
        except Exception:
            pass
    if payload.log_every_batch:
        try:
            cfg.trainer.logging.log_freq = 1
        except Exception:
            pass
    elif payload.log_freq is not None and "logging" in cfg.trainer:
        cfg.trainer.logging.log_freq = int(payload.log_freq)
    # Language backbone tuning (text alignment preservation)
    if payload.language_backbone_lr is not None:
        try:
            cfg.scratch.lr_language_backbone = float(payload.language_backbone_lr)
        except Exception:
            pass
    if payload.freeze_language_backbone:
        try:
            cfg.scratch.lr_language_backbone = 0.0
        except Exception:
            pass
    # Balance strategy/config
    if payload.balance_strategy is not None:
        cfg.dataset.balance_strategy = payload.balance_strategy
        cfg.dataset.class_balance = payload.balance_strategy != "none"
    if payload.balance_classes is not None:
        cfg.dataset.class_balance = bool(payload.balance_classes)
    if payload.balance_power is not None:
        cfg.dataset.balance_power = float(payload.balance_power)
    if payload.balance_clip is not None:
        cfg.dataset.balance_clip = float(payload.balance_clip)
    if payload.balance_beta is not None:
        cfg.dataset.balance_beta = float(payload.balance_beta)
    if payload.balance_gamma is not None:
        cfg.dataset.balance_gamma = float(payload.balance_gamma)
    cfg.trainer.checkpoint.save_dir = f"{cfg.launcher.experiment_log_dir}/checkpoints"
    if "meters" in cfg.trainer and "val" in cfg.trainer.meters:
        try:
            cfg.trainer.meters.val.roboflow100.detection.dump_dir = f"{cfg.launcher.experiment_log_dir}/dumps/local"
            cfg.trainer.meters.val.roboflow100.detection.pred_file_evaluators[0].gt_path = cfg.paths.val_ann_file
            # Apply val tuning
            if payload.val_max_dets is not None:
                cfg.trainer.meters.val.roboflow100.detection.maxdets = int(payload.val_max_dets)
        except Exception:
            pass
    # Prompt vocab overrides: allow multiple variants per class and optional randomization during training
    user_prompts = payload.prompt_variants or {}
    prompt_map: Dict[int, List[str]] = {}
    classes = meta.get("classes") or []
    if classes and user_prompts:
        def _normalise_variants(raw: Any) -> List[str]:
            if raw is None:
                return []
            if isinstance(raw, str):
                parts = [p.strip() for p in raw.replace(";", ",").split(",") if p.strip()]
                return parts if parts else [raw.strip()] if raw.strip() else []
            if isinstance(raw, (list, tuple, set)):
                return [str(p).strip() for p in raw if str(p).strip()]
            return []

        for idx, label in enumerate(classes):
            # allow lookup by label or by (1-based) category id
            cat_id = idx + 1
            custom = (
                user_prompts.get(label)
                or user_prompts.get(str(label))
                or user_prompts.get(cat_id)
                or user_prompts.get(str(cat_id))
            )
            variants = _normalise_variants(custom)
            if variants:
                prompt_map[cat_id] = variants

    if prompt_map:
        prompt_randomize = bool(payload.prompt_randomize) if payload.prompt_randomize is not None else True
        # Train loader
        try:
            train_loader_cfg = cfg.trainer.data.train.dataset.get("coco_json_loader")  # type: ignore[index]
        except Exception:
            train_loader_cfg = None
        if train_loader_cfg is None:
            cfg.trainer.data.train.dataset["coco_json_loader"] = {}
            train_loader_cfg = cfg.trainer.data.train.dataset.get("coco_json_loader")  # type: ignore[index]
        try:
            train_loader_cfg["_target_"] = "sam3.train.data.coco_json_loaders.COCO_FROM_JSON"
            train_loader_cfg["_partial_"] = True
            train_loader_cfg["prompts"] = prompt_map
            train_loader_cfg["prompt_randomize"] = prompt_randomize
        except Exception:
            pass
        # Val loader (deterministic prompts)
        try:
            val_loader_cfg = cfg.trainer.data.val.dataset.coco_json_loader  # type: ignore[assignment]
        except Exception:
            val_loader_cfg = None
        if val_loader_cfg is None:
            try:
                cfg.trainer.data.val.dataset["coco_json_loader"] = {}
                val_loader_cfg = cfg.trainer.data.val.dataset.coco_json_loader  # type: ignore[assignment]
            except Exception:
                val_loader_cfg = None
        if val_loader_cfg is not None:
            try:
                val_loader_cfg["_target_"] = "sam3.train.data.coco_json_loaders.COCO_FROM_JSON"
                val_loader_cfg["_partial_"] = True
                val_loader_cfg["prompts"] = prompt_map
                val_loader_cfg["prompt_randomize"] = False
            except Exception:
                pass
    cfg.launcher.num_nodes = 1
    cfg.submitit.use_cluster = False
    cfg.submitit.cpus_per_task = max(cfg.scratch.num_train_workers, cfg.submitit.cpus_per_task or 0)
    Path(cfg.paths.experiment_log_dir).mkdir(parents=True, exist_ok=True)
    return cfg, int(cfg.launcher.gpus_per_node)


@app.post("/sam3/train/jobs")
def create_sam3_training_job(payload: Sam3TrainRequest):
    meta = _resolve_sam3_dataset_meta(payload.dataset_id)
    job_id = uuid.uuid4().hex
    prep_logs: List[str] = []
    cfg, num_gpus = _build_sam3_config(payload, meta, job_id, prep_logs)
    config_dict = OmegaConf.to_container(cfg, resolve=False)  # type: ignore[arg-type]
    job = Sam3TrainingJob(job_id=job_id, config=config_dict)
    with SAM3_TRAINING_JOBS_LOCK:
        SAM3_TRAINING_JOBS[job_id] = job
        for msg in prep_logs:
            _sam3_job_log(job, msg)
        _sam3_job_log(job, "Job queued")
    logger.info("[sam3-train %s] dataset=%s gpus=%s", job_id[:8], payload.dataset_id, num_gpus)
    _start_sam3_training_worker(
        job,
        cfg,
        num_gpus,
        val_score_thresh=payload.val_score_thresh,
        val_max_dets=payload.val_max_dets,
    )
    return {"job_id": job_id}


@app.get("/sam3/train/jobs")
def list_sam3_training_jobs():
    _prune_job_registry(SAM3_TRAINING_JOBS, SAM3_TRAINING_JOBS_LOCK)
    with SAM3_TRAINING_JOBS_LOCK:
        jobs = sorted(SAM3_TRAINING_JOBS.values(), key=lambda job: job.created_at, reverse=True)
        return [_serialize_sam3_job(job) for job in jobs]


@app.get("/sam3/train/jobs/{job_id}")
def get_sam3_training_job(job_id: str):
    job = _get_sam3_job(job_id)
    return _serialize_sam3_job(job)


@app.post("/sam3/train/jobs/{job_id}/cancel")
def cancel_sam3_training_job(job_id: str):
    job = _get_sam3_job(job_id)
    with SAM3_TRAINING_JOBS_LOCK:
        if job.status in {"succeeded", "failed", "cancelled"}:
            raise HTTPException(status_code=HTTP_428_PRECONDITION_REQUIRED, detail="job_not_cancellable")
        if job.cancel_event.is_set():
            return {"status": job.status}
        job.cancel_event.set()
        if job.process and job.process.poll() is None:
            try:
                job.process.terminate()
            except Exception:  # noqa: BLE001
                pass
        next_status = job.status if job.status not in {"running", "queued"} else "cancelling"
        _sam3_job_update(job, status=next_status, message="Cancellation requested ...")
    return {"status": job.status}


@app.post("/yolo/train/jobs")
def create_yolo_training_job(payload: YoloTrainRequest):
    if not payload.accept_tos:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="yolo_tos_required")
    job_id = uuid.uuid4().hex
    run_dir = _yolo_run_dir(job_id, create=True)
    dataset_info = _resolve_yolo_training_dataset(payload)
    if payload.task == "segment" and dataset_info.get("yolo_ready") and not dataset_info.get("yolo_seg_ready"):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="yolo_seg_requires_polygons")
    config = payload.dict(exclude_none=True)
    config["paths"] = {"run_dir": str(run_dir)}
    config["dataset"] = dataset_info
    message = "Queued (training not started)"
    status = "queued"
    if not dataset_info.get("yolo_ready"):
        status = "blocked"
        message = "Dataset is not YOLO-ready; conversion required."
    job = YoloTrainingJob(job_id=job_id, config=config, message=message, status=status)
    with YOLO_TRAINING_JOBS_LOCK:
        YOLO_TRAINING_JOBS[job_id] = job
        _yolo_job_log(job, job.message)
    _yolo_write_run_meta(
        run_dir,
        {
            "job_id": job_id,
            "status": job.status,
            "message": job.message,
            "config": job.config,
        },
    )
    if job.status != "blocked":
        _start_yolo_training_worker(job)
    return {"job_id": job_id}


@app.get("/yolo/train/jobs")
def list_yolo_training_jobs():
    _prune_job_registry(YOLO_TRAINING_JOBS, YOLO_TRAINING_JOBS_LOCK)
    with YOLO_TRAINING_JOBS_LOCK:
        jobs = sorted(YOLO_TRAINING_JOBS.values(), key=lambda job: job.created_at, reverse=True)
        return [_serialize_yolo_job(job) for job in jobs]


@app.get("/yolo/train/jobs/{job_id}")
def get_yolo_training_job(job_id: str):
    job = _get_yolo_job(job_id)
    return _serialize_yolo_job(job)


@app.post("/yolo/train/jobs/{job_id}/cancel")
def cancel_yolo_training_job(job_id: str):
    job = _get_yolo_job(job_id)
    with YOLO_TRAINING_JOBS_LOCK:
        if job.status in {"succeeded", "failed", "cancelled"}:
            raise HTTPException(status_code=HTTP_428_PRECONDITION_REQUIRED, detail="job_not_cancellable")
        if job.cancel_event.is_set():
            return {"status": job.status}
        job.cancel_event.set()
        next_status = job.status if job.status not in {"running", "queued"} else "cancelled"
        _yolo_job_update(job, status=next_status, message="Cancellation requested ...")
        run_dir = _yolo_run_dir(job.job_id, create=False)
        _yolo_write_run_meta(
            run_dir,
            {
                "job_id": job.job_id,
                "status": job.status,
                "message": job.message,
                "config": job.config,
            },
        )
    return {"status": job.status}


@app.post("/rfdetr/train/jobs")
def create_rfdetr_training_job(payload: RfDetrTrainRequest):
    if not payload.accept_tos:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="rfdetr_tos_required")
    job_id = uuid.uuid4().hex
    run_dir = _rfdetr_run_dir(job_id, create=True)
    dataset_info = _resolve_rfdetr_training_dataset(payload)
    config = payload.dict(exclude_none=True)
    config["paths"] = {"run_dir": str(run_dir)}
    config["dataset"] = dataset_info
    message = "Queued (training not started)"
    status = "queued"
    job = RfDetrTrainingJob(job_id=job_id, config=config, message=message, status=status)
    with RFDETR_TRAINING_JOBS_LOCK:
        RFDETR_TRAINING_JOBS[job_id] = job
        _rfdetr_job_log(job, job.message)
    _rfdetr_write_run_meta(
        run_dir,
        {
            "job_id": job_id,
            "status": job.status,
            "message": job.message,
            "config": job.config,
            "created_at": job.created_at,
            "updated_at": job.updated_at,
        },
    )
    _start_rfdetr_training_worker(job)
    return {"job_id": job_id}


@app.get("/rfdetr/train/jobs")
def list_rfdetr_training_jobs():
    _prune_job_registry(RFDETR_TRAINING_JOBS, RFDETR_TRAINING_JOBS_LOCK)
    with RFDETR_TRAINING_JOBS_LOCK:
        jobs = sorted(RFDETR_TRAINING_JOBS.values(), key=lambda job: job.created_at, reverse=True)
        return [_serialize_rfdetr_job(job) for job in jobs]


@app.get("/rfdetr/train/jobs/{job_id}")
def get_rfdetr_training_job(job_id: str):
    job = _get_rfdetr_job(job_id)
    return _serialize_rfdetr_job(job)


@app.post("/rfdetr/train/jobs/{job_id}/cancel")
def cancel_rfdetr_training_job(job_id: str):
    job = _get_rfdetr_job(job_id)
    with RFDETR_TRAINING_JOBS_LOCK:
        if job.status in {"succeeded", "failed", "cancelled"}:
            raise HTTPException(status_code=HTTP_428_PRECONDITION_REQUIRED, detail="job_not_cancellable")
        if job.cancel_event.is_set():
            return {"status": job.status}
        job.cancel_event.set()
        next_status = job.status if job.status not in {"running", "queued"} else "cancelled"
        _rfdetr_job_update(job, status=next_status, message="Cancellation requested ...")
        run_dir = _rfdetr_run_dir(job.job_id, create=False)
        _rfdetr_write_run_meta(
            run_dir,
            {
                "job_id": job.job_id,
                "status": job.status,
                "message": job.message,
                "config": job.config,
                "created_at": job.created_at,
                "updated_at": job.updated_at,
            },
        )
    return {"status": job.status}


@app.get("/rfdetr/variants")
def list_rfdetr_variants(task: Optional[str] = Query(None)):
    if task:
        task_norm = task.strip().lower()
        return [v for v in RFDETR_VARIANTS if v.get("task") == task_norm]
    return RFDETR_VARIANTS


@app.get("/rfdetr/runs")
def list_rfdetr_runs():
    return _list_rfdetr_runs()


@app.get("/rfdetr/active")
def get_rfdetr_active():
    return _load_rfdetr_active()


@app.post("/rfdetr/active")
def set_rfdetr_active(payload: RfDetrActiveRequest):
    run_dir = _rfdetr_run_dir(payload.run_id, create=False)
    if not run_dir.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="rfdetr_run_not_found")
    best_path = _rfdetr_best_checkpoint(run_dir)
    if not best_path:
        raise HTTPException(status_code=HTTP_412_PRECONDITION_FAILED, detail="rfdetr_best_missing")
    meta = _rfdetr_load_run_meta(run_dir)
    config = meta.get("config") or {}
    dataset = config.get("dataset") or {}
    active_payload = {
        "run_id": payload.run_id,
        "run_name": config.get("run_name") or dataset.get("label") or payload.run_id,
        "best_path": best_path,
        "labelmap_path": str(run_dir / "labelmap.txt") if (run_dir / "labelmap.txt").exists() else None,
        "task": config.get("task") or dataset.get("task"),
        "variant": config.get("variant"),
    }
    return _save_rfdetr_active(active_payload)


@app.get("/detectors/default")
def get_default_detector():
    return _load_detector_default()


class DetectorDefaultRequest(BaseModel):
    mode: str


@app.post("/detectors/default")
def set_default_detector(payload: DetectorDefaultRequest):
    data = {"mode": payload.mode}
    return _save_detector_default(data)


@app.get("/rfdetr/runs/{run_id}/download")
def download_rfdetr_run(run_id: str):
    run_dir = _rfdetr_run_dir(run_id, create=False)
    if not run_dir.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="rfdetr_run_not_found")
    meta = _rfdetr_load_run_meta(run_dir)
    run_name = meta.get("config", {}).get("run_name") or meta.get("job_id") or run_id
    safe_name = _sanitize_yolo_run_id(run_name)
    buffer = io.BytesIO()
    with zipfile.ZipFile(buffer, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        for filename in sorted(RFDETR_KEEP_FILES):
            path = run_dir / filename
            if path.exists():
                zf.write(path, arcname=filename)
    buffer.seek(0)
    headers = {"Content-Disposition": f'attachment; filename="{safe_name}.zip"'}
    return StreamingResponse(buffer, media_type="application/zip", headers=headers)


@app.get("/yolo/runs/{run_id}/summary")
def yolo_run_summary(run_id: str):
    run_dir = _yolo_run_dir(run_id, create=False)
    if not run_dir.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="yolo_run_not_found")
    meta = _yolo_load_run_meta(run_dir)
    config = meta.get("config") or {}
    dataset = config.get("dataset") or {}
    run_name = config.get("run_name") or dataset.get("label") or dataset.get("id") or run_id
    labelmap = _read_labelmap_lines(run_dir / "labelmap.txt")
    metrics = _clean_metric_summary(_yolo_metrics_summary(run_dir))
    return {
        "run_id": run_id,
        "run_name": run_name,
        "dataset_label": dataset.get("label"),
        "dataset_id": dataset.get("id") or dataset.get("dataset_id"),
        "task": config.get("task"),
        "variant": config.get("variant"),
        "labelmap": labelmap,
        "metrics": metrics,
    }


@app.get("/rfdetr/runs/{run_id}/summary")
def rfdetr_run_summary(run_id: str):
    run_dir = _rfdetr_run_dir(run_id, create=False)
    if not run_dir.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="rfdetr_run_not_found")
    meta = _rfdetr_load_run_meta(run_dir)
    config = meta.get("config") or {}
    dataset = config.get("dataset") or {}
    run_name = config.get("run_name") or dataset.get("label") or dataset.get("id") or run_id
    labelmap = _read_labelmap_lines(run_dir / "labelmap.txt")
    metrics = _clean_metric_summary(_rfdetr_metrics_summary(run_dir))
    return {
        "run_id": run_id,
        "run_name": run_name,
        "dataset_label": dataset.get("label"),
        "dataset_id": dataset.get("id") or dataset.get("dataset_id"),
        "task": config.get("task"),
        "variant": config.get("variant"),
        "labelmap": labelmap,
        "metrics": metrics,
    }

@app.delete("/rfdetr/runs/{run_id}")
def delete_rfdetr_run(run_id: str):
    run_dir = _rfdetr_run_dir(run_id, create=False)
    if not run_dir.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="rfdetr_run_not_found")
    try:
        shutil.rmtree(run_dir)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=str(exc))
    return {"status": "deleted", "run_id": run_id}


@app.get("/yolo/variants")
def list_yolo_variants(task: Optional[str] = Query(None)):
    if task:
        task_norm = task.strip().lower()
        return [v for v in YOLO_VARIANTS if v.get("task") == task_norm]
    return YOLO_VARIANTS


@app.get("/yolo/runs")
def list_yolo_runs():
    return _list_yolo_runs()


@app.get("/yolo/active")
def get_yolo_active():
    return _load_yolo_active()


@app.post("/yolo/active")
def set_yolo_active(payload: YoloActiveRequest):
    run_dir = _yolo_run_dir(payload.run_id, create=False)
    if not run_dir.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="yolo_run_not_found")
    best_path = run_dir / "best.pt"
    if not best_path.exists():
        raise HTTPException(status_code=HTTP_412_PRECONDITION_FAILED, detail="yolo_best_missing")
    meta = _yolo_load_run_meta(run_dir)
    config = meta.get("config") or {}
    dataset = config.get("dataset") or {}
    active_payload = {
        "run_id": payload.run_id,
        "run_name": config.get("run_name") or dataset.get("label") or payload.run_id,
        "best_path": str(best_path),
        "labelmap_path": str(run_dir / "labelmap.txt") if (run_dir / "labelmap.txt").exists() else None,
        "task": config.get("task"),
        "variant": config.get("variant"),
    }
    return _save_yolo_active(active_payload)


@app.post("/yolo/predict_region", response_model=YoloRegionResponse)
def yolo_predict_region(payload: YoloRegionRequest):
    model, labelmap, task = _ensure_yolo_inference_runtime()
    task_name = str(task).lower() if task else None
    if not task_name:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="yolo_task_unknown")
    if "segment" in task_name:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="yolo_region_detect_requires_bbox")
    pil_img, _np_img, _token = _resolve_detector_image(payload.image_base64, payload.image_token)
    img_w, img_h = pil_img.size
    full_w = int(payload.full_width) if payload.full_width else img_w
    full_h = int(payload.full_height) if payload.full_height else img_h
    x, y, w, h = [float(v) for v in payload.region[:4]]
    left = max(0.0, min(full_w, x))
    top = max(0.0, min(full_h, y))
    right = max(left + 1.0, min(full_w, x + w))
    bottom = max(top + 1.0, min(full_h, y + h))
    warnings: List[str] = []
    if payload.image_is_cropped:
        if payload.full_width is None or payload.full_height is None:
            warnings.append("full_size_missing")
        if abs((right - left) - img_w) > 0.5 or abs((bottom - top) - img_h) > 0.5:
            warnings.append("region_crop_mismatch")
        right = min(full_w, left + img_w)
        bottom = min(full_h, top + img_h)
        crop = pil_img
    else:
        crop = pil_img.crop((left, top, right, bottom))
    conf = float(payload.conf) if payload.conf is not None else 0.25
    iou = float(payload.iou) if payload.iou is not None else 0.45
    max_det = int(payload.max_det) if payload.max_det is not None else 300
    if conf < 0 or conf > 1:
        conf = min(1.0, max(0.0, conf))
        warnings.append("conf_clamped")
    if iou < 0 or iou > 1:
        iou = min(1.0, max(0.0, iou))
        warnings.append("iou_clamped")
    if max_det < 1:
        max_det = 1
        warnings.append("max_det_clamped")
    if max_det > 5000:
        max_det = 5000
        warnings.append("max_det_clamped")
    expected = payload.expected_labelmap or []
    if expected and not labelmap:
        warnings.append("labelmap_missing")
    elif expected and labelmap and expected != labelmap:
        warnings.append("labelmap_mismatch")
    with YOLO_INFER_LOCK:
        with YOLO_INFER_LOCK:
            results = model.predict(crop, conf=conf, iou=iou, max_det=max_det, verbose=False)
    detections: List[YoloRegionDetection] = []
    if results:
        det_boxes = results[0].boxes
        if det_boxes is not None and det_boxes.xyxy is not None:
            xyxy = det_boxes.xyxy.cpu().numpy()
            confs = det_boxes.conf.cpu().numpy() if det_boxes.conf is not None else None
            classes = det_boxes.cls.cpu().numpy() if det_boxes.cls is not None else None
            for idx, box in enumerate(xyxy):
                x1, y1, x2, y2 = [float(v) for v in box[:4]]
                cx = (x1 + x2) / 2 + left
                cy = (y1 + y2) / 2 + top
                if payload.center_only and not (left <= cx <= right and top <= cy <= bottom):
                    continue
                abs_x = max(0.0, min(full_w, x1 + left))
                abs_y = max(0.0, min(full_h, y1 + top))
                abs_w = max(0.0, min(full_w - abs_x, (x2 - x1)))
                abs_h = max(0.0, min(full_h - abs_y, (y2 - y1)))
                class_id = int(classes[idx]) if classes is not None else -1
                class_name = None
                if class_id >= 0 and class_id < len(labelmap):
                    class_name = labelmap[class_id]
                score = float(confs[idx]) if confs is not None else None
                detections.append(
                    YoloRegionDetection(
                        bbox=[abs_x, abs_y, abs_w, abs_h],
                        class_id=class_id,
                        class_name=class_name,
                        score=score,
                    )
                )
    return YoloRegionResponse(detections=detections, labelmap=labelmap, warnings=warnings or None)


@app.post("/rfdetr/predict_region", response_model=RfDetrRegionResponse)
def rfdetr_predict_region(payload: RfDetrRegionRequest):
    model, labelmap, task = _ensure_rfdetr_inference_runtime()
    task_name = str(task).lower() if task else None
    if not task_name:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="rfdetr_task_unknown")
    if "segment" in task_name:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="rfdetr_region_detect_requires_bbox")
    pil_img, _np_img, _token = _resolve_detector_image(payload.image_base64, payload.image_token)
    img_w, img_h = pil_img.size
    full_w = int(payload.full_width) if payload.full_width else img_w
    full_h = int(payload.full_height) if payload.full_height else img_h
    x, y, w, h = [float(v) for v in payload.region[:4]]
    left = max(0.0, min(full_w, x))
    top = max(0.0, min(full_h, y))
    right = max(left + 1.0, min(full_w, x + w))
    bottom = max(top + 1.0, min(full_h, y + h))
    warnings: List[str] = []
    if payload.image_is_cropped:
        if payload.full_width is None or payload.full_height is None:
            warnings.append("full_size_missing")
        if abs((right - left) - img_w) > 0.5 or abs((bottom - top) - img_h) > 0.5:
            warnings.append("region_crop_mismatch")
        right = min(full_w, left + img_w)
        bottom = min(full_h, top + img_h)
        crop = pil_img
    else:
        crop = pil_img.crop((left, top, right, bottom))
    conf = float(payload.conf) if payload.conf is not None else 0.25
    max_det = int(payload.max_det) if payload.max_det is not None else 300
    if conf < 0 or conf > 1:
        conf = min(1.0, max(0.0, conf))
        warnings.append("conf_clamped")
    if max_det < 1:
        max_det = 1
        warnings.append("max_det_clamped")
    if max_det > 5000:
        max_det = 5000
        warnings.append("max_det_clamped")
    expected = payload.expected_labelmap or []
    if expected and not labelmap:
        warnings.append("labelmap_missing")
    elif expected and labelmap and expected != labelmap:
        warnings.append("labelmap_mismatch")
    detections: List[RfDetrRegionDetection] = []
    try:
        with RFDETR_INFER_LOCK:
            results = model.predict(crop, threshold=conf)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"rfdetr_predict_failed:{exc}") from exc
    if results is not None:
        xyxy = getattr(results, "xyxy", None)
        scores = getattr(results, "confidence", None)
        class_ids = getattr(results, "class_id", None)
        if xyxy is not None and len(xyxy):
            raw_entries: List[Tuple[Optional[float], RfDetrRegionDetection]] = []
            labelmap_shifted = False
            for idx, box in enumerate(xyxy):
                x1, y1, x2, y2 = [float(v) for v in box[:4]]
                cx = (x1 + x2) / 2 + left
                cy = (y1 + y2) / 2 + top
                if payload.center_only and not (left <= cx <= right and top <= cy <= bottom):
                    continue
                abs_x = max(0.0, min(full_w, x1 + left))
                abs_y = max(0.0, min(full_h, y1 + top))
                abs_w = max(0.0, min(full_w - abs_x, (x2 - x1)))
                abs_h = max(0.0, min(full_h - abs_y, (y2 - y1)))
                class_id = int(class_ids[idx]) if class_ids is not None else -1
                if labelmap and class_id >= len(labelmap) and 0 <= class_id - 1 < len(labelmap):
                    class_id -= 1
                    labelmap_shifted = True
                class_name = None
                if class_id >= 0 and class_id < len(labelmap):
                    class_name = labelmap[class_id]
                score = float(scores[idx]) if scores is not None else None
                raw_entries.append(
                    (
                        score,
                        RfDetrRegionDetection(
                            bbox=[abs_x, abs_y, abs_w, abs_h],
                            class_id=class_id,
                            class_name=class_name,
                            score=score,
                        ),
                    )
                )
            if raw_entries:
                if any(score is not None for score, _ in raw_entries):
                    raw_entries.sort(key=lambda item: item[0] if item[0] is not None else -1.0, reverse=True)
                detections = [entry for _, entry in raw_entries[:max_det]]
            if labelmap_shifted:
                warnings.append("labelmap_shifted")
    return RfDetrRegionResponse(detections=detections, labelmap=labelmap, warnings=warnings or None)


def _yolo_extract_detections(
    results: Any,
    labelmap: List[str],
    offset_x: float,
    offset_y: float,
    full_w: int,
    full_h: int,
) -> List[Dict[str, Any]]:
    detections: List[Dict[str, Any]] = []
    if not results:
        return detections
    det_boxes = results[0].boxes if results else None
    if det_boxes is None or det_boxes.xyxy is None:
        return detections
    xyxy = det_boxes.xyxy.cpu().numpy()
    confs = det_boxes.conf.cpu().numpy() if det_boxes.conf is not None else None
    classes = det_boxes.cls.cpu().numpy() if det_boxes.cls is not None else None
    for idx, box in enumerate(xyxy):
        x1, y1, x2, y2 = [float(v) for v in box[:4]]
        abs_x = max(0.0, min(full_w, x1 + offset_x))
        abs_y = max(0.0, min(full_h, y1 + offset_y))
        abs_w = max(0.0, min(full_w - abs_x, (x2 - x1)))
        abs_h = max(0.0, min(full_h - abs_y, (y2 - y1)))
        class_id = int(classes[idx]) if classes is not None else -1
        class_name = labelmap[class_id] if 0 <= class_id < len(labelmap) else None
        score = float(confs[idx]) if confs is not None else None
        detections.append(
            {
                "bbox": [abs_x, abs_y, abs_w, abs_h],
                "class_id": class_id,
                "class_name": class_name,
                "score": score,
            }
        )
    return detections


def _rfdetr_extract_detections(
    results: Any,
    labelmap: List[str],
    offset_x: float,
    offset_y: float,
    full_w: int,
    full_h: int,
) -> Tuple[List[Dict[str, Any]], bool]:
    detections: List[Dict[str, Any]] = []
    labelmap_shifted = False
    if results is None:
        return detections, labelmap_shifted
    xyxy = getattr(results, "xyxy", None)
    scores = getattr(results, "confidence", None)
    class_ids = getattr(results, "class_id", None)
    if xyxy is None or not len(xyxy):
        return detections, labelmap_shifted
    for idx, box in enumerate(xyxy):
        x1, y1, x2, y2 = [float(v) for v in box[:4]]
        abs_x = max(0.0, min(full_w, x1 + offset_x))
        abs_y = max(0.0, min(full_h, y1 + offset_y))
        abs_w = max(0.0, min(full_w - abs_x, (x2 - x1)))
        abs_h = max(0.0, min(full_h - abs_y, (y2 - y1)))
        class_id = int(class_ids[idx]) if class_ids is not None else -1
        if labelmap and class_id >= len(labelmap) and 0 <= class_id - 1 < len(labelmap):
            class_id -= 1
            labelmap_shifted = True
        class_name = labelmap[class_id] if 0 <= class_id < len(labelmap) else None
        score = float(scores[idx]) if scores is not None else None
        detections.append(
            {
                "bbox": [abs_x, abs_y, abs_w, abs_h],
                "class_id": class_id,
                "class_name": class_name,
                "score": score,
            }
        )
    return detections, labelmap_shifted


def _resolve_detector_image(
    image_base64: Optional[str],
    image_token: Optional[str],
) -> Tuple[Image.Image, np.ndarray, str]:
    if image_token:
        for variant in ("sam1", "sam3"):
            cached = _fetch_preloaded_image(image_token, variant)
            if cached is not None:
                pil_img = Image.fromarray(cached)
                return pil_img, cached, image_token
        if image_base64:
            pil_img, np_img = _decode_image_base64(image_base64)
            token = hashlib.md5(np_img.tobytes()).hexdigest()
            _store_preloaded_image(token, np_img, "sam1")
            return pil_img, np_img, token
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="image_token_not_found")
    pil_img, np_img = _decode_image_base64(image_base64)
    token = hashlib.md5(np_img.tobytes()).hexdigest()
    _store_preloaded_image(token, np_img, "sam1")
    return pil_img, np_img, token


@app.post("/yolo/predict_full", response_model=YoloRegionResponse)
def yolo_predict_full(payload: YoloFullRequest):
    model, labelmap, task = _ensure_yolo_inference_runtime()
    task_name = str(task).lower() if task else None
    if not task_name:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="yolo_task_unknown")
    if "segment" in task_name:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="yolo_full_detect_requires_bbox")
    pil_img, _np_img, _token = _resolve_detector_image(payload.image_base64, payload.image_token)
    img_w, img_h = pil_img.size
    warnings: List[str] = []
    conf = _clamp_conf_value(float(payload.conf) if payload.conf is not None else 0.25, warnings)
    iou = _clamp_iou_value(float(payload.iou) if payload.iou is not None else 0.45, warnings)
    max_det = _clamp_max_det_value(int(payload.max_det) if payload.max_det is not None else 300, warnings)
    _apply_expected_labelmap_warnings(payload.expected_labelmap, labelmap, warnings)
    with YOLO_INFER_LOCK:
        results = model.predict(pil_img, conf=conf, iou=iou, max_det=max_det, verbose=False)
    raw = _yolo_extract_detections(results, labelmap, 0.0, 0.0, img_w, img_h)
    detections = [YoloRegionDetection(**item) for item in raw]
    return YoloRegionResponse(detections=detections, labelmap=labelmap, warnings=warnings or None)


@app.post("/yolo/predict_windowed", response_model=YoloRegionResponse)
def yolo_predict_windowed(payload: YoloWindowedRequest):
    model, labelmap, task = _ensure_yolo_inference_runtime()
    task_name = str(task).lower() if task else None
    if not task_name:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="yolo_task_unknown")
    if "segment" in task_name:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="yolo_windowed_detect_requires_bbox")
    pil_img, _np_img, _token = _resolve_detector_image(payload.image_base64, payload.image_token)
    img_w, img_h = pil_img.size
    warnings: List[str] = []
    conf = _clamp_conf_value(float(payload.conf) if payload.conf is not None else 0.25, warnings)
    iou = _clamp_iou_value(float(payload.iou) if payload.iou is not None else 0.45, warnings)
    max_det = _clamp_max_det_value(int(payload.max_det) if payload.max_det is not None else 300, warnings)
    slice_size = int(payload.slice_size) if payload.slice_size is not None else 640
    overlap = float(payload.overlap) if payload.overlap is not None else 0.2
    merge_iou = float(payload.merge_iou) if payload.merge_iou is not None else 0.5
    slice_size, overlap, merge_iou = _clamp_slice_params(slice_size, overlap, merge_iou, img_w, img_h, warnings)
    _apply_expected_labelmap_warnings(payload.expected_labelmap, labelmap, warnings)
    slices, starts = _slice_image_sahi(pil_img, slice_size, overlap)
    raw_detections: List[Dict[str, Any]] = []
    for tile, start in zip(slices, starts):
        offset_x, offset_y = float(start[0]), float(start[1])
        crop = Image.fromarray(tile)
        results = model.predict(crop, conf=conf, iou=iou, max_det=max_det, verbose=False)
        raw_detections.extend(_yolo_extract_detections(results, labelmap, offset_x, offset_y, img_w, img_h))
    merged = _merge_detections_nms(raw_detections, merge_iou, max_det)
    detections = [YoloRegionDetection(**item) for item in merged]
    return YoloRegionResponse(detections=detections, labelmap=labelmap, warnings=warnings or None)


@app.post("/rfdetr/predict_full", response_model=RfDetrRegionResponse)
def rfdetr_predict_full(payload: RfDetrFullRequest):
    model, labelmap, task = _ensure_rfdetr_inference_runtime()
    task_name = str(task).lower() if task else None
    if not task_name:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="rfdetr_task_unknown")
    if "segment" in task_name:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="rfdetr_full_detect_requires_bbox")
    pil_img, _np_img, _token = _resolve_detector_image(payload.image_base64, payload.image_token)
    img_w, img_h = pil_img.size
    warnings: List[str] = []
    conf = _clamp_conf_value(float(payload.conf) if payload.conf is not None else 0.25, warnings)
    max_det = _clamp_max_det_value(int(payload.max_det) if payload.max_det is not None else 300, warnings)
    _apply_expected_labelmap_warnings(payload.expected_labelmap, labelmap, warnings)
    try:
        with RFDETR_INFER_LOCK:
            results = model.predict(pil_img, threshold=conf)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"rfdetr_predict_failed:{exc}") from exc
    raw, labelmap_shifted = _rfdetr_extract_detections(results, labelmap, 0.0, 0.0, img_w, img_h)
    raw.sort(key=lambda det: float(det.get("score") or 0.0), reverse=True)
    detections = [RfDetrRegionDetection(**item) for item in raw[:max_det]]
    if labelmap_shifted:
        warnings.append("labelmap_shifted")
    return RfDetrRegionResponse(detections=detections, labelmap=labelmap, warnings=warnings or None)


@app.post("/rfdetr/predict_windowed", response_model=RfDetrRegionResponse)
def rfdetr_predict_windowed(payload: RfDetrWindowedRequest):
    model, labelmap, task = _ensure_rfdetr_inference_runtime()
    task_name = str(task).lower() if task else None
    if not task_name:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="rfdetr_task_unknown")
    if "segment" in task_name:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="rfdetr_windowed_detect_requires_bbox")
    pil_img, _np_img, _token = _resolve_detector_image(payload.image_base64, payload.image_token)
    img_w, img_h = pil_img.size
    warnings: List[str] = []
    conf = _clamp_conf_value(float(payload.conf) if payload.conf is not None else 0.25, warnings)
    max_det = _clamp_max_det_value(int(payload.max_det) if payload.max_det is not None else 300, warnings)
    slice_size = int(payload.slice_size) if payload.slice_size is not None else 640
    overlap = float(payload.overlap) if payload.overlap is not None else 0.2
    merge_iou = float(payload.merge_iou) if payload.merge_iou is not None else 0.5
    slice_size, overlap, merge_iou = _clamp_slice_params(slice_size, overlap, merge_iou, img_w, img_h, warnings)
    _apply_expected_labelmap_warnings(payload.expected_labelmap, labelmap, warnings)
    slices, starts = _slice_image_sahi(pil_img, slice_size, overlap)
    raw_detections: List[Dict[str, Any]] = []
    labelmap_shifted = False
    for tile, start in zip(slices, starts):
        offset_x, offset_y = float(start[0]), float(start[1])
        crop = Image.fromarray(tile)
        try:
            with RFDETR_INFER_LOCK:
                results = model.predict(crop, threshold=conf)
        except Exception as exc:  # noqa: BLE001
            raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"rfdetr_predict_failed:{exc}") from exc
        extracted, shifted = _rfdetr_extract_detections(results, labelmap, offset_x, offset_y, img_w, img_h)
        labelmap_shifted = labelmap_shifted or shifted
        raw_detections.extend(extracted)
    merged = _merge_detections_nms(raw_detections, merge_iou, max_det)
    detections = [RfDetrRegionDetection(**item) for item in merged]
    if labelmap_shifted:
        warnings.append("labelmap_shifted")
    return RfDetrRegionResponse(detections=detections, labelmap=labelmap, warnings=warnings or None)


@app.get("/yolo/runs/{run_id}/download")
def download_yolo_run(run_id: str):
    run_dir = _yolo_run_dir(run_id, create=False)
    if not run_dir.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="yolo_run_not_found")
    meta = _yolo_load_run_meta(run_dir)
    run_name = meta.get("config", {}).get("run_name") or meta.get("job_id") or run_id
    safe_name = _sanitize_yolo_run_id(run_name)
    buffer = io.BytesIO()
    with zipfile.ZipFile(buffer, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        for filename in sorted(YOLO_KEEP_FILES):
            path = run_dir / filename
            if path.exists():
                zf.write(path, arcname=filename)
    buffer.seek(0)
    headers = {"Content-Disposition": f'attachment; filename="{safe_name}.zip"'}
    return StreamingResponse(buffer, media_type="application/zip", headers=headers)


@app.delete("/yolo/runs/{run_id}")
def delete_yolo_run(run_id: str):
    run_dir = _yolo_run_dir(run_id, create=False)
    if not run_dir.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="yolo_run_not_found")
    try:
        shutil.rmtree(run_dir)
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=str(exc))
    return {"status": "deleted", "run_id": run_id}


@app.get("/sam3/train/cache_size")
def sam3_train_cache_size():
    cache_root = SAM3_JOB_ROOT / "splits"
    return {"bytes": _dir_size_bytes(cache_root)}


@app.post("/sam3/train/cache/purge")
def sam3_train_cache_purge():
    cache_root = SAM3_JOB_ROOT / "splits"
    deleted = _purge_directory(cache_root)
    return {"status": "ok", "deleted_bytes": deleted}


@app.get("/sam3/storage/runs")
def list_sam3_runs(variant: str = Query("sam3")):
    # SAM3-lite removed; always use sam3
    return _list_sam3_runs("sam3")


@app.delete("/sam3/storage/runs/{run_id}")
def delete_sam3_run(run_id: str, variant: str = Query("sam3"), scope: str = Query("all")):
    normalized = "sam3"
    if scope not in SAM3_STORAGE_SCOPES:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="invalid_scope")
    run_dir = _run_dir_for_request(run_id, normalized)
    active_paths = _active_run_paths_for_variant(normalized)
    if run_dir.resolve() in active_paths:
        raise HTTPException(status_code=HTTP_409_CONFLICT, detail="sam3_run_active")
    deleted, freed = _delete_run_scope(run_dir, scope)
    return {"deleted": deleted, "freed_bytes": freed}


@app.post("/sam3/storage/runs/{run_id}/promote")
def promote_sam3_run(run_id: str, variant: str = Query("sam3")):
    return _promote_run(run_id, "sam3")


@app.get("/sam3/models/available")
def list_sam3_available_models(
    variant: str = Query("sam3"),
    promoted_only: bool = Query(False),
):
    """List run checkpoints for prompt model selection."""
    runs = _list_sam3_runs("sam3")
    models: List[Dict[str, Any]] = []
    # Always expose the base/active env model if available
    # Env/base model entry (always listed)
    env_base_path = SAM3_CHECKPOINT_PATH if SAM3_CHECKPOINT_PATH else None
    models.append(
        {
            "id": "Base SAM3",
            "key": "base",
            # Use None so activation loads from HF if no local checkpoint is present.
            "path": env_base_path,
            "size_bytes": None,
            "promoted": False,
            "active": active_sam3_checkpoint in {None, env_base_path},
            "variant": "sam3",
            "run_path": None,
            "source": "env",
        }
    )
    # Current active model entry (if different from env/base)
    if active_sam3_checkpoint and active_sam3_checkpoint != env_base_path:
        models.append(
            {
                "id": active_sam3_metadata.get("label") or active_sam3_metadata.get("id") or "active",
                "key": f"active:{active_sam3_checkpoint}",
                "path": active_sam3_checkpoint,
                "size_bytes": None,
                "promoted": False,
                "active": True,
                "variant": "sam3",
                "run_path": None,
                "source": active_sam3_metadata.get("source") or "env",
            }
        )
    for run in runs:
        if promoted_only and not run.get("promoted"):
            continue
        if run.get("active"):
            # allow listing active too, but mark status
            pass
        ckpts = run.get("checkpoints") or []
        if not ckpts:
            continue
        # prefer last.ckpt
        chosen = None
        for ck in ckpts:
            if ck.get("file") == "last.ckpt":
                chosen = ck
                break
        if chosen is None:
            chosen = ckpts[0]
        models.append(
            {
                "id": run.get("id"),
                "path": chosen.get("path"),
                "size_bytes": chosen.get("size_bytes"),
                "promoted": run.get("promoted", False),
                "active": run.get("active", False),
                "variant": run.get("variant"),
                "run_path": run.get("path"),
            }
        )
    return models

@app.post("/sam3/models/activate")
def activate_sam3_model(payload: Sam3ModelActivateRequest):
    global active_sam3_checkpoint, active_sam3_model_id, active_sam3_metadata, active_sam3_enable_segmentation
    checkpoint_path = payload.checkpoint_path
    source = "huggingface"
    resolved_path: Optional[Path] = None
    if checkpoint_path:
        resolved_path = Path(checkpoint_path).resolve()
        if not resolved_path.exists():
            raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="sam3_checkpoint_not_found")
        checkpoint_path = str(resolved_path)
        source = "custom"
    enable_seg = active_sam3_enable_segmentation if checkpoint_path is not None else True
    if payload.enable_segmentation is not None:
        enable_seg = bool(payload.enable_segmentation)
    active_sam3_checkpoint = checkpoint_path
    active_sam3_enable_segmentation = enable_seg
    active_sam3_model_id = payload.label or (resolved_path.stem if resolved_path else "facebook/sam3")
    active_sam3_metadata = {
        "id": active_sam3_model_id,
        "label": payload.label or active_sam3_model_id,
        "checkpoint": active_sam3_checkpoint,
        "source": source,
        "enable_segmentation": active_sam3_enable_segmentation,
    }
    _reset_sam3_runtime()
    return {"active": active_sam3_metadata}


@app.post("/qwen/train/jobs")
def create_qwen_training_job(payload: QwenTrainRequest):
    if QWEN_TRAINING_IMPORT_ERROR is not None or train_qwen_model is None:
        raise HTTPException(
            status_code=HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"qwen_training_unavailable:{QWEN_TRAINING_IMPORT_ERROR}",
        )
    if not payload.dataset_id:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="dataset_id_required")
    job_id = uuid.uuid4().hex
    prep_logs: List[str] = []
    config = _build_qwen_config(payload, job_id, prep_logs)
    config_dict = asdict(config)
    job = QwenTrainingJob(job_id=job_id, config=config_dict)
    logger.info(
        "[qwen-train %s] create job accelerator=%s devices=%s dataset=%s",
        job_id[:8],
        getattr(payload, "accelerator", None) or config_dict.get("accelerator"),
        getattr(payload, "devices", None) or config_dict.get("devices"),
        payload.dataset_id,
    )
    with QWEN_TRAINING_JOBS_LOCK:
        QWEN_TRAINING_JOBS[job_id] = job
        for msg in prep_logs:
            _qwen_job_log(job, msg)
        _qwen_job_log(job, "Job queued")
    _start_qwen_training_worker(job, config)
    return {"job_id": job_id}


@app.get("/qwen/train/jobs")
def list_qwen_training_jobs(request: Request):
    _prune_job_registry(QWEN_TRAINING_JOBS, QWEN_TRAINING_JOBS_LOCK)
    with QWEN_TRAINING_JOBS_LOCK:
        jobs = sorted(QWEN_TRAINING_JOBS.values(), key=lambda job: job.created_at, reverse=True)
        _log_qwen_get_request(str(request.url.path), jobs)
        return [_serialize_qwen_job(job) for job in jobs]


@app.get("/qwen/train/jobs/{job_id}")
def get_qwen_training_job(job_id: str, request: Request):
    job = _get_qwen_job(job_id)
    _log_qwen_get_request(str(request.url.path), [job])
    return _serialize_qwen_job(job)


@app.post("/qwen/train/jobs/{job_id}/cancel")
def cancel_qwen_training_job(job_id: str):
    job = _get_qwen_job(job_id)
    with QWEN_TRAINING_JOBS_LOCK:
        if job.status in {"succeeded", "failed", "cancelled"}:
            raise HTTPException(status_code=HTTP_428_PRECONDITION_REQUIRED, detail="job_not_cancellable")
        if job.cancel_event.is_set():
            return {"status": job.status}
        job.cancel_event.set()
        next_status = job.status if job.status not in {"running", "queued"} else "cancelling"
        _qwen_job_update(job, status=next_status, message="Cancellation requested ...")
        return {"status": next_status}


@app.get("/qwen/train/cache_size")
def qwen_train_cache_size():
    cache_root = QWEN_JOB_ROOT / "splits"
    return {"bytes": _dir_size_bytes(cache_root)}


@app.post("/qwen/train/cache/purge")
def qwen_train_cache_purge():
    cache_root = QWEN_JOB_ROOT / "splits"
    deleted = _purge_directory(cache_root)
    return {"status": "ok", "deleted_bytes": deleted}


@app.get("/qwen/models")
def list_qwen_models():
    default_entry = {
        "id": "default",
        "label": "Base Qwen 3",
        "type": "builtin",
        "metadata": _default_qwen_metadata(),
        "path": None,
        "created_at": None,
        "active": active_qwen_model_id == "default",
    }
    entries = _list_qwen_model_entries()
    data = [default_entry]
    for entry in entries:
        entry["active"] = entry.get("id") == active_qwen_model_id
        data.append(entry)
    return {
        "active": active_qwen_model_id,
        "models": data,
    }


@app.post("/qwen/models/activate")
def activate_qwen_model(payload: QwenModelActivateRequest):
    model_id = (payload.model_id or "").strip()
    if not model_id:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="model_id_required")
    if model_id == "default":
        _set_active_qwen_model_default()
    else:
        entry = _get_qwen_model_entry(model_id)
        if not entry:
            raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="qwen_model_not_found")
        latest = entry.get("path")
        if not latest or not Path(latest).exists():
            raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="qwen_model_missing_checkpoint")
        _set_active_qwen_model_custom(model_id, Path(latest), entry.get("metadata") or {})
    return {
        "active": active_qwen_model_id,
        "metadata": active_qwen_metadata,
    }


@app.get("/clip/active_model", response_model=ActiveModelResponse)
def get_active_model():
    return _current_active_payload()


@app.post("/clip/active_model", response_model=ActiveModelResponse)
def set_active_model(payload: ActiveModelRequest):
    global clf, clip_model, clip_preprocess, clip_model_name, clip_initialized
    global active_classifier_path, active_labelmap_path, active_label_list, clip_last_error
    global active_encoder_type, active_encoder_model
    global dinov3_model, dinov3_processor, dinov3_model_name, dinov3_initialized, dinov3_model_device
    global active_classifier_meta, active_head_normalize_embeddings, active_classifier_head

    classifier_path = _normalise_optional_path(payload.classifier_path) or active_classifier_path
    labelmap_path = _normalise_optional_path(payload.labelmap_path)
    labelmap_provided = "labelmap_path" in payload.__fields_set__
    requested_clip_model = _normalise_optional_path(payload.clip_model)

    if not classifier_path:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="classifier_required")
    classifier_path_abs = os.path.abspath(classifier_path)
    if not os.path.isfile(classifier_path_abs):
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="classifier_not_found")
    allowed_root = (UPLOAD_ROOT / "classifiers").resolve()
    if not str(Path(classifier_path_abs).resolve()).startswith(str(allowed_root)):
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="classifier_path_not_allowed")
    _validate_upload_extension(classifier_path_abs, CLASSIFIER_ALLOWED_EXTS, "classifier_extension_not_allowed")

    try:
        new_clf = joblib.load(classifier_path_abs)
    except Exception as exc:  # noqa: BLE001
        clip_last_error = str(exc)
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"classifier_load_failed:{exc}") from exc

    meta_clip_model = None
    meta_encoder_type = "clip"
    meta_encoder_model = None
    meta_found = False
    meta_obj: Optional[Dict[str, Any]] = None
    meta_path = os.path.splitext(classifier_path_abs)[0] + ".meta.pkl"
    if os.path.exists(meta_path):
        try:
            meta_candidate = joblib.load(meta_path)
            if isinstance(meta_candidate, dict):
                meta_obj = meta_candidate
                meta_found = True
                meta_clip_model = meta_obj.get("clip_model")
                meta_encoder_type = meta_obj.get("encoder_type") or "clip"
                meta_encoder_model = meta_obj.get("encoder_model") or meta_clip_model
        except Exception:
            meta_clip_model = None
            meta_encoder_type = "clip"
            meta_encoder_model = None
            meta_found = False
    encoder_type_norm = str(meta_encoder_type or "clip").strip().lower()
    encoder_model_norm = str(meta_encoder_model or "").strip() or (str(meta_clip_model).strip() if meta_clip_model else "")

    embed_dim = None
    try:
        if isinstance(new_clf, dict):
            embed_dim = new_clf.get("embedding_dim")
            if embed_dim is None:
                layers = new_clf.get("layers")
                if isinstance(layers, list) and layers:
                    weight = layers[0].get("weight")
                    if weight is not None and hasattr(weight, "shape"):
                        embed_dim = weight.shape[1]
        else:
            coef = getattr(new_clf, "coef_", None)
            if coef is not None:
                embed_dim = coef.shape[1]
    except Exception:
        embed_dim = None
    new_clip_model = None
    new_preprocess = None
    new_dinov3_model = None
    new_dinov3_processor = None
    encoder_model_for_active = None

    if not meta_found:
        if isinstance(new_clf, dict) and str(new_clf.get("classifier_type") or new_clf.get("head_type") or "") == "mlp":
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="classifier_meta_required")
        if embed_dim is not None and int(embed_dim) not in {512, 768}:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="classifier_meta_required")
        if encoder_type_norm != "clip":
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="classifier_meta_required")

    if encoder_type_norm == "clip":
        clip_name = requested_clip_model or str(meta_clip_model or "").strip() or clip_model_name or DEFAULT_CLIP_MODEL
        inferred = _infer_clip_model_from_embedding_dim(embed_dim, active_name=clip_model_name or DEFAULT_CLIP_MODEL)
        if inferred and inferred != clip_name and not requested_clip_model:
            clip_name = inferred
        if clip_name not in SUPPORTED_CLIP_MODELS:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="clip_model_not_allowed")
        need_new_clip = clip_model is None or clip_model_name != clip_name
        if need_new_clip:
            try:
                new_clip_model, new_preprocess = clip.load(clip_name, device=device)
            except Exception as exc:  # noqa: BLE001
                raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"clip_load_failed:{exc}") from exc
        else:
            new_clip_model = clip_model
            new_preprocess = clip_preprocess
        clip_dim = getattr(getattr(new_clip_model, "visual", None), "output_dim", None)
        if embed_dim is not None and clip_dim is not None and embed_dim != clip_dim:
            inferred = _infer_clip_model_from_embedding_dim(embed_dim, active_name=clip_name)
            if inferred and inferred != clip_name:
                try:
                    new_clip_model, new_preprocess = clip.load(inferred, device=device)
                except Exception as exc:  # noqa: BLE001
                    raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"clip_load_failed:{exc}") from exc
                clip_name = inferred
                clip_dim = getattr(getattr(new_clip_model, "visual", None), "output_dim", None)
                logger.warning(
                    "CLIP classifier embedding dim %s mismatched requested backbone; falling back to %s.",
                    embed_dim,
                    inferred,
                )
        if embed_dim is not None and clip_dim is not None and embed_dim != clip_dim:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"dimension_mismatch:{embed_dim}!={clip_dim}")
        encoder_model_for_active = clip_name
    elif encoder_type_norm == "dinov3":
        if not encoder_model_norm:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="encoder_model_required")
        try:
            target_device = _dinov3_resolve_device(device)
            new_dinov3_model, new_dinov3_processor = _load_dinov3_backbone(
                encoder_model_norm,
                target_device,
                raise_on_error=True,
            )
        except RuntimeError as exc:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"dinov3_load_failed:{exc}") from exc
        dino_dim = None
        try:
            cfg = getattr(new_dinov3_model, "config", None)
            dino_dim = getattr(cfg, "hidden_size", None) or getattr(cfg, "embed_dim", None)
        except Exception:
            dino_dim = None
        if embed_dim is not None and dino_dim is not None and int(embed_dim) != int(dino_dim):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=f"dimension_mismatch:{embed_dim}!={dino_dim}")
        encoder_model_for_active = encoder_model_norm
    else:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="clip_encoder_type_unsupported")

    labelmap_path_abs = None
    labelmap_entries: List[str] = []
    if labelmap_path is not None:
        labelmap_path_abs = os.path.abspath(labelmap_path)
        allowed_label_roots = [
            (UPLOAD_ROOT / "labelmaps").resolve(),
            (UPLOAD_ROOT / "classifiers").resolve(),
        ]
        if not any(str(Path(labelmap_path_abs).resolve()).startswith(str(root)) for root in allowed_label_roots):
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="labelmap_path_not_allowed")
        _validate_upload_extension(labelmap_path_abs, LABELMAP_ALLOWED_EXTS, "labelmap_extension_not_allowed")
        labelmap_entries = _load_labelmap_file(labelmap_path_abs, strict=True)
    elif not labelmap_provided and active_labelmap_path:
        labelmap_path_abs = active_labelmap_path
        labelmap_entries = list(active_label_list)
    classes_raw = getattr(new_clf, "classes_", None)
    if classes_raw is None and isinstance(new_clf, dict):
        classes_list = [str(c) for c in list(new_clf.get("classes") or [])]
    else:
        classes_list = [str(c) for c in list(classes_raw)] if classes_raw is not None else []
    clf_classes = len(classes_list) if classes_list else None
    if clf_classes is not None:
        if not labelmap_entries:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="labelmap_required_for_classifier")
        bg_indices = _clip_head_background_indices(classes_list)
        non_bg_classes = [c for idx, c in enumerate(classes_list) if idx not in bg_indices]
        label_norm = {_normalize_class_name_for_match(n) for n in labelmap_entries if n}
        clf_norm = {_normalize_class_name_for_match(n) for n in non_bg_classes if n}
        if label_norm != clf_norm:
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="labelmap_classifier_class_mismatch")

    with clip_lock:
        clf = new_clf
        if encoder_type_norm == "clip":
            clip_model = new_clip_model
            clip_preprocess = new_preprocess
            clip_model_name = encoder_model_for_active
            clip_initialized = True
        else:
            clip_initialized = bool(clip_model is not None and clip_preprocess is not None)
        active_classifier_path = classifier_path_abs
        active_labelmap_path = labelmap_path_abs
        active_label_list = labelmap_entries
        active_encoder_type = encoder_type_norm
        active_encoder_model = encoder_model_for_active
        active_classifier_meta = dict(meta_obj) if isinstance(meta_obj, dict) else {}
        active_head_normalize_embeddings = _resolve_active_head_normalize_embeddings(meta_obj, new_clf, default=True)
        try:
            active_classifier_head = _load_clip_head_from_classifier(Path(classifier_path_abs))
        except Exception:
            active_classifier_head = None
        if payload.logit_adjustment_inference is not None and isinstance(active_classifier_head, dict):
            active_classifier_head["logit_adjustment_inference"] = bool(payload.logit_adjustment_inference)
        clip_last_error = None
    if encoder_type_norm == "dinov3":
        with dinov3_lock:
            dinov3_model = new_dinov3_model
            dinov3_processor = new_dinov3_processor
            dinov3_model_name = encoder_model_for_active
            dinov3_model_device = target_device
            dinov3_initialized = bool(dinov3_model is not None and dinov3_processor is not None)

    return _current_active_payload()


# note this one is actually not used. For a while I thought it would be cool to send a smaller crop to SAM but I'm not sure it makes sense since
# now I'm caching / checking the file that is currently loaded in the predictor and not updating on every call so it's actually waaaay faster and we have the whole image
# ---------------------------------------------------------------------------
# SAM preload endpoint
# ---------------------------------------------------------------------------

@app.post("/sam_preload", response_model=SamPreloadResponse)
def sam_preload(payload: SamPreloadRequest):
    variant = _default_variant(payload.sam_variant)
    try:
        slot_name = predictor_manager.resolve_slot(payload.slot, allow_disabled_fallback=False)
        return sam_preload_manager.submit(
            variant=variant,
            generation=payload.preload_generation,
            image_token=payload.image_token,
            image_base64=payload.image_base64,
            image_name=payload.image_name,
            slot=slot_name,
        )
    except HTTPException:
        raise
    except ValueError as exc:
        raise HTTPException(status_code=HTTP_409_CONFLICT, detail=str(exc)) from exc
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=500, detail=f"sam_preload_failed:{exc}") from exc


@app.get("/sam_slots", response_model=List[SamSlotStatus])
def sam_slots():
    return predictor_manager.status()


@app.post("/sam_activate_slot", response_model=SamActivateResponse)
def sam_activate_slot(payload: SamActivateRequest):
    variant = _default_variant(payload.sam_variant)
    slot = predictor_manager.get_slot_for_image(payload.image_name, variant)
    if slot is None:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="slot_not_found")
    promoted = predictor_manager.promote_slot(slot.name)
    if not promoted and slot.name != "current":
        raise HTTPException(status_code=HTTP_409_CONFLICT, detail="slot_busy")
    return SamActivateResponse(status="promoted", slot="current", token=slot.token)


def _predictor_settings_payload() -> PredictorSettings:
    min_cap, max_cap = predictor_manager.capacity_limits()
    current_cap = predictor_manager.get_capacity()
    active = predictor_manager.active_slot_count()
    loaded = predictor_manager.loaded_slot_count()
    image_memory = predictor_manager.total_image_memory_bytes()
    gpu_total_mb = None
    gpu_free_mb = None
    gpu_cc = None
    gpu_count = None
    if torch.cuda.is_available():
        try:
            free_bytes, total_bytes = torch.cuda.mem_get_info()
            gpu_total_mb = _bytes_to_mb(int(total_bytes))
            gpu_free_mb = _bytes_to_mb(int(free_bytes))
            major, minor = torch.cuda.get_device_capability(torch.cuda.current_device())
            gpu_cc = f"{major}.{minor}"
            gpu_count = torch.cuda.device_count()
        except Exception:
            gpu_total_mb = None
            gpu_free_mb = None
            gpu_count = None
    vm = psutil.virtual_memory()
    process = psutil.Process(os.getpid())
    process_mb = _bytes_to_mb(process.memory_info().rss)
    total_mb = _bytes_to_mb(int(vm.total))
    available_mb = _bytes_to_mb(int(vm.available))
    image_mb = _bytes_to_mb(image_memory)
    return PredictorSettings(
        max_predictors=current_cap,
        min_predictors=min_cap,
        max_supported_predictors=max_cap,
        active_predictors=active,
        loaded_predictors=loaded,
        process_ram_mb=process_mb,
        total_ram_mb=total_mb,
        available_ram_mb=available_mb,
        image_ram_mb=image_mb,
        gpu_total_mb=gpu_total_mb,
        gpu_free_mb=gpu_free_mb,
        gpu_compute_capability=gpu_cc,
        gpu_device_count=gpu_count,
    )


@app.get("/predictor_settings", response_model=PredictorSettings)
def get_predictor_settings():
    return _predictor_settings_payload()


@app.post("/predictor_settings", response_model=PredictorSettings)
def update_predictor_settings(payload: PredictorSettingsUpdate):
    min_cap, max_cap = predictor_manager.capacity_limits()
    try:
        requested = int(payload.max_predictors)
    except Exception:
        requested = min_cap
    normalized = max(min_cap, min(max_cap, requested))
    predictor_manager.set_capacity(normalized)
    return _predictor_settings_payload()


@app.get("/qwen/status")
def qwen_status():
    dependency_error = str(QWEN_IMPORT_ERROR) if QWEN_IMPORT_ERROR else None
    device_guess = qwen_device
    pending_error = qwen_last_error
    if not device_guess and not dependency_error:
        try:
            device_guess = _resolve_qwen_device()
        except RuntimeError as exc:  # noqa: BLE001
            pending_error = str(exc)
            device_guess = None
    return {
        "available": dependency_error is None,
        "loaded": qwen_model is not None,
        "model_name": QWEN_MODEL_NAME,
        "model_family": (active_qwen_metadata or {}).get("model_family", "qwen3"),
        "device": device_guess,
        "max_new_tokens": QWEN_MAX_NEW_TOKENS,
        "min_pixels": QWEN_MIN_PIXELS,
        "max_pixels": QWEN_MAX_PIXELS,
        "last_error": pending_error,
        "dependency_error": dependency_error,
        "active_model": active_qwen_model_id,
        "active_metadata": active_qwen_metadata,
    }


@app.get("/qwen/settings", response_model=QwenRuntimeSettings)
def qwen_settings():
    return QwenRuntimeSettings(trust_remote_code=QWEN_TRUST_REMOTE_CODE)


@app.post("/qwen/settings", response_model=QwenRuntimeSettings)
def update_qwen_settings(payload: QwenRuntimeSettingsUpdate):
    global QWEN_TRUST_REMOTE_CODE
    if payload.trust_remote_code is not None:
        desired = bool(payload.trust_remote_code)
        if desired != QWEN_TRUST_REMOTE_CODE:
            QWEN_TRUST_REMOTE_CODE = desired
            _unload_qwen_runtime()
    return QwenRuntimeSettings(trust_remote_code=QWEN_TRUST_REMOTE_CODE)


@app.post("/runtime/unload")
def unload_all_runtimes():
    _unload_inference_runtimes()
    return {"status": "unloaded"}


@app.post("/qwen/unload")
def qwen_unload():
    _unload_qwen_runtime()
    return {"status": "unloaded"}


@app.post("/qwen/infer", response_model=QwenInferenceResponse)
def qwen_infer(payload: QwenInferenceRequest):
    prompt_type = payload.prompt_type.lower()
    if prompt_type not in {"bbox", "point", "bbox_sam"}:
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="invalid_prompt_type")
    pil_img, np_img, token = resolve_image_payload(
        payload.image_base64,
        payload.image_token,
        getattr(payload, "sam_variant", None),
    )
    manual_prompt = (payload.prompt or "").strip()
    if manual_prompt:
        final_prompt = manual_prompt
    else:
        item_list = (payload.item_list or "").strip()
        final_prompt = _render_qwen_prompt(
            prompt_type,
            items=item_list,
            image_type=(payload.image_type or "").strip() or None,
            extra_context=(payload.extra_context or "").strip() or None,
        )
    try:
        qwen_text, proc_w, proc_h = _run_qwen_inference(final_prompt, pil_img)
    except HTTPException:
        raise
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=f"qwen_inference_failed:{exc}") from exc
    print("[Qwen prompt]", final_prompt)
    print("[Qwen raw output]", qwen_text)
    warnings: List[str] = []
    try:
        _, items = _extract_qwen_json_block(qwen_text)
    except HTTPException as exc:
        detail = exc.detail if isinstance(exc.detail, str) else str(exc.detail)
        warnings.append(f"parse_error:{detail}")
        print(f"[Qwen parse error] {detail}; raw text follows:\n{qwen_text}")
        return QwenInferenceResponse(
            boxes=[],
            raw_response=qwen_text,
            prompt=final_prompt,
            prompt_type=prompt_type,  # type: ignore[arg-type]
            warnings=warnings,
            image_token=token,
        )
    normalized_items = _qwen_items_from_payload(items)
    if not normalized_items:
        print("[Qwen parsed but empty list]", qwen_text)
        warnings.append("no_results")
        return QwenInferenceResponse(
            boxes=[],
            raw_response=qwen_text,
            prompt=final_prompt,
            prompt_type=prompt_type,  # type: ignore[arg-type]
            warnings=warnings,
            image_token=token,
        )
    variant = _default_variant(getattr(payload, "sam_variant", None))
    limit = payload.max_results or 8
    image_name = getattr(payload, "image_name", None)
    if prompt_type == "bbox":
        boxes = _qwen_bbox_results(normalized_items, proc_w, proc_h, pil_img.width, pil_img.height, limit=limit)
    elif prompt_type == "bbox_sam":
        boxes = _qwen_bbox_sam_results(
            normalized_items,
            proc_w,
            proc_h,
            pil_img,
            np_img,
            token,
            variant,
            image_name=image_name,
            limit=limit,
        )
    else:
        boxes = _qwen_point_results(
            normalized_items,
            proc_w,
            proc_h,
            pil_img,
            np_img,
            token,
            variant,
            image_name=image_name,
            limit=limit,
        )
    if not boxes:
        warnings.append("no_results")
    return QwenInferenceResponse(
        boxes=boxes,
        raw_response=qwen_text,
        prompt=final_prompt,
        prompt_type=prompt_type,  # type: ignore[arg-type]
        warnings=warnings,
        image_token=token,
    )


@app.post("/qwen/caption", response_model=QwenCaptionResponse)
def qwen_caption(payload: QwenCaptionRequest):
    fast_mode = bool(payload.fast_mode)
    force_unload = payload.force_unload
    if force_unload is None:
        force_unload = QWEN_CAPTION_CACHE_LIMIT == 0
    if fast_mode:
        force_unload = False
    multi_model_cache = bool(payload.multi_model_cache or fast_mode)
    active_model_id: Optional[str] = None
    active_runtime: Optional[Tuple[Any, Any]] = None
    request_model_cache: Dict[str, Tuple[Any, Any]] = {}

    def get_runtime(model_id: Optional[str]) -> Tuple[Any, Any]:
        nonlocal active_model_id, active_runtime
        if multi_model_cache:
            key = model_id or "__active__"
            cached = request_model_cache.get(key)
            if cached:
                return cached
            if model_id:
                runtime = _ensure_qwen_ready_for_caption(model_id)
            else:
                runtime = _ensure_qwen_ready()
            request_model_cache[key] = runtime
            return runtime
        if active_runtime is not None and active_model_id != model_id:
            logger.info(
                "[qwen-caption] switching model %s -> %s; unloading current runtime",
                active_model_id,
                model_id,
            )
            _unload_qwen_runtime()
            active_runtime = None
            active_model_id = None
        if active_runtime is None:
            if model_id:
                active_runtime = _ensure_qwen_ready_for_caption(model_id)
                active_model_id = model_id
            else:
                active_runtime = _ensure_qwen_ready()
                active_model_id = None
        return active_runtime

    try:
        if payload.unload_others and not fast_mode:
            _unload_non_qwen_runtimes()
        pil_img, _, _ = resolve_image_payload(payload.image_base64, payload.image_token, None)
        user_prompt = (payload.user_prompt or "").strip()
        include_counts = bool(payload.include_counts)
        include_coords = bool(payload.include_coords)
        max_boxes = payload.max_boxes if payload.max_boxes is not None else 0
        max_new_tokens = payload.max_new_tokens if payload.max_new_tokens is not None else 128
        label_hints = payload.label_hints or []
        allowed_labels = _allowed_caption_labels(label_hints)
        image_width = payload.image_width or pil_img.width
        image_height = payload.image_height or pil_img.height
        caption_mode = payload.caption_mode or "full"
        restrict_to_labels = payload.restrict_to_labels if payload.restrict_to_labels is not None else True
        caption_all_windows = True if caption_mode == "windowed" else bool(payload.caption_all_windows)
        detailed_mode = caption_mode == "windowed"
        glossary_map = _caption_glossary_map(
            payload.labelmap_glossary,
            [hint.label for hint in label_hints if hint.label],
        )
        allowed_labels_prompt = (
            [_caption_preferred_label(label, glossary_map) for label in allowed_labels]
            if allowed_labels
            else []
        )
        prompt_text, counts, used_boxes, truncated = _build_qwen_caption_prompt(
            user_prompt,
            label_hints,
            image_width,
            image_height,
            include_counts,
            include_coords,
            max_boxes,
            detailed_mode,
            restrict_to_labels=restrict_to_labels,
            labelmap_glossary=payload.labelmap_glossary,
        )
        glossary_line = ""
        if payload.labelmap_glossary:
            glossary_line = (
                "Glossary (label synonyms): "
                f"{payload.labelmap_glossary}. "
                "Use glossary terms as optional synonym hints; do not copy the glossary verbatim. "
                "Never output labelmap class names (especially tokens with underscores)."
            )
            prompt_text = f"{prompt_text}\n{glossary_line}"
        base_model_id = (active_qwen_metadata or {}).get("model_id") or QWEN_MODEL_NAME
        variant = payload.model_variant or "auto"
        model_id_override = payload.model_id or ""
        if model_id_override:
            desired_model_id = model_id_override
        else:
            desired_model_id = _resolve_qwen_variant_model_id(base_model_id, variant)
        if active_qwen_model_path and desired_model_id != base_model_id:
            logger.info(
                "[qwen-caption] using base model override (%s) while adapter %s is active",
                desired_model_id,
                active_qwen_model_id,
            )
        caption_base_model_id = desired_model_id if model_id_override else base_model_id
        final_only = bool(payload.final_answer_only)
        two_stage = bool(payload.two_stage_refine)
        is_thinking = "Thinking" in desired_model_id or variant == "Thinking"
        decode_params = _resolve_qwen_caption_decode(payload, is_thinking)
        deterministic_decode = {"do_sample": False}
        if is_thinking:
            prompt_text = _adjust_prompt_for_thinking(prompt_text)
        # Keep caption max_new_tokens consistent across full/windowed/refine paths; cap at 2000.
        # Avoid per-path caps here (we previously caused repeated CUDA asserts by diverging).
        if is_thinking:
            max_new_tokens = max(max_new_tokens, 2000)
        max_new_tokens = min(max_new_tokens, 2000)
        refine_max_tokens = max_new_tokens
        system_prompt = (
            "You are a detailed captioning assistant. Use the image as truth. "
            "Provide a rich, multi-sentence caption when there is a lot to see. "
            "Do not mention labels, hints, bounding boxes, coordinates, or that counts were provided. "
            "Do not output labelmap tags (e.g., light_vehicle). Use natural words like car, van, SUV. "
            "Avoid any label tokens that contain underscores. "
            "If the hints conflict with the image, mention the uncertainty briefly."
        )
        system_prompt = f"{system_prompt} Respond in English only."
        if final_only:
            system_prompt = f"{system_prompt} Return only the final caption. Do not include reasoning or preamble."
        if final_only and is_thinking and not two_stage:
            system_prompt = f"{system_prompt} Respond with exactly <final>...</final> and nothing else."
        use_caption_cache = True
        if active_qwen_model_path and not model_id_override and desired_model_id == base_model_id and variant == "auto":
            use_caption_cache = False

        def resolve_main_runtime() -> Tuple[Any, Any]:
            if model_id_override:
                return get_runtime(desired_model_id)
            if use_caption_cache:
                return get_runtime(desired_model_id)
            return get_runtime(None)

        windowed_captions: List[Tuple[int, int, int, str]] = []
        cleanup_count = 0
        refine_count = 0
        merge_count = 0
        if caption_mode == "windowed":
            overlap = _resolve_qwen_window_overlap(payload.window_overlap)
            window_size = _resolve_qwen_window_size(None, image_width, image_height, overlap=overlap)
            force_two = True
            x_positions = _window_positions(image_width, window_size, overlap, force_two=force_two)
            y_positions = _window_positions(image_height, window_size, overlap, force_two=force_two)
            grouped_hints = _group_hints_by_window(label_hints, x_positions, y_positions, window_size)
            window_model_id = desired_model_id
            window_base_model_id = window_model_id
            window_is_thinking = "Thinking" in window_model_id
            for y0 in y_positions:
                for x0 in x_positions:
                    window_hints = grouped_hints.get((x0, y0), [])
                    if not window_hints and not caption_all_windows:
                        continue
                    window_allowed = _allowed_caption_labels(window_hints)
                    window_glossary_map = _caption_glossary_map(
                        payload.labelmap_glossary,
                        [hint.label for hint in window_hints if hint.label],
                    )
                    window_allowed_prompt = (
                        [_caption_preferred_label(label, window_glossary_map) for label in window_allowed]
                        if window_allowed
                        else []
                    )
                    window_prompt, window_counts, _, _ = _build_qwen_caption_prompt(
                        user_prompt,
                        window_hints,
                        window_size,
                        window_size,
                        include_counts,
                        include_coords,
                        max_boxes,
                        detailed_mode=True,
                        restrict_to_labels=restrict_to_labels,
                        labelmap_glossary=payload.labelmap_glossary,
                    )
                    window_prompt = (
                        f"Window region in full image: [{x0}, {y0}] to [{x0 + window_size}, {y0 + window_size}].\n"
                        "Focus only on this region.\n"
                        "Write 1-3 detailed sentences about this region only. No reasoning or preamble. "
                        "Do not mention labels, hints, counts, or coordinates. "
                        "Do not output labelmap tags (e.g., light_vehicle); use natural words like car or van. "
                        "Avoid any token with underscores.\n"
                        f"{window_prompt}"
                    )
                    if glossary_line:
                        window_prompt = f"{window_prompt}\n{glossary_line}"
                    if restrict_to_labels and window_allowed_prompt:
                        window_prompt = (
                            f"{window_prompt}\nAllowed classes: {', '.join(window_allowed_prompt)}. "
                            "Do not introduce any other entity types."
                        )
                    if window_is_thinking:
                        window_prompt = _adjust_prompt_for_thinking(window_prompt)
                    window_img = pil_img.crop((x0, y0, x0 + window_size, y0 + window_size))
                    window_max_tokens = max_new_tokens
                    qwen_text, _, _ = _run_qwen_inference(
                        window_prompt,
                        window_img,
                        max_new_tokens=window_max_tokens,
                        system_prompt_override=system_prompt,
                        runtime_override=get_runtime(window_model_id),
                        decode_override=decode_params,
                    )
                    window_caption, _ = _extract_caption_from_text(qwen_text, marker=None)
                    window_caption = _sanitize_qwen_caption(window_caption)
                    if window_is_thinking and _thinking_caption_needs_cleanup(window_caption, qwen_text):
                        cleanup_model = _resolve_qwen_variant_model_id(window_base_model_id, "Instruct")
                        window_caption = _run_qwen_caption_cleanup(
                            window_caption,
                            window_img,
                            window_max_tokens,
                            window_base_model_id,
                            use_caption_cache,
                            model_id_override=cleanup_model,
                            runtime_override=get_runtime(cleanup_model),
                            allowed_labels=window_allowed_prompt if restrict_to_labels and window_allowed_prompt else None,
                            strict=True,
                            minimal_edit=True,
                        )
                        cleanup_count += 1
                    if _caption_is_degenerate(window_caption):
                        cleanup_model = _resolve_qwen_variant_model_id(window_base_model_id, "Instruct")
                        window_caption = _run_qwen_caption_cleanup(
                            window_caption,
                            window_img,
                            window_max_tokens,
                            window_base_model_id,
                            use_caption_cache,
                            model_id_override=cleanup_model,
                            runtime_override=get_runtime(cleanup_model),
                            allowed_labels=window_allowed_prompt if restrict_to_labels and window_allowed_prompt else None,
                            strict=True,
                            minimal_edit=True,
                        )
                        cleanup_count += 1
                    if _caption_needs_completion(window_caption) or _caption_has_meta(window_caption):
                        cleanup_model = _resolve_qwen_variant_model_id(window_base_model_id, "Instruct")
                        window_caption = _run_qwen_caption_cleanup(
                            window_caption,
                            window_img,
                            window_max_tokens,
                            window_base_model_id,
                            use_caption_cache,
                            model_id_override=cleanup_model,
                            runtime_override=get_runtime(cleanup_model),
                            allowed_labels=window_allowed_prompt if restrict_to_labels and window_allowed_prompt else None,
                            strict=True,
                            minimal_edit=True,
                        )
                        cleanup_count += 1
                    needs_refine, missing = _caption_needs_refine(
                        window_caption,
                        window_counts,
                        detailed_mode=True,
                        include_counts=include_counts,
                        glossary_map=window_glossary_map,
                    )
                    if needs_refine:
                        refine_model = _resolve_qwen_variant_model_id(window_base_model_id, "Instruct")
                        allowed_note = ""
                        if restrict_to_labels and window_allowed_prompt:
                            allowed_note = (
                                f"Allowed classes: {', '.join(window_allowed_prompt)}. "
                                "Do not introduce any other entity types."
                            )
                        elif not restrict_to_labels:
                            allowed_note = "You may mention additional visible objects beyond the hints."
                        missing_note = (
                            f"Ensure the caption mentions: {', '.join(missing)}."
                            if missing
                            else "Ensure all labeled classes in this window are mentioned."
                        )
                        refine_prompt = f"{window_prompt}\nDraft caption: {window_caption}\n{missing_note}"
                        if allowed_note:
                            refine_prompt = f"{refine_prompt}\n{allowed_note}"
                        refine_prompt = (
                            f"{refine_prompt}\n"
                            "Edit the draft with minimal changes. Do not introduce new objects or actions. "
                            "Return only a concise, complete caption (1-3 sentences) with no coordinates."
                        )
                        refine_system = (
                            "You are a concise captioning assistant. Return only the final caption in English."
                        )
                        refine_text, _, _ = _run_qwen_inference(
                            refine_prompt,
                            window_img,
                            max_new_tokens=refine_max_tokens,
                            system_prompt_override=refine_system,
                            runtime_override=get_runtime(refine_model),
                            decode_override=deterministic_decode,
                        )
                        window_caption, _ = _extract_caption_from_text(refine_text, marker=None)
                        window_caption = _sanitize_qwen_caption(window_caption)
                        refine_count += 1
                    if window_caption:
                        windowed_captions.append((x0, y0, window_size, window_caption))
                        _emit_caption_window(x0, y0, window_size, window_caption)
            if windowed_captions:
                window_lines = ["Close-up observations from subregions (use these to enrich the final caption):"]
                for x0, y0, size, caption in windowed_captions:
                    x_center = x0 + size / 2.0
                    y_center = y0 + size / 2.0
                    horiz = "left" if x_center < image_width / 3.0 else "right" if x_center > image_width * 2 / 3.0 else "center"
                    vert = "top" if y_center < image_height / 3.0 else "bottom" if y_center > image_height * 2 / 3.0 else "middle"
                    region = f"{vert}-{horiz}"
                    window_lines.append(
                        f"- {region} ([{x0},{y0},{x0 + size},{y0 + size}]): {caption}"
                    )
                if include_counts and restrict_to_labels:
                    window_lines.append(
                        "Now describe the full image in detail. Use all labeled object counts and the close-up observations. "
                        "Mention every class that appears in the hints, and summarize repetitive objects (e.g., many cars as a parking lot) "
                        "unless only a few are present or a specific action stands out. "
                        "Do not mention labels, hints, counts, or coordinates."
                    )
                    window_lines.append(
                        "If window observations conflict, trust the full image and the authoritative counts. Avoid self-contradictions."
                    )
                else:
                    window_lines.append(
                        "Now describe the full image in detail using the close-up observations. "
                        "Use detector hints as suggestions, and mention other visible objects. "
                        "Do not mention labels, hints, or coordinates."
                    )
                    window_lines.append(
                        "If window observations conflict, trust the full image. Avoid self-contradictions."
                    )
                window_lines.append(
                    "In the final caption, preserve specific details from the windows "
                    "(e.g., people counts, actions, and notable objects). "
                    "Do not compress away window details; longer captions are preferred."
                )
                prompt_text = f"{prompt_text}\n" + "\n".join(window_lines)
        if two_stage and is_thinking:
            draft_prompt = (
                "Step 1: Look at the image and form a draft caption.\n"
                "Respond with: DRAFT: <caption>"
            )
            draft_system = f"{system_prompt} Return only a line starting with 'DRAFT:'."
            draft_text, _, _ = _run_qwen_inference(
                draft_prompt,
                pil_img,
                max_new_tokens=max_new_tokens,
                system_prompt_override=draft_system,
                runtime_override=resolve_main_runtime(),
                decode_override=decode_params,
            )
            draft_caption, _ = _extract_caption_from_text(draft_text, marker="DRAFT")
            draft_caption = _sanitize_qwen_caption(draft_caption)
            allowed_note = ""
            if restrict_to_labels and allowed_labels_prompt:
                allowed_note = (
                    f"Allowed classes: {', '.join(allowed_labels_prompt)}. Do not introduce any other entity types."
                )
            elif not restrict_to_labels:
                allowed_note = "You may mention additional visible objects beyond the hints."
            refine_prompt = f"{prompt_text}\nDraft caption: {draft_caption}"
            if allowed_note:
                refine_prompt = f"{refine_prompt}\n{allowed_note}"
            refine_prompt = (
                f"{refine_prompt}\n"
                "Edit the draft with minimal changes. Do not introduce new objects or actions. "
                "Return only the final caption."
            )
            refine_system = (
                "You are a captioning assistant. Use the image as truth. "
                "Return only the final caption. Respond in English only."
            )
            refine_model = _resolve_qwen_variant_model_id(caption_base_model_id, "Instruct")
            qwen_text, _, _ = _run_qwen_inference(
                refine_prompt,
                pil_img,
                max_new_tokens=refine_max_tokens,
                system_prompt_override=refine_system,
                runtime_override=get_runtime(refine_model),
                decode_override=deterministic_decode,
            )
            caption_text, _ = _extract_caption_from_text(qwen_text, marker=None)
            if final_only or is_thinking:
                caption_text = _sanitize_qwen_caption(caption_text)
        else:
            qwen_text, _, _ = _run_qwen_inference(
                prompt_text,
                pil_img,
                max_new_tokens=max_new_tokens,
                system_prompt_override=system_prompt,
                runtime_override=resolve_main_runtime(),
                decode_override=decode_params,
            )
            caption_text, _ = _extract_caption_from_text(qwen_text, marker=None)
            if final_only or is_thinking:
                caption_text = _sanitize_qwen_caption(caption_text)
            if is_thinking and _thinking_caption_needs_cleanup(caption_text, qwen_text):
                cleanup_model = _resolve_qwen_variant_model_id(caption_base_model_id, "Instruct")
                caption_text = _run_qwen_caption_cleanup(
                    caption_text,
                    pil_img,
                    refine_max_tokens,
                    caption_base_model_id,
                    use_caption_cache,
                    model_id_override=cleanup_model,
                    runtime_override=get_runtime(cleanup_model),
                    allowed_labels=allowed_labels_prompt if restrict_to_labels and allowed_labels_prompt else None,
                    strict=True,
                    minimal_edit=True,
                )
                cleanup_count += 1
        if caption_mode == "windowed" and windowed_captions and caption_text:
            merge_tokens = min(refine_max_tokens, 256)
            caption_text = _run_qwen_caption_merge(
                caption_text,
                windowed_captions,
                pil_img=pil_img,
                base_model_id=caption_base_model_id,
                runtime_resolver=get_runtime,
                max_new_tokens=merge_tokens,
                glossary_line=glossary_line or None,
            )
            merge_count += 1
            if final_only or is_thinking:
                caption_text = _sanitize_qwen_caption(caption_text)
        if _caption_is_degenerate(caption_text):
            cleanup_model = _resolve_qwen_variant_model_id(caption_base_model_id, "Instruct")
            caption_text = _run_qwen_caption_cleanup(
                caption_text,
                pil_img,
                refine_max_tokens,
                caption_base_model_id,
                use_caption_cache,
                model_id_override=cleanup_model,
                runtime_override=get_runtime(cleanup_model),
                allowed_labels=allowed_labels_prompt if restrict_to_labels and allowed_labels_prompt else None,
                strict=True,
                minimal_edit=True,
            )
            cleanup_count += 1
        if _caption_needs_completion(caption_text) or _caption_has_meta(caption_text):
            cleanup_model = _resolve_qwen_variant_model_id(caption_base_model_id, "Instruct")
            caption_text = _run_qwen_caption_cleanup(
                caption_text,
                pil_img,
                refine_max_tokens,
                caption_base_model_id,
                use_caption_cache,
                model_id_override=cleanup_model,
                runtime_override=get_runtime(cleanup_model),
                allowed_labels=allowed_labels_prompt if restrict_to_labels and allowed_labels_prompt else None,
                strict=True,
                minimal_edit=True,
            )
            cleanup_count += 1
        if caption_mode == "windowed" and "4B" in desired_model_id and _caption_needs_short_form(caption_text):
            cleanup_model = _resolve_qwen_variant_model_id(caption_base_model_id, "Instruct")
            caption_text = _run_qwen_caption_cleanup(
                caption_text,
                pil_img,
                refine_max_tokens,
                caption_base_model_id,
                use_caption_cache,
                model_id_override=cleanup_model,
                runtime_override=get_runtime(cleanup_model),
                allowed_labels=allowed_labels_prompt if restrict_to_labels and allowed_labels_prompt else None,
                strict=True,
                minimal_edit=True,
            )
            cleanup_count += 1
        needs_refine, missing = _caption_needs_refine(
            caption_text,
            counts,
            detailed_mode=detailed_mode,
            include_counts=include_counts,
            glossary_map=glossary_map,
        )
        if needs_refine:
            refine_model = _resolve_qwen_variant_model_id(caption_base_model_id, "Instruct")
            allowed_note = ""
            if restrict_to_labels and allowed_labels_prompt:
                allowed_note = (
                    f"Allowed classes: {', '.join(allowed_labels_prompt)}. Do not introduce any other entity types."
                )
            elif not restrict_to_labels:
                allowed_note = "You may mention additional visible objects beyond the hints."
            missing_note = (
                f"Ensure the caption mentions: {', '.join(missing)}."
                if missing
                else "Ensure all labeled classes are mentioned."
            )
            refine_prompt = f"{prompt_text}\nDraft caption: {caption_text}\n{missing_note}"
            if allowed_note:
                refine_prompt = f"{refine_prompt}\n{allowed_note}"
            refine_prompt = (
                f"{refine_prompt}\n"
                "Edit the draft with minimal changes. Do not introduce new objects or actions. "
                "Return only the final caption with no coordinates."
            )
            refine_system = "You are a captioning assistant. Return only the final caption in English."
            refine_text, _, _ = _run_qwen_inference(
                refine_prompt,
                pil_img,
                max_new_tokens=refine_max_tokens,
                system_prompt_override=refine_system,
                runtime_override=get_runtime(refine_model),
                decode_override=deterministic_decode,
            )
            caption_text, _ = _extract_caption_from_text(refine_text, marker=None)
            caption_text = _sanitize_qwen_caption(caption_text)
            refine_count += 1
        if caption_text and _caption_needs_english_rewrite(caption_text):
            rewrite_model = _resolve_qwen_variant_model_id(base_model_id, "Instruct")
            rewrite_prompt = (
                "Rewrite the caption in English only, preserving meaning and brevity.\n"
                f"Caption: {caption_text}"
            )
            rewrite_system = "Return only the rewritten caption in English."
            rewrite_text, _, _ = _run_qwen_inference(
                rewrite_prompt,
                pil_img,
                max_new_tokens=refine_max_tokens,
                system_prompt_override=rewrite_system,
                runtime_override=get_runtime(rewrite_model),
                decode_override=deterministic_decode,
            )
            caption_text, _ = _extract_caption_from_text(rewrite_text, marker=None)
            if final_only or is_thinking:
                caption_text = _sanitize_qwen_caption(caption_text)
        response = QwenCaptionResponse(
            caption=caption_text,
            used_counts=counts,
            used_boxes=used_boxes,
            truncated=truncated,
        )
        word_count = len(caption_text.split()) if caption_text else 0
        logger.info(
            "[qwen-caption] hints=%s used=%s truncated=%s variant=%s model=%s final_only=%s windows=%s cleanup=%s refine=%s merge=%s words=%s",
            len(payload.label_hints or []),
            used_boxes,
            truncated,
            variant,
            desired_model_id,
            final_only,
            len(windowed_captions) if caption_mode == "windowed" else 0,
            cleanup_count,
            refine_count,
            merge_count,
            word_count,
        )
    except HTTPException:
        if force_unload:
            logger.warning("[qwen-caption] exception -> forcing unload")
            request_model_cache.clear()
            _unload_qwen_runtime()
            active_runtime = None
            active_model_id = None
        raise
    except Exception as exc:  # noqa: BLE001
        if force_unload:
            logger.warning("[qwen-caption] exception=%s -> forcing unload", exc)
            request_model_cache.clear()
            _unload_qwen_runtime()
            active_runtime = None
            active_model_id = None
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=f"qwen_caption_failed:{exc}") from exc
    if force_unload:
        request_model_cache.clear()
        _unload_qwen_runtime()
        active_runtime = None
        active_model_id = None
    return response


@app.post("/qwen/prepass", response_model=QwenPrepassResponse)
def qwen_prepass(payload: QwenPrepassRequest):
    try:
        payload = payload.copy(update={"prepass_only": True})
        return _run_prepass_annotation(payload)
    except HTTPException:
        raise
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_503_SERVICE_UNAVAILABLE, detail=f"qwen_prepass_failed:{exc}") from exc


@app.post("/calibration/jobs")
def start_calibration_job(payload: CalibrationRequest = Body(...)):
    job = _start_calibration_job(payload)
    return _serialize_calibration_job(job)


@app.get("/calibration/jobs")
def list_calibration_jobs():
    _prune_job_registry(CALIBRATION_JOBS, CALIBRATION_JOBS_LOCK)
    with CALIBRATION_JOBS_LOCK:
        jobs = list(CALIBRATION_JOBS.values())
    jobs.sort(key=lambda j: j.created_at, reverse=True)
    return [_serialize_calibration_job(job) for job in jobs]


@app.get("/calibration/jobs/{job_id}")
def get_calibration_job(job_id: str):
    with CALIBRATION_JOBS_LOCK:
        job = CALIBRATION_JOBS.get(job_id)
    if not job:
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="calibration_job_not_found")
    return _serialize_calibration_job(job)


@app.post("/calibration/jobs/{job_id}/cancel")
def cancel_calibration_job(job_id: str):
    job = _cancel_calibration_job(job_id)
    return _serialize_calibration_job(job)


@app.get("/prepass/recipes")
def list_prepass_recipes():
    return _list_prepass_recipes()


@app.get("/prepass/recipes/{recipe_id}", response_model=PrepassRecipeResponse)
def get_prepass_recipe(recipe_id: str):
    recipe_dir = _prepass_recipe_dir(recipe_id)
    meta = _load_prepass_recipe_meta(recipe_dir)
    return PrepassRecipeResponse(
        id=meta.get("id") or recipe_id,
        name=meta.get("name") or recipe_id,
        description=meta.get("description"),
        created_at=float(meta.get("created_at") or time.time()),
        updated_at=float(meta.get("updated_at") or time.time()),
        config=meta.get("config") or {},
        glossary=meta.get("glossary"),
        schema_version=int(meta.get("schema_version") or PREPASS_RECIPE_SCHEMA_VERSION),
    )


@app.post("/prepass/recipes", response_model=PrepassRecipeResponse)
def save_prepass_recipe(payload: PrepassRecipeRequest):
    recipe_id = payload.recipe_id or uuid.uuid4().hex
    recipe_dir = _prepass_recipe_dir(recipe_id, create=True)
    now = time.time()
    existing = {}
    meta_path = recipe_dir / PREPASS_RECIPE_META
    if meta_path.exists():
        try:
            existing = json.loads(meta_path.read_text())
        except Exception:
            existing = {}
    created_at = float(existing.get("created_at") or now)
    recipe_meta = {
        "id": recipe_id,
        "schema_version": PREPASS_RECIPE_SCHEMA_VERSION,
        "name": payload.name.strip(),
        "description": (payload.description or "").strip(),
        "config": payload.config or {},
        "glossary": _normalize_labelmap_glossary(payload.glossary),
        "created_at": created_at,
        "updated_at": now,
    }
    _write_prepass_recipe_meta(recipe_dir, recipe_meta)
    return PrepassRecipeResponse(
        id=recipe_id,
        name=recipe_meta["name"],
        description=recipe_meta.get("description"),
        created_at=created_at,
        updated_at=now,
        config=recipe_meta["config"],
        glossary=recipe_meta.get("glossary") or None,
    )


@app.delete("/prepass/recipes/{recipe_id}")
def delete_prepass_recipe(recipe_id: str):
    recipe_dir = _prepass_recipe_dir(recipe_id)
    if not recipe_dir.exists():
        raise HTTPException(status_code=HTTP_404_NOT_FOUND, detail="prepass_recipe_not_found")
    shutil.rmtree(recipe_dir, ignore_errors=True)
    return {"status": "deleted", "id": recipe_id}


def _collect_recipe_assets(recipe_meta: Dict[str, Any], temp_dir: Path) -> Dict[str, Any]:
    assets: Dict[str, Any] = {"copied": [], "missing": []}
    config = recipe_meta.get("config") or {}
    glossary_text = recipe_meta.get("glossary") or ""
    if glossary_text:
        glossary_path = temp_dir / "glossary.json"
        glossary_path.write_text(json.dumps({"glossary": glossary_text}, indent=2), encoding="utf-8")
        assets["copied"].append(
            {
                "path": "glossary.json",
                "size": glossary_path.stat().st_size,
                "sha256": _sha256_path(glossary_path),
            }
        )

    labelmap_lines: List[str] = []
    if isinstance(config.get("labelmap"), list):
        labelmap_lines = [str(x).strip() for x in config.get("labelmap") or [] if str(x).strip()]
    if not labelmap_lines:
        dataset_id = config.get("dataset_id")
        if isinstance(dataset_id, str) and dataset_id.strip():
            labelmap_lines, _ = _agent_load_labelmap_meta(dataset_id)
    if not labelmap_lines and active_labelmap_path:
        try:
            labelmap_lines = _read_labelmap_lines(Path(active_labelmap_path))
        except Exception:
            labelmap_lines = []
    if labelmap_lines:
        labelmap_path = temp_dir / "labelmap.txt"
        labelmap_path.write_text("\n".join(labelmap_lines) + "\n", encoding="utf-8")
        assets["copied"].append(
            {
                "path": "labelmap.txt",
                "size": labelmap_path.stat().st_size,
                "sha256": _sha256_path(labelmap_path),
            }
        )

    def _copy_run(root: Path, run_id: Optional[str], keep: Optional[set[str]], kind: str):
        if not run_id:
            return
        run_dir = root / _sanitize_yolo_run_id(run_id)
        if not run_dir.exists():
            assets["missing"].append({"kind": kind, "id": run_id})
            return
        dest = temp_dir / "models" / kind / run_dir.name
        assets["copied"].extend(_copy_tree_filtered(run_dir, dest, keep_files=keep))

    _copy_run(YOLO_JOB_ROOT, config.get("yolo_id"), None, "yolo_runs")
    _copy_run(RFDETR_JOB_ROOT, config.get("rfdetr_id"), RFDETR_KEEP_FILES, "rfdetr_runs")

    copied_qwen_ids: set[str] = set()

    def _copy_qwen_run(model_id: Optional[str]) -> None:
        if not model_id:
            return
        if model_id in copied_qwen_ids:
            return
        entry = _get_qwen_model_entry(str(model_id))
        if not entry:
            assets["missing"].append({"kind": "qwen_model", "id": model_id})
            return
        raw_path = entry.get("path")
        if not raw_path:
            assets["missing"].append({"kind": "qwen_model", "id": model_id})
            return
        run_path = Path(str(raw_path)).resolve()
        if run_path.name == "latest":
            run_path = run_path.parent
        if not run_path.exists():
            assets["missing"].append({"kind": "qwen_model", "id": model_id})
            return
        dest = temp_dir / "models" / "qwen_runs" / run_path.name
        assets["copied"].extend(_copy_tree_filtered(run_path, dest, keep_files=None))
        copied_qwen_ids.add(model_id)

    _copy_qwen_run(config.get("model_id"))
    _copy_qwen_run(config.get("prepass_caption_model_id"))

    classifier_id = config.get("classifier_id")
    if classifier_id:
        try:
            classifier_path = _resolve_agent_clip_classifier_path(classifier_id)
        except HTTPException:
            classifier_path = None
        if classifier_path and classifier_path.exists():
            dest = temp_dir / "models" / "classifiers"
            dest.mkdir(parents=True, exist_ok=True)
            target = dest / classifier_path.name
            shutil.copy2(classifier_path, target)
            assets["copied"].append(
                {
                    "path": str(target.relative_to(temp_dir)),
                    "size": target.stat().st_size,
                    "sha256": _sha256_path(target),
                }
            )
        else:
            assets["missing"].append({"kind": "classifier", "id": classifier_id})

    job_id = config.get("ensemble_job_id")
    if job_id:
        job_dir = CALIBRATION_ROOT / _sanitize_yolo_run_id(job_id)
        if job_dir.exists():
            dest = temp_dir / "models" / "calibration_jobs" / job_dir.name
            assets["copied"].extend(_copy_tree_filtered(job_dir, dest, keep_files=None))
        else:
            assets["missing"].append({"kind": "calibration_job", "id": job_id})

    return assets


@app.post("/prepass/recipes/{recipe_id}/export")
def export_prepass_recipe(recipe_id: str):
    recipe_dir = _prepass_recipe_dir(recipe_id)
    meta = _load_prepass_recipe_meta(recipe_dir)
    temp_dir = Path(
        tempfile.mkdtemp(prefix=f"prepass_recipe_{recipe_id}_", dir=PREPASS_RECIPE_EXPORT_ROOT)
    )
    try:
        meta_copy = json.loads(json.dumps(meta))
        config_copy = meta_copy.get("config") or {}
        if isinstance(config_copy, dict) and "dataset_id" in config_copy:
            config_copy = dict(config_copy)
            config_copy.pop("dataset_id", None)
            meta_copy["config"] = config_copy
        meta_path = temp_dir / PREPASS_RECIPE_META
        meta_path.write_text(json.dumps(meta_copy, indent=2), encoding="utf-8")
        assets = _collect_recipe_assets(meta_copy, temp_dir)
        manifest = {
            "schema_version": PREPASS_RECIPE_SCHEMA_VERSION,
            "recipe_id": meta.get("id") or recipe_id,
            "generated_at": time.time(),
            "assets": assets,
        }
        manifest_path = temp_dir / "manifest.json"
        manifest_path.write_text(json.dumps(manifest, indent=2), encoding="utf-8")
        zip_path = temp_dir.with_suffix(".zip")
        shutil.make_archive(zip_path.with_suffix("").as_posix(), "zip", temp_dir.as_posix())
        return FileResponse(
            path=str(zip_path),
            media_type="application/zip",
            filename=f"prepass_recipe_{recipe_id}.zip",
        )
    finally:
        # temp dir is left for FileResponse streaming; cleanup deferred by OS
        pass


def _import_prepass_recipe_from_zip(zip_path: Path) -> PrepassRecipeResponse:
    temp_dir = Path(tempfile.mkdtemp(prefix="prepass_recipe_import_"))
    try:
        extract_dir = temp_dir / "extract"
        extract_dir.mkdir(parents=True, exist_ok=True)
        with zipfile.ZipFile(zip_path) as zf:
            zf.extractall(extract_dir)
        manifest_path = extract_dir / "manifest.json"
        if not manifest_path.exists():
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="prepass_recipe_missing_manifest")
        manifest = json.loads(manifest_path.read_text())
        meta_path = extract_dir / PREPASS_RECIPE_META
        if not meta_path.exists():
            raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="prepass_recipe_missing_meta")
        meta = json.loads(meta_path.read_text())
        _validate_prepass_recipe_manifest(manifest, extract_dir)
        config = meta.get("config") or {}
        if isinstance(config, dict):
            config = dict(config)
            config.pop("dataset_id", None)
        glossary = meta.get("glossary") or ""
        labelmap_file = None
        for candidate in (extract_dir / "labelmap.txt", extract_dir / "labelmaps" / "labelmap.txt"):
            if candidate.exists():
                labelmap_file = candidate
                break
        if labelmap_file and not isinstance(config.get("labelmap"), list):
            try:
                lines = _read_labelmap_lines(labelmap_file)
            except Exception:
                lines = []
            if lines:
                config["labelmap"] = lines

        def _run_dir_matches(src: Path, dest: Path, keep_files: Optional[set[str]] = None) -> bool:
            if not dest.exists() or not dest.is_dir():
                return False
            for item in src.iterdir():
                if not item.is_file():
                    continue
                if keep_files is not None and item.name not in keep_files:
                    continue
                target = dest / item.name
                if not target.exists():
                    return False
                if target.stat().st_size != item.stat().st_size:
                    return False
            return True

        def _copy_run_assets(kind: str, root: Path, keep_files: Optional[set[str]] = None) -> Optional[str]:
            src_root = extract_dir / "models" / kind
            if not src_root.exists():
                return None
            for run_dir in src_root.iterdir():
                if not run_dir.is_dir():
                    continue
                existing = root / run_dir.name
                if _run_dir_matches(run_dir, existing, keep_files=keep_files):
                    return run_dir.name
                new_id = uuid.uuid4().hex
                dest = root / new_id
                dest.mkdir(parents=True, exist_ok=True)
                for item in run_dir.iterdir():
                    if item.is_file():
                        if keep_files is not None and item.name not in keep_files:
                            continue
                        shutil.copy2(item, dest / item.name)
                return new_id
            return None

        yolo_id = _copy_run_assets("yolo_runs", YOLO_JOB_ROOT, keep_files=None)
        rfdetr_id = _copy_run_assets("rfdetr_runs", RFDETR_JOB_ROOT, keep_files=RFDETR_KEEP_FILES)
        if yolo_id:
            config["yolo_id"] = yolo_id
        if rfdetr_id:
            config["rfdetr_id"] = rfdetr_id

        qwen_id_map: Dict[str, str] = {}
        qwen_root = extract_dir / "models" / "qwen_runs"
        if qwen_root.exists():
            for run_dir in qwen_root.iterdir():
                if not run_dir.is_dir():
                    continue
                meta_file = run_dir / QWEN_METADATA_FILENAME
                meta_payload: Dict[str, Any] = {}
                if meta_file.exists():
                    try:
                        meta_payload = json.loads(meta_file.read_text())
                    except Exception:
                        meta_payload = {}
                old_id = str(meta_payload.get("id") or run_dir.name)
                new_id = uuid.uuid4().hex
                dest = QWEN_JOB_ROOT / new_id
                dest.mkdir(parents=True, exist_ok=True)
                for item in run_dir.iterdir():
                    if item.is_file():
                        shutil.copy2(item, dest / item.name)
                # Update metadata id to match new run id if we can.
                meta_dest = dest / QWEN_METADATA_FILENAME
                if meta_dest.exists():
                    try:
                        payload = json.loads(meta_dest.read_text())
                    except Exception:
                        payload = {}
                    payload["id"] = new_id
                    meta_dest.write_text(json.dumps(payload, indent=2), encoding="utf-8")
                qwen_id_map[old_id] = new_id

        if qwen_id_map:
            for key in ("model_id", "prepass_caption_model_id"):
                val = config.get(key)
                if isinstance(val, str) and val in qwen_id_map:
                    config[key] = qwen_id_map[val]

        classifier_root = extract_dir / "models" / "classifiers"
        if classifier_root.exists():
            classifier_root.mkdir(parents=True, exist_ok=True)
            for item in classifier_root.iterdir():
                if item.is_file():
                    dest = (UPLOAD_ROOT / "classifiers") / item.name
                    dest.parent.mkdir(parents=True, exist_ok=True)
                    if not dest.exists() or dest.stat().st_size != item.stat().st_size:
                        shutil.copy2(item, dest)
                    config["classifier_id"] = str(dest.relative_to(UPLOAD_ROOT / "classifiers"))
                    break

        calib_root = extract_dir / "models" / "calibration_jobs"
        if calib_root.exists():
            for job_dir in calib_root.iterdir():
                if not job_dir.is_dir():
                    continue
                existing = CALIBRATION_ROOT / job_dir.name
                if _run_dir_matches(job_dir, existing, keep_files=None):
                    config["ensemble_job_id"] = job_dir.name
                else:
                    new_job = uuid.uuid4().hex
                    dest = CALIBRATION_ROOT / new_job
                    dest.mkdir(parents=True, exist_ok=True)
                    for item in job_dir.iterdir():
                        if item.is_file():
                            shutil.copy2(item, dest / item.name)
                    config["ensemble_job_id"] = new_job
                break

        original_name = meta.get("name") or f"Imported recipe {uuid.uuid4().hex[:8]}"
        unique_name, renamed_from = _unique_prepass_recipe_name(original_name)
        notice = None
        if renamed_from:
            notice = f"Recipe name '{renamed_from}' already exists. Imported as '{unique_name}'."
        recipe_id = uuid.uuid4().hex
        recipe_dir = _prepass_recipe_dir(recipe_id, create=True)
        now = time.time()
        recipe_meta = {
            "id": recipe_id,
            "schema_version": PREPASS_RECIPE_SCHEMA_VERSION,
            "name": unique_name,
            "description": meta.get("description") or "",
            "config": config,
            "glossary": _normalize_labelmap_glossary(glossary),
            "created_at": now,
            "updated_at": now,
        }
        _write_prepass_recipe_meta(recipe_dir, recipe_meta)
        return PrepassRecipeResponse(
            id=recipe_id,
            name=recipe_meta["name"],
            description=recipe_meta.get("description"),
            created_at=now,
            updated_at=now,
            config=recipe_meta["config"],
            glossary=recipe_meta.get("glossary") or None,
            renamed_from=renamed_from,
            notice=notice,
        )
    finally:
        shutil.rmtree(temp_dir, ignore_errors=True)


@app.post("/prepass/recipes/import", response_model=PrepassRecipeResponse)
def import_prepass_recipe(file: UploadFile = File(...)):  # noqa: B008
    temp_dir = Path(tempfile.mkdtemp(prefix="prepass_recipe_import_"))
    try:
        zip_path = temp_dir / "upload.zip"
        with zip_path.open("wb") as f:
            shutil.copyfileobj(file.file, f)
        return _import_prepass_recipe_from_zip(zip_path)
    finally:
        shutil.rmtree(temp_dir, ignore_errors=True)


@app.post("/prepass/recipes/import-raw", response_model=PrepassRecipeResponse)
async def import_prepass_recipe_raw(request: Request):
    if "application/zip" not in (request.headers.get("content-type") or ""):
        raise HTTPException(status_code=HTTP_415_UNSUPPORTED_MEDIA_TYPE, detail="prepass_recipe_invalid_media")
    temp_dir = Path(tempfile.mkdtemp(prefix="prepass_recipe_import_raw_"))
    try:
        zip_path = temp_dir / "upload.zip"
        with zip_path.open("wb") as f:
            async for chunk in request.stream():
                f.write(chunk)
        return _import_prepass_recipe_from_zip(zip_path)
    finally:
        shutil.rmtree(temp_dir, ignore_errors=True)

@app.post("/sam3/text_prompt", response_model=Sam3TextPromptResponse)
def sam3_text_prompt(payload: Sam3TextPrompt):
    variant = _default_variant(payload.sam_variant or "sam3")
    if variant != "sam3":
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="sam3_text_requires_sam3")
    pil_img, np_img, token = resolve_image_payload(payload.image_base64, payload.image_token, variant)
    effective_limit = payload.max_results
    detections, masks_arr = _run_sam3_text_inference(
        pil_img,
        payload.text_prompt,
        payload.threshold,
        payload.mask_threshold,
        effective_limit,
        return_masks=True,
        min_size=payload.min_size,
        simplify_epsilon=payload.simplify_epsilon,
    )
    warnings: List[str] = []
    if not detections:
        warnings.append("no_results")
    encoded_masks = None
    if detections:
        encoded_masks = []
        for idx, det in enumerate(detections):
            payload = det.mask if isinstance(det, QwenDetection) else None
            if payload is None and masks_arr is not None and idx < len(masks_arr) and masks_arr[idx] is not None:
                try:
                    payload = encode_binary_mask(masks_arr[idx])
                except Exception:
                    payload = None
            encoded_masks.append(payload)
        if all(m is None for m in encoded_masks):
            encoded_masks = None
    return Sam3TextPromptResponse(
        detections=detections,
        warnings=warnings,
        image_token=token,
        masks=encoded_masks,
    )


@app.post("/sam3/text_prompt_auto", response_model=Sam3TextPromptAutoResponse)
def sam3_text_prompt_auto(payload: Sam3TextPrompt):
    variant = _default_variant(payload.sam_variant or "sam3")
    if variant != "sam3":
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="sam3_text_requires_sam3")
    if not _active_encoder_ready():
        return Sam3TextPromptAutoResponse(
            detections=[],
            warnings=["clip_unavailable"],
            image_token=None,
        )
    pil_img, np_img, token = resolve_image_payload(payload.image_base64, payload.image_token, variant)
    effective_limit = payload.max_results
    detections, masks_arr = _run_sam3_text_inference(
        pil_img,
        payload.text_prompt,
        payload.threshold,
        payload.mask_threshold,
        effective_limit,
        return_masks=True,
        min_size=payload.min_size,
        simplify_epsilon=payload.simplify_epsilon,
    )
    # TODO: enrich with masks for polygon mode consumers.
    responses: List[SamPointAutoResponse] = []
    warnings: List[str] = []
    if not detections:
        warnings.append("no_results")
    for idx, det in enumerate(detections):
        mask = masks_arr[idx] if masks_arr is not None and idx < len(masks_arr) else None
        mask_payload = det.mask if hasattr(det, "mask") else None
        if mask_payload is None and mask is not None:
            mask_payload = encode_binary_mask(mask)
        if mask is not None:
            try:
                x_min, y_min, x_max, y_max = mask_to_bounding_box(mask)
            except Exception:
                x_min, y_min, x_max, y_max = yolo_to_corners(det.bbox, pil_img.width, pil_img.height)
        else:
            x_min, y_min, x_max, y_max = yolo_to_corners(det.bbox, pil_img.width, pil_img.height)
        li = max(0, int(x_min))
        ti = max(0, int(y_min))
        ri = min(pil_img.width, int(x_max))
        bi = min(pil_img.height, int(y_max))
        if ri <= li or bi <= ti:
            responses.append(
                SamPointAutoResponse(
                    prediction="unknown",
                    bbox=det.bbox,
                    uuid=str(uuid.uuid4()),
                    error="empty_mask",
                    image_token=token,
                    score=det.score,
                    mask=mask_payload,
                    simplify_epsilon=getattr(det, "simplify_epsilon", None),
                )
            )
            continue
        subarr = np_img[ti:bi, li:ri, :]
        final_pil = Image.fromarray(subarr)
        feats_np = _encode_pil_batch_for_active([final_pil])
        if feats_np is None or not isinstance(feats_np, np.ndarray) or feats_np.size == 0:
            responses.append(
                SamPointAutoResponse(
                    prediction="unknown",
                    bbox=det.bbox,
                    uuid=str(uuid.uuid4()),
                    error="clip_unavailable",
                    image_token=token,
                    score=det.score,
                    mask=mask_payload,
                    simplify_epsilon=getattr(det, "simplify_epsilon", None),
                )
            )
            continue
        details = _clip_auto_predict_details(feats_np)
        err = details.get("error")
        if isinstance(err, str) and err.startswith("classifier_error") and err not in warnings:
            warnings.append(err)
        responses.append(
            SamPointAutoResponse(
                prediction=str(details.get("label") or "unknown"),
                proba=details.get("proba"),
                second_label=details.get("second_label"),
                second_proba=details.get("second_proba"),
                margin=details.get("margin"),
                bbox=det.bbox,
                uuid=str(uuid.uuid4()),
                image_token=token,
                score=det.score,
                mask=mask_payload,
                simplify_epsilon=getattr(det, "simplify_epsilon", None),
                error=err,
            )
        )
    return Sam3TextPromptAutoResponse(detections=responses, warnings=warnings, image_token=token)


@app.post("/sam3/visual_prompt", response_model=Sam3TextPromptResponse)
def sam3_visual_prompt(payload: Sam3VisualPrompt):
    variant = _default_variant(payload.sam_variant or "sam3")
    if variant != "sam3":
        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail="sam3_visual_requires_sam3")
    pil_img, np_img, token = resolve_image_payload(payload.image_base64, payload.image_token, variant)
    effective_limit = payload.max_results
    try:
        if payload.bboxes:
            detections, masks_arr = _run_sam3_visual_inference_multi(
                pil_img,
                [tuple(bx) for bx in payload.bboxes],
                payload.bbox_labels,
                payload.threshold,
                payload.mask_threshold,
                effective_limit,
                return_masks=True,
                min_size=payload.min_size,
                simplify_epsilon=payload.simplify_epsilon,
            )
        else:
            detections, masks_arr = _run_sam3_visual_inference(
                pil_img,
                (
                    float(payload.bbox_left),
                    float(payload.bbox_top),
                    float(payload.bbox_width),
                    float(payload.bbox_height),
                ),
                payload.threshold,
                payload.mask_threshold,
                effective_limit,
                return_masks=True,
                min_size=payload.min_size,
                simplify_epsilon=payload.simplify_epsilon,
            )
    except HTTPException:
        raise
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=f"sam3_visual_failed:{exc}") from exc
    warnings: List[str] = []
    if not detections:
        warnings.append("no_results")
    encoded_masks = None
    if detections:
        encoded_masks = []
        for idx, det in enumerate(detections):
            payload_mask = det.mask if isinstance(det, QwenDetection) else None
            if payload_mask is None and masks_arr is not None and idx < len(masks_arr) and masks_arr[idx] is not None:
                try:
                    payload_mask = encode_binary_mask(masks_arr[idx])
                except Exception:
                    payload_mask = None
            encoded_masks.append(payload_mask)
        if all(m is None for m in encoded_masks):
            encoded_masks = None
    return Sam3TextPromptResponse(
        detections=detections,
        warnings=warnings,
        image_token=token,
        masks=encoded_masks,
    )


@app.post("/sam_point", response_model=YoloBboxOutput)
def sam_point(prompt: PointPrompt):
    pil_img, np_img, token = resolve_image_payload(
        prompt.image_base64,
        getattr(prompt, "image_token", None),
        getattr(prompt, "sam_variant", None),
    )
    coords = np.array([[prompt.point_x, prompt.point_y]])
    labels = np.array([1])
    variant = _default_variant(getattr(prompt, "sam_variant", None))
    masks, _, _ = _predict_with_cache(
        np_img,
        token,
        variant,
        image_name=getattr(prompt, "image_name", None),
        point_coords=coords,
        point_labels=labels,
        multimask_output=False,
    )
    mask_arr = np.asarray(masks[0])
    if mask_arr.dtype != np.uint8:
        mask_arr = (mask_arr > 0).astype(np.uint8)
    left, top, right, bottom = mask_to_bounding_box(mask_arr)
    yolo_box = to_yolo(pil_img.width, pil_img.height, left, top, right, bottom)
    return YoloBboxOutput(
        class_id="0",
        bbox=yolo_box,
        uuid=prompt.uuid,
        image_token=token,
        mask=encode_binary_mask(mask_arr),
        simplify_epsilon=None,
    )


@app.post("/sam_bbox_auto", response_model=SamPointAutoResponse)
def sam_bbox_auto(prompt: BboxPrompt):
    if not _active_encoder_ready():
        return SamPointAutoResponse(prediction=ERROR_MESSAGE, bbox=[], uuid=prompt.uuid)

    pil_img, np_img, token = resolve_image_payload(
        prompt.image_base64,
        getattr(prompt, "image_token", None),
        getattr(prompt, "sam_variant", None),
    )
    full_h, full_w = pil_img.height, pil_img.width
    left = max(0, prompt.bbox_left)
    top = max(0, prompt.bbox_top)
    right = min(full_w, left + prompt.bbox_width)
    bottom = min(full_h, top + prompt.bbox_height)
    if right <= left or bottom <= top:
        return SamPointAutoResponse(
            prediction="unknown",
            bbox=[0, 0, 0, 0],
            uuid=prompt.uuid,
            error="invalid_bbox",
            image_token=token,
        )
    sub_box = np.array([left, top, right, bottom], dtype=np.float32)
    variant = _default_variant(getattr(prompt, "sam_variant", None))
    masks, _, _ = _predict_with_cache(
        np_img,
        token,
        variant,
        image_name=getattr(prompt, "image_name", None),
        box=sub_box,
        multimask_output=False,
    )
    mask_arr = np.asarray(masks[0])
    if mask_arr.dtype != np.uint8:
        mask_arr = (mask_arr > 0).astype(np.uint8)
    x_min, y_min, x_max, y_max = mask_to_bounding_box(mask_arr)
    yolo_box = to_yolo(full_w, full_h, x_min, y_min, x_max, y_max)
    gx_min_i = max(0, int(x_min))
    gy_min_i = max(0, int(y_min))
    gx_max_i = min(full_w, int(x_max))
    gy_max_i = min(full_h, int(y_max))
    if gx_max_i <= gx_min_i or gy_max_i <= gy_min_i:
        return SamPointAutoResponse(
            prediction="unknown",
            bbox=yolo_box,
            uuid=prompt.uuid,
            error="empty_mask",
            image_token=token,
        )
    subarr = np_img[gy_min_i:gy_max_i, gx_min_i:gx_max_i, :]
    final_pil = Image.fromarray(subarr)
    feats_np = _encode_pil_batch_for_active([final_pil])
    if feats_np is None or not isinstance(feats_np, np.ndarray) or feats_np.size == 0:
        return SamPointAutoResponse(prediction="unknown", bbox=yolo_box, uuid=prompt.uuid, error="clip_unavailable", image_token=token)
    details = _clip_auto_predict_details(feats_np)
    return SamPointAutoResponse(
        prediction=str(details.get("label") or "unknown"),
        proba=details.get("proba"),
        second_label=details.get("second_label"),
        second_proba=details.get("second_proba"),
        margin=details.get("margin"),
        bbox=yolo_box,
        uuid=prompt.uuid,
        image_token=token,
        mask=encode_binary_mask(mask_arr),
        simplify_epsilon=None,
        error=details.get("error"),
    )


@app.post("/sam_point_auto", response_model=SamPointAutoResponse)
def sam_point_auto(prompt: PointPrompt):
    if not _active_encoder_ready():
        return SamPointAutoResponse(prediction=ERROR_MESSAGE, bbox=[], uuid=prompt.uuid)

    pil_img, np_img, token = resolve_image_payload(
        prompt.image_base64,
        getattr(prompt, "image_token", None),
        getattr(prompt, "sam_variant", None),
    )
    coords = np.array([[prompt.point_x, prompt.point_y]])
    labels = np.array([1])
    variant = _default_variant(getattr(prompt, "sam_variant", None))
    masks, _, _ = _predict_with_cache(
        np_img,
        token,
        variant,
        image_name=getattr(prompt, "image_name", None),
        point_coords=coords,
        point_labels=labels,
        multimask_output=False,
    )
    mask_arr = np.asarray(masks[0])
    if mask_arr.dtype != np.uint8:
        mask_arr = (mask_arr > 0).astype(np.uint8)
    left, top, right, bottom = mask_to_bounding_box(mask_arr)
    yolo_box = to_yolo(pil_img.width, pil_img.height, left, top, right, bottom)
    li = max(0, int(left))
    ti = max(0, int(top))
    ri = min(pil_img.width, int(right))
    bi = min(pil_img.height, int(bottom))
    if ri <= li or bi <= ti:
        return SamPointAutoResponse(
            prediction="unknown",
            bbox=yolo_box,
            uuid=prompt.uuid,
            error="empty_mask",
            image_token=token,
            mask=encode_binary_mask(mask_arr),
            simplify_epsilon=None,
        )
    subarr = np_img[ti:bi, li:ri, :]
    final_pil = Image.fromarray(subarr)
    feats_np = _encode_pil_batch_for_active([final_pil])
    if feats_np is None or not isinstance(feats_np, np.ndarray) or feats_np.size == 0:
        return SamPointAutoResponse(
            prediction="unknown",
            bbox=yolo_box,
            uuid=prompt.uuid,
            error="clip_unavailable",
            image_token=token,
            mask=encode_binary_mask(mask_arr),
            simplify_epsilon=None,
        )
    details = _clip_auto_predict_details(feats_np)
    return SamPointAutoResponse(
        prediction=str(details.get("label") or "unknown"),
        proba=details.get("proba"),
        second_label=details.get("second_label"),
        second_proba=details.get("second_proba"),
        margin=details.get("margin"),
        bbox=yolo_box,
        uuid=prompt.uuid,
        image_token=token,
        mask=encode_binary_mask(mask_arr),
        simplify_epsilon=None,
        error=details.get("error"),
    )


@app.post("/sam_point_multi", response_model=YoloBboxOutput)
def sam_point_multi(prompt: MultiPointPrompt):
    positive = prompt.positive_points or []
    negative = prompt.negative_points or []
    if len(positive) == 0:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="positive_points_required")

    pil_img, np_img, token = resolve_image_payload(
        prompt.image_base64,
        getattr(prompt, "image_token", None),
        getattr(prompt, "sam_variant", None),
    )
    coords = np.array(positive + negative, dtype=np.float32)
    labels = np.array([1] * len(positive) + [0] * len(negative), dtype=np.int64)
    variant = _default_variant(getattr(prompt, "sam_variant", None))
    masks, _, _ = _predict_with_cache(
        np_img,
        token,
        variant,
        image_name=getattr(prompt, "image_name", None),
        point_coords=coords,
        point_labels=labels,
        multimask_output=False,
    )
    mask_arr = np.asarray(masks[0])
    if mask_arr.dtype != np.uint8:
        mask_arr = (mask_arr > 0).astype(np.uint8)
    left, top, right, bottom = mask_to_bounding_box(mask_arr)
    yolo_box = to_yolo(pil_img.width, pil_img.height, left, top, right, bottom)
    return YoloBboxOutput(
        class_id="0",
        bbox=yolo_box,
        uuid=prompt.uuid,
        image_token=token,
        mask=encode_binary_mask(mask_arr),
        simplify_epsilon=None,
    )


@app.post("/sam_point_multi_auto", response_model=SamPointAutoResponse)
def sam_point_multi_auto(prompt: MultiPointPrompt):
    if not _active_encoder_ready():
        return SamPointAutoResponse(prediction=ERROR_MESSAGE, bbox=[], uuid=prompt.uuid)

    positive = prompt.positive_points or []
    negative = prompt.negative_points or []
    if len(positive) == 0:
        raise HTTPException(status_code=HTTP_422_UNPROCESSABLE_ENTITY, detail="positive_points_required")

    pil_img, np_img, token = resolve_image_payload(
        prompt.image_base64,
        getattr(prompt, "image_token", None),
        getattr(prompt, "sam_variant", None),
    )
    coords = np.array(positive + negative, dtype=np.float32)
    labels = np.array([1] * len(positive) + [0] * len(negative), dtype=np.int64)
    variant = _default_variant(getattr(prompt, "sam_variant", None))
    masks, _, _ = _predict_with_cache(
        np_img,
        token,
        variant,
        image_name=getattr(prompt, "image_name", None),
        point_coords=coords,
        point_labels=labels,
        multimask_output=False,
    )
    mask_arr = np.asarray(masks[0])
    if mask_arr.dtype != np.uint8:
        mask_arr = (mask_arr > 0).astype(np.uint8)
    left, top, right, bottom = mask_to_bounding_box(mask_arr)
    yolo_box = to_yolo(pil_img.width, pil_img.height, left, top, right, bottom)
    li = max(0, int(left))
    ti = max(0, int(top))
    ri = min(pil_img.width, int(right))
    bi = min(pil_img.height, int(bottom))
    if ri <= li or bi <= ti:
        return SamPointAutoResponse(prediction="unknown", bbox=yolo_box, uuid=prompt.uuid, error="empty_mask", image_token=token)
    subarr = np_img[ti:bi, li:ri, :]
    final_pil = Image.fromarray(subarr)
    feats_np = _encode_pil_batch_for_active([final_pil])
    if feats_np is None or not isinstance(feats_np, np.ndarray) or feats_np.size == 0:
        return SamPointAutoResponse(prediction="unknown", bbox=yolo_box, uuid=prompt.uuid, error="clip_unavailable", image_token=token)
    details = _clip_auto_predict_details(feats_np)
    return SamPointAutoResponse(
        prediction=str(details.get("label") or "unknown"),
        proba=details.get("proba"),
        second_label=details.get("second_label"),
        second_proba=details.get("second_proba"),
        margin=details.get("margin"),
        bbox=yolo_box,
        uuid=prompt.uuid,
        image_token=token,
        mask=encode_binary_mask(mask_arr),
        simplify_epsilon=None,
        error=details.get("error"),
    )


@app.post("/sam_bbox", response_model=YoloBboxOutput)
def sam_bbox(prompt: BboxPrompt):
    pil_img, np_img, token = resolve_image_payload(
        prompt.image_base64,
        getattr(prompt, "image_token", None),
        getattr(prompt, "sam_variant", None),
    )
    full_h, full_w = pil_img.height, pil_img.width
    left = max(0, prompt.bbox_left)
    top = max(0, prompt.bbox_top)
    right = min(full_w, left + prompt.bbox_width)
    bottom = min(full_h, top + prompt.bbox_height)
    if right <= left or bottom <= top:
        return YoloBboxOutput(
            class_id="0",
            bbox=[0, 0, 0, 0],
            uuid=prompt.uuid
        )
    sub_box = np.array([left, top, right, bottom], dtype=np.float32)
    variant = _default_variant(getattr(prompt, "sam_variant", None))
    masks, _, _ = _predict_with_cache(
        np_img,
        token,
        variant,
        image_name=getattr(prompt, "image_name", None),
        box=sub_box,
        multimask_output=False,
    )
    mask_arr = np.asarray(masks[0])
    if mask_arr.dtype != np.uint8:
        mask_arr = (mask_arr > 0).astype(np.uint8)
    x_min, y_min, x_max, y_max = mask_to_bounding_box(mask_arr)
    yolo_box = to_yolo(full_w, full_h, x_min, y_min, x_max, y_max)
    gx_min_i = max(0, int(x_min))
    gy_min_i = max(0, int(y_min))
    gx_max_i = min(full_w, int(x_max))
    gy_max_i = min(full_h, int(y_max))
    if gx_max_i <= gx_min_i or gy_max_i <= gy_min_i:
        return YoloBboxOutput(
            class_id="0",
            bbox=yolo_box,
            uuid=prompt.uuid,
            image_token=token,
        )
    return YoloBboxOutput(
        class_id="0",
        bbox=yolo_box,
        uuid=prompt.uuid,
        image_token=token,
        mask=encode_binary_mask(mask_arr),
        simplify_epsilon=None,
    )

@app.post("/crop_zip_init")
def crop_zip_init():
    jobId = str(uuid.uuid4())
    job_store[jobId] = []
    return {"jobId": jobId}

@app.post("/crop_zip_chunk")
def crop_zip_chunk(request: CropZipRequest, jobId: str = Query(...)):
    if jobId not in job_store:
        raise HTTPException(status_code=400, detail="Invalid jobId")
    job_store[jobId].extend(request.images)
    return {"status": "ok", "count": len(request.images)}

@app.get("/crop_zip_finalize")
def crop_zip_finalize(jobId: str):
    if jobId not in job_store:
        raise HTTPException(status_code=400, detail="Invalid jobId")
    all_images = job_store[jobId]
    if len(all_images) == 0:
        empty_buffer = io.BytesIO()
        with zipfile.ZipFile(empty_buffer, mode="w") as zf:
            pass
        empty_buffer.seek(0)
        del job_store[jobId]
        return StreamingResponse(
            empty_buffer,
            media_type="application/x-zip-compressed",
            headers={"Content-Disposition": "attachment; filename=crops.zip"}
        )
    zip_buffer = io.BytesIO()
    with zipfile.ZipFile(zip_buffer, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
        for i, cropImage in enumerate(all_images):
            img_data = base64.b64decode(cropImage.image_base64)
            pil_img = Image.open(io.BytesIO(img_data)).convert("RGB")
            for bindex, bbox in enumerate(cropImage.bboxes):
                left = bbox.x
                top = bbox.y
                right = left + bbox.width
                bottom = top + bbox.height
                left = max(0, min(left, pil_img.width))
                right = max(0, min(right, pil_img.width))
                top = max(0, min(top, pil_img.height))
                bottom = max(0, min(bottom, pil_img.height))
                if right <= left or bottom <= top:
                    continue
                sub_img = pil_img.crop((left, top, right, bottom))
                stem = cropImage.originalName.rsplit(".",1)[0]
                out_name = f"{stem}-{bbox.className}-{bindex}.jpg"
                crop_buffer = io.BytesIO()
                sub_img.save(crop_buffer, format="JPEG")
                crop_buffer.seek(0)
                zf.writestr(out_name, crop_buffer.read())
    zip_buffer.seek(0)
    del job_store[jobId]
    return StreamingResponse(
        zip_buffer,
        media_type="application/x-zip-compressed",
        headers={"Content-Disposition": "attachment; filename=crops.zip"}
    )


if os.environ.get("COORD_ROUNDTRIP_TEST") == "1":
    _coord_roundtrip_smoke()
