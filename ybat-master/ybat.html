<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>ü•î Tator Annotation Tool</title>
    <link href="ybat.css?v=20251220" rel="stylesheet">

    <!-- 1) Keep scripts in an order that ensures ybat.js is available  
         before we call listenImageCrop().
    -->
    <script src="canvas.min.js?v=20251220"></script>
    <script src="jszip.min.js?v=20251220"></script>
    <script src="filesaver.min.js?v=20251220"></script>
    <script src="ybat.js?v=20251220"></script>
</head>
<body>
    <div id="ingestProgress" class="ingest-progress" aria-live="polite" aria-atomic="true">
        <div id="ingestProgressLabel" class="ingest-progress__label">Loading‚Ä¶</div>
        <div id="ingestProgressDetail" class="ingest-progress__detail">0/0</div>
    </div>
    <div id="backgroundLoadModal" class="modal" aria-hidden="true">
        <div class="modal__backdrop" data-modal-dismiss="background"></div>
        <div class="modal__dialog" role="dialog" aria-modal="true" aria-labelledby="backgroundLoadTitle">
            <div class="modal__title" id="backgroundLoadTitle">Still loading‚Ä¶</div>
            <p class="modal__body" id="backgroundLoadMessage">Images and annotations are still being prepared in the background.</p>
            <div class="modal__actions">
                <button type="button" class="modal__btn modal__btn--primary" id="backgroundLoadDismiss">OK</button>
            </div>
        </div>
    </div>
    <div id="batchTweakModal" class="modal" aria-hidden="true">
        <div class="modal__backdrop" data-modal-dismiss="batchTweak"></div>
        <div class="modal__dialog" role="dialog" aria-modal="true" aria-labelledby="batchTweakTitle">
            <div class="modal__title" id="batchTweakTitle">Apply SAM tweak?</div>
            <p class="modal__body" id="batchTweakMessage">
                Apply SAM bbox tweak to all boxes in <span id="batchTweakClass">this class</span>?
            </p>
            <div class="modal__actions">
                <button type="button" class="modal__btn modal__btn--primary" id="batchTweakConfirm">Yes</button>
                <button type="button" class="modal__btn" id="batchTweakCancel">No</button>
            </div>
        </div>
	    </div>
	    <div id="trainingPackagingModal" class="modal" aria-hidden="true">
	        <div class="modal__backdrop" data-modal-dismiss="trainingPackaging"></div>
	        <div class="modal__dialog" role="dialog" aria-modal="true" aria-labelledby="trainingPackagingTitle">
            <div class="modal__title" id="trainingPackagingTitle">Packaging dataset‚Ä¶</div>
            <p class="training-packaging__stats" id="trainingPackagingStats">Estimating selection‚Ä¶</p>
            <div class="training-packaging-progress">
                <div class="training-packaging-progress__fill" id="trainingPackagingProgressFill"></div>
            </div>
            <p class="training-packaging__progress" id="trainingPackagingProgressText">Preparing‚Ä¶</p>
            <p class="training-packaging__eta" id="trainingPackagingEta">Estimating upload time‚Ä¶</p>
            <p class="training-packaging__elapsed" id="trainingPackagingElapsed">Elapsed: 0s</p>
            <p class="training-packaging__hint" id="trainingPackagingHint">Keep this tab open while we stage files and upload them to the server. Larger datasets can take a few minutes.</p>
            <div class="modal__actions">
                <button type="button" class="modal__btn" id="trainingPackagingDismiss">Hide</button>
            </div>
	        </div>
	    </div>
	    <div id="taskQueue" class="task-queue" aria-live="polite" aria-atomic="true"></div>
	    <div id="classScrollIndicator" class="class-scroll-indicator" aria-live="polite" aria-atomic="true"></div>
	    <div class="tab-shell">
	        <div class="tab-bar" role="tablist">
	            <button class="tab-button active" data-tab="labeling" id="tabLabelingButton" type="button">Label Images</button>
            <button class="tab-button" data-tab="training" id="tabTrainingButton" type="button">Train Class Predictor</button>
            <button class="tab-button" data-tab="qwen-train" id="tabQwenTrainButton" type="button">Train Qwen</button>
            <button class="tab-button" data-tab="sam3-train" id="tabSam3TrainButton" type="button">Train SAM3</button>
            <button class="tab-button" data-tab="yolo-train" id="tabYoloTrainButton" type="button">Train YOLO</button>
            <button class="tab-button" data-tab="rfdetr-train" id="tabRfDetrTrainButton" type="button">Train RF-DETR</button>
            <button class="tab-button" data-tab="agent-mining" id="tabAgentMiningButton" type="button">SAM3 Recipe Mining</button>
            <button class="tab-button" data-tab="prompt-helper" id="tabPromptHelperButton" type="button">SAM3 Vocabulary Explorer</button>
            <button class="tab-button" data-tab="datasets" id="tabDatasetsButton" type="button">Dataset Management</button>
            <button class="tab-button" data-tab="sam3-prompt-models" id="tabSam3PromptModelsButton" type="button">SAM Model Selection</button>
            <button class="tab-button" data-tab="active" id="tabActiveButton" type="button">Class Predictor Settings</button>
            <button class="tab-button" data-tab="qwen" id="tabQwenButton" type="button">Qwen Models</button>
            <button class="tab-button" data-tab="predictors" id="tabPredictorsButton" type="button">SAM Predictors</button>
            <button class="tab-button" data-tab="settings" id="tabSettingsButton" type="button">Backend Config</button>
        </div>
        <div class="tab-panels">
            <div class="tab-panel active" id="tabLabeling" data-tab-panel="labeling">
                <div class="container" id="container">
                    <div class="left">
                        <form action="">
                            <label for="images">Images:</label>
                            <input type="file" id="images" name="images[]" accept="image/jpeg, image/png" multiple class="file-input-hidden" />
                            <label for="images" id="imagesSelect" class="training-button" role="button" tabindex="0">Choose Images‚Ä¶</label>
                            <button type="button" name="cropImages" id="cropImages" class="training-button">Crop &amp; Save</button>
                            <br />
                            <label for="imageSearch">Search:</label>
                            <input type="text" id="imageSearch" name="imageSearch" />
                            <label for="imageList"></label>
                            <select name="imageList" id="imageList" size="10" multiple></select>
                            <div id="imageInformation"></div>
                            <div id="datasetTypeBadge" class="training-help"></div>
                            <div class="training-actions">
                                <button type="button" id="polygonDrawToggle" class="training-button secondary" aria-pressed="true">
                                    Polygon draw: On (P)
                                </button>
                            </div>
                            <div class="training-help">Seg mode only: P toggles polygon drawing. When off, clicks select/move polygons without adding new ones.</div>
                            <label for="classes">Classes:</label>
                            <input type="file" id="classes" name="classes" accept="text/plain" class="file-input-hidden" />
                            <label for="classes" id="classesSelect" class="training-button" role="button" tabindex="0">Load Classes‚Ä¶</label>
                            <label for="classList"></label>
                            <select name="classList" id="classList" size="10" multiple></select>
                            <div id="bboxInformation"></div>
                            <hr />
                            <div class="training-field training-field--inline">
                                <label for="autoMode">Auto Class</label>
                                <input type="checkbox" id="autoMode" name="autoMode" />
                            </div>
                            <div class="training-field training-field--inline">
                                <label for="autoClassMarginEnabled">Auto-class clarity guard<span class="help-icon" title="Optional. Require the top CLIP class to beat the runner-up by a minimum margin before we accept it. Helps avoid ambiguous auto-class suggestions.">?</span></label>
                                <input type="checkbox" id="autoClassMarginEnabled" />
                            </div>
                            <div class="training-field">
                                <label for="autoClassMarginValue">Auto-class min margin<span class="help-icon" title="Minimum difference between the best and second-best CLIP class. If the gap is smaller, we keep the current class instead of auto-assigning.">?</span></label>
                                <input type="number" id="autoClassMarginValue" value="0.2" min="0" max="1" step="0.01" />
                                <div class="training-help">Use higher values for stricter auto-class picks (fewer ambiguous labels).</div>
                            </div>
                            <div class="training-field training-field--inline">
                                <label for="autoClassMarginWarn">Show ambiguity notice<span class="help-icon" title="If the clarity guard rejects a class, show a quick notice like ‚ÄúCLIP class unclear between car and truck.‚Äù">?</span></label>
                                <input type="checkbox" id="autoClassMarginWarn" />
                            </div>
                            <div class="training-field training-field--inline">
                                <label for="samMode">SAM Mode</label>
                                <input type="checkbox" id="samMode" name="samMode" />
                            </div>
                            <div class="training-field">
                                <label for="samVariant">SAM Model</label>
                                <select id="samVariant" name="samVariant">
                                    <option value="sam1">SAM 1</option>
                                    <option value="sam3" selected>SAM 3</option>
                                </select>
                            </div>
                            <div class="training-field training-field--inline">
                                <label for="samPreload">Preload SAM</label>
                                <input type="checkbox" id="samPreload" name="samPreload" />
                            </div>
                            <div class="training-field training-field--inline">
                                <label for="pointMode">Point Mode</label>
                                <input type="checkbox" id="pointMode" name="pointMode" />
                            </div>
                            <div class="training-field training-field--inline">
                                <label for="multiPointMode">Multi-Point Mode</label>
                                <input type="checkbox" id="multiPointMode" name="multiPointMode" />
                            </div>
                            <div class="training-field">
                                <label for="regionDetector">Region detector</label>
                                <select id="regionDetector">
                                    <option value="yolo" selected>YOLOv8</option>
                                    <option value="rfdetr">RF-DETR</option>
                                </select>
                                <div class="training-help">Used when you hold R and drag a region on the canvas.</div>
                            </div>
                            <div class="training-field">
                                <label for="yoloRegionConf">Region conf<span class="help-icon" title="Hold R and drag to run the selected detector only on the region. Zooming in gives the model more detail. Lower = more detections, higher = stricter.">?</span></label>
                                <input type="number" id="yoloRegionConf" value="0.25" min="0" max="1" step="0.01" />
                                <div class="training-help">Runs the selected detector on the crop at full model resolution. Detections are kept if their center lands inside the region.</div>
                            </div>
                            <div class="training-field">
                                <label for="yoloRegionIou">Region IoU<span class="help-icon" title="Non-max suppression threshold (YOLO only). Higher keeps more overlapping boxes.">?</span></label>
                                <input type="number" id="yoloRegionIou" value="0.45" min="0" max="1" step="0.01" />
                            </div>
                            <div class="training-field">
                                <label for="yoloRegionMaxDet">Region max dets<span class="help-icon" title="Max boxes returned for the selected region.">?</span></label>
                                <input type="number" id="yoloRegionMaxDet" value="100" min="1" step="1" />
                            </div>
                            <div class="training-field" id="polygonSimplifyField">
                                <label for="polygonSimplifyEpsilon">Polygon detail</label>
                                <input type="range" id="polygonSimplifyEpsilon" name="polygonSimplifyEpsilon" min="0" max="40" step="0.5" value="20" />
                                <div class="training-help">Left = simpler polygons, right = more detail (more points).</div>
                            </div>
                            <div id="samStatus" class="sam-status" role="status" aria-live="polite"></div>
                            <div id="samStatusProgress" class="sam-status-progress" aria-hidden="true">
                                <div id="samStatusProgressFill"></div>
                            </div>
                            <br />
                            <div id="sam3SimilarityRow" class="training-actions" style="margin-bottom: 12px;">
                                <button type="button" id="sam3SimilarityButton" class="training-button secondary" title="Shift-click/drag selects positives (green). Shift+Alt selects negatives (red). Runs on the selected boxes for this image.">
                                    SAM3 similarity prompt (use selected box(es))
                                </button>
                                <label for="sam3SimilarityThreshold" class="training-help" style="margin-top: 6px;">Similarity score (higher = stricter)</label>
                                <input type="range" id="sam3SimilarityThreshold" min="0" max="1" step="0.05" value="0.5" />
                            </div>
                            <section class="qwen-panel" aria-labelledby="qwenPanelTitle">
                                <div class="qwen-panel__header">
                                    <span id="qwenPanelTitle">Qwen 2.5 Assist</span>
                                    <span id="qwenActiveModelLabel" class="qwen-active-model-label"></span>
                                    <span id="qwenStatusLabel" class="qwen-status-label">Checking‚Ä¶</span>
                                </div>
                                <label for="qwenItems">Objects / keywords</label>
                                <textarea id="qwenItems" rows="3" placeholder="List what to detect (comma or line separated)"></textarea>
                                <button type="button" id="qwenAdvancedToggle" class="qwen-advanced-toggle" aria-expanded="false">Show advanced overrides</button>
                                <div id="qwenAdvancedPanel" class="qwen-advanced" aria-hidden="true">
                                    <label for="qwenCustomPrompt">Full prompt override</label>
                                    <textarea id="qwenCustomPrompt" rows="3" placeholder="Optional: supply the exact prompt to send to Qwen"></textarea>
                                    <label for="qwenImageType">Image type description</label>
                                    <input type="text" id="qwenImageType" placeholder="Pulled from active Qwen model" />
                                    <label for="qwenExtraContext">Extra context</label>
                                    <textarea id="qwenExtraContext" rows="2" placeholder="Optional notes (lighting, camera, etc.)"></textarea>
                                </div>
                                <label for="qwenClassSelect">Assign detections to class</label>
                                <select id="qwenClassSelect"></select>
                                <label for="qwenPromptType">Prompt output</label>
                                <select id="qwenPromptType">
                                    <option value="bbox" selected>Bounding boxes</option>
                                    <option value="bbox_sam">Bounding boxes ‚Üí SAM cleanup</option>
                                    <option value="point">Points ‚Üí SAM boxes</option>
                                </select>
                                <label for="qwenMaxResults">Max detections</label>
                                <input type="number" id="qwenMaxResults" min="1" max="50" value="8" />
                                <button type="button" id="qwenRunButton" class="training-button" disabled>Use Qwen</button>
                                <p class="qwen-hint">Uses the prompts saved with the active Qwen model; add extra context via the advanced overrides when needed.</p>
                                <section class="sam3-text-panel" id="sam3TextPanel" aria-labelledby="sam3TextTitle">
                                    <div class="sam3-text-panel__header">
                                        <span id="sam3TextTitle"><strong>SAM3 Text Prompt</strong></span>
                                        <span id="sam3TextStatus" class="sam3-text-status" aria-live="polite"></span>
                                    </div>
                                    <p class="sam3-text-hint">Runs text-driven segmentation with SAM3. Can be used alongside the bbox tools.</p>
                                    <label for="sam3TextPrompt">Prompt (describe what to segment)</label>
                                    <input type="text" id="sam3TextPrompt" placeholder="e.g., red helmet" />
                                    <div class="sam3-text-grid">
                                        <label for="sam3Threshold">Score threshold</label>
                                        <input type="number" id="sam3Threshold" step="0.05" min="0" max="1" value="0.5" />
                                        <label for="sam3MaskThreshold">Mask threshold</label>
                                        <input type="number" id="sam3MaskThreshold" step="0.05" min="0" max="1" value="0.5" />
                                        <label for="sam3MaxResults">Max results</label>
                                        <input type="number" id="sam3MaxResults" min="1" max="100" value="20" />
                                        <label for="sam3MinSize">Min size (px¬≤)</label>
                                        <input type="number" id="sam3MinSize" min="0" value="0" title="Ignore detections smaller than this pixel area. 0 keeps all." />
                                        <label for="sam3MaxPoints">Max points/polygon</label>
                                        <input type="number" id="sam3MaxPoints" min="10" max="5000" value="500" title="Simplification cap per polygon. Lower for smoother shapes; higher preserves detail." />
                                        <label for="sam3SimplifyEpsilon">Simplify epsilon (px)</label>
                                        <input type="number" id="sam3SimplifyEpsilon" min="0" step="0.1" value="1.0" title="Douglas‚ÄìPeucker epsilon in pixels on the downsampled mask: higher = smoother polygons, lower = more detail." />
                                    </div>
                                    <label for="sam3ClassSelect">Assign detections to class</label>
                                    <select id="sam3ClassSelect"></select>
                                    <div class="sam3-text-buttons">
                                        <button type="button" id="sam3RunButton" class="training-button">Run SAM3</button>
                                        <button type="button" id="sam3TextCascadeToggle" class="training-button secondary">Start cascade</button>
                                    </div>
                                    <div id="sam3TextCascadePanel" class="sam3-text-cascade" hidden>
                                        <div class="sam3-text-cascade__header">
                                            <strong>Text prompt cascade</strong>
                                            <div class="sam3-text-cascade__actions">
                                                <button type="button" id="sam3TextCascadeAdd" class="training-button secondary">+ Add step</button>
                                                <button type="button" id="sam3TextCascadeRun" class="training-button">Run cascade</button>
                                                <button type="button" id="sam3TextCascadeStop" class="training-button danger" disabled>Stop</button>
                                                <button type="button" id="sam3TextCascadeClear" class="training-button secondary">Clear</button>
                                            </div>
                                        </div>
                                        <p class="training-help" style="margin-top: 4px;">
                                            Chain multiple SAM3 text prompts with their own thresholds and target classes. Steps run in order on the current image.
                                        </p>
                                        <div id="sam3TextCascadeSteps" class="sam3-text-cascade__steps"></div>
                                    </div>
                                    <div class="sam3-text-batch" style="margin-top: 10px;">
                                        <div class="sam3-text-batch__count">
                                            <label for="sam3BatchCount">Apply to next N images</label>
                                            <input type="number" id="sam3BatchCount" min="1" max="999" value="5" />
	                                        </div>
	                                        <div class="sam3-text-batch__options">
	                                            <label class="sam3-text-batch__option" for="sam3BatchIncludeCurrent">
	                                                <input type="checkbox" id="sam3BatchIncludeCurrent" checked />
	                                                Include current image
	                                            </label>
	                                        </div>
	                                        <div class="sam3-text-batch__actions">
	                                            <button type="button" id="sam3BatchRunButton" class="training-button secondary">Run batch</button>
	                                            <button type="button" id="sam3BatchStopButton" class="training-button danger" disabled>Stop batch</button>
	                                        </div>
                                    </div>
                                    <p class="training-help" style="margin-top: 6px;">
                                        Runs the current prompt sequentially across the next N images. If cascade is enabled, runs every cascade step per image.
                                    </p>
                                </section>
                            </section>
                            <br />
                            <hr />
                            <label for="bboxes">Bboxes:</label>
                            <input type="file" id="bboxes" name="bboxes[]" accept="text/plain,application/zip" disabled multiple class="file-input-hidden" />
                            <label for="bboxes" id="bboxesSelect" class="training-button button-disabled" aria-disabled="true" role="button" tabindex="-1">Import Bboxes‚Ä¶</label>
                            <input type="file" id="bboxesFolder" name="bboxesFolder[]" accept="text/plain" disabled multiple webkitdirectory directory class="file-input-hidden" />
                            <label for="bboxesFolder" id="bboxesSelectFolder" class="training-button button-disabled" aria-disabled="true" role="button" tabindex="-1">Import Bboxes Folder‚Ä¶</label>
                            <button type="button" name="saveBboxes" id="saveBboxes" class="training-button">Save YOLO</button>
                            <hr />
                            <div id="description">
                                SHORTCUTS:
                                <ul>
                                    <li><strong>X</strong> ‚Äî press with a bbox selected to run the SAM tweak</li>
                                    <li><strong>X</strong> twice ‚Äî batch tweak all bboxes in the class currently selected in the list</li>
                                    <li>Shift + click ‚Äî add/remove selection (green)</li>
                                    <li>Shift + drag ‚Äî selection box (green)</li>
                                    <li>Shift + Alt + click/drag ‚Äî select negatives for SAM3 similarity (red)</li>
                                    <li>Hold R + drag ‚Äî Region detect (YOLO/RF-DETR; keeps detections whose center is inside; zoom in for more detail)</li>
                                    <li>Mouse wheel ‚Äî zoom</li>
                                    <li>Shift + wheel ‚Äî pan</li>
                                    <li>Right-click drag ‚Äî pan</li>
                                    <li>‚Üê / ‚Üí ‚Äî cycle images</li>
                                    <li>‚Üë / ‚Üì ‚Äî cycle classes</li>
                                    <li>Delete / Backspace / W ‚Äî remove selected bbox</li>
                                    <li>Q ‚Äî delete the most recently created bbox</li>
                                    <li>Enter ‚Äî submit multi-point mask</li>
                                    <li>Hold Z ‚Äî temporarily disable Auto Class and all SAM modes so you can select or delete the active bbox</li>
                                    <li>A ‚Äî toggle auto class</li>
                                    <li><strong>1</strong> ‚Äî SAM3 similarity prompt using selected boxes (pos/neg)</li>
                                    <li>S ‚Äî toggle SAM</li>
                                    <li>D ‚Äî toggle SAM point mode</li>
                                    <li>M ‚Äî toggle SAM multi-point</li>
                                    <li>F ‚Äî add positive point</li>
                                    <li>G ‚Äî add negative point</li>
                                </ul>
                            </div>
                        </form>
                    </div>
                    <div class="right" id="right">
                        <div id="clipProgressBar"><div id="clipProgressFill"></div></div>
                        <canvas id="canvas"></canvas>
                        <canvas id="hiddenCanvas"></canvas>
                    </div>
                </div>
            </div>
            <div class="tab-panel" id="tabTraining" data-tab-panel="training">
                <div class="training-wrapper two-col">
                    <section class="training-form-section">
                        <h2>Class Predictor Settings</h2>
                        <div class="training-help" style="margin: 8px 0 14px; padding: 10px 12px; border: 1px solid #e2e8f0; border-radius: 12px; background: #f8fafc;">
                            <div style="font-weight: 600; color: #0f172a;">Backbone guide (recent comparison on qwen_dataset)</div>
                            <div style="margin-top: 6px;">
                                <strong>DINOv3 LVD (ViT-L/16)</strong> ranked highest in our test, followed by LVD ViT-B/16, then LVD ViT-S/16.
                                <strong>CLIP</strong> trailed but enables text prompts and prompt prefiltering.
                                <strong>SAT backbones</strong> were slower and did not beat LVD in that test.
                            </div>
                            <div style="margin-top: 6px;">
                                <strong>Compute / VRAM / latency (rough bands)</strong><br />
                                CLIP ViT-B/32: Low (<=8&nbsp;GB) ‚Ä¢ Fast ¬∑ CLIP ViT-B/16: Medium (8-12&nbsp;GB) ‚Ä¢ Medium ¬∑ CLIP ViT-L/14: High (12-16&nbsp;GB) ‚Ä¢ Slow<br />
                                DINOv3 LVD ViT-S/B: Medium (12-16&nbsp;GB) ‚Ä¢ Slow ¬∑ DINOv3 LVD ViT-L/H+: High (16-24&nbsp;GB+) ‚Ä¢ Slower ¬∑ DINOv3 7B/SAT: Very high (48&nbsp;GB+) ‚Ä¢ Very slow
                            </div>
                            <div style="margin-top: 6px; color: #475569;">Choose the largest model your GPU can handle if accuracy matters; use smaller models for faster iteration. Results vary by dataset.</div>
                        </div>
                        <div class="training-field">
                            <label for="trainEncoderType">Encoder type</label>
                            <select id="trainEncoderType">
                                <option value="clip" selected>CLIP (image + text)</option>
                                <option value="dinov3">DINOv3 (image-only)</option>
                            </select>
                            <div class="training-help">CLIP is the default and enables prompt prefiltering; DINOv3 is image-only and skips text-based prompt filters.</div>
                        </div>
                        <div class="training-field" id="clipBackboneRow">
                            <label for="clipBackboneSelect">CLIP Backbone</label>
                            <select id="clipBackboneSelect"></select>
                            <div class="training-help">Default is the largest backbone (ViT-L/14) for best accuracy. Choose a smaller model for faster training or lower VRAM.</div>
                        </div>
                        <div class="training-field" id="dinov3BackboneRow" hidden>
                            <label for="dinov3BackboneSelect">DINOv3 Backbone</label>
                            <select id="dinov3BackboneSelect"></select>
                            <div class="training-help">Frozen image encoder only (no text). Larger models are more accurate but need more VRAM.</div>
                        </div>
                        <div class="training-field">
                            <label for="qwenTrainDevices">Device IDs</label>
                            <input type="text" id="qwenTrainDevices" placeholder="e.g., 0,1" />
                            <div class="training-help">Leave blank to use a single GPU. Enter comma-separated CUDA device indices to split across multiple GPUs.</div>
                        </div>
                        <div class="training-field logreg-only">
                            <label for="trainSolver">Solver</label>
                            <select id="trainSolver">
                                <option value="saga" selected>SAGA (elastic net-friendly)</option>
                                <option value="lbfgs">LBFGS</option>
                                <option value="liblinear">LibLinear</option>
                                <option value="sag">SAG</option>
                                <option value="newton-cg">Newton-CG</option>
                            </select>
                        </div>
                        <div class="training-field">
                            <label for="trainClassifierType">Classifier head</label>
                            <select id="trainClassifierType">
                                <option value="logreg">LogReg (fast, linear)</option>
                                <option value="mlp" selected>MLP (nonlinear, slower)</option>
                            </select>
                            <div class="training-help">MLP can improve class separation at the cost of extra training time. LogReg is fast and stable.</div>
                            <div class="training-help">MLP input size equals the encoder embedding size (CLIP ViT-B = 512, ViT-L = 768, DINOv3 varies). Use hidden sizes around 0.5‚Äì1.0√ó the embedding for best results.</div>
                            <div class="training-help mlp-only" hidden>LogReg-only tuning (solver, C, hard mining, max iter) is hidden when MLP is selected.</div>
                        </div>
                        <div class="training-field">
                            <label for="trainDatasetSelect">Cached dataset (YOLO)</label>
                            <div class="training-picker">
                                <select id="trainDatasetSelect">
                                    <option value="">Select a dataset‚Ä¶</option>
                                </select>
                                <button type="button" class="training-button secondary" id="trainDatasetRefresh">Refresh</button>
                            </div>
                            <div class="training-help" id="trainDatasetSummary">Choose a cached YOLO dataset (bbox or seg). Use Dataset Management to upload datasets.</div>
                        </div>
                        <div class="training-field">
                            <label for="trainUploadCurrentDatasetBtn">Current labeling dataset</label>
                            <div class="training-picker">
                                <button type="button" class="training-button" id="trainUploadCurrentDatasetBtn">Upload current dataset</button>
                                <button type="button" class="training-button secondary" id="trainOpenDatasetManagerBtn">Open Dataset Management</button>
                            </div>
                            <div class="training-help">Uploads the dataset from the labeling tab as YOLO so it appears in the cached list above.</div>
                        </div>
                        <div class="training-field-double">
                            <div>
                                <label for="trainModelFilename">Model Filename</label>
                                <input type="text" id="trainModelFilename" value="my_logreg_model.pkl" />
                            </div>
                            <div>
                                <label for="trainLabelmapFilename">Labelmap Filename</label>
                                <input type="text" id="trainLabelmapFilename" value="my_label_list.pkl" />
                            </div>
                        </div>
                        <div class="training-help">
                            Saved automatically to <code>uploads/classifiers/</code> (model + meta) and <code>uploads/labelmaps/</code> (labelmap).
                        </div>
                        <div class="training-advanced-grid">
                            <div>
                                <label for="trainTestSize">Test Size</label>
                                <input type="number" id="trainTestSize" min="0" max="0.9" step="0.05" value="0.2" />
                                <div class="training-help">Fraction of images reserved for evaluation.</div>
                            </div>
                            <div>
                                <label for="trainRandomSeed">Seed</label>
                                <input type="number" id="trainRandomSeed" value="42" />
                                <div class="training-help">Controls deterministic train/test split.</div>
                            </div>
                            <div>
                                <label for="trainBatchSize">Batch Size</label>
                                <input type="number" id="trainBatchSize" value="64" min="1" />
                                <div class="training-help">Images encoded per CLIP forward pass.</div>
                            </div>
                            <div class="logreg-only">
                                <label for="trainMaxIter">Max Iter</label>
                                <input type="number" id="trainMaxIter" value="1000" min="100" step="50" />
                                <div class="training-help">Upper bound on solver iterations.</div>
                            </div>
                            <div>
                                <label for="trainMinPerClass">Min / Class</label>
                                <input type="number" id="trainMinPerClass" value="2" min="1" />
                                <div class="training-help">Drops classes with fewer samples before training.</div>
                            </div>
                            <div class="logreg-only">
                                <label for="trainRegC">C</label>
                                <input type="number" id="trainRegC" value="1.0" step="0.1" min="0.01" />
                                <div class="training-help">Inverse regularisation strength (higher = less regularisation).</div>
                            </div>
                            <div class="mlp-only" hidden>
                                <label for="trainMlpHiddenSizes">MLP hidden sizes</label>
                                <div class="training-picker">
                                    <input type="text" id="trainMlpHiddenSizes" value="256" />
                                    <button type="button" class="training-button secondary" id="trainMlpHiddenSizesAuto">Use recommended</button>
                                </div>
                                <div class="training-help">Comma-separated layer sizes (e.g., 768,384). Use recommended to match the selected backbone. Smaller layers train faster, larger layers boost accuracy.</div>
                            </div>
                            <div class="mlp-only" hidden>
                                <label for="trainMlpDropout">MLP dropout</label>
                                <input type="number" id="trainMlpDropout" value="0.1" min="0" max="0.9" step="0.05" />
                                <div class="training-help">Dropout between hidden layers.</div>
                            </div>
                            <div class="mlp-only" hidden>
                                <label for="trainMlpEpochs">MLP epochs</label>
                                <input type="number" id="trainMlpEpochs" value="50" min="1" step="1" />
                                <div class="training-help">Training epochs for the MLP head.</div>
                            </div>
                            <div class="mlp-only" hidden>
                                <label for="trainMlpLr">MLP learning rate</label>
                                <input type="number" id="trainMlpLr" value="0.001" min="0.000001" step="0.0001" />
                                <div class="training-help">AdamW learning rate.</div>
                            </div>
                            <div class="mlp-only" hidden>
                                <label for="trainMlpWeightDecay">MLP weight decay</label>
                                <input type="number" id="trainMlpWeightDecay" value="0.0001" min="0" step="0.0001" />
                                <div class="training-help">L2 regularisation for the MLP head.</div>
                            </div>
                            <div class="mlp-only" hidden>
                                <label for="trainMlpLabelSmoothing">Soft targets (label smoothing)</label>
                                <input type="number" id="trainMlpLabelSmoothing" value="0.1" min="0" max="0.3" step="0.01" />
                                <div class="training-help">Softens targets to improve class separation (0 disables).</div>
                            </div>
                            <div class="mlp-only" hidden>
                                <label for="trainMlpLossType">MLP loss</label>
                                <select id="trainMlpLossType">
                                    <option value="ce" selected>Cross-entropy (default)</option>
                                    <option value="focal">Focal loss (hard examples)</option>
                                </select>
                                <div class="training-help">Focal loss downweights easy examples so the head focuses on harder mistakes.</div>
                            </div>
                            <div class="mlp-only" hidden>
                                <label for="trainMlpActivation">MLP activation</label>
                                <select id="trainMlpActivation">
                                    <option value="relu">ReLU (fast)</option>
                                    <option value="gelu" selected>GELU (smoother)</option>
                                </select>
                                <div class="training-help">GELU can improve separation but is a bit slower.</div>
                            </div>
                            <div class="mlp-only" hidden>
                                <label for="trainMlpLayerNorm">MLP layer norm</label>
                                <div class="training-toggle-row" style="margin: 6px 0 0;">
                                    <input type="checkbox" id="trainMlpLayerNorm" checked />
                                    <span>Normalize each hidden layer</span>
                                </div>
                                <div class="training-help">Helps stabilize training when embeddings vary a lot.</div>
                            </div>
                            <div class="mlp-only mlp-focal-only" hidden>
                                <label for="trainMlpFocalGamma">Focal gamma</label>
                                <input type="number" id="trainMlpFocalGamma" value="2.0" min="0" max="5" step="0.1" />
                                <div class="training-help">Higher = stronger focus on hard examples (2.0 is typical).</div>
                            </div>
                            <div class="mlp-only mlp-focal-only" hidden>
                                <label for="trainMlpFocalAlpha">Focal alpha</label>
                                <input type="number" id="trainMlpFocalAlpha" value="-1" min="-1" max="1" step="0.05" />
                                <div class="training-help">Optional extra scaling (set -1 to disable).</div>
                            </div>
                            <div class="mlp-only" hidden>
                                <label for="trainMlpSampler">MLP sampler</label>
                                <select id="trainMlpSampler">
                                    <option value="balanced" selected>Balanced (recommended)</option>
                                    <option value="shuffle">Shuffle (natural)</option>
                                    <option value="none">None (sequential)</option>
                                </select>
                                <div class="training-help">Balanced sampling helps small classes show up more often.</div>
                            </div>
                            <div class="mlp-only" hidden>
                                <label for="trainMlpMixupAlpha">Mixup alpha</label>
                                <input type="number" id="trainMlpMixupAlpha" value="0.1" min="0" max="2" step="0.05" />
                                <div class="training-help">Blends embeddings to improve generalization (0 disables).</div>
                            </div>
                            <div class="mlp-only" hidden>
                                <label for="trainMlpNormalizeEmbeddings">Normalize embeddings</label>
                                <div class="training-toggle-row" style="margin: 6px 0 0;">
                                    <input type="checkbox" id="trainMlpNormalizeEmbeddings" checked />
                                    <span>Normalize (L2) before training</span>
                                </div>
                                <div class="training-help">Stabilizes MLP training when embedding scales vary.</div>
                            </div>
                            <div>
                                <label for="trainEmbeddingCenter">Center embeddings</label>
                                <div class="training-toggle-row" style="margin: 6px 0 0;">
                                    <input type="checkbox" id="trainEmbeddingCenter" />
                                    <span>Subtract mean (per-dimension)</span>
                                </div>
                                <div class="training-help">Reduces CLIP/DINO embedding anisotropy.</div>
                            </div>
                            <div>
                                <label for="trainEmbeddingStandardize">Standardize embeddings</label>
                                <div class="training-toggle-row" style="margin: 6px 0 0;">
                                    <input type="checkbox" id="trainEmbeddingStandardize" checked />
                                    <span>Divide by per-dimension std</span>
                                </div>
                                <div class="training-help">Acts like whitening; can boost separation for hard classes.</div>
                            </div>
                            <div class="mlp-only" hidden>
                                <label for="trainMlpPatience">MLP early-stop patience</label>
                                <input type="number" id="trainMlpPatience" value="6" min="1" step="1" />
                                <div class="training-help">Stop when val loss stops improving.</div>
                            </div>
                            <div class="mlp-only hard-mining-only" hidden>
                                <label for="trainMlpHardMiningEpochs">MLP hard-mining epochs</label>
                                <input type="number" id="trainMlpHardMiningEpochs" value="5" min="1" step="1" />
                                <div class="training-help">Extra epochs that focus on misclassified / low-confidence samples.</div>
                            </div>
                            <div>
                                <label for="trainClassWeight">Class Weight</label>
                                <select id="trainClassWeight">
                                    <option value="none">None</option>
                                    <option value="balanced">Balanced</option>
                                    <option value="effective" selected>Effective-number (rare class focus)</option>
                                </select>
                                <div class="training-help">`balanced` reweights rare classes; `effective` uses the effective-number formula; `none` leaves raw counts.</div>
                            </div>
                            <div>
                                <label for="trainEffectiveBeta">Effective beta</label>
                                <input type="number" id="trainEffectiveBeta" value="0.9999" min="0.5" max="0.99999" step="0.0001" />
                                <div class="training-help">Only used when Class Weight is ‚ÄúEffective‚Äù. Higher = more weight on rare classes.</div>
                            </div>
                            <div>
                                <label for="trainLogitAdjustmentMode">Long-tail mode (logit adjustment)</label>
                                <div class="training-toggle-row" style="margin: 6px 0 0;">
                                    <input type="checkbox" id="trainLogitAdjustmentMode" />
                                    <span>Enable logit adjustment (MLP trains + inference; LogReg inference only)</span>
                                </div>
                                <div class="training-help">Adds a class-prior adjustment to logits so rare classes are less suppressed. For LogReg, only inference is adjusted.</div>
                            </div>
                            <div>
                                <label for="trainDeviceOverride">Device Override</label>
                                <input type="text" id="trainDeviceOverride" placeholder="cpu or cuda" />
                            </div>
                            <div>
                                <label for="trainCalibrationMode">Calibration</label>
                                <select id="trainCalibrationMode">
                                    <option value="none">None</option>
                                    <option value="temperature" selected>Temperature scaling</option>
                                </select>
                                <div class="training-help">Calibrates probabilities after training (does not change argmax classes).</div>
                            </div>
                            <div class="hard-mining-only">
                                <label for="trainHardMisWeight">Hard W (misclass)</label>
                                <input type="number" id="trainHardMisWeight" value="3.0" min="1" step="0.1" />
                                <div class="training-help">Multiplier applied to misclassified samples in hard mining.</div>
                            </div>
                            <div class="hard-mining-only">
                                <label for="trainHardLowConfWeight">Hard W (low conf)</label>
                                <input type="number" id="trainHardLowConfWeight" value="2.0" min="1" step="0.1" />
                                <div class="training-help">Multiplier for low-confidence samples (below the thresholds).</div>
                            </div>
                            <div class="hard-mining-only">
                                <label for="trainHardLowConfThreshold">Low-conf threshold</label>
                                <input type="number" id="trainHardLowConfThreshold" value="0.65" min="0" max="0.9999" step="0.01" />
                                <div class="training-help">Samples with max probability below this value get boosted.</div>
                            </div>
                            <div class="hard-mining-only">
                                <label for="trainHardMarginThreshold">Margin threshold</label>
                                <input type="number" id="trainHardMarginThreshold" value="0.15" min="0" max="1" step="0.01" />
                                <div class="training-help">Boost samples whose top-1 vs top-2 gap falls under this margin.</div>
                            </div>
                            <div class="logreg-only">
                                <label for="trainConvergenceTol">Convergence tol</label>
                                <input type="number" id="trainConvergenceTol" value="0.0001" min="0" step="0.00001" />
                                <div class="training-help">Lower tolerance forces more iterations before convergence.</div>
                            </div>
                        </div>
                        <div class="training-divider"></div>
                        <div class="training-field">
                            <label for="trainBgClassCount">Background classes (1‚Äì10)<span class="help-icon" title="Adds hidden ‚Äúbackground‚Äù classes trained on empty crops. These help CLIP reject false positives by learning what ‚Äúnone of the above‚Äù looks like. Use 1‚Äì3 for quick runs; 6‚Äì10 for tougher datasets. We cluster negatives into multiple groups so each class is coherent (better than one overly-general background class).">?</span></label>
                            <input type="number" id="trainBgClassCount" min="1" max="10" step="1" value="2" />
                            <div class="training-help">
                                Hidden negative classes sampled from empty areas. More classes give finer ‚Äúbackground‚Äù coverage, but increase training time.
                            </div>
                        </div>
                        <button type="button" id="startTrainingBtn">Start Training</button>
                        <div class="training-toggle-row">
                            <label title="Speeds up repeat runs by reusing cached embeddings. If you change labels or class definitions, disable this once to rebuild the cache."><input type="checkbox" id="trainReuseEmbeddings" checked /> Cache and reuse embeddings</label>
                            <label><input type="checkbox" id="trainHardMining" checked /> Hard example mining</label>
                        </div>
                        <div class="training-message" id="trainingMessage" role="status"></div>
                    </section>
                    <section class="training-status-section">
                        <h2>Training Status</h2>
                        <div class="training-progress" id="trainingProgressBar">
                            <div class="training-progress-fill" id="trainingProgressFill"></div>
                        </div>
                        <div class="training-status-text" id="trainingStatusText">Idle</div>
                        <div class="training-actions">
                            <button type="button" class="training-button training-button-danger" id="cancelTrainingBtn" disabled>Cancel Job</button>
                        </div>
                        <div class="training-summary" id="trainingSummary"></div>
                        <div class="training-chart">
                            <h3>Validation Metric</h3>
                            <canvas id="trainingMetricCanvas" height="160"></canvas>
                            <div class="training-help" id="trainingChartStatus">Metrics appear after training completes.</div>
                        </div>
                        <div class="training-logs">
                            <h3>Logs</h3>
                            <pre id="trainingLog"></pre>
                        </div>
                        <div class="training-history">
                            <h3>Recent Jobs</h3>
                            <div id="trainingHistory"></div>
                        </div>
                    </section>
                </div>
            </div>
            <div class="tab-panel" id="tabQwenTrain" data-tab-panel="qwen-train">
                <div class="training-wrapper two-col">
	                    <section class="training-form-section">
	                        <h2>Fine-tune Qwen 2.5 VL</h2>
	                        <p class="qwen-train-intro">
	                            Fine-tune Qwen on a dataset managed in the Dataset Management tab. Upload a YOLO / YOLO-seg dataset first, then build Qwen
	                            annotations (JSONL) so Qwen can learn to return JSON detections (bbox + point) for every class in your label map.
	                        </p>
                        <div class="qwen-train-callouts">
                            <div>
                                <strong>LoRA</strong>
                                <p>Trains lightweight adapters on top of the base FP16 model. Fast and easy when you have ‚â•24&nbsp;GB VRAM.</p>
                            </div>
                            <div>
                                <strong>QLoRA</strong>
                                <p>Quantizes the base model to 4-bit (NF4) and only trains adapters. Ideal for 18‚Äì24&nbsp;GB GPUs.</p>
                            </div>
                        </div>
                        <p class="qwen-train-note">
                            Each training sample reuses your description plus the system prompt here. For every epoch we randomly ask Qwen to return
                            either bounding boxes or click points, and we vary the scope of the request: sometimes all classes, sometimes a single class
                            that exists in the image, and sometimes a small subset. The targets are filtered to match that instruction so the adapters learn
                            both broad sweeps and class-specific prompts without duplicating the dataset on disk.
                        </p>
                        <div class="training-field">
                            <label for="qwenTrainRunName">Run name / subfolder</label>
                            <input type="text" id="qwenTrainRunName" placeholder="Optional; defaults to job id" />
                            <div class="training-help">Checkpoints land in <code>uploads/qwen_runs/&lt;run_name&gt;</code>.</div>
                        </div>
	                        <div class="training-field">
	                            <label>Dataset</label>
	                            <div class="qwen-dataset-cache-controls">
	                                <select id="qwenDatasetSelect"></select>
	                                <button type="button" class="training-button" id="qwenDatasetRefresh">Refresh</button>
	                            </div>
	                            <div class="training-help" id="qwenDatasetSummary">If the selected dataset says "needs Qwen build", go to Dataset Management and click "Build Qwen".</div>
	                        </div>
                        <div class="training-grid">
                            <div class="checkbox-row">
                                <label><input type="checkbox" id="qwenTrainRandomSplit" checked /> Random split (ignore existing train/val)</label>
                            </div>
                            <div>
                                <label for="qwenTrainValPercent">Val %</label>
                                <input type="number" id="qwenTrainValPercent" min="1" max="90" value="30" />
                            </div>
                            <div>
                                <label for="qwenTrainSplitSeed">Split seed</label>
                                <input type="number" id="qwenTrainSplitSeed" value="42" />
                            </div>
                        </div>
                        <div class="training-help">We rebuild the train/val split per job using this seed and percentage.</div>
                        <div class="training-grid">
                            <div class="training-help" id="qwenCacheInfo">Split cache: ‚Ä¶</div>
                            <div>
                                <button id="qwenCachePurge" class="secondary">Purge split cache</button>
                            </div>
                        </div>
                        <div class="training-field">
                            <label for="qwenTrainModelId">Base model repo</label>
                            <input type="text" id="qwenTrainModelId" value="Qwen/Qwen2.5-VL-3B-Instruct" />
                        </div>
                        <div class="training-field">
                            <label for="qwenTrainSystemPrompt">System prompt</label>
                            <textarea id="qwenTrainSystemPrompt" rows="3">You are an annotation assistant that only returns JSON shaped like {"detections":[{"label":"class","bbox":[x1,y1,x2,y2]} or {"label":"class","point":[x,y]}]}. Respond with JSON only.</textarea>
                            <div class="training-help">Shared system message for every conversation. The user prompt still toggles between bbox vs. point instructions per sample.</div>
                        </div>
                        <div class="training-field">
                            <label for="qwenTrainPromptNoise">System prompt noise (0‚Äì0.30)</label>
                            <input type="number" id="qwenTrainPromptNoise" min="0" max="0.3" step="0.01" value="0.05" />
                            <div class="training-help">We randomly drop this fraction of characters from the system prompt for each sample to make the model more robust.</div>
                        </div>
                        <div class="training-field">
                            <label for="qwenTrainAccelerator">Accelerator</label>
                            <select id="qwenTrainAccelerator">
                                <option value="gpu" selected>GPU (recommended)</option>
                                <option value="auto">Auto</option>
                                <option value="cpu">CPU (debug only)</option>
                            </select>
                        </div>
                        <fieldset class="qwen-radio-group">
                            <legend>Adapter strategy</legend>
                            <label class="qwen-radio">
                                <input type="radio" name="qwenLoraMode" value="qlora" checked />
                                <span>
                                    <strong>QLoRA (default)</strong>
                                    <small>Quantize the backbone to 4-bit NF4 and train adapters. Lower VRAM, same accuracy.</small>
                                </span>
                            </label>
                            <label class="qwen-radio">
                                <input type="radio" name="qwenLoraMode" value="lora" />
                                <span>
                                    <strong>LoRA</strong>
                                    <small>Keep the base model in FP16/BF16 and only train adapters. Requires more VRAM but slightly simpler.</small>
                                </span>
                            </label>
                        </fieldset>
                        <div class="training-advanced-grid">
                            <div>
                                <label for="qwenTrainBatchSize">Batch size</label>
                                <input type="number" id="qwenTrainBatchSize" value="1" min="1" />
                            </div>
                            <div>
                                <label for="qwenTrainEpochs">Max epochs</label>
                                <input type="number" id="qwenTrainEpochs" value="10" min="1" />
                            </div>
                            <div>
                                <label for="qwenTrainLR">Learning rate</label>
                                <input type="number" step="0.00001" id="qwenTrainLR" value="0.0002" />
                            </div>
                            <div>
                                <label for="qwenTrainAccumulate">Gradient accumulation</label>
                                <input type="number" id="qwenTrainAccumulate" value="8" min="1" />
                            </div>
                            <div>
                                <label for="qwenTrainDevices">Device IDs</label>
                                <input type="text" id="qwenTrainDevices" placeholder="e.g., 0,1" />
                                <div class="training-help">Leave blank for a single GPU. Enter comma-separated CUDA IDs to train on multiple GPUs.</div>
                            </div>
                            <div>
                                <label for="qwenTrainLoraRank">LoRA rank</label>
                                <input type="number" id="qwenTrainLoraRank" value="8" min="1" />
                            </div>
                            <div>
                                <label for="qwenTrainLoraAlpha">LoRA alpha</label>
                                <input type="number" id="qwenTrainLoraAlpha" value="16" min="1" />
                            </div>
                            <div>
                                <label for="qwenTrainLoraDropout">LoRA dropout</label>
                                <input type="number" id="qwenTrainLoraDropout" value="0.05" step="0.01" min="0" max="1" />
                            </div>
                            <div>
                                <label for="qwenTrainPatience">Early stop patience</label>
                                <input type="number" id="qwenTrainPatience" value="3" min="1" />
                            </div>
                            <div>
                                <label for="qwenTrainMaxImageDim">Max image dimension (px)</label>
                                <input type="number" id="qwenTrainMaxImageDim" value="1024" min="256" max="4096" step="64" />
                                <div class="training-help">Images larger than this shrink on the longest side before reaching Qwen.</div>
                            </div>
                            <div>
                                <label for="qwenTrainMaxDetections">Max detections per sample</label>
                                <input type="number" id="qwenTrainMaxDetections" value="200" min="1" max="200" />
                                <div class="training-help">Uses the same per-class-aware cap from the trainer; drop below 200 to ease VRAM pressure.</div>
                            </div>
                        </div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="qwenTrainStartBtn">Start Training</button>
                            <button type="button" class="training-button training-button-danger" id="qwenTrainCancelBtn" disabled>Cancel Job</button>
                        </div>
                        <div class="training-message" id="qwenTrainMessage" role="status"></div>
                    </section>
                    <section class="training-status-section">
                        <h2>Qwen Training Status</h2>
                        <div class="training-progress" id="qwenTrainProgressBar">
                            <div class="training-progress-fill" id="qwenTrainProgressFill"></div>
                        </div>
                        <div class="training-status-text" id="qwenTrainStatusText">Idle</div>
                        <div class="training-epoch-detail" id="qwenTrainEpochDetail">Waiting for telemetry‚Ä¶</div>
                        <div class="training-chart">
                            <div class="training-chart-header">
                                <h3>Training Loss</h3>
                                <label class="chart-smoothing">
                                    Smoothing
                                    <select id="qwenTrainChartSmoothing">
                                        <option value="1">None</option>
                                        <option value="5">5-point</option>
                                        <option value="15" selected>15-point</option>
                                    </select>
                                </label>
                            </div>
                            <canvas id="qwenTrainLossCanvas"></canvas>
                            <div class="training-help" id="qwenTrainChartStatus">Loss telemetry will appear while a job is running.</div>
                        </div>
                        <div class="qwen-sample-panel">
                            <div class="qwen-sample-header">
                                <h3>Random Training Sample</h3>
                                <button type="button" class="training-button" id="qwenSampleBtn">Generate random Qwen data sample</button>
                            </div>
                            <div class="qwen-sample-canvas">
                                <canvas id="qwenSampleCanvas" width="320" height="240"></canvas>
                                <div class="qwen-sample-status" id="qwenSampleMessage">Load images + label map, then click the button to preview how a training conversation is constructed.</div>
                            </div>
                            <div class="qwen-sample-meta" id="qwenSampleMeta"></div>
                            <div class="qwen-sample-details">
                                <label>Prompt sent to Qwen</label>
                                <pre id="qwenSamplePrompt"></pre>
                                <label>Expected JSON response</label>
                                <pre id="qwenSampleExpected"></pre>
                            </div>
                        </div>
                        <div class="training-summary" id="qwenTrainSummary"></div>
                        <div class="training-logs">
                            <h3>Logs</h3>
                            <pre id="qwenTrainLog"></pre>
                        </div>
                        <div class="training-history">
                            <h3>Recent Qwen Jobs</h3>
                            <div id="qwenTrainHistory"></div>
                        </div>
                    </section>
                </div>
            </div>
            <div class="tab-panel" id="tabSam3Train" data-tab-panel="sam3-train">
                <div class="training-wrapper two-col">
                    <section class="training-form-section">
                        <h2>Train SAM3 (text-promptable by default)</h2>
                        <p class="training-help">Pick a cached dataset (Qwen uploads or YOLO folders). By default we keep the SAM3 segmentation head so the resulting checkpoint works with text prompting; masks are only required if you opt in below.</p>
                        <div class="training-field">
                            <label>Dataset</label>
                            <div class="sam3-dataset-row">
                                <select id="sam3DatasetSelect"></select>
                                <button type="button" class="training-button" id="sam3DatasetRefresh">Refresh</button>
                                <button type="button" class="training-button" id="sam3DatasetConvert">Convert</button>
                            </div>
                            <div class="training-help" id="sam3DatasetSummary">We‚Äôll auto-convert to COCO when needed.</div>
                        </div>
                        <div class="training-grid">
                            <div class="checkbox-row">
                                <label><input type="checkbox" id="sam3RandomSplit" checked /> Random split (ignore existing train/val)</label>
                            </div>
                            <div>
                                <label for="sam3ValPercent">Val %</label>
                                <input type="number" id="sam3ValPercent" min="1" max="90" value="20" />
                            </div>
                            <div>
                                <label for="sam3SplitSeed">Split seed</label>
                                <input type="number" id="sam3SplitSeed" value="42" />
                            </div>
                        </div>
                        <div class="training-help">Train/val splits are rebuilt deterministically per job using this seed.</div>
                        <div class="training-grid">
                            <div class="training-help" id="sam3CacheInfo">Split cache: ‚Ä¶</div>
                            <div>
                                <button id="sam3CachePurge" class="secondary">Purge split cache</button>
                            </div>
                        </div>
                        <div class="training-field">
                            <label for="sam3RunName">Run name / log dir</label>
                            <input type="text" id="sam3RunName" placeholder="Optional; defaults to job id" />
                            <div class="training-help">Logs + checkpoints land in <code>uploads/sam3_runs/&lt;run_name&gt;</code>.</div>
                        </div>
                        <div class="training-grid">
                            <div>
                                <label for="sam3TrainBatch">Train batch size</label>
                                <input type="number" id="sam3TrainBatch" min="1" value="1" />
                            </div>
                            <div>
                                <label for="sam3ValBatch">Val batch size</label>
                                <input type="number" id="sam3ValBatch" min="1" value="1" />
                            </div>
                            <div>
                                <label for="sam3TrainWorkers">Train workers</label>
                                <input type="number" id="sam3TrainWorkers" min="0" value="4" />
                            </div>
                            <div>
                                <label for="sam3ValWorkers">Val workers</label>
                                <input type="number" id="sam3ValWorkers" min="0" value="2" />
                            </div>
                            <div>
                                <label for="sam3Epochs">Max epochs</label>
                                <input type="number" id="sam3Epochs" min="1" value="20" />
                            </div>
                            <div>
                                <label for="sam3Resolution">Resolution</label>
                                <input type="number" id="sam3Resolution" min="256" value="1008" step="16" />
                            </div>
                            <div>
                                <label for="sam3LrScale">LR scale</label>
                                <input type="number" id="sam3LrScale" step="0.01" value="0.1" />
                            </div>
                            <div>
                                <label for="sam3GradAccum">Grad accumulation</label>
                                <input type="number" id="sam3GradAccum" min="1" value="1" />
                            </div>
                            <div>
                                <label for="sam3ValFreq">Val every N epochs</label>
                                <input type="number" id="sam3ValFreq" min="1" value="10" />
                            </div>
                            <div>
                                <label for="sam3ValScoreThresh">Val score threshold</label>
                                <input type="number" id="sam3ValScoreThresh" step="0.01" min="0" max="1" value="0.2" title="Drop detections below this score before COCO eval." />
                            </div>
                            <div>
                                <label for="sam3ValMaxDets">Val max detections per image</label>
                                <input type="number" id="sam3ValMaxDets" min="1" value="1000" title="Keep only the top N detections per image before COCO eval." />
                            </div>
                            <div class="checkbox-row">
                                <label><input type="checkbox" id="sam3CapEpoch" checked /> Cap epoch size</label>
                                <label for="sam3TargetEpochSize">Batches per epoch</label>
                                <input type="number" id="sam3TargetEpochSize" min="1" value="1500" title="Number of batches to treat as one epoch. When capped, epochs stop after this many batches even if the dataset is larger." />
                            </div>
                            <div class="checkbox-row">
                                <label><input type="checkbox" id="sam3CapVal" /> Cap validation size</label>
                                <label for="sam3ValCapSize">Val images</label>
                                <input type="number" id="sam3ValCapSize" min="1" value="200" disabled title="Optional cap on number of validation images for quick smoke tests." />
                            </div>
                            <div>
                                <label for="sam3BalanceStrategy" title="Choose how to rebalance rare classes.">
                                    Balance strategy
                                </label>
                                <select id="sam3BalanceStrategy">
                                    <option value="none" selected>None (uniform sampling)</option>
                                    <option value="inv_sqrt">Mild: 1/sqrt(freq)</option>
                                    <option value="clipped_inv">Clipped inverse-freq</option>
                                    <option value="effective_num">Effective number of samples</option>
                                    <option value="focal">Focal sampling</option>
                                </select>
                                <div id="sam3BalanceDescription" class="training-help"></div>
                            </div>
                            <div class="balance-param-row sam3-balance-param" data-param="power">
                                <label for="sam3BalancePower" title="Exponent for inverse-frequency weighting (1/freq^power). Lower = milder.">
                                    Inverse power
                                </label>
                                <input type="number" id="sam3BalancePower" step="0.1" min="0" value="0.5" />
                            </div>
                            <div class="balance-param-row sam3-balance-param" data-param="clip">
                                <label for="sam3BalanceClip" title="Clamp the ratio between highest and lowest weights. e.g., 10 means rare classes get at most 10x the weight of common classes.">
                                    Clip ratio (max/min)
                                </label>
                                <input type="number" id="sam3BalanceClip" step="1" min="1" value="10" />
                            </div>
                            <div class="balance-param-row sam3-balance-param" data-param="beta">
                                <label for="sam3BalanceBeta" title="Beta for effective number of samples weighting. Higher (0.99-0.999) = stronger emphasis on rare classes.">
                                    Beta (effective num)
                                </label>
                                <input type="number" id="sam3BalanceBeta" step="0.001" min="0" max="0.9999" value="0.99" />
                            </div>
                            <div class="balance-param-row sam3-balance-param" data-param="gamma">
                                <label for="sam3BalanceGamma" title="Gamma for focal-style sampling. Higher = more boost to rare/low-freq classes.">
                                    Gamma (focal)
                                </label>
                                <input type="number" id="sam3BalanceGamma" step="0.1" min="0" value="0.5" />
                            </div>
                            <div>
                                <label for="sam3Warmup">Warmup steps</label>
                                <input type="number" id="sam3Warmup" min="0" value="20" title="Warmup steps: start the LR near zero and ramp up over this many steps. Higher = gentler/safer start, lower = hotter start. 0 disables warmup." />
                            </div>
                            <div>
                                <label for="sam3Timescale">Scheduler timescale</label>
                                <input type="number" id="sam3Timescale" min="1" value="20" title="Scheduler timescale: stretches the LR schedule. Higher = slower decay, steadier training. Lower = faster decay, spikier early changes." />
                            </div>
                        </div>
                        <div class="training-toggle-row">
                            <label title="For debugging only: prints a log line every minibatch (instead of every Nth). Creates very large logs."><input type="checkbox" id="sam3LogAll" /> Log every minibatch (very verbose)</label>
                        </div>
                        <div class="training-toggle-row">
                            <label title="Stops updating the language/text encoder to preserve base vocabulary. Overrides the language LR override below."><input type="checkbox" id="sam3FreezeLanguage" /> Freeze language backbone (preserve base text prompts)</label>
                        </div>
                        <div class="training-field">
                            <label for="sam3LanguageLr">Language backbone LR override</label>
                            <input type="number" id="sam3LanguageLr" step="0.000001" min="0" placeholder="‚âà5e-5 √ó LR scale (default)" />
                            <div class="training-help">Optional: set a specific LR for the text encoder. Set to 0 (or check ‚ÄúFreeze language‚Äù) to keep base vocabulary; leave blank to use the config default.</div>
                        </div>
                        <div class="training-field">
                            <label for="sam3PromptVariants">Prompt variants per class (text grounding)</label>
                            <textarea id="sam3PromptVariants" rows="3" placeholder="class_one: variant a, variant b&#10;hard_hat: helmet, safety helmet"></textarea>
                            <div class="training-help">One class per line: <code>class_name: alt1, alt2</code>. We swap the class name for one of these phrases during training; leave blank to use the class names as-is.</div>
                            <div class="training-toggle-row">
                                <label title="When on, pick a random variant per datapoint during training; when off, always use the first variant. Validation always uses the first variant."><input type="checkbox" id="sam3PromptRandomize" checked /> Randomize variants per datapoint</label>
                            </div>
                        </div>
                        <div class="training-toggle-row">
                            <label title="Keep the SAM3 segmentation head loaded so text prompting works. Uses the base head weights if you don‚Äôt train masks."><input type="checkbox" id="sam3SegHead" checked /> Keep segmentation head for text prompts</label>
                        </div>
                        <div class="training-toggle-row">
                            <label title="Only turn this on if your dataset includes segmentation masks. It will feed masks into training and update the segmentation head; leave off for bbox-only datasets (head stays frozen for prompting)."><input type="checkbox" id="sam3SegTrain" /> Train segmentation head with masks (requires masks)</label>
                            <div class="training-help">Default OFF for bbox-only runs. Turn ON only when you have mask annotations and want the segmentation head to learn them; otherwise the head stays frozen but still serves text prompts.</div>
                        </div>
                        <div class="training-toggle-row">
                            <label title="Disables the segmentation head entirely. Smaller checkpoints, faster training, BUT no text prompting and only bbox refinement works."><input type="checkbox" id="sam3BBoxOnly" /> Bbox-refiner only mode (no text prompts, no masks)</label>
                            <div class="training-help">Use only if you explicitly want a bbox-only refiner. Default OFF so text prompting remains available.</div>
                        </div>
                        <button type="button" id="sam3StartBtn" class="training-button">Start SAM3 Training</button>
                            <div class="training-note">
                                <strong>Tuning tips:</strong><br />
                                ‚Ä¢ Lower LR scale (e.g., 0.05 or 0.02) to reduce early loss spikes.<br />
                                ‚Ä¢ Meta‚Äôs default LR scale is 0.1; that makes transformer LR 8e-5, vision 2.5e-5, language 5e-6. Halving/dropping LR scale makes starts gentler.<br />
                                ‚Ä¢ Warmup steps: start near-zero LR and ramp up over these steps; higher is gentler/safer, lower is hotter/risks spikes (try 20‚Äì100).<br />
                                ‚Ä¢ Timescale: stretches the LR schedule; higher keeps LR steady longer, lower decays faster (try 20‚Äì40 for short runs).<br />
                                ‚Ä¢ To simulate larger batches without extra VRAM, keep batch size at 1 and raise grad accumulation (e.g., 2‚Äì4).<br />
                                ‚Ä¢ Typical recipe: resolution 1008, LR scale 0.05, grad accumulation 2, batch sizes 1/1, epochs 20.
                            </div>
                        <div class="training-note">
                            <strong>Text prompt alignment (read this):</strong><br />
                            ‚Ä¢ SAM3 class scores come from text embeddings. If you leave variants blank, we use your raw class names as prompts.<br />
                            ‚Ä¢ Weird names (e.g., <code>light_vehicle</code>) often don‚Äôt match the base model. Add synonyms per class or unfreeze the language backbone (set a small LR, or uncheck ‚ÄúFreeze language‚Äù).<br />
                            ‚Ä¢ Freezing the language encoder is only sensible if you stick to the base vocabulary and just fine-tune boxes/masks.<br />
                            ‚Ä¢ COCO eval is class-ID based; bad prompt alignment ‚áí wrong class IDs ‚áí low AP even if boxes look reasonable.
                        </div>
                        <div class="training-message" id="sam3Message" role="status"></div>
                    </section>
                    <section class="training-status-section">
                        <h2>Training Status</h2>
                        <div class="training-progress" id="sam3ProgressBar">
                            <div class="training-progress-fill" id="sam3ProgressFill"></div>
                        </div>
                        <div class="training-status-text" id="sam3StatusText">Idle</div>
                        <div class="training-help" id="sam3EtaText">ETA: estimating‚Ä¶</div>
                        <div class="training-actions">
                            <button type="button" class="training-button training-button-danger" id="sam3CancelBtn" disabled>Cancel Job</button>
                        </div>
                        <div class="training-summary" id="sam3Summary"></div>
                        <div class="training-summary" id="sam3BalanceSummary"></div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="sam3ActivateBtn" disabled>Activate checkpoint</button>
                        </div>
                        <div class="training-chart">
                            <h3>Loss</h3>
                            <canvas id="sam3LossChart" height="120"></canvas>
                            <div class="training-chart-legend">
                                <span class="legend-item legend-blue">Blue: rolling average loss (last ~10 batches; computed every batch even if not all are logged).</span>
                                <span class="legend-item legend-orange">Orange: logged batch loss (printed every Nth minibatch; some batches are skipped in logs).</span>
                                <span class="legend-item">Log lines show <code>batch=</code> and <code>avg10=</code> (last ~10 minibatches).</span>
                                <div class="training-help">
                                    Smoothed trend (EMA): <input type="range" id="sam3TrendSmooth" min="0.01" max="0.5" step="0.01" value="0.05" />
                                    <span id="sam3TrendSmoothValue">0.05</span> (lower = smoother)
                                </div>
                            </div>
                        </div>
                        <div class="training-chart">
                            <h3>Validation (COCO bbox)</h3>
                            <div id="sam3ValMetrics" class="training-help">No validation metrics yet.</div>
                        </div>
                        <div class="training-logs">
                            <h3>Logs</h3>
                            <pre id="sam3Log"></pre>
                        </div>
                        <div class="training-history">
                            <h3>Recent SAM3 Jobs</h3>
                            <div id="sam3TrainingHistory"></div>
                        </div>
                        <div class="training-history">
                            <div class="storage-header">
                                <h3>Run storage (logs & checkpoints)</h3>
                                <button type="button" class="training-button" id="sam3StorageRefresh">Refresh</button>
                            </div>
                            <div class="training-help">Lists folders under <code>uploads/sam3_runs/</code>; delete logs/dumps/checkpoints you no longer need.</div>
                            <div id="sam3StorageList" class="storage-list"></div>
                        </div>
                    </section>
                </div>
            </div>
            <div class="tab-panel" id="tabYoloTrain" data-tab-panel="yolo-train">
                <div class="training-wrapper two-col">
                    <section class="training-form-section">
                        <h2>Train YOLOv8</h2>
                        <p class="training-help">Train YOLOv8 on any dataset managed in the Dataset Management tab. We keep only <code>best.pt</code> + metrics to avoid disk bloat.</p>
                        <div class="training-field">
                            <label>Dataset</label>
                            <div class="sam3-dataset-row">
                                <select id="yoloDatasetSelect"></select>
                                <button type="button" class="training-button" id="yoloDatasetRefresh">Refresh</button>
                            </div>
                            <div class="training-help" id="yoloDatasetSummary">Select a dataset, then start training.</div>
                        </div>
                        <div class="training-field">
                            <label for="yoloRunName">Run name (optional)</label>
                            <input type="text" id="yoloRunName" placeholder="qwen_yolov8s_2025-01-10" />
                            <div class="training-help">Used in run storage + history. Leave blank to auto-name runs.</div>
                        </div>
                        <div class="training-grid">
                            <div>
                                <label for="yoloTask">Task</label>
                                <select id="yoloTask">
                                    <option value="detect" selected>Detect (bbox)</option>
                                    <option value="segment">Segment (mask)</option>
                                </select>
                            </div>
                            <div>
                                <label for="yoloHeadType">Head type</label>
                                <select id="yoloHeadType" title="P2 adds a higher-resolution head for small objects. No official P2 pretrained weights are provided by Ultralytics.">
                                    <option value="standard" selected>Standard (P3-P5)</option>
                                    <option value="p2">P2 (small-object head)</option>
                                </select>
                                <div class="training-help">P2 variants use a custom P2 head and start from scratch unless you supply base weights.</div>
                            </div>
                            <div>
                                <label for="yoloScale">Model scale</label>
                                <select id="yoloScale" title="Scale controls width/depth. Larger = more accuracy, more VRAM.">
                                    <option value="n" selected>Nano (fastest)</option>
                                    <option value="s">Small</option>
                                    <option value="m">Medium</option>
                                    <option value="l">Large</option>
                                    <option value="x">XLarge</option>
                                </select>
                                <div class="training-help">Scale is baked into the model config (n/s/m/l/x). Larger scales train slower and need more VRAM.</div>
                            </div>
                            <div>
                                <label for="yoloResolvedVariant">Resolved model id</label>
                                <input type="text" id="yoloResolvedVariant" readonly />
                                <div class="training-help" id="yoloVariantHelp">Standard uses official YOLOv8 pretrained weights when available.</div>
                            </div>
                            <div class="checkbox-row">
                                <label><input type="checkbox" id="yoloFromScratch" /> Train from scratch (no pretrained weights)</label>
                            </div>
                            <div>
                                <label for="yoloBaseWeights">Base weights (optional)</label>
                                <input type="text" id="yoloBaseWeights" placeholder="yolov8n.pt or /path/to/weights" />
                                <div class="training-help">For P2 heads there are no official pretrained weights, so leave blank to start from scratch or point to your own weights.</div>
                            </div>
                        </div>
                        <div class="training-grid">
                            <div>
                                <label for="yoloEpochs">Epochs</label>
                                <input type="number" id="yoloEpochs" min="1" value="50" />
                            </div>
                            <div>
                                <label for="yoloImgSize">Image size</label>
                                <input type="number" id="yoloImgSize" min="256" step="32" value="640" />
                            </div>
                            <div>
                                <label for="yoloBatch">Batch</label>
                                <input type="number" id="yoloBatch" min="1" value="16" />
                            </div>
                            <div>
                                <label for="yoloWorkers">Workers</label>
                                <input type="number" id="yoloWorkers" min="0" value="4" />
                            </div>
                            <div>
                                <label for="yoloDevices">Devices (comma-separated)</label>
                                <input type="text" id="yoloDevices" placeholder="0,1" />
                            </div>
                            <div>
                                <label for="yoloSeed">Seed</label>
                                <input type="number" id="yoloSeed" value="42" />
                            </div>
                        </div>
                        <div class="training-subsection__title">Augmentations</div>
                        <div class="training-grid">
                            <div>
                                <label for="yoloAugFlipLR">Flip LR</label>
                                <input type="number" id="yoloAugFlipLR" min="0" max="1" step="0.05" value="0.5" />
                            </div>
                            <div>
                                <label for="yoloAugFlipUD">Flip UD</label>
                                <input type="number" id="yoloAugFlipUD" min="0" max="1" step="0.05" value="0.0" />
                            </div>
                            <div>
                                <label for="yoloAugHsvH">HSV H</label>
                                <input type="number" id="yoloAugHsvH" min="0" max="0.5" step="0.01" value="0.015" />
                            </div>
                            <div>
                                <label for="yoloAugHsvS">HSV S</label>
                                <input type="number" id="yoloAugHsvS" min="0" max="1" step="0.01" value="0.7" />
                            </div>
                            <div>
                                <label for="yoloAugHsvV">HSV V</label>
                                <input type="number" id="yoloAugHsvV" min="0" max="1" step="0.01" value="0.4" />
                            </div>
                            <div>
                                <label for="yoloAugMosaic">Mosaic</label>
                                <input type="number" id="yoloAugMosaic" min="0" max="1" step="0.05" value="1.0" />
                            </div>
                            <div>
                                <label for="yoloAugMixup">Mixup</label>
                                <input type="number" id="yoloAugMixup" min="0" max="1" step="0.05" value="0.0" />
                            </div>
                            <div>
                                <label for="yoloAugCopyPaste">Copy-paste</label>
                                <input type="number" id="yoloAugCopyPaste" min="0" max="1" step="0.05" value="0.0" />
                            </div>
                            <div>
                                <label for="yoloAugScale">Scale</label>
                                <input type="number" id="yoloAugScale" min="0" max="1" step="0.05" value="0.5" />
                            </div>
                            <div>
                                <label for="yoloAugTranslate">Translate</label>
                                <input type="number" id="yoloAugTranslate" min="0" max="1" step="0.05" value="0.1" />
                            </div>
                            <div>
                                <label for="yoloAugDegrees">Degrees</label>
                                <input type="number" id="yoloAugDegrees" min="0" max="45" step="1" value="0" />
                            </div>
                        </div>
                        <div class="checkbox-row">
                            <label><input type="checkbox" id="yoloAcceptTos" /> I have read and accept the Ultralytics YOLO license/terms.</label>
                            <div class="training-help">Required to start training.</div>
                        </div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="yoloTrainStartBtn">Start YOLO Training</button>
                            <button type="button" class="training-button training-button-danger" id="yoloTrainCancelBtn" disabled>Cancel Job</button>
                        </div>
                        <div class="training-message" id="yoloTrainMessage" role="status"></div>
                    </section>
                    <section class="training-status-section">
                        <h2>YOLO Training Status</h2>
                        <div class="training-progress" id="yoloTrainProgressBar">
                            <div class="training-progress-fill" id="yoloTrainProgressFill"></div>
                        </div>
                        <div class="training-status-text" id="yoloTrainStatusText">Idle</div>
                        <div class="training-chart">
                            <h3>Validation Metric</h3>
                            <canvas id="yoloTrainMetricCanvas" height="160"></canvas>
                            <div class="training-help" id="yoloTrainChartStatus">Metrics appear after training completes.</div>
                        </div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="yoloTrainRefreshBtn">Refresh status</button>
                        </div>
                        <div class="training-logs">
                            <h3>Logs</h3>
                            <pre id="yoloTrainLog"></pre>
                        </div>
                        <div class="training-history">
                            <h3>Recent YOLO Jobs</h3>
                            <div id="yoloTrainHistory"></div>
                        </div>
                        <div class="training-history">
                            <div class="storage-header">
                                <h3>Saved YOLO Runs</h3>
                                <button type="button" class="training-button" id="yoloRunsRefresh">Refresh</button>
                            </div>
                            <div class="training-help">Each run stores <code>best.pt</code> + metrics only. Download a zip to move it elsewhere.</div>
                            <div class="training-field">
                                <label for="yoloRunSelect">Saved runs</label>
                                <select id="yoloRunSelect"></select>
                            </div>
                            <div class="training-actions">
                                <button type="button" class="training-button" id="yoloRunActivate">Set active</button>
                                <button type="button" class="training-button" id="yoloRunDownload">Download zip</button>
                                <button type="button" class="training-button training-button-danger" id="yoloRunDelete">Delete run</button>
                            </div>
                            <div class="training-help" id="yoloRunSummary">No runs yet.</div>
                        </div>
                    </section>
                </div>
            </div>
            <div class="tab-panel" id="tabRfDetrTrain" data-tab-panel="rfdetr-train">
                <div class="training-wrapper two-col">
                    <section class="training-form-section">
                        <h2>Train RF-DETR</h2>
                        <p class="training-help">Train RF-DETR on datasets from the Dataset Management tab. We keep only best checkpoints + metrics.</p>
                        <div class="training-field">
                            <label>Dataset</label>
                            <div class="sam3-dataset-row">
                                <select id="rfdetrDatasetSelect"></select>
                                <button type="button" class="training-button" id="rfdetrDatasetRefresh">Refresh</button>
                            </div>
                            <div class="training-help" id="rfdetrDatasetSummary">Select a dataset, then start training.</div>
                        </div>
                        <div class="training-field">
                            <label for="rfdetrRunName">Run name (optional)</label>
                            <input type="text" id="rfdetrRunName" placeholder="qwen_rfdetr_medium_2025-01-10" />
                            <div class="training-help">Used in run storage + history. Leave blank to auto-name runs.</div>
                        </div>
                        <div class="training-grid">
                            <div>
                                <label for="rfdetrTask">Task</label>
                                <select id="rfdetrTask">
                                    <option value="detect" selected>Detect (bbox)</option>
                                    <option value="segment">Segment (mask)</option>
                                </select>
                            </div>
                            <div>
                                <label for="rfdetrVariant">Model variant</label>
                                <select id="rfdetrVariant"></select>
                                <div class="training-help" id="rfdetrVariantHelp">Variants are loaded from the backend.</div>
                            </div>
                            <div>
                                <label for="rfdetrResolution">Resolution (optional)</label>
                                <input type="number" id="rfdetrResolution" min="256" step="32" placeholder="auto" />
                                <div class="training-help">Overrides the default resolution for the selected variant.</div>
                            </div>
                            <div class="checkbox-row">
                                <label><input type="checkbox" id="rfdetrMultiScale" checked /> Multi-scale resize (default on)</label>
                                <div class="training-help">Randomly resizes images to several scales around the target resolution. Helps small objects but adds variance.</div>
                            </div>
                            <div class="checkbox-row">
                                <label><input type="checkbox" id="rfdetrExpandedScales" checked /> Expanded scale range (default on)</label>
                                <div class="training-help">Wider scale range for multi-scale (more small‚Äëobject coverage, slower). Ignored if multi-scale is off.</div>
                            </div>
                            <div class="checkbox-row">
                                <label><input type="checkbox" id="rfdetrFromScratch" /> Train from scratch (no pretrained weights)</label>
                                <div class="training-help">Starts with random weights. Use only for large datasets or extreme domain shifts (slower + lower early accuracy).</div>
                            </div>
                            <div>
                                <label for="rfdetrPretrainWeights">Pretrained weights (optional)</label>
                                <input type="text" id="rfdetrPretrainWeights" placeholder="rf-detr-medium.pth or /path/to/weights" />
                                <div class="training-help">Leave blank to use the default weights for the variant.</div>
                            </div>
                            <div class="checkbox-row">
                                <label><input type="checkbox" id="rfdetrUseEma" checked /> Use EMA weights</label>
                            </div>
                            <div class="checkbox-row">
                                <label><input type="checkbox" id="rfdetrEarlyStop" /> Enable early stopping</label>
                            </div>
                            <div>
                                <label for="rfdetrEarlyStopPatience">Early-stop patience</label>
                                <input type="number" id="rfdetrEarlyStopPatience" min="1" value="10" />
                            </div>
                        </div>
                        <div class="training-grid">
                            <div>
                                <label for="rfdetrEpochs">Epochs</label>
                                <input type="number" id="rfdetrEpochs" min="1" value="100" />
                            </div>
                            <div>
                                <label for="rfdetrBatch">Batch</label>
                                <input type="number" id="rfdetrBatch" min="1" value="4" />
                            </div>
                            <div>
                                <label for="rfdetrGradAccum">Grad accumulation</label>
                                <input type="number" id="rfdetrGradAccum" min="1" value="4" />
                            </div>
                            <div>
                                <label for="rfdetrWorkers">Workers</label>
                                <input type="number" id="rfdetrWorkers" min="0" value="2" />
                            </div>
                            <div>
                                <label for="rfdetrDevices">Devices (comma-separated)</label>
                                <input type="text" id="rfdetrDevices" placeholder="0,1" />
                                <div class="training-help">Leave blank to use all GPUs. Use "0" to pin to one GPU. Multi-GPU uses distributed training.</div>
                            </div>
                            <div>
                                <label for="rfdetrSeed">Seed</label>
                                <input type="number" id="rfdetrSeed" value="42" />
                            </div>
                        </div>
                        <div class="checkbox-row">
                            <label><input type="checkbox" id="rfdetrAcceptTos" /> I have read and accept the RF-DETR license/terms.</label>
                            <div class="training-help">Required to start training.</div>
                        </div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="rfdetrTrainStartBtn">Start RF-DETR Training</button>
                            <button type="button" class="training-button training-button-danger" id="rfdetrTrainCancelBtn" disabled>Cancel Job</button>
                        </div>
                        <div class="training-message" id="rfdetrTrainMessage" role="status"></div>
                    </section>
                    <section class="training-status-section">
                        <h2>RF-DETR Training Status</h2>
                        <div class="training-progress" id="rfdetrTrainProgressBar">
                            <div class="training-progress-fill" id="rfdetrTrainProgressFill"></div>
                        </div>
                        <div class="training-status-text" id="rfdetrTrainStatusText">Idle</div>
                        <div class="training-chart">
                            <h3>Validation Metric</h3>
                            <canvas id="rfdetrTrainMetricCanvas" height="160"></canvas>
                            <div class="training-help" id="rfdetrTrainChartStatus">Metrics appear after training completes.</div>
                        </div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="rfdetrTrainRefreshBtn">Refresh status</button>
                        </div>
                        <div class="training-logs">
                            <h3>Logs</h3>
                            <pre id="rfdetrTrainLog"></pre>
                        </div>
                        <div class="training-history">
                            <h3>Recent RF-DETR Jobs</h3>
                            <div id="rfdetrTrainHistory"></div>
                        </div>
                        <div class="training-history">
                            <div class="storage-header">
                                <h3>Saved RF-DETR Runs</h3>
                                <button type="button" class="training-button" id="rfdetrRunsRefresh">Refresh</button>
                            </div>
                            <div class="training-help">Each run stores best checkpoints + metrics only. Download a zip to move it elsewhere.</div>
                            <div class="training-field">
                                <label for="rfdetrRunSelect">Saved runs</label>
                                <select id="rfdetrRunSelect"></select>
                            </div>
                            <div class="training-actions">
                                <button type="button" class="training-button" id="rfdetrRunActivate">Set active</button>
                                <button type="button" class="training-button" id="rfdetrRunDownload">Download zip</button>
                                <button type="button" class="training-button training-button-danger" id="rfdetrRunDelete">Delete run</button>
                            </div>
                            <div class="training-help" id="rfdetrRunSummary">No runs yet.</div>
                        </div>
                    </section>
                </div>
            </div>
            <div class="tab-panel" id="tabAgentMining" data-tab-panel="agent-mining">
	                <div class="training-wrapper two-col">
		                    <section class="training-form-section">
		                        <h2>SAM3 Recipe Mining</h2>
			                        <p class="training-help">Automatically discover ‚Äúrecipes‚Äù that help SAM3 find each class in your dataset. A recipe is a portable zip (JSON + embedded pretrained CLIP head) you can reuse in the labeling panel.</p>
			                        <div class="training-help" style="color: #b91c1c; font-weight: 600;">Recipe mining UI build: 2025-12-20 (flat layout)</div>
			                        <div id="agentMiningHowItWorks" style="margin-bottom: 12px; display: block !important; padding: 12px 14px; border: 1px solid #e2e8f0; border-radius: 12px; background: #f8fafc;">
			                            <div style="font-weight: 700; margin-bottom: 6px;">How recipe mining works</div>
			                            <div class="training-help">
			                                <strong>Prerequisite:</strong> Train a CLIP head first (CLIP tab). Recipe mining uses that head to score detections and to auto-tune ‚Äúcleanliness‚Äù (precision) thresholds. This is required.
			                            </div>
			                            <div class="training-help" style="margin-top: 6px; color: #475569;">
			                                Defaults are <strong>recall-first</strong> (lower thresholds, more candidates). Raise cleanliness or thresholds when you need higher precision.
			                            </div>
			                            <div class="training-help" style="margin-top: 10px;">
			                                <div style="font-weight: 600;">Full flow + how to tune it</div>
			                                <div style="font-weight: 600; margin: 10px 0 6px;">High-level flow</div>
			                                <pre class="training-help" style="white-space: pre; overflow-x: auto; padding: 10px; background: #f8fafc; border: 1px solid #e2e8f0; border-radius: 8px;">Dataset (YOLO/COCO) + trained CLIP head
        ‚îÇ
        ‚îú‚îÄ Sample images (deterministic by seed)
        ‚îÇ
        ‚îú‚îÄ Build prompt list per class
        ‚îÇ    - class name + your extra prompts
        ‚îÇ    - optional GPT prompt expansion
        ‚îÇ    - base prompts (e.g., ‚Äúobject‚Äù, ‚Äúsmall object‚Äù)
        ‚îÇ
        ‚îú‚îÄ Text-candidate eval (sample, text-only)
        ‚îÇ    - run SAM3 text inference per prompt
        ‚îÇ    - build a candidate-threshold curve per prompt (coverage vs false positives)
        ‚îÇ
        ‚îú‚îÄ Build a multi-step recipe (per class)
        ‚îÇ    - pick a short chain of prompt steps (usually 3‚Äì8)
        ‚îÇ    - each step has its own text threshold (from the prompt curve)
        ‚îÇ
        ‚îú‚îÄ Run the full steps pipeline (sample)
        ‚îÇ    - for each step:
        ‚îÇ        1) text candidates ‚Üí pick diverse expansion candidates
        ‚îÇ        2) similarity-based search (SAM3 visual expansion)
        ‚îÇ        3) de-dupe
        ‚îÇ
        ‚îú‚îÄ CLIP head gating (sample)
        ‚îÇ    - crop each detection and score with the pretrained CLIP head
        ‚îÇ    - auto-tune min prob + (optional) margin to hit your ‚Äúcleanliness‚Äù target
        ‚îÇ
        ‚îú‚îÄ Background suppression (default on)
        ‚îÇ    - if a __bg_* class scores higher than the target, drop the detection
        ‚îÇ    - optional margin can make this stricter
        ‚îÇ
        ‚îî‚îÄ Save portable recipe zip
             - recipe.json (schema v2, multi-step)
             - clip_head/head.npz + meta.json (embedded head + tuned thresholds)</pre>
			                                <div style="margin-top: 10px;">
			                                    <strong>Why recipes may look ‚Äútext-only‚Äù:</strong> in schema‚Äëv2, the ‚Äúsimilarity prompt‚Äù is SAM3‚Äôs <em>visual</em> stage (candidate boxes ‚Üí expand similar masks/boxes). The recipe stores the <em>text</em> prompts + thresholds; it does <em>not</em> need a fixed crop-bank of exemplars because candidates are chosen per image at inference time.
			                                </div>
			                                <div style="font-weight: 600; margin: 12px 0 6px;">What we optimize for (‚Äúcleanliness‚Äù)</div>
			                                <div>
			                                    We optimize to <strong>maximize coverage</strong> (find as many GT objects as possible) while meeting your <strong>precision target</strong>:
			                                    <ul style="margin: 6px 0 0 18px;">
			                                        <li><strong>Match</strong> = a detection overlaps an unmatched GT box for that class with IoU ‚â• ‚ÄúEval IoU‚Äù.</li>
			                                        <li><strong>Precision</strong> = matches / (matches + false positives). Higher = fewer wrong boxes.</li>
			                                        <li><strong>Coverage</strong> = matches / GT count. Higher = finds more objects.</li>
			                                        <li><strong>Duplicates</strong> = extra detections on the same GT object (tracked separately).</li>
			                                    </ul>
			                                </div>
			                                <div style="font-weight: 600; margin: 12px 0 6px;">Multi-step tuning modes (what to enable)</div>
			                                <ul style="margin: 6px 0 0 18px;">
			                                    <li><strong>Default (no extra tuning):</strong> candidate curves ‚Üí greedy prompt selection ‚Üí full pipeline ‚Üí CLIP thresholds tuned to match your cleanliness.</li>
			                                    <li><strong>Refine prompt subset:</strong> quick ‚Äútry a few swaps‚Äù pass (text-candidate stage only; no extra full SAM runs). Use when the chosen prompts look ‚Äúobviously wrong‚Äù.</li>
			                                    <li><strong>Tier‚Äë1 grid search:</strong> small search over <code>visual_expansion_score</code> (SAM3) + <code>candidates/step</code> (shared across steps). Good first knob search if Global is off.</li>
			                                    <li><strong>Tier‚Äë2 IoU search:</strong> small search over de‚Äëdupe overlaps (IoU) (shared across steps). Helps duplicates / false positives.</li>
			                                    <li><strong>Global optimizer (best quality):</strong> tries many full recipe variants and keeps the best (slowest, most thorough).</li>
			                                </ul>
			                                <div class="training-help" style="margin-top: 8px; color: #475569;">
			                                    Note: when the Global optimizer is on, Tier‚Äë1 and Tier‚Äë2 are ignored (Global already tunes those knobs).
			                                </div>
			                                <div style="font-weight: 600; margin: 12px 0 6px;">Global optimizer: what knobs it can change</div>
			                                <div>
			                                    A <strong>trial</strong> is a full per-class recipe (ordered steps + per-step settings). We create new trials by making small changes to the current best:
			                                    <ul style="margin: 6px 0 0 18px;">
			                                        <li><strong>Prompt subset:</strong> add / drop / swap a step‚Äôs prompt.</li>
			                                        <li><strong>Per-step text threshold:</strong> how strict the text prompt must be before a box becomes a candidate.</li>
			                                        <li><strong>Per-step visual expansion score (SAM3)</strong> and <strong>candidates per image</strong> (how many candidates we expand).</li>
			                                        <li><strong>Per-step de-dupe overlap (IoU):</strong> candidate de-dupe and output de-dupe.</li>
			                                        <li><strong>Optional:</strong> step order and per-step max dets/image.</li>
			                                    </ul>
			                                    Each trial is scored by running the <em>real</em> steps pipeline on the same (repeatable) subset of sampled images, then sweeping CLIP thresholds to match your cleanliness target.
			                                </div>
			                                <div style="font-weight: 600; margin: 12px 0 6px;">Breadth vs depth (simple mental model)</div>
			                                <ul style="margin: 6px 0 0 18px;">
			                                    <li><strong>Breadth (‚Äúexploration‚Äù):</strong> try more recipe ideas. Increase when results feel ‚Äústuck‚Äù.</li>
			                                    <li><strong>Depth (‚Äúreliability‚Äù):</strong> test each idea on more sampled images. Increase when results feel noisy or unstable.</li>
			                                    <li><strong>Rule of thumb:</strong> if you raise breadth a lot, add some depth too.</li>
			                                </ul>
			                                <div style="font-weight: 600; margin: 12px 0 6px;">Global optimizer controls (what they mean)</div>
			                                <ul style="margin: 6px 0 0 18px;">
			                                    <li><strong>Stage‚Äë1/2/3 eval cap:</strong> how many sampled images we use per stage (bigger = more reliable, slower). Stage‚Äë1 ranks quickly; Stage‚Äë2/3 re-test the best candidates.</li>
			                                    <li><strong>Max trials:</strong> how many candidate recipe configs we consider per round (higher = more thorough, slower).</li>
			                                    <li><strong>Keep ratio:</strong> how aggressively we prune candidates each stage (0.5 keeps the top half).</li>
			                                    <li><strong>Rounds</strong> + <strong>Variants per round:</strong> how long we keep exploring around the current best recipe.</li>
			                                    <li><strong>Tune step order</strong> / <strong>Tune max dets/image:</strong> optional extra dimensions the optimizer is allowed to change.</li>
			                                </ul>
			                                <div style="font-weight: 600; margin: 12px 0 6px;">How to influence results (practical)</div>
			                                <ul style="margin: 6px 0 0 18px;">
			                                    <li><strong>Cleaner / fewer FPs:</strong> raise ‚ÄúRecipe cleanliness‚Äù. Expect lower coverage.</li>
			                                    <li><strong>More coverage:</strong> lower ‚ÄúRecipe cleanliness‚Äù, increase Max recipe steps, and consider lowering Candidate curve floor (lets per-step thresholds go below the base).</li>
			                                    <li><strong>More thorough search:</strong> set ‚ÄúSearch budget‚Äù to Balanced/Best and increase Max trials / Rounds / Variants per round (slower).</li>
			                                    <li><strong>Faster runs:</strong> reduce ‚ÄúEval images‚Äù; lower Stage‚Äë3 eval cap; lower Max trials.</li>
			                                </ul>
			                                <div style="margin-top: 12px; color: #64748b;">
			                                    Note: we no longer generate the old crop-bank recipe style; new recipes are schema‚Äëv2 multi-step and always embed a pretrained CLIP head. Imported legacy recipes can still be applied for backward compatibility.
			                                </div>
			                            </div>
			                        </div>
		                        <div class="training-field">
		                            <label for="agentDatasetSelect">Dataset<span class="help-icon" title="Choose the dataset to mine recipes from.&#10;Datasets are created/converted in the Dataset Management tab.">?</span></label>
		                            <div class="sam3-dataset-row">
		                                <select id="agentDatasetSelect"></select>
		                                <button type="button" class="training-button" id="agentDatasetRefresh">Refresh</button>
		                            </div>
	                            <div class="training-help" id="agentDatasetSummary">Pick a converted SAM3/Qwen dataset.</div>
			                        </div>
				                        <div class="training-grid">
			                            <div>
			                                <label for="agentEvalImageCount">Eval images<span class="help-icon" title="How many images to sample for mining and scoring.&#10;Smaller = faster smoke test; larger = more reliable scores.&#10;We always sample deterministically from the seed.">?</span></label>
			                                <input type="number" id="agentEvalImageCount" min="1" max="50000" value="100" title="Number of images to sample for mining + scoring." />
			                            </div>
			                            <div>
			                                <label for="agentSplitSeed">Sample seed<span class="help-icon" title="Random seed for sampling evaluation images.&#10;Same seed = same sample (repeatable).&#10;Change it to try a different set of images.">?</span></label>
			                                <input type="number" id="agentSplitSeed" value="42" title="Same seed = same sample. Change it if you want a different random sample." />
			                            </div>
				                            <div class="training-field training-field--inline">
				                                <label for="agentReuseCache">Reuse cached SAM results<span class="help-icon" title="Speeds up repeated runs by reusing cached SAM3 prompt evaluations and expansions when possible.&#10;Turn off if you changed the SAM3 prompt model/checkpoint and want to be extra safe.&#10;Use ‚ÄúPurge cache‚Äù to free disk space.">?</span></label>
				                                <input type="checkbox" id="agentReuseCache" checked />
				                            </div>
					                            <div>
					                                <label for="agentIouThreshold">Match overlap (Eval IoU)<span class="help-icon" title="Used only for scoring (evaluation), not for running the recipe.&#10;IoU = how much the predicted box overlaps the ground-truth box (0‚Äì1).&#10;0.5 ‚âà ‚Äúat least about half overlap‚Äù.&#10;Higher = stricter scoring (coverage may look lower).">?</span></label>
					                                <input type="number" id="agentIouThreshold" step="0.05" min="0" max="1" value="0.5" />
					                            </div>
						                            <div>
						                                <label for="agentWorkersPerGpu">Predictors per GPU<span class="help-icon" title="How many SAM3 workers to run per GPU during mining.&#10;1 = one worker per GPU (safest).&#10;Higher can speed up mining if you have VRAM headroom, but uses more GPU memory and can trigger CUDA OOM.">?</span></label>
						                                <input type="number" id="agentWorkersPerGpu" min="1" max="8" value="1" />
						                            </div>
						                            <div>
						                                <label for="agentMaxWorkers">Max predictors total (optional)<span class="help-icon" title="Optional cap on total parallel SAM3 workers across all GPUs.&#10;Leave blank to use all available GPUs with your ‚ÄúPredictors per GPU‚Äù setting.&#10;Lower values reduce VRAM usage and can make the UI more responsive.">?</span></label>
						                                <input type="number" id="agentMaxWorkers" min="1" max="256" placeholder="auto" />
						                            </div>
						                            <div>
						                                <label for="agentSearchMode">Search mode<span class="help-icon" title="Recipe mining now always uses Multi-step: it builds a chain of prompt steps and tunes CLIP head thresholds for your cleanliness target.&#10;Defaults are recall-first; raise cleanliness or thresholds for precision.&#10;Legacy modes are deprecated and are no longer exposed in the UI.">?</span></label>
						                                <div class="training-help">Multi-step (recall-first defaults)</div>
						                                <input type="hidden" id="agentSearchMode" value="steps" />
					                            </div>
					                        </div>
							                        <div id="agentStepsOptions" style="margin-bottom: 12px; display: block !important; padding: 12px 14px; border: 1px solid #e2e8f0; border-radius: 12px; background: #f8fafc;">
							                            <div style="font-weight: 700; margin-bottom: 6px;">Recipe search settings</div>
							                            <div class="training-help" style="margin-bottom: 10px; color: #475569;">Need a refresher? Open <strong>How recipe mining works</strong> above. Defaults are recall-heavy; raise cleanliness for precision.</div>
						                                <div class="training-grid">
						                                    <div>
						                                        <label for="agentStepsMaxSteps">Max recipe steps<span class="help-icon" title="How many prompt steps we are allowed to select for a class.&#10;More steps can increase coverage but costs more compute and can add false positives.&#10;A good starting point is 4‚Äì8.">?</span></label>
						                                        <input type="number" id="agentStepsMaxSteps" min="1" max="50" value="6" />
					                                    </div>
					                                    <div>
					                                        <label for="agentStepsMaxSeedsPerStep">Similarity candidates per step<span class="help-icon" title="Per step and per image, how many candidate detections we expand with similarity-based search (SAM3 visual prompting).&#10;Higher can improve coverage on diverse objects, but is slower.&#10;Typical: 5‚Äì15 (recall-first: 10‚Äì20).">?</span></label>
						                                        <input type="number" id="agentStepsMaxSeedsPerStep" min="0" max="500" value="10" />
						                                    </div>
						                                </div>
						                                <div class="training-subsection" style="margin-top: 10px;">
						                                    <div class="training-subsection__title">Search budget</div>
						                                    <div class="training-grid" style="margin-top: 4px;">
						                                        <div>
							                                            <label for="agentStepsGlobalPreset">Global optimizer preset<span class="help-icon" title="Sets a good starting point for the Global optimizer.&#10;&#10;Off: no global optimizer (fastest).&#10;Fast/Balanced/Best: enable the optimizer and set both:&#10;- Breadth (how many recipe ideas we try)&#10;- Depth (how carefully we score each idea)&#10;Custom: keep your current numbers.&#10;&#10;How it works (successive halving): start with many candidates ‚Üí test quickly on Stage‚Äë1 images ‚Üí keep the best fraction ‚Üí retest on Stage‚Äë2/3 to pick a winner.">?</span></label>
						                                            <select id="agentStepsGlobalPreset">
						                                                <option value="off" selected>Off (fastest)</option>
						                                                <option value="fast">Fast</option>
						                                                <option value="balanced">Balanced (recommended)</option>
						                                                <option value="best">Best (slowest)</option>
						                                                <option value="custom">Custom</option>
						                                            </select>
							                                            <div class="training-help" style="margin-top: 6px; color: #64748b;">
							                                                Global optimizer runs a small ‚Äútournament‚Äù (successive halving). Use <strong>Breadth</strong> to explore more ideas and <strong>Depth</strong> to make comparisons more reliable.
							                                            </div>
						                                            <div class="training-help" style="margin-top: 6px; color: #64748b;">
						                                                Tip: changing any advanced optimizer fields below will switch ‚ÄúSearch budget‚Äù to Custom.
						                                            </div>
						                                        </div>
						                                    </div>
						                                    <div class="training-help" style="margin-top: 10px; padding: 10px 12px; border: 1px solid #e2e8f0; border-radius: 12px; background: #f8fafc;">
						                                        <div style="display: flex; align-items: center; justify-content: space-between; gap: 10px;">
						                                            <div style="font-weight: 700; color: #0f172a;">
						                                                Compute estimate<span class="help-icon" title="Rough, relative compute estimate (not seconds). Updates live as you change settings.&#10;&#10;What it tracks: Sample images √ó (steps √ó (1 + candidates/step)), plus extra work from the Global optimizer.&#10;&#10;Global optimizer cost grows with:&#10;- Breadth (trials, rounds, variants/round, max step changes)&#10;- Depth (stage eval caps, keep ratio)&#10;&#10;Real runtime depends on GPU, model, image sizes, caching, and dataset content. Use this to see how cost scales as you turn knobs.">?</span>
						                                            </div>
						                                            <span id="agentStepsBudgetBadge" class="badge">‚Ä¶</span>
						                                        </div>
						                                        <div id="agentStepsBudgetText" class="training-help" style="margin-top: 6px; color: #475569; white-space: pre-wrap;">‚Ä¶</div>
						                                        <div class="training-progress" style="margin-top: 8px;">
						                                            <div id="agentStepsBudgetFill" class="training-progress-fill"></div>
						                                        </div>
						                                    </div>
						                                    <div class="training-subsection" style="margin-top: 12px;">
						                                        <div class="training-subsection__title">Speed helpers</div>
						                                        <div class="training-grid" style="margin-top: 8px;">
						                                            <div>
						                                                <label for="agentStepsEarlyStop">Early-stop<span class="help-icon" title="Stop adding prompt steps when coverage stalls or candidates are far below your cleanliness target.&#10;This can reduce runtime, but may miss rare edge cases.&#10;Off is safer for recall-first runs; Balanced is a good speed default.">?</span></label>
						                                                <div class="training-field training-field--inline" style="margin-bottom: 6px;">
						                                                    <input type="checkbox" id="agentStepsEarlyStop" checked />
						                                                    <span class="training-help">On (default)</span>
						                                                </div>
						                                                <label for="agentStepsEarlyStopMode">Strictness<span class="help-icon" title="Conservative keeps searching longer; Aggressive stops sooner.">?</span></label>
						                                                <select id="agentStepsEarlyStopMode">
						                                                    <option value="conservative">Conservative</option>
						                                                    <option value="balanced" selected>Balanced</option>
						                                                    <option value="aggressive">Aggressive</option>
						                                                </select>
						                                            </div>
						                                            <div>
						                                                <label for="agentStepsPromptPrefilter">CLIP prompt prefilter<span class="help-icon" title="Use CLIP similarity to filter weak prompts before running SAM3.&#10;This can greatly reduce candidate-eval time. Base prompts are always kept.">?</span></label>
						                                                <div class="training-field training-field--inline" style="margin-bottom: 6px;">
						                                                    <input type="checkbox" id="agentStepsPromptPrefilter" checked />
						                                                    <span class="training-help">On</span>
						                                                </div>
						                                                <label for="agentStepsPromptPrefilterMode">Strength<span class="help-icon" title="Keep more prompts = slower but safer; keep fewer = faster but riskier.">?</span></label>
						                                                <select id="agentStepsPromptPrefilterMode">
						                                                    <option value="conservative">Keep more prompts</option>
						                                                    <option value="balanced" selected>Balanced</option>
						                                                    <option value="aggressive">Keep fewer prompts</option>
						                                                </select>
						                                            </div>
						                                            <div>
						                                                <label for="agentStepsPromptBgDrop">Prompt background drop<span class="help-icon" title="Use background classes in your CLIP head to skip prompts that mostly hit background.&#10;This can cut a lot of wasted SAM3 runs on noisy prompts.&#10;Requires a head trained with __bg_* classes.">?</span></label>
						                                                <div class="training-field training-field--inline" style="margin-bottom: 6px;">
						                                                    <input type="checkbox" id="agentStepsPromptBgDrop" checked />
						                                                    <span class="training-help">On</span>
						                                                </div>
						                                                <label for="agentStepsPromptBgDropMode">Strictness<span class="help-icon" title="Conservative keeps more prompts; Aggressive drops more prompt candidates and is faster.">?</span></label>
						                                                <select id="agentStepsPromptBgDropMode">
						                                                    <option value="conservative">Conservative</option>
						                                                    <option value="balanced" selected>Balanced</option>
						                                                    <option value="aggressive">Aggressive</option>
						                                                </select>
						                                            </div>
						                                        </div>
						                                    </div>
							                                    <div class="training-help" style="margin-top: 8px; color: #64748b;">
							                                        Advanced (below): Global optimizer breadth/depth knobs, Tier‚Äë1/Tier‚Äë2, prompt subset refinement, and candidate-curve floors.
							                                    </div>
						                                </div>
						                            </div>
											                                <div id="agentStepsAdvancedDetails" style="margin-top: 12px; padding: 12px 14px; border: 1px solid #e2e8f0; border-radius: 12px; background: #ffffff;">
											                                    <div style="font-weight: 700; margin-bottom: 6px;">Advanced optimization</div>
											                                        <div class="training-help" style="color: #475569; margin-bottom: 10px;">
											                                            Optional knobs for Global optimizer breadth/depth, Tier‚Äë1/Tier‚Äë2 tuning, prompt subset refinement, and candidate-curve floors.
											                                        </div>
							                                        <div style="font-weight: 600; margin-top: 10px;">Global optimizer (successive halving)</div>
							                                        <div class="training-help" style="margin-top: 6px; color: #475569;">
							                                            Tune it with two levers:
							                                            <strong>Breadth</strong> = try more recipe ideas ‚Ä¢ <strong>Depth</strong> = score each idea more carefully.
						                                        </div>
						                                        <div class="two-col-grid" style="margin-top: 10px;">
						                                            <div class="training-subsection">
						                                                <div class="training-subsection__title">Breadth (exploration)</div>
						                                                <div class="training-help">Try more candidates. Helpful when results plateau or prompts look ‚Äúobviously wrong‚Äù.</div>
						                                                <div class="training-grid" style="margin-top: 8px;">
						                                                    <div>
						                                                        <label for="agentStepsGlobalMaxTrials">Max trials<span class="help-icon" title="Breadth.&#10;How many candidate recipes we test in Stage‚Äë1 (per class).&#10;Higher = explores more ideas (slower).&#10;Typical: 16‚Äì64.">?</span></label>
						                                                        <input type="number" id="agentStepsGlobalMaxTrials" min="1" max="4096" value="36" />
						                                                    </div>
						                                                    <div>
						                                                        <label for="agentStepsGlobalRounds">Rounds<span class="help-icon" title="Breadth.&#10;How many times we repeat the tournament around the current best recipe.&#10;More rounds = more chances to improve (slower).&#10;Typical: 1‚Äì3.">?</span></label>
						                                                        <input type="number" id="agentStepsGlobalRounds" min="1" max="20" value="2" />
						                                                    </div>
						                                                    <div>
						                                                        <label for="agentStepsGlobalMutationsPerRound">Variants per round<span class="help-icon" title="Breadth.&#10;How many new recipe variants we generate around the current best per round.&#10;Higher = explores more tweaks (slower).&#10;Typical: 12‚Äì40.">?</span></label>
						                                                        <input type="number" id="agentStepsGlobalMutationsPerRound" min="1" max="10000" value="24" />
						                                                    </div>
						                                                    <div>
						                                                        <label for="agentStepsGlobalMaxStepsMutated">Max steps changed per variant<span class="help-icon" title="Breadth.&#10;How big each candidate change is allowed to be.&#10;1 = tiny, stable tweaks; 2‚Äì3 = coordinated changes (more powerful, less predictable).">?</span></label>
						                                                        <input type="number" id="agentStepsGlobalMaxStepsMutated" min="1" max="10" value="2" />
												                                    </div>
												                                </div>
						                                                <div class="training-help" style="margin-top: 10px;">
						                                                    <div class="training-field training-field--inline">
						                                                        <label style="display: inline-flex; align-items: center; gap: 8px;" title="Breadth (optional). Allows the optimizer to swap step order. Usually small effect, but can matter when Max dets/image is small.">
						                                                            <input type="checkbox" id="agentStepsGlobalEnableOrdering" />
						                                                            Tune step order
						                                                        </label>
						                                                    </div>
						                                                    <div class="training-field training-field--inline">
						                                                        <label style="display: inline-flex; align-items: center; gap: 8px;" title="Breadth (optional). Allows the optimizer to tune per-step Max dets/image. Can reduce false positives, but can also miss objects and change runtime.">
						                                                            <input type="checkbox" id="agentStepsGlobalEnableMaxResults" />
						                                                            Tune max dets/image
						                                                        </label>
						                                                    </div>
						                                                </div>
						                                            </div>
						                                            <div class="training-subsection">
						                                                <div class="training-subsection__title">Depth (evaluation reliability)</div>
					                                                <div class="training-help">Score candidates on more sampled images and prune less aggressively.</div>
						                                                <div class="training-grid" style="margin-top: 8px;">
						                                                    <div>
						                                                        <label for="agentStepsGlobalEvalCap1">Stage‚Äë1 eval cap<span class="help-icon" title="Depth.&#10;How many sampled images we use for the first (fast) ranking stage.&#10;Low = fast but noisy rankings.&#10;Typical: 25‚Äì100.">?</span></label>
						                                                        <input type="number" id="agentStepsGlobalEvalCap1" min="1" max="50000" value="50" />
						                                                    </div>
						                                                    <div>
						                                                        <label for="agentStepsGlobalEvalCap2">Stage‚Äë2 eval cap<span class="help-icon" title="Depth.&#10;How many sampled images we use for the second stage.&#10;Higher = more reliable (slower).&#10;Typical: 100‚Äì500.">?</span></label>
						                                                        <input type="number" id="agentStepsGlobalEvalCap2" min="1" max="50000" value="200" />
						                                                    </div>
						                                                    <div>
						                                                        <label for="agentStepsGlobalEvalCap3">Stage‚Äë3 eval cap<span class="help-icon" title="Depth.&#10;How many sampled images we use for the final stage.&#10;Typical: 500‚Äì2000.&#10;If your sample is smaller, we automatically use the full sample.">?</span></label>
						                                                        <input type="number" id="agentStepsGlobalEvalCap3" min="1" max="50000" value="1000" />
						                                                    </div>
						                                                    <div>
						                                                        <label for="agentStepsGlobalKeepRatio">Keep ratio<span class="help-icon" title="Depth.&#10;How many candidates survive each stage. 0.5 keeps the top half.&#10;Higher = safer (less likely to prune a good candidate early), slower.&#10;Lower = faster, riskier.">?</span></label>
						                                                        <input type="number" id="agentStepsGlobalKeepRatio" step="0.05" min="0.1" max="0.9" value="0.5" />
						                                                    </div>
						                                                </div>
						                                            </div>
						                                        </div>
						                                        <div class="training-help" style="margin-top: 6px; color: #64748b;">
							                                            Note: When ‚ÄúSearch budget‚Äù is not Off, Tier‚Äë1 and Tier‚Äë2 toggles are ignored.
						                                        </div>
					                                        <hr style="margin: 12px 0; border: none; border-top: 1px solid #e2e8f0;" />
					                                        <label style="display: inline-flex; align-items: center; gap: 8px;" title="Optional (advanced).&#10;When enabled, we try a small grid of candidate settings to improve coverage at your cleanliness target.&#10;This tunes the recipe‚Äôs visual expansion score (SAM3) + candidates per step (shared across steps).&#10;Requires a pretrained CLIP head.&#10;Slower than the default.">
					                                            <input type="checkbox" id="agentStepsTier1Optimize" />
					                                            Tier‚Äë1 grid search: visual score &amp; candidate count
					                                        </label>
					                                        <div class="training-grid" style="margin-top: 10px;">
					                                            <div>
					                                            <label for="agentStepsTier1EvalCap">Tier‚Äë1 grid search eval cap (images)<span class="help-icon" title="How many sampled images we use while comparing candidate settings.&#10;Higher = more reliable tuning (slower).&#10;Lower = faster but noisier.&#10;Typical: 100‚Äì500.">?</span></label>
					                                                <input type="number" id="agentStepsTier1EvalCap" min="10" max="50000" value="200" />
					                                            </div>
					                                            <div>
					                                                <label for="agentStepsTier1MaxTrials">Tier‚Äë1 grid search max trials<span class="help-icon" title="How many candidate settings we try during Tier‚Äë1 grid search.&#10;More trials can find better settings but costs time.&#10;Typical: 6‚Äì12.">?</span></label>
					                                                <input type="number" id="agentStepsTier1MaxTrials" min="1" max="256" value="9" />
					                                            </div>
					                                        </div>
					                                        <div class="training-help" style="margin-top: 6px; color: #64748b;">
					                                            Note: Tier‚Äë1 grid search is only used in Multi-step mode and only when a pretrained CLIP head is selected.
					                                        </div>
					                                        <div class="training-help" style="margin-top: 10px;">
					                                            <label style="display: inline-flex; align-items: center; gap: 8px;" title="Optional (advanced).&#10;When enabled, we try a small number of candidate de-duplication settings to reduce duplicates and false positives at your cleanliness target.&#10;This tunes the candidate de-dupe IoU + output de-dupe IoU for the class (applied across all steps).&#10;Requires a pretrained CLIP head.&#10;Slower than the default.">
					                                                <input type="checkbox" id="agentStepsTier2Optimize" />
					                                                Tier‚Äë2 IoU search: de‚Äëdupe overlaps
					                                            </label>
					                                            <div class="training-grid" style="margin-top: 10px;">
					                                                <div>
					                                                    <label for="agentStepsTier2EvalCap">Tier‚Äë2 IoU search eval cap (images)<span class="help-icon" title="How many sampled images we use while comparing candidate de-dupe settings.&#10;Higher = more reliable tuning (slower).&#10;Lower = faster but noisier.&#10;Typical: 100‚Äì500.">?</span></label>
					                                                    <input type="number" id="agentStepsTier2EvalCap" min="10" max="50000" value="200" />
					                                                </div>
					                                                <div>
					                                                    <label for="agentStepsTier2MaxTrials">Tier‚Äë2 IoU search max trials<span class="help-icon" title="How many candidate de-dupe settings we try during Tier‚Äë2 IoU search.&#10;More trials can find better settings but costs time.&#10;Typical: 6‚Äì16.">?</span></label>
					                                                    <input type="number" id="agentStepsTier2MaxTrials" min="1" max="256" value="12" />
					                                                </div>
					                                            </div>
					                                            <div class="training-help" style="margin-top: 6px; color: #64748b;">
					                                                Note: Tier‚Äë2 currently applies one set of IoU values per class (shared across all steps).
					                                            </div>
					                                        </div>
					                                        <div class="training-help" style="margin-top: 10px;">
					                                            <label style="display: inline-flex; align-items: center; gap: 8px;" title="Optional (advanced).&#10;After the initial prompt selection, do a small local search that can add/drop/swap prompt steps.&#10;This is text-candidate stage only (fast) and does not run extra SAM evaluations.&#10;Useful when the greedy selection gets stuck in a suboptimal prompt set.">
					                                                <input type="checkbox" id="agentStepsRefinePromptSubset" />
					                                                Refine prompt subset (local search)
					                                            </label>
					                                            <div class="training-grid" style="margin-top: 10px;">
					                                                <div>
					                                                    <label for="agentStepsRefineMaxIters">Refine iterations<span class="help-icon" title="How many local-search improvement steps to try.&#10;0 = disabled even if the checkbox is on.&#10;Typical: 3‚Äì10.">?</span></label>
					                                                    <input type="number" id="agentStepsRefineMaxIters" min="0" max="100" value="6" />
					                                                </div>
					                                                <div>
					                                                    <label for="agentStepsRefineTopK">Candidates per iter<span class="help-icon" title="How many add/swap candidates to consider per iteration (higher = more thorough, slower).&#10;Typical: 4‚Äì10.">?</span></label>
					                                                    <input type="number" id="agentStepsRefineTopK" min="1" max="50" value="6" />
					                                                </div>
					                                            </div>
					                                        </div>
					                                        <hr style="margin: 12px 0; border: none; border-top: 1px solid #e2e8f0;" />
					                                        <div class="training-help" style="margin-top: 6px;">
					                                            <div style="font-weight: 600; margin-bottom: 6px;">Candidate-threshold curve collection</div>
					                                            <div class="training-grid">
					                                                <div>
					                                                    <label for="agentStepsSeedEvalFloor">Candidate curve floor (optional)<span class="help-icon" title="Candidate-threshold curve = we sweep the text score from low ‚Üí high and record how many true objects vs false positives we get. That curve shows the trade-off, and we auto-pick per-step thresholds from it.&#10;&#10;Auto (default) uses your base Text threshold.&#10;Set a lower floor (e.g., 0.0‚Äì0.01) so the curve can include lower thresholds.&#10;Warning: lower floor can be slower.">?</span></label>
					                                                    <input type="number" id="agentStepsSeedEvalFloor" step="0.01" min="0" max="1" placeholder="auto" />
					                                                </div>
					                                                <div>
					                                                    <label for="agentStepsSeedEvalMaxResults">Candidate curve max dets (optional)<span class="help-icon" title="Safety valve for very low candidate curve floors.&#10;Caps how many candidate detections we consider per prompt per image while building curves.&#10;Lower = faster/cleaner curves, but can miss low-score matches.&#10;Leave blank to reuse Max dets/image.">?</span></label>
					                                                    <input type="number" id="agentStepsSeedEvalMaxResults" min="1" max="5000" placeholder="auto" />
					                                                </div>
					                                            </div>
					                                        </div>
											                                </div>
						                        <div class="training-grid">
						                            <div>
						                                <label for="agentSeedThreshold">Base text threshold (candidates)<span class="help-icon" title="Base SAM3 text score used to collect candidate curves.&#10;Per-step text thresholds are auto-selected from those curves (default).&#10;Lower = more candidates (better recall, more noise, slower).&#10;Higher = fewer candidates (cleaner/faster, can miss objects).">?</span></label>
						                                <input type="number" id="agentSeedThreshold" step="0.01" min="0" max="1" value="0.02" />
			                            </div>
			                            <div>
			                                <label for="agentExpandThreshold">Visual expansion score (SAM3)<span class="help-icon" title="Similarity-based search threshold for SAM3‚Äôs visual prompt stage.&#10;Higher = fewer but cleaner expansions; lower = more but noisier expansions.&#10;Per-step values can be tuned by Tier‚Äë1 or the Global optimizer; otherwise this base value is used.">?</span></label>
			                                <input type="number" id="agentExpandThreshold" step="0.01" min="0" max="1" value="0.15" />
			                            </div>
			                            <div>
			                                <label for="agentSeedDedupeIou">Candidate de-dupe overlap (IoU)<span class="help-icon" title="After the text-candidate pass, we remove near-duplicate candidate boxes.&#10;If two candidate boxes overlap more than this IoU, we keep one.&#10;Higher (e.g., 0.9) only removes almost-identical boxes.&#10;Lower removes more overlaps (can collapse crowded scenes).">?</span></label>
			                                <input type="number" id="agentSeedDedupeIou" step="0.05" min="0" max="1" value="0.9" />
			                            </div>
			                            <div>
			                                <label for="agentDedupeIou">Output de-dupe overlap (IoU)<span class="help-icon" title="Final de-duplication across all detections the recipe outputs.&#10;If two output boxes overlap more than this IoU, we keep the best one.&#10;Lower = more aggressive de-dup (fewer duplicates; may drop close objects).&#10;Higher = allows more overlaps (risk duplicate labels).">?</span></label>
			                                <input type="number" id="agentDedupeIou" step="0.05" min="0" max="1" value="0.5" />
			                            </div>
			                        </div>
			                        <div class="training-grid">
			                            <div>
			                                <label for="agentMaskThreshold">Mask threshold<span class="help-icon" title="Polygons only.&#10;Controls how the mask is turned into a polygon (higher trims uncertain pixels).&#10;Higher = tighter/smaller shapes; lower = looser/bigger shapes.">?</span></label>
			                                <input type="number" id="agentMaskThreshold" step="0.05" min="0" max="1" value="0.5" />
			                            </div>
			                            <div>
			                                <label for="agentMaxResults">Max dets/image<span class="help-icon" title="Safety cap: maximum number of detections kept per image per prompt/pass.&#10;Prevents runaway outputs on noisy prompts.&#10;If too low, crowded images may be clipped.">?</span></label>
			                                <input type="number" id="agentMaxResults" min="1" value="1000" />
			                            </div>
			                            <div>
			                                <label for="agentMinSize">Min area (px¬≤)<span class="help-icon" title="Drop detections smaller than this pixel area.&#10;Useful to remove tiny specks.&#10;0 keeps all (including very small objects).">?</span></label>
			                                <input type="number" id="agentMinSize" min="0" value="0" />
			                            </div>
			                            <div>
			                                <label for="agentSimplifyEps">Simplify Œµ<span class="help-icon" title="Polygons only.&#10;Douglas‚ÄìPeucker simplification in pixels.&#10;Higher = fewer points (simpler shapes).&#10;0 = no simplification (max detail).">?</span></label>
			                                <input type="number" id="agentSimplifyEps" min="0" step="0.1" value="0.0" />
			                            </div>
			                        </div>
				                        <div class="training-grid training-grid--single">
					                            <div>
					                                <label for="agentClipHeadSelect">Pretrained CLIP head (required)<span class="help-icon" title="Required for recipe mining.&#10;Agent Mining uses a pretrained CLIP classifier head (trained earlier on your dataset) to filter detections and tune ‚Äúcleanliness‚Äù (precision).&#10;The head (and its tuned thresholds) is embedded into saved recipe zips (portable).&#10;Tip: you can also tighten CLIP thresholds later at inference time per cascade step (cumulative).">?</span></label>
						                                <div class="training-picker">
							                                <select id="agentClipHeadSelect">
							                                    <option value="">Select a CLIP head‚Ä¶</option>
							                                </select>
							                                <button type="button" class="training-button secondary" id="agentClipHeadRefresh">Refresh</button>
						                                </div>
						                                <div class="training-help">Required. Select a classifier under <code>uploads/classifiers/</code> (trained via the CLIP tab or uploaded). Recommended: keep auto-tune on and use a high target precision for clean recipes.</div>
						                                <div class="training-help" id="agentClipHeadMeta"></div>
						                            </div>
					                            <div class="training-field training-field--inline">
					                                <label for="agentClipHeadAutoTune">Auto-tune thresholds<span class="help-icon" title="CLIP head mode only (recommended).&#10;Automatically searches per-class CLIP head thresholds on the sampled images so recipes are clean (few false positives).&#10;By default this tunes min prob and margin (see ‚ÄúAuto-tune margin‚Äù below).&#10;These tuned thresholds are baked into the saved recipe ZIP and used whenever the recipe runs.&#10;Turn this off only if you want to force fixed thresholds for all classes.">?</span></label>
					                                <input type="checkbox" id="agentClipHeadAutoTune" checked />
					                            </div>
					                            <div class="training-field training-field--inline">
					                                <label for="agentClipHeadTuneMargin">Auto-tune margin<span class="help-icon" title="When auto-tuning CLIP thresholds, also search a per-class margin (target must beat other classes by this amount).&#10;Disable this to keep a fixed margin while min prob is auto-tuned.">?</span></label>
					                                <input type="checkbox" id="agentClipHeadTuneMargin" checked />
					                            </div>
					                            <div>
					                                <label for="agentClipHeadTargetPrecision">Recipe cleanliness (precision target)<span class="help-icon" title="CLIP head auto-tune only.&#10;&#10;What ‚Äúprecision‚Äù means here:&#10;- Precision = matches / (matches + false positives) on the sampled images.&#10;- A detection is a ‚Äúmatch‚Äù if it overlaps an unmatched ground-truth box for that class with IoU ‚â• the job‚Äôs IoU threshold.&#10;- Detections that match no ground-truth box count as false positives.&#10;- Extra detections on the same object are counted as ‚ÄúDuplicates‚Äù (shown separately).&#10;&#10;How to tune:&#10;- Move toward ‚ÄúCleaner‚Äù to reduce false positives (but you may miss objects).&#10;- Typical clean recipes: 0.85‚Äì0.98. Recall-first runs: 0.70‚Äì0.80.&#10;&#10;This sets the per-class CLIP thresholds that get baked into the saved recipe ZIP.">?</span></label>
					                                <div class="training-help" style="display: flex; align-items: center; gap: 10px; margin-top: 4px;">
					                                    <span style="white-space: nowrap;" title="Lower target precision = more detections, but noisier (more false positives).">More detections (noisier)</span>
					                                    <input type="range" id="agentClipHeadTargetPrecision" min="0.5" max="0.99" step="0.01" value="0.75" style="flex: 1;" />
					                                    <span id="agentClipHeadTargetPrecisionValue" style="min-width: 3.5em; text-align: right;">0.75</span>
					                                    <span style="white-space: nowrap;" title="Higher target precision = cleaner results (fewer false positives), but can miss objects.">Cleaner (fewer false positives)</span>
					                                </div>
					                                <div class="training-help" style="margin-top: 6px; color: #64748b;">Default is recall-first (0.75). Increase for cleaner results.</div>
					                                <div class="training-help" style="margin-top: 6px;">
					                                    <div style="font-weight: 600; margin-bottom: 6px;">Advanced</div>
					                                    <label style="display: inline-flex; align-items: center; gap: 8px;" title="Enable precision targets below 0.50 (0.10‚Äì0.99). This is mainly useful for recall-first debugging and usually produces very noisy recipes.">
					                                        <input type="checkbox" id="agentClipHeadAllowLowPrecision" />
					                                        Allow very low precision targets (0.10‚Äì0.99)
					                                    </label>
					                                    <div class="training-help" style="margin-top: 6px; color: #b91c1c;">
					                                        Warning: targets below 0.50 often create extremely noisy recipes (huge false positives) and can increase mining time + output size. Use only for recall-first experiments / debugging.
					                                    </div>
					                                </div>
					                            </div>
				                        </div>
				                        <div class="training-grid">
				                            <div class="training-field training-field--inline">
				                                <label for="agentClipHeadBgGuard">Background suppression (negative classes)<span class="help-icon" title="Use the __bg_* classes in your CLIP head to suppress false positives.&#10;If a detection scores higher for background than the target class, we drop it.&#10;Requires a head trained with background classes.">?</span></label>
				                                <input type="checkbox" id="agentClipHeadBgGuard" checked />
				                            </div>
				                            <div>
				                                <label for="agentClipHeadBgMargin">Background margin<span class="help-icon" title="Optional margin for background suppression.&#10;Require the target class to beat the best background class by at least this margin.&#10;0 = only require target ‚â• background.">?</span></label>
				                                <input type="number" id="agentClipHeadBgMargin" step="0.01" min="0" max="1" value="0.0" />
				                            </div>
				                            <div>
				                                <label for="agentClipHeadBgApply">Apply background suppression<span class="help-icon" title="Choose where to apply background suppression.&#10;Final only = safest for recall.&#10;Seeds only or both can reduce noise earlier but may lower recall.">?</span></label>
				                                <select id="agentClipHeadBgApply">
				                                    <option value="final">Final detections only (default)</option>
				                                    <option value="seed">Seed candidates only</option>
				                                    <option value="both">Seeds + final</option>
				                                </select>
				                            </div>
				                        </div>
				                        <div class="training-grid">
				                            <div>
				                                <label for="agentClipHeadBgPenalty">Background penalty (optimizer)<span class="help-icon" title="Optional penalty during optimization: recipes that are often classified as background are scored lower.&#10;0 disables the penalty.">?</span></label>
				                                <input type="number" id="agentClipHeadBgPenalty" step="0.01" min="0" max="2" value="0.0" />
				                            </div>
				                            <div class="training-help" style="align-self: center;">
				                                Uses the __bg_* classes to suppress background-like detections. Default is ON for cleaner recipes.
				                            </div>
				                        </div>
				                        <div class="training-grid">
				                            <div class="training-field training-field--inline">
				                                <label for="agentClipHeadBgAutoTune">Auto-tune background margin<span class="help-icon" title="When auto-tuning CLIP thresholds, also search the best background margin per class.&#10;Requires background classes and auto-tune enabled.">?</span></label>
				                                <input type="checkbox" id="agentClipHeadBgAutoTune" checked />
				                            </div>
				                            <div class="training-field training-field--inline">
				                                <label for="agentStepsHardNegExport">Hard-negative replay export<span class="help-icon" title="Export false positives (background-like crops) during final tuning to reuse as negatives later.&#10;Saved under uploads/clip_negative_replay/.">?</span></label>
				                                <input type="checkbox" id="agentStepsHardNegExport" checked />
				                            </div>
				                            <div>
				                                <label for="agentStepsHardNegMaxCrops">Max replay crops<span class="help-icon" title="Maximum number of hard-negative crops to save per class.&#10;0 disables export.">?</span></label>
				                                <input type="number" id="agentStepsHardNegMaxCrops" min="0" max="5000" value="200" />
				                            </div>
				                            <div>
				                                <label for="agentStepsHardNegMinProb">Min CLIP prob<span class="help-icon" title="Minimum target-class probability for a detection to be saved as a hard negative.">?</span></label>
				                                <input type="number" id="agentStepsHardNegMinProb" step="0.01" min="0" max="1" value="0.1" />
				                            </div>
				                        </div>
				                        <div class="training-grid">
					                            <div>
					                                <label for="agentClipHeadMinProb">Manual head min prob<span class="help-icon" title="CLIP head manual mode only (auto-tune OFF).&#10;Minimum probability required for the target class.&#10;Higher = fewer detections, cleaner results; lower = more detections (more false positives).&#10;This value is baked into the saved recipe ZIP.">?</span></label>
					                                <input type="number" id="agentClipHeadMinProb" step="0.05" min="0" max="1" value="0.5" />
					                            </div>
				                            <div>
				                                <label for="agentClipHeadMargin">Head margin<span class="help-icon" title="Used when auto-tune is OFF or when ‚ÄúAuto-tune margin‚Äù is disabled.&#10;Require the target class probability to beat the best other class by this margin.&#10;0 disables the margin check.&#10;Higher can reduce confusion between similar classes.&#10;This value is baked into the saved recipe ZIP.">?</span></label>
				                                <input type="number" id="agentClipHeadMargin" step="0.05" min="0" max="1" value="0.0" />
				                            </div>
				                        </div>
		                        <div class="two-col-grid">
		                            <div>
		                                <label for="agentClasses">Class IDs (optional)<span class="help-icon" title="Optional: restrict mining to specific class IDs. Leave blank to mine all classes.&#10;Example: 1,2,5">?</span></label>
		                                <input type="text" id="agentClasses" placeholder="blank = all" />
		                            </div>
		                            <div>
		                                <label for="agentQwenMaxPrompts">Extra text prompts/class<span class="help-icon" title="Ask GPT-OSS for additional words to try for each class (in addition to the class name).&#10;0 = only use the class name; higher values try more alternative phrases.">?</span></label>
		                                <input type="number" id="agentQwenMaxPrompts" min="0" max="20" value="0" title="0 = only use the class name; higher values try more alternative phrases." />
		                            </div>
		                            <div>
		                                <label for="agentPromptReasoning">GPT-OSS reasoning<span class="help-icon" title="Controls how much the model ‚Äúthinks‚Äù before proposing extra words.&#10;Higher can be slower and less stable.&#10;We recommend ‚Äúnone‚Äù for now.">?</span></label>
		                                <select id="agentPromptReasoning" title="How much GPT-OSS ‚Äúthinks‚Äù before proposing extra words. Higher uses more time and can be less stable.">
		                                    <option value="none" selected>none (fastest)</option>
	                                    <option value="low">low</option>
	                                    <option value="medium">medium</option>
	                                    <option value="high">high</option>
	                                </select>
	                                <div class="training-help" style="color: #b91c1c;">Only ‚Äúnone‚Äù is stable right now.</div>
		                            </div>
		                            <div>
		                                <label for="agentPromptMaxTokens">GPT-OSS max tokens<span class="help-icon" title="Maximum length of the model output when proposing extra prompts.&#10;Higher can be slower and use more GPU memory.&#10;If you see truncated lists, increase this.">?</span></label>
		                                <input type="number" id="agentPromptMaxTokens" min="16" max="400" value="160" title="Maximum length of GPT-OSS output when proposing extra words." />
		                            </div>
		                        </div>
				                        <div class="training-field">
				                            <label for="agentExtraPrompts">Extra prompts (optional)<span class="help-icon" title="Optional JSON dict of extra prompts to try.&#10;Keys: class names.&#10;Values: lists of strings.&#10;Special key ‚Äú__base__‚Äù applies to all classes.&#10;These are appended to GPT-OSS suggestions.">?</span></label>
				                            <textarea id="agentExtraPrompts" rows="8" style="min-width: 480px;" placeholder='{"__base__": ["object", "small object"], "light_vehicle": ["car", "sedan"], "person": ["human", "pedestrian"]}' title="Optional JSON dict of class_name -> list of extra prompt phrases. Special key &quot;__base__&quot; applies to all classes. Example: {&quot;__base__&quot;: [&quot;object&quot;, &quot;small object&quot;], &quot;light_vehicle&quot;: [&quot;car&quot;, &quot;sedan&quot;]}"></textarea>
				                            <div class="training-help">Optional extra words/phrases to try. They are appended to GPT-OSS suggestions. Use &quot;__base__&quot; to add prompts for all classes.</div>
				                            <div class="training-help" id="agentExtraPromptsParseStatus" aria-live="polite"></div>
				                        </div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="agentRunBtn">Run SAM3 Recipe Mining</button>
                            <button type="button" class="training-button secondary" id="agentRefreshBtn">Refresh latest</button>
	                            <button type="button" class="training-button danger" id="agentCancelBtn">Cancel job</button>
	                        </div>
	                        <div class="training-message" id="agentStatus" role="status">Idle</div>
	                        <div class="training-help" id="agentCacheSize">Cache: ‚Ä¶</div>
	                        <div class="training-progress" aria-hidden="true">
	                            <div id="agentProgressFill" class="training-progress-fill"></div>
	                        </div>
	                        <div class="training-help" id="agentProgressText" aria-live="polite"></div>
	                    </section>
	                    <section class="training-status-section">
			                        <div class="training-card">
			                            <div class="training-card__header">
			                                <div class="training-card__title">Recipe mining quick reference</div>
			                            </div>
			                            <div class="training-card__body">
			                                <p><strong>Workflow:</strong> Train a CLIP head ‚Üí run mining (start with a small sample) ‚Üí review per-class results ‚Üí save recipes ‚Üí apply (and optionally chain) recipes from the labeling tab.</p>
			                                <p class="training-help" style="margin-top: 10px;"><strong>How to read scores:</strong></p>
			                                <ul class="training-help" style="margin-top: 6px;">
			                                    <li><strong>Coverage</strong> = matches / GT objects (higher is better).</li>
			                                    <li><strong>Precision</strong> = matches / (matches + false positives) (higher is cleaner).</li>
			                                    <li><strong>FPs</strong> = detections that match no ground-truth box for that class (lower is better).</li>
			                                    <li><strong>Duplicates</strong> = extra detections on the same GT object (lower is better; tracked separately).</li>
			                                    <li><strong>Det rate</strong> = % of sampled images that produced at least one detection (useful ‚Äúis anything happening?‚Äù sanity check).</li>
			                                </ul>
					                                <p class="training-help" style="margin-top: 10px;"><strong>If results look ‚Äúdead‚Äù:</strong> verify your CLIP head includes the dataset class names, and try lowering ‚ÄúCandidate curve floor‚Äù or temporarily lowering ‚ÄúRecipe cleanliness‚Äù to confirm SAM3 is producing candidates before tightening filters.</p>
			                                <div style="font-weight: 600; margin: 12px 0 6px;">Knob glossary (all controls)</div>
			                                <ul style="margin: 6px 0 0 18px;">
			                                    <li><strong>Eval images</strong>: how many images are sampled for mining + scoring.</li>
			                                    <li><strong>Sample seed</strong>: deterministic sample selector (same seed = same sample).</li>
			                                    <li><strong>Reuse cached SAM results</strong>: reuse cached SAM3 outputs between runs.</li>
			                                    <li><strong>Match overlap (Eval IoU)</strong>: IoU threshold for scoring matches only.</li>
			                                    <li><strong>Predictors per GPU</strong>: SAM3 workers per GPU (more = faster, more VRAM).</li>
			                                    <li><strong>Max predictors total</strong>: optional cap on total workers.</li>
			                                    <li><strong>Max recipe steps</strong>: max prompt steps per class.</li>
			                                    <li><strong>Similarity candidates per step</strong>: how many candidate boxes are expanded per image.</li>
			                                    <li><strong>Search budget</strong>: enables Global optimizer presets (Off/Fast/Balanced/Best).</li>
			                                    <li><strong>Compute estimate</strong>: relative cost indicator (not seconds).</li>
			                                    <li><strong>Early-stop</strong>: stop adding steps when coverage stalls or precision is too low.</li>
			                                    <li><strong>CLIP prompt prefilter</strong>: drop weak prompts before SAM3 (fast).</li>
			                                    <li><strong>Global optimizer (breadth)</strong>: trials, rounds, variants, step changes.</li>
			                                    <li><strong>Global optimizer (depth)</strong>: stage eval caps, keep ratio.</li>
			                                    <li><strong>Tier‚Äë1 grid search</strong>: tunes visual expansion score + candidates/step.</li>
			                                    <li><strong>Tier‚Äë2 IoU search</strong>: tunes candidate/output de‚Äëdupe IoUs.</li>
			                                    <li><strong>Refine prompt subset</strong>: small add/drop/swap search on prompts.</li>
			                                    <li><strong>Candidate curve floor / max dets</strong>: text-candidate curve controls.</li>
			                                    <li><strong>Text threshold (candidates)</strong>: score cutoff for text prompt detections.</li>
			                                    <li><strong>Visual expansion score (SAM3)</strong>: score cutoff for similarity-based expansion.</li>
			                                    <li><strong>Candidate de‚Äëdupe IoU</strong>: remove near-duplicate candidate boxes.</li>
			                                    <li><strong>Output de‚Äëdupe IoU</strong>: remove near-duplicate final boxes.</li>
			                                    <li><strong>Mask threshold</strong>: polygon mask binarization (segmentation only).</li>
			                                    <li><strong>Max dets/image</strong>: safety cap on outputs per image.</li>
			                                    <li><strong>Min area (px¬≤)</strong>: drop tiny detections.</li>
			                                    <li><strong>Simplify Œµ</strong>: polygon simplification amount.</li>
			                                    <li><strong>Pretrained CLIP head</strong>: required classifier for mining.</li>
			                                    <li><strong>Auto-tune thresholds</strong>: per-class CLIP min prob/margin tuning.</li>
			                                    <li><strong>Recipe cleanliness</strong>: target precision for CLIP tuning.</li>
			                                    <li><strong>Manual head min prob / margin</strong>: used when auto‚Äëtune is off.</li>
			                                    <li><strong>Class IDs</strong>: optional subset of classes to mine.</li>
			                                    <li><strong>Extra text prompts/class</strong>: GPT-OSS prompt expansion count.</li>
			                                    <li><strong>GPT‚ÄëOSS reasoning / max tokens</strong>: prompt generator settings.</li>
			                                    <li><strong>Extra prompts JSON</strong>: user-provided prompts by class.</li>
			                                </ul>
			                            </div>
			                        </div>
                        <h3>Progress & Logs</h3>
                        <div class="training-help">Live status and logs for the latest SAM3 Recipe Mining job.</div>
                        <div class="training-actions">
                            <button type="button" class="training-button danger" id="agentPurgeCacheBtn" title="Purge cached detections to free disk space.">Purge cache</button>
                        </div>
                        <div class="training-logs" style="margin-bottom: 12px;">
                            <div id="agentLogs" class="training-log" style="max-height: 220px;"></div>
                        </div>
	                        <div class="training-divider"></div>
	                        <h3>Results</h3>
	                        <div class="training-help">Coverage = % of ground-truth objects found. FPs = extra detections that shouldn‚Äôt be there. Higher coverage and lower FPs is better.</div>
	                        <div id="agentResults"></div>
		                        <div class="training-divider"></div>
		                        <h3>Saved Recipes</h3>
		                        <div class="training-grid agent-recipe-controls">
		                            <div class="training-field">
		                                <label for="agentRecipeSelect">Saved recipes<span class="help-icon" title="Saved recipe zips on the backend. Use Refresh if you just mined or imported a recipe.">?</span></label>
		                                <select id="agentRecipeSelect"></select>
		                            </div>
		                        </div>
		                        <div class="training-grid training-grid--buttons agent-recipe-actions">
		                            <button type="button" class="training-button" id="agentRecipeRefresh">Refresh list</button>
		                            <button type="button" class="training-button secondary" id="agentRecipeDownload">Download zip</button>
		                            <button type="button" class="training-button danger" id="agentRecipeDelete">Delete recipe</button>
		                        </div>
		                        <div class="training-field">
			                            <label for="agentRecipeFile">Import recipe zip<span class="help-icon" title="Import a recipe zip from another machine or previous run.&#10;New zips contain recipe.json + embedded CLIP head artifacts (clip_head/). Legacy zips may also include example crops.">?</span></label>
		                            <div class="training-grid training-grid--buttons">
		                                <input type="file" id="agentRecipeFile" accept=".zip" />
		                                <button type="button" class="training-button secondary" id="agentRecipeImport">Import</button>
		                            </div>
			                            <div class="training-help">Packages include recipe.json plus embedded CLIP head artifacts (and legacy crops when present) so they are portable between machines.</div>
		                        </div>
		                        <div class="training-help">Refresh, download, import, or delete saved recipes. Apply recipes to images from the labeling tab.</div>
		                    </section>
		                </div>
		            </div>
            <div class="tab-panel" id="tabPromptHelper" data-tab-panel="prompt-helper">
                <div class="training-wrapper">
                    <section class="training-form-section">
                        <h2>SAM3 Vocabulary Explorer</h2>
                        <p class="training-help">Generate alternative text prompts for your dataset classes and score them with SAM3 on a sampled subset to find the best wording.</p>
                        <div class="training-field">
                            <label>Dataset</label>
                            <div class="sam3-dataset-row">
                                <select id="promptHelperDatasetSelect"></select>
                                <button type="button" class="training-button" id="promptHelperDatasetRefresh">Refresh</button>
                            </div>
                            <div class="training-help" id="promptHelperDatasetSummary">Pick a converted SAM3/Qwen dataset.</div>
                        </div>
                        <div class="training-grid">
                            <div>
                                <label for="promptHelperSampleSize">Images per class</label>
                                <input type="number" id="promptHelperSampleSize" min="1" value="20" title="How many images per class to sample for scoring prompts." />
                            </div>
                            <div>
                                <label for="promptHelperMaxSynonyms">Max extra prompts per class</label>
                                <input type="number" id="promptHelperMaxSynonyms" min="0" max="10" value="3" title="Number of alternative phrases to try (original name is always included)." />
                            </div>
                            <div>
                                <label for="promptHelperScoreThresh">Score threshold</label>
                                <input type="number" id="promptHelperScoreThresh" step="0.01" min="0" max="1" value="0.2" title="Drop detections below this score when scoring prompts." />
                            </div>
                            <div>
                                <label for="promptHelperMaxDets">Max detections/image</label>
                                <input type="number" id="promptHelperMaxDets" min="1" value="100" title="Keep only the top N detections per image when scoring prompts." />
                            </div>
                            <div>
                                <label for="promptHelperIouThresh">IoU threshold</label>
                                <input type="number" id="promptHelperIouThresh" step="0.05" min="0" max="1" value="0.5" title="IoU threshold for a match when comparing SAM3 boxes to ground truth." />
                            </div>
                            <div>
                                <label for="promptHelperSeed">Random seed</label>
                                <input type="number" id="promptHelperSeed" value="42" title="Controls image sampling so reruns are reproducible." />
                            </div>
                        </div>
                        <div class="training-toggle-row">
                            <label title="Use Qwen to brainstorm human-friendly phrases. Turn off to only use your raw class names + simple cleaned variants."><input type="checkbox" id="promptHelperUseQwen" checked /> Use Qwen to propose phrases</label>
                        </div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="promptHelperGenerateBtn">Generate prompts</button>
                            <button type="button" class="training-button" id="promptHelperEvaluateBtn" disabled>Evaluate with SAM3</button>
                        </div>
                        <div class="training-field">
                            <label>Prompt presets</label>
                            <div class="training-grid">
                                <div>
                                    <input type="text" id="promptHelperPresetName" placeholder="Preset name (optional)" />
                                    <button type="button" class="training-button" id="promptHelperPresetSave">Save prompts</button>
                                </div>
                                <div>
                                    <select id="promptHelperPresetSelect"></select>
                                    <button type="button" class="training-button" id="promptHelperPresetLoad">Load preset</button>
                                </div>
                            </div>
                            <div class="training-help">Save/load prompt sets for reuse with this dataset.</div>
                        </div>
                        <div class="training-note">
                            <strong>Flow:</strong> generate/edit prompts, then run evaluation. Scoring samples images per class, runs SAM3 text prompts one at a time, and reports precision/recall-style stats against ground truth boxes.
                        </div>
                        <div class="training-message" id="promptHelperMessage" role="status"></div>
                        <div class="training-divider"></div>
                        <h3>Prompt Search (beta)</h3>
                        <p class="training-help">Use your edited prompts above, then run a targeted search that samples positives and negatives to find the safest wording (prioritizes recall but penalizes prompts below a precision floor).</p>
                        <div class="training-field">
                            <label for="promptSearchClassSelect">Class to search</label>
                            <select id="promptSearchClassSelect" style="width: 100%; min-width: 200px;"></select>
                            <div class="training-help">Default is all classes. Pick one to iterate faster on a single label (generate prompts first to populate).</div>
                        </div>
                        <div class="training-grid">
                            <div>
                                <label for="promptSearchSampleSize">Positive images/class</label>
                                <input type="number" id="promptSearchSampleSize" min="1" value="20" title="How many images that contain the class to sample." />
                            </div>
                            <div>
                                <label for="promptSearchNegatives">Negative images/class</label>
                                <input type="number" id="promptSearchNegatives" min="0" value="20" title="How many images without the class to include (catches false positives)." />
                            </div>
                            <div>
                                <label for="promptSearchPrecisionFloor">Precision floor</label>
                                <input type="number" id="promptSearchPrecisionFloor" step="0.05" min="0" max="1" value="0.9" title="Prompts below this precision get penalized heavily in the search score." />
                            </div>
                            <div>
                                <label for="promptSearchScoreThresh">Score threshold</label>
                                <input type="number" id="promptSearchScoreThresh" step="0.01" min="0" max="1" value="0.2" title="Drop detections below this score when scoring prompts." />
                            </div>
                            <div>
                                <label for="promptSearchMaxDets">Max detections/image</label>
                                <input type="number" id="promptSearchMaxDets" min="1" value="100" title="Keep only the top N detections per image when searching." />
                            </div>
                            <div>
                                <label for="promptSearchIouThresh">IoU threshold</label>
                                <input type="number" id="promptSearchIouThresh" step="0.05" min="0" max="1" value="0.5" title="IoU threshold for a match when comparing SAM3 boxes to ground truth." />
                            </div>
                            <div>
                                <label for="promptSearchSeed">Random seed</label>
                                <input type="number" id="promptSearchSeed" value="42" title="Controls sampling so reruns are reproducible." />
                            </div>
                        </div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="promptSearchRunBtn">Search best prompts</button>
                        </div>
                        <div class="training-message" id="promptSearchMessage" role="status"></div>
                        <div class="training-note">
                            Uses the current prompt list above (including any edits or presets). Positive samples only include images with the class; negatives are sampled from images without the class to reveal false positives.
                        </div>
                        <div class="training-divider"></div>
                        <h3>Prompt Recipe Mining</h3>
                        <p class="training-help">Find an ordered set of prompts + thresholds that covers all GTs for a class with zero (or minimal) false positives.</p>
                        <div class="training-field">
                            <label for="promptRecipeClassSelect">Class to target</label>
                            <select id="promptRecipeClassSelect"></select>
                            <div class="training-help">Runs on a deterministic sample for this class.</div>
                        </div>
                        <div class="training-grid">
                            <div>
                                <label for="promptRecipeSampleSize">Positive images</label>
                                <input type="number" id="promptRecipeSampleSize" min="1" value="30" title="How many images containing the class to sample." />
                            </div>
                            <div>
                                <label for="promptRecipeNegatives">Negative images</label>
                                <input type="number" id="promptRecipeNegatives" min="0" value="10" title="How many images without the class to include for FP checks." />
                            </div>
                            <div>
                                <label for="promptRecipeThresholds">Thresholds (comma sep)</label>
                                <input type="text" id="promptRecipeThresholds" value="0.2,0.3,0.4" title="Score thresholds to try for each prompt." />
                            </div>
                            <div>
                                <label for="promptRecipeMaxDets">Max detections/image</label>
                                <input type="number" id="promptRecipeMaxDets" min="1" value="100" />
                            </div>
                            <div>
                                <label for="promptRecipeIouThresh">IoU threshold</label>
                                <input type="number" id="promptRecipeIouThresh" step="0.05" min="0" max="1" value="0.5" />
                            </div>
                            <div>
                                <label for="promptRecipeSeed">Random seed</label>
                                <input type="number" id="promptRecipeSeed" value="42" />
                            </div>
                            <div>
                                <label for="promptRecipeExpandCount">Qwen expand (max)</label>
                                <input type="number" id="promptRecipeExpandCount" min="0" max="50" value="10" title="How many new prompts to request from Qwen." />
                            </div>
                        </div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="promptRecipeExpandBtn">Expand prompts with Qwen</button>
                            <button type="button" class="training-button" id="promptRecipeRunBtn">Mine recipe</button>
                            <button type="button" class="training-button secondary" id="promptRecipeApplyBtn" title="Copy the last mined recipe prompts/thresholds into the prompt editor so you can save as a preset.">
                                Apply last recipe to prompts
                            </button>
                        </div>
                        <div class="training-message" id="promptRecipeMessage" role="status"></div>
                        <div class="training-note">Uses prompts for the selected class (edit in the list above). Runs on positives + optional negatives; tries each threshold per prompt and proposes an ordered recipe.</div>
                    </section>
                    <section class="training-status-section">
                        <h2>Prompts & Results</h2>
                        <div class="training-help" id="promptHelperStatus">Idle</div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="promptHelperApplyBtn" disabled>Apply top prompts to SAM3 training</button>
                        </div>
                        <div class="training-subsection">
                            <h4>Prompts to evaluate</h4>
                            <div id="promptHelperPrompts"></div>
                        </div>
                        <div class="training-subsection">
                            <h4>Evaluation log</h4>
                            <div id="promptHelperLogs" class="training-log"></div>
                        </div>
                        <div class="training-summary" id="promptHelperSummary"></div>
                        <div id="promptHelperResults" class="training-history"></div>
                        <div class="training-subsection">
                            <h4>Prompt Search (beta)</h4>
                            <div class="training-help" id="promptSearchStatus">Idle</div>
                            <div class="training-subsection">
                                <h5>Search log</h5>
                                <div id="promptSearchLogs" class="training-log"></div>
                            </div>
                            <div id="promptSearchResults" class="training-history"></div>
                            <div class="training-subsection">
                                <h5>Prompt Recipe Mining</h5>
                                <div class="training-help" id="promptRecipeStatus">Idle</div>
                                <div class="training-subsection">
                                    <h5>Recipe log</h5>
                                    <div id="promptRecipeLogs" class="training-log"></div>
                                </div>
                                <div id="promptRecipeResults" class="training-history"></div>
                            </div>
                        </div>
                    </section>
                </div>
            </div>
            <div class="tab-panel" id="tabDatasets" data-tab-panel="datasets">
                <div class="training-wrapper dataset-wrapper">
                    <div class="two-col-grid">
                        <div class="training-card">
                            <div class="training-card__header">
                                <div class="training-card__title">Dataset Management</div>
                                <div class="training-help">Upload YOLO/YOLO-seg datasets once and reuse them across CLIP, Qwen, and SAM3 training.</div>
                            </div>
	                        <div class="training-card__body dataset-manager">
	                                <div class="training-subsection">
	                                    <div class="training-subsection__title">Upload current labeling session</div>
	                                    <div class="training-help">Packages the images + labels you have loaded in the labeling tab and uploads them as a YOLO / YOLO-seg dataset (train split) so you can reuse it for CLIP, Qwen, and SAM3 flows.</div>
	                                    <div class="training-grid">
	                                        <div>
	                                            <label for="datasetUploadCurrentName">Dataset name (optional)</label>
	                                            <input type="text" id="datasetUploadCurrentName" placeholder="Defaults to labeling_session" />
	                                        </div>
	                                        <div>
	                                            <label for="datasetUploadCurrentContext">Dataset context (optional)</label>
	                                            <input type="text" id="datasetUploadCurrentContext" placeholder="Optional: used for Qwen instruction text" />
	                                        </div>
	                                    </div>
	                                    <div class="training-actions">
	                                        <button type="button" id="datasetUploadCurrentBtn" class="training-button">Save as YOLO dataset</button>
	                                        <div id="datasetUploadCurrentSummary" class="training-help"></div>
	                                    </div>
	                                </div>
                                <div class="training-subsection">
                                    <div class="training-subsection__title">Upload YOLO/YOLO-seg zip</div>
                                    <label for="datasetUploadFile">Zip file (root: labelmap.txt + images + labels)</label>
                                    <div class="training-help">
                                        Expect exactly: labelmap.txt (class names, one per line) at the root, plus images/ and labels/ in YOLO format. If you already have train/val splits, include them; otherwise a single images/labels pair is fine.
                                    </div>
                                    <input type="file" id="datasetUploadFile" accept=".zip" />
                                    <div class="training-grid">
                                        <div>
                                            <label for="datasetUploadName">Dataset name (optional)</label>
                                            <input type="text" id="datasetUploadName" placeholder="Defaults to zip name" />
                                        </div>
                                        <div>
                                            <label for="datasetUploadType">Dataset type</label>
                                            <select id="datasetUploadType">
                                                <option value="">Auto-detect</option>
                                                <option value="bbox">BBox (YOLO)</option>
                                                <option value="seg">Polygon masks (YOLO-seg)</option>
                                            </select>
                                        </div>
                                    </div>
                                    <div class="training-actions">
                                        <button type="button" id="datasetUploadBtn" class="training-button">Upload dataset</button>
                                        <button type="button" id="datasetListRefresh" class="training-button training-button--ghost">Refresh list</button>
                                    </div>
                                    <div id="datasetUploadMessage" class="training-message"></div>
                                </div>
                            </div>
                        </div>
                        <div class="training-card">
                            <div class="training-card__header">
                                <div class="training-card__title">Datasets on disk</div>
                                <div class="training-help">All cached datasets discovered on disk. Use the refresh button after adding or removing datasets manually.</div>
                            </div>
                            <div class="training-card__body">
                                <div class="training-actions">
                                    <button type="button" id="datasetListRefreshTop" class="training-button">Refresh list</button>
                                </div>
                                <div id="datasetList" class="training-history"></div>
                            </div>
                        </div>
                    </div>
                    <div class="training-card">
                        <div class="training-card__header">
                            <div class="training-card__title">Build segmentation datasets from bbox datasets</div>
                            <div class="training-help">Creates YOLO polygon masks using SAM (bbox ‚Üí polygon). Originals stay untouched.</div>
                        </div>
                        <div class="training-card__body">
                            <label for="segBuilderDatasetSelect">Source bbox dataset</label>
                            <select id="segBuilderDatasetSelect"></select>
                            <div id="segBuilderDatasetSummary" class="training-help">Pick a dataset to convert.</div>
                            <label for="segBuilderOutputName">Output dataset name</label>
                            <input type="text" id="segBuilderOutputName" placeholder="Defaults to &lt;dataset&gt;_seg" />
                            <label for="segBuilderVariant">SAM variant</label>
                            <select id="segBuilderVariant">
                                <option value="sam3">SAM 3</option>
                                <option value="sam1">SAM 1</option>
                            </select>
                            <div class="training-help">Polygons only (YOLO-seg). Output is kept separate; originals stay untouched.</div>
                            <div class="training-actions">
                                <button type="button" id="segBuilderStartBtn" class="training-button">Start build</button>
                                <button type="button" id="segBuilderRefreshBtn" class="training-button training-button--ghost">Refresh datasets</button>
                            </div>
                            <div id="segBuilderMessage" class="training-message"></div>
                            <div id="segBuilderLog" class="training-log"></div>
                        </div>
                        <div class="training-card__footer">
                            <div class="training-subsection__title">Segmentation build jobs</div>
                            <div id="segBuilderJobs" class="training-history"></div>
                            <div class="training-actions">
                                <button type="button" id="segBuilderJobsRefresh" class="training-button training-button--ghost">Refresh jobs</button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="tab-panel" id="tabSam3PromptModels" data-tab-panel="sam3-prompt-models">
                <div class="training-wrapper single-wide">
                    <section class="training-form-section">
                        <h2>SAM Model Selection</h2>
                        <p class="training-help">Pick a locally trained SAM checkpoint to use for SAM3 prompting in the labeling page.</p>
                        <div class="training-field">
                            <label for="sam3PromptModelSelect">Available models</label>
                            <div class="sam3-dataset-row">
                                <select id="sam3PromptModelSelect"></select>
                                <button type="button" class="training-button" id="sam3PromptRefresh">Refresh</button>
                            </div>
                            <div class="training-help" id="sam3PromptModelSummary"></div>
                            <div class="training-help" id="sam3PromptActiveLabel"></div>
                        </div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="sam3PromptActivate">Activate model</button>
                        </div>
                        <div class="training-message" id="sam3PromptMessage" role="status"></div>
                    </section>
                </div>
            </div>
            <div class="tab-panel" id="tabActive" data-tab-panel="active">
                <div class="active-wrapper">
                    <section class="active-controls">
                        <h2>Switch Models</h2>
                        <div class="active-message" id="activeMessage" role="status"></div>
                        <p class="active-help">When you activate a trained classifier, the system loads the backbone saved in its metadata. Use these controls if you need to override the paths manually.</p>
                        <div class="active-field">
                            <label for="activeClassifierSelect">Saved classifiers</label>
                            <div class="active-picker">
                                <select id="activeClassifierSelect"></select>
                                <button type="button" class="training-button" id="activeClassifierRefresh">Refresh</button>
                            </div>
                            <div class="active-actions">
                                <button type="button" class="training-button secondary" id="activeClassifierUse">Use selected</button>
                                <button type="button" class="training-button secondary" id="activeClassifierDownload">Download zip</button>
                                <button type="button" class="training-button secondary" id="activeClassifierDelete">Delete</button>
                            </div>
                            <div class="active-help">Pick a classifier stored under <code>uploads/classifiers/</code>. Download zip includes the classifier, meta (encoder type/model), and labelmap (if found).</div>
                        </div>
                        <div class="active-field">
                            <label for="activeLabelmapSelect">Saved labelmaps</label>
                            <div class="active-picker">
                                <select id="activeLabelmapSelect"></select>
                                <button type="button" class="training-button" id="activeLabelmapRefresh">Refresh</button>
                            </div>
                            <div class="active-actions">
                                <button type="button" class="training-button secondary" id="activeLabelmapUse">Use selected</button>
                                <button type="button" class="training-button secondary" id="activeLabelmapDownload">Download</button>
                                <button type="button" class="training-button secondary" id="activeLabelmapDelete">Delete</button>
                            </div>
                            <div class="active-help">Labelmaps are copied into <code>uploads/labelmaps/</code> after training.</div>
                        </div>
                        <div class="active-field">
                            <label for="activeClipSelect">CLIP Backbone (CLIP heads only)</label>
                            <select id="activeClipSelect"></select>
                            <div class="active-help">Used only when the active head is CLIP-based. DINOv3 heads ignore this setting.</div>
                        </div>
                        <div class="active-field">
                            <label for="activeClassifierPath">Classifier Path</label>
                            <div class="active-picker">
                                <input type="text" id="activeClassifierPath" placeholder="./my_logreg_model.pkl" />
                                <button type="button" class="training-button" id="activeClassifierBrowse">Upload‚Ä¶</button>
                            </div>
                            <div class="active-help">Upload a `.pkl` classifier. It will be stored under <code>uploads/classifiers/</code> and appear in the saved list.</div>
                        </div>
                        <div class="active-field">
                            <label for="activeLabelmapPath">Labelmap Path</label>
                            <div class="active-picker">
                                <input type="text" id="activeLabelmapPath" placeholder="Optional" />
                                <button type="button" class="training-button" id="activeLabelmapBrowse">Upload‚Ä¶</button>
                            </div>
                            <div class="active-help">Optional `.pkl`/`.txt` mapping to keep YOLO order (saved under <code>uploads/labelmaps/</code>).</div>
                        </div>
                        <div class="active-field active-fallback">
                            <h3>Fallback: Dilate Crop on Low Confidence</h3>
                            <p class="active-help">When Auto Class is on, CLIP may occasionally return a low-confidence score for a rough bbox. Enable this fallback to automatically enlarge (dilate) the crop and retry the prediction before surfacing an error.</p>
                            <label class="active-inline">
                                <input type="checkbox" id="useFallbackDilate" name="useFallbackDilate" />
                                Turn on fallback dilation when CLIP probability is below the threshold.
                            </label>
                            <div class="fallback-options">
                                <label for="minProba">Min CLIP confidence (0‚Äì1)</label>
                                <input type="number" id="minProba" name="minProba" min="0" max="1" step="0.05" value="0.55" />
                                <label for="dilateRatio">Dilate ratio</label>
                                <input type="number" id="dilateRatio" name="dilateRatio" min="0" max="1" step="0.05" value="0.10" />
                            </div>
                            <p class="active-help">Example: a 0.55 threshold + 0.10 ratio expands each low-confidence bbox by 10% in every direction before rerunning CLIP. Leave disabled if you prefer to review low-confidence crops manually.</p>
                        </div>
                        <div class="active-buttons">
                            <button type="button" class="training-button" id="activateLatestModelBtn" disabled>Activate Latest Training Run</button>
                            <button type="button" class="training-button" id="applyActiveModelBtn">Apply Changes</button>
                            <button type="button" class="training-button" id="refreshActiveModelBtn">Refresh</button>
                        </div>
                        <input type="file" id="activeClassifierUpload" accept=".pkl" class="file-input-hidden" />
                        <input type="file" id="activeLabelmapUpload" accept=".pkl,.txt" class="file-input-hidden" />
                    </section>
                </div>
            </div>
            <div class="tab-panel" id="tabQwen" data-tab-panel="qwen">
                <div class="qwen-models-wrapper">
                    <section class="qwen-models-list">
                        <h2>Qwen Models</h2>
                        <p class="qwen-models-help">Pick which fine-tuned adapters should power the Assist panel. Each run stores the exact prompts, context, and class list it learned from.</p>
                        <div class="qwen-model-status" id="qwenModelStatus">Loading‚Ä¶</div>
                        <div class="qwen-model-toolbar">
                            <button type="button" class="training-button" id="qwenModelRefreshBtn">Refresh</button>
                        </div>
                        <div class="qwen-model-grid" id="qwenModelList"></div>
                    </section>
                    <section class="qwen-model-details">
                        <h2>Active Model Details</h2>
                        <div id="qwenModelDetails" class="qwen-model-details__body"></div>
                    </section>
                </div>
            </div>
            <div class="tab-panel" id="tabPredictors" data-tab-panel="predictors">
                <div class="predictor-panel">
                    <div class="predictor-card">
                        <h2>Predictor Budget</h2>
                        <p>Choose how many SAM predictors stay loaded so you can flip between images without delays.</p>
                        <div class="predictor-control">
                            <label for="predictorCount">Concurrent predictors</label>
                            <div class="predictor-control-inline">
                                <input type="number" id="predictorCount" min="1" max="3" value="3" />
                                <button type="button" class="training-button" id="predictorApply">Apply</button>
                            </div>
                        </div>
                        <div class="predictor-message" id="predictorMessage"></div>
                    </div>
                    <div class="predictor-grid">
                        <div class="predictor-stat">
                            <span>Active slots</span>
                            <strong id="predictorActiveCount">--</strong>
                        </div>
                        <div class="predictor-stat">
                            <span>Loaded predictors</span>
                            <strong id="predictorLoadedCount">--</strong>
                        </div>
                        <div class="predictor-stat">
                            <span>Predictor RAM</span>
                            <strong id="predictorImageRam">--</strong>
                        </div>
                        <div class="predictor-stat">
                            <span>Process RAM</span>
                            <strong id="predictorProcessRam">--</strong>
                        </div>
                        <div class="predictor-stat">
                            <span>GPU free / total (fallback: system)</span>
                            <strong id="predictorSystemFreeRam">--</strong>
                        </div>
                    </div>
                </div>
            </div>
            <div class="tab-panel" id="tabSettings" data-tab-panel="settings">
                <div class="settings-panel">
                    <h3>Backend Connection</h3>
                    <p>The UI talks to the FastAPI server via an HTTP base URL. Enter the URL (including port) below to point the interface at a local or remote backend.</p>
                    <label for="settingsApiRoot">API base URL</label>
                    <input type="text" id="settingsApiRoot" placeholder="http://localhost:8000" />
                    <div class="settings-actions">
                        <button type="button" id="settingsApply" class="training-button">Save</button>
                        <button type="button" id="settingsTest" class="training-button">Test Connection</button>
                    </div>
                    <div id="settingsStatus" class="settings-status" role="status" aria-live="polite"></div>
                    <p class="settings-help">Tip: when tunnelling to a remote GPU box (e.g. <code>ssh -L 8000:127.0.0.1:8000 user@gpu-host</code>), set the URL to <code>http://localhost:8000</code> so the browser routes through the tunnel.</p>
                    <div class="training-divider"></div>
                    <h3>Backend Fuzzer</h3>
                    <p class="settings-help">Runs a quick smoke test with random data against key API endpoints (SAM/Qwen). Useful to sanity-check that the backend is alive and models are loaded. Tests may fail if weights are missing.</p>
                    <div class="training-grid">
                            <label><input type="checkbox" id="fuzzerIncludeQwen" checked /> Include Qwen tests</label>
                            <label><input type="checkbox" id="fuzzerIncludeSam3" /> Include SAM3 tests</label>
                            <label><input type="checkbox" id="fuzzerIncludeClip" /> Include CLIP tests</label>
                            <label><input type="checkbox" id="fuzzerIncludeAgent" /> Include Agent Mining tests</label>
                    </div>
                    <div class="settings-actions">
                        <button type="button" id="runBackendFuzzer" class="training-button">Run fuzzer</button>
                    </div>
                    <div id="backendFuzzerStatus" class="settings-status" role="status" aria-live="polite"></div>
                    <pre id="backendFuzzerLog" class="training-log" style="max-height: 220px; overflow-y: auto;"></pre>
                    <div class="training-divider"></div>
                    <h3>Default upload and payload limits</h3>
                    <p class="settings-help">
                        Defaults are tuned for LAN use. You can override any of these on the backend by setting
                        environment variables (e.g. <code>CLIP_TRAIN_UPLOAD_QUOTA_BYTES</code>,
                        <code>DATASET_ZIP_MAX_BYTES</code>, <code>AGENT_RECIPE_MAX_BYTES</code>,
                        <code>AGENT_MINING_CACHE_MAX_BYTES</code>, <code>MAX_RESPONSE_DETECTIONS</code>,
                        <code>SAM_PRELOAD_MAX_BYTES</code>).
                        If the backend is configured with lower limits, uploads over those caps will be rejected.
                    </p>
                    <ul class="settings-help">
                        <li>Dataset zips (SAM3/Qwen): up to ~100GB zip, ~50GB per entry.</li>
                        <li>CLIP/Qwen chunked uploads: ~10GB per file, ~100GB per job quota.</li>
                        <li>Classifier/labelmap/general assets: ~10GB per file, ~100GB per uploads quota.</li>
                        <li>Agent recipe zips: ~2GB total; crops capped (~1000 pngs / ~512MB total).</li>
                        <li>Responses: max ~5000 detections / 2000 masks; SAM preload cache ~2GB.</li>
                        <li>Agent mining cache: ~80GB default; TTL off by default (set <code>AGENT_MINING_CACHE_TTL_HOURS</code> to enable purge).</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>

    <script>
        document.addEventListener("DOMContentLoaded", function() {
          if (typeof listenImageCrop === "function") {
            listenImageCrop();
          }
        });
        </script>
</body>
</html>
