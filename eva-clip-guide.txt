Using and Training EVA-CLIP-18B for Image Classification
Obtaining EVA-CLIP-18B Weights

The EVA-CLIP-18B model weights are publicly available. The easiest source is the official Hugging Face repository: BAAI/EVA-CLIP-18B
huggingface.co
. This repo provides a Transformers-compatible model (with integrated image and text encoders) and also hosts the original PyTorch checkpoint (approximately 36.7 GB in FP16)
huggingface.co
.

To download via Hugging Face, you can use transformers.AutoModel.from_pretrained("BAAI/EVA-CLIP-18B", ...) with trust_remote_code=True (since custom model code is used). Alternatively, the raw PyTorch checkpoint file (e.g., EVA_CLIP_18B_psz14_s6B.fp16.pt) can be obtained from the repository and loaded using the official codebase (see the BAAI EVA GitHub)
huggingface.co
.

Ensure you have sufficient disk space and a robust internet connection for the download. The model is 18 billion parameters (approx. 18.1B, with ~17.5B in the image tower alone)
huggingface.co
, so both storage and memory requirements are significant.

Loading the EVA-CLIP-18B Image Encoder in PyTorch

To use only the image tower of EVA-CLIP-18B (ignoring the text encoder), you can still load the entire CLIP model and then focus on the image encoding functionality. For example:

import torch
from transformers import AutoModel

model_name = "BAAI/EVA-CLIP-18B"
# Load the model in half-precision. Use trust_remote_code because the repository has custom model definitions.
model = AutoModel.from_pretrained(
    model_name, torch_dtype=torch.float16, trust_remote_code=True
).to("cuda")  # move to GPU if available
model.eval()  # set to evaluation mode (no dropout)


This will download and initialize the model. Internally, the AutoModel call instantiates EVA-CLIP's custom CLIP model class, which includes both the image and text encoders (with methods like model.encode_image and model.encode_text). Since we only need the image encoder:

We will use model.encode_image(...) for inference and feature extraction.

We can ignore model.encode_text and the text tokenizer. If memory is a concern, you could free some memory by moving the text encoder’s weights to CPU or deleting them after loading, but by default they will reside in GPU memory along with the vision encoder. (Note: EVA-CLIP-18B’s text encoder is relatively small – on the order of 0.6B params – compared to the 17.5B-param vision encoder
huggingface.co
.)

If using limited GPU memory, consider loading the model with model sharding or 8-bit quantization to reduce usage. For example, Hugging Face Accelerate allows splitting the model across multiple GPUs or CPU. The EVA-CLIP authors also suggest using DeepSpeed ZeRO-Inference to initialize the model under tight memory constraints, with a utility function to load the checkpoint in partitions. These techniques can help if you don’t have a single GPU with ~40GB of VRAM.

Note on GPU requirements: In FP16, the full EVA-CLIP-18B model occupies on the order of 35–37 GB of VRAM
huggingface.co
huggingface.co
. A single NVIDIA A100 40GB GPU can just about hold it for inference. If your GPU has less memory (e.g. 24GB), you will need to use multi-GPU or offload some weights to CPU. Using torch.cuda.amp.autocast() (as shown below) will keep operations in half precision which also helps conserve memory during inference.

Image Preprocessing Pipeline

Before passing images to EVA-CLIP-18B’s encoder, they must be preprocessed similarly to how CLIP models are typically preprocessed:

Resolution: EVA-CLIP models are generally trained on 224×224 resolution images (the EVA-CLIP-18B checkpoint is for 224px images)
huggingface.co
. (There is also a 448px variant for EVA-CLIP-8B, but for the 18B model, use 224 unless you specifically fine-tune at a higher resolution.)

Transforms: Apply the standard CLIP image transforms:

Resize the image, preserving aspect ratio, so that the shortest side is 224 pixels,

Center-crop to a 224×224 square,

Convert to RGB (if not already),

Convert to tensor and normalize color channels. Use the same normalization means and stds as CLIP: mean = (0.48145466, 0.4578275, 0.40821073), std = (0.26862954, 0.26130258, 0.27577711). These values come from the original CLIP preprocessing and are used for EVA-CLIP as well.

Using Transformers Processor: You can use Hugging Face’s CLIPImageProcessor to handle these steps. For example:

from transformers import CLIPImageProcessor
from PIL import Image
processor = CLIPImageProcessor.from_pretrained("openai/clip-vit-large-patch14")
image = Image.open("example.jpg")
inputs = processor(images=image, return_tensors="pt")
pixel_values = inputs["pixel_values"].to("cuda")  # shape: [1, 3, 224, 224]


The above uses the same preprocessing as OpenAI CLIP ViT-L/14 (which is compatible with EVA-CLIP’s needs). This processor under the hood will resize and crop the image to 224×224 and apply the CLIP normalization.

Alternative: If you prefer not to use CLIPImageProcessor, you can manually compose transforms with PIL and TorchVision. For example, in the EVA-CLIP model card, the authors provide a sample using torchvision.transforms to resize and normalize the image identically to CLIP’s defaults.

Make sure the resulting tensor is float and normalized before feeding it into model.encode_image. If using a batch of images, stack them into a tensor of shape [batch, 3, 224, 224] and ensure that device (CPU/GPU) matches the model.

Extracting Image Embeddings

Once the model and preprocessing are ready, you can extract embeddings for images using the EVA-CLIP image encoder:

# Assuming `model` (EVA-CLIP-18B) and `pixel_values` (preprocessed image batch) are ready
with torch.no_grad(), torch.cuda.amp.autocast():
    image_features = model.encode_image(pixel_values)  # forward pass through image tower
# image_features is a tensor of shape [batch_size, embedding_dim]
print(image_features.shape)


This returns the CLIP image embeddings (often called the image feature vectors). Typically, CLIP image features are L2-normalized before comparing with text features or using for similarity-based classification. However, if you plan to use these features as input to a classifier, you can use them either normalized or unnormalized (a linear classifier can learn an appropriate scale).

Key points when extracting embeddings:

Batch Processing: You can process a batch of images at once, as shown above. Be mindful of memory: with such a large model, even inference can be heavy. If you encounter memory issues, use smaller batches and accumulate results. You may also consider gradient checkpointing or splitting the model to CPU for part of the batch processing if absolutely necessary.

Embedding Dimensionality: The dimensionality of EVA-CLIP-18B’s image embeddings is determined by the model architecture. (OpenAI CLIP ViT-L/14, for example, yields a 768-dimensional vector; EVA-CLIP may have a larger dimension given its scale.) You can check this by printing the shape of image_features. For instance, if you get [N, 1024], then the image embedding dimension is 1024. This dimension will be the input size for any downstream classifier.

No Gradients: Note the use of torch.no_grad() in the extraction code – if you are just extracting features (and not fine-tuning the backbone), wrap the encoding in no-grad to save memory and computation. The model is so large that computing gradients for it (when not needed) would be wasteful.

Training a Linear Classification Head

Using EVA-CLIP-18B’s image embeddings, you can train a lightweight classifier (e.g., a linear layer or logistic regression) to perform supervised image classification. This is often referred to as a linear probe (freezing the backbone and training a linear head on top). The general procedure:

Feature Extraction: First, obtain embeddings for all images in your training set using the frozen EVA-CLIP image encoder. You can do this in advance (offline) and save them, or on the fly during training. For a large dataset, offline extraction is often more efficient:

Loop over your training images, apply the preprocessing, and use model.encode_image to get the feature vector for each image (do this with torch.no_grad() and in batches). Save these feature vectors along with their labels.

Do the same for your validation set (to use for model evaluation).

Prepare the Classifier: Define a linear layer that maps the image embedding to your target classes. For example, in PyTorch:

import torch.nn as nn
num_classes = 100  # change this to the number of classes in your dataset
embed_dim = image_features.shape[1]  # dimension of EVA-CLIP embeddings
linear_classifier = nn.Linear(embed_dim, num_classes).to("cuda")


This linear layer will be trained to predict the class from the EVA-CLIP features. You might initialize it with bias and weights zero or random (the default nn.Linear initialization is usually fine).

Freeze the Backbone: If you haven’t already, freeze the EVA-CLIP model’s parameters so they won’t update during classifier training:

for param in model.parameters():
    param.requires_grad = False
model.eval()  # ensure we're in eval mode (no dropout, etc.)


We only want to train the linear_classifier parameters for now.

Training Loop: Train the linear classifier on the extracted features. If you precomputed features, your training loop will load batches of saved features and feed them to the classifier. If you choose to do it on-the-fly, your loop will look like:

import torch.optim as optim
import torch.nn.functional as F

optimizer = optim.AdamW(linear_classifier.parameters(), lr=1e-3)  # choose LR, optimizer
for images, labels in train_loader:  # iterate over your dataset
    images = images.to("cuda")
    labels = labels.to("cuda")
    # Extract features with the frozen backbone
    with torch.no_grad():
        feats = model.encode_image(images)  # [batch, embed_dim]
    # Forward pass through the linear classifier
    logits = linear_classifier(feats)       # [batch, num_classes]
    loss = F.cross_entropy(logits, labels)  # compute classification loss
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()


In this training scheme, model.encode_image produces features but does not compute gradients (since we wrapped it in no_grad() and froze its params). Only the linear layer's weights get updated. Train for as many epochs as needed until the classifier converges (common practice is on the order of tens of epochs if training from scratch, but it may converge faster since features are high-quality).

Evaluation: After training, evaluate the classifier on your validation set. Compute the predicted labels (pred = logits.argmax(dim=1)) and compare to true labels to calculate accuracy. You should find that even though the EVA-CLIP model was not specifically trained on your classification task, its features are very discriminative.

This linear probe approach has been shown to yield very strong results. For instance, on ImageNet-1K, EVA-CLIP-18B’s frozen features combined with a linear classifier achieve about 88.9% top-1 accuracy
marktechpost.com
 – outperforming prior models like InternImage and OpenCLIP on the same task. This underscores the quality of the learned visual representations.

(Side note: If you have a smaller dataset, you might choose to fine-tune the linear layer with a higher learning rate and possibly use regularization like weight decay. If the dataset is large (e.g., ImageNet), standard training practices (lr schedulers, etc.) for linear classification apply. The EVA-CLIP paper’s linear probe used the ImageNet training set labels to achieve the 88.9% mentioned above.)

Fine-Tuning the Image Encoder (Full or Partial)

In some cases, you may want to fine-tune the EVA-CLIP-18B image encoder on a specific classification task (rather than keeping it completely frozen). Here are strategies and considerations for fine-tuning such a large model:

Partial Fine-Tuning (Layer-wise freezing): A practical approach is to unfreeze only the last few layers of the image encoder, or use a smaller subset of the model, while keeping the rest frozen. For example, you might allow gradient updates only in the last Transformer block(s) of the ViT and in the classification head. This way, you update, say, the top 10% of the model’s parameters (which can still be billions in a model this size!) and avoid touching earlier layers that capture more general features. This reduces the memory and compute burden and can mitigate overfitting. Many times, high-level features are task-specific, so fine-tuning just those can yield improvements.

Full Fine-Tuning: This means updating all 18B parameters on your dataset. This is computationally intensive and typically only feasible with a multi-GPU setup:

Hardware: You will likely need multiple high-memory GPUs. The authors originally trained EVA-CLIP-18B using 360 A100 GPUs (40GB each)
huggingface.co
 for the contrastive learning phase. Fine-tuning on a smaller classification dataset won’t require that scale, but expect to need at least several GPUs (or iterations with model sharding) to fit the model and its gradients. If you have access to an 80GB A100, it might be possible to do single-GPU fine-tuning at half precision with a smaller batch size, but this is at the edge of feasibility.

Memory Management: Use mixed precision (FP16 or bfloat16 training) to cut memory usage in half. Enable gradient checkpointing on the model’s layers to trade compute for memory (this can significantly reduce memory usage by not storing intermediate activations). You should also consider distributed training frameworks. For example, Fully Sharded Data Parallel (FSDP) or DeepSpeed can shard the model’s weights, gradients, and optimizer states across GPUs. The EVA-CLIP repository specifically notes that using deepspeed.zero.Init() (ZeRO Stage 3) is helpful for handling the model with limited memory, and provides a utility load_zero_partitions() for loading the checkpoint in this manner.

Optimizers and Hyperparameters: Use a smaller learning rate for the backbone when fine-tuning. A common strategy is a two-stage learning rate: e.g., LR = 1e-5 for the backbone and 1e-3 for the new classification head. This ensures the pre-trained features aren’t drastically changed at once. Also use weight decay (e.g., 0.1) and perhaps layer-wise learning rate decay (lower LR for lower layers, higher for later layers) as is often done with large ViT fine-tuning.

Training Schedule: Fine-tuning might only need a few epochs if the dataset is small, or up to, say, 10-20 epochs on something like ImageNet-1K, since the model starts with a very good initialization. Monitor validation accuracy – large models can overfit quickly if the dataset is not huge, so you might stop early if you see overfitting.

Augmentation: Use data augmentation (random crops, flips, etc.) as appropriate for your dataset to avoid overfitting during fine-tuning. Even though EVA-CLIP’s pre-training data had variety, your fine-tune should still include augmentations if the dataset is smaller (this is standard practice in ImageNet fine-tuning as well).

Parameter-Efficient Tuning: If full fine-tuning is too resource-heavy, consider techniques like LoRA (Low-Rank Adapters) or other adapter-based fine-tuning. These approaches insert small trainable modules (with far fewer parameters than the whole model) into the network and freeze the rest of the model. This way, you can still adapt the model to your task with a fraction of the compute and memory cost. For example, adding LoRA adapters to each attention module of the ViT and training those (with the rest of EVA-CLIP frozen) could be a viable strategy to get some performance gain without needing to update all 18B weights.

Remember that fine-tuning such a large model is an advanced undertaking. Always begin with a lower-risk approach (like linear classifier or partial fine-tuning) and only attempt full-model fine-tuning if absolutely necessary and you have the infrastructure to support it.

GPU Requirements and Best Practices

Hardware Considerations: EVA-CLIP-18B’s image encoder by itself is around 17.5 billion parameters (17.5B in the vision Transformer, plus the text model brings it to 18.1B)
huggingface.co
. In half precision, just loading the vision tower requires ~35 GB GPU memory
huggingface.co
. For inference or feature extraction, an NVIDIA A100 40GB (or greater) is recommended to comfortably load the model. If you only have smaller GPUs (e.g., 2 × 24GB), you can try sharding the model across them (using device_map in Hugging Face or manual placement of different layers on different GPUs). High-bandwidth interconnect (NVLink or NVSwitch) will help if splitting across devices.

For training or fine-tuning, multi-GPU setups are ideal:

If doing a linear probe (only training a small classifier), a single GPU is fine since the backbone is frozen and can even be run on CPU to get features if necessary (at a much slower speed).

If fine-tuning part or all of the model, you’ll benefit from multiple GPUs. Use Distributed Data Parallel (DDP) for data parallelism and/or model parallelism (with frameworks like Megatron-LM, DeepSpeed, or FSDP as mentioned) for splitting the model. Ensure your batch size per GPU isn’t too large to avoid out-of-memory errors. It may be useful to start with a very small batch (even 1 or 2 images per GPU) to confirm the model loads and then scale up gradually.

Batch Size and Gradient Accumulation: The original EVA-CLIP training was done with extremely large batches (e.g., global batch 108K for 18B model)
huggingface.co
, which is beneficial for contrastive learning. For supervised fine-tuning, you don’t need such extremes, but larger batch sizes can still help convergence up to a point. Use gradient accumulation if you need a larger effective batch size but are limited by GPU memory – e.g., accumulate gradients over 4 iterations of batch 32 to simulate batch 128. Just be mindful of adjusting the learning rate appropriately if you change batch size (linear scaling rule).

Learning Rate Tips: Large models can be sensitive to learning rate. It’s often effective to use a relatively low learning rate when fine-tuning (to avoid destabilizing the pre-trained weights), and possibly warm up the learning rate for the first few hundred steps. The EVA-CLIP paper doesn’t give fine-tuning LRs, but as a reference, ViT models fine-tuned on ImageNet often use LR on the order of 1e-5 to 5e-5 for the backbone (with warmup), and maybe 1e-3 for new layers.

Hyperparameters from Public Implementations: While specific fine-tuning hyperparameters for EVA-CLIP-18B may not be publicly detailed, we can glean some hints:

The authors report that EVA-CLIP-18B achieves 80.7% zero-shot accuracy on average across 27 benchmarks
huggingface.co
, and when fine-tuned (or linear-probed) on ImageNet-1K, about 88-89% top-1 accuracy
marktechpost.com
. Achieving these results likely required optimization steps similar to those used for large ViTs (e.g., AdamW optimizer, cosine learning rate decay, etc.).

If you use frameworks like FlagAI or the BAAI EVA code, check their configuration files or README for any mention of fine-tuning settings. Often, they might have example scripts for linear evaluation or fine-tuning smaller EVA models which can be extrapolated to the 18B case.

A safe approach is: optimizer = AdamW, loss = CrossEntropy for classification, epochs = 10-20 (for a large dataset), and evaluate on a validation set each epoch to monitor progress. If the validation accuracy starts dropping or diverging, reduce learning rate or stop early.

References and Resources:

Official Paper: “EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters” by Sun et al. (2024) – the paper introduces EVA-CLIP-18B and reports its performance in zero-shot, linear probe, and fine-tuning scenarios
huggingface.co
. It’s a good reference to understand the training setup and the capabilities of the model.

Hugging Face Model Card: The Hugging Face model card for EVA-CLIP-18B contains example code for usage. Notably, it shows how to load the model with AutoModel and how to perform a zero-shot classification by encoding an image and a few text prompts. This can serve as a starting point for writing your own inference or training code.

BAAI EVA GitHub: The code repository by BAAI (Beijing Academy of AI) for the EVA series includes scripts for EVA-CLIP. It provides functions like create_model_and_transforms to load the model and processing pipeline in PyTorch natively. The GitHub may also include configuration files or instructions for fine-tuning and evaluation.

Demo and Usage Examples: There is a Hugging Face Spaces demo comparing EVA-CLIP to OpenAI CLIP
huggingface.co
 (for the 8B model, but the usage is analogous). It demonstrates zero-shot classification usage. Additionally, community forums or blogs (e.g., the MarkTechPost article summarizing EVA-CLIP-18B
marktechpost.com
) can provide insight into how others are using the model and the kind of results to expect.

By following this guide, you should be able to load EVA-CLIP-18B’s image encoder, extract high-quality image features, and build classifiers on top of them. Despite the engineering challenges posed by the model’s scale, EVA-CLIP-18B offers unprecedented visual representation power in open-source form. Good luck with your image classification projects!
