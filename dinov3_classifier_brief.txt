DINOv3 Classifier Integration Brief (CLIP-Compatible, Frozen Encoder Only)

Goal
- Add DINOv3 as an optional frozen image encoder for classifier heads without replacing CLIP.
- Preserve full backward compatibility with existing CLIP heads, metadata, cache, and inference.
- Keep the current pipeline structure (train head -> save meta -> activate -> use for recipe mining + auto-class).

Current pipeline (baseline we must keep intact)
- Training entrypoint: /clip/train (localinferenceapi.py) -> tools/clip_training.py::train_clip_from_yolo.
- Embedding extraction: CLIP image encoder produces normalized features, cached under uploads/clip_embeddings.
- Classifier head: logistic regression trained on embeddings, saved as .pkl with sidecar .meta.pkl.
- Meta fields: clip_model, classes/labelmap, embedding_dim, proba_mode, background indices, etc.
- Runtime: active classifier is loaded in localinferenceapi.py, and auto-class uses _clip_head_predict_proba.
- Recipe mining: uses CLIP prompt prefilter (text encoder) and CLIP head gating for cleanliness.

Constraints / non-goals
- Do NOT replace CLIP. DINOv3 is a new encoder_type alongside clip.
- Frozen encoder only (no DINOv3 fine-tuning).
- No CLIP text encoder when DINOv3 head is active (prompt prefilter must be disabled).
- Maintain compatibility with existing heads and cached embeddings.

Design summary
- Treat encoder selection as metadata on the head: encoder_type + encoder_model + embedding_dim.
- CLIP remains the default when encoder_type is missing.
- DINOv3 uses transformers AutoImageProcessor + AutoModel to extract pooler_output (fallback CLS token).
- The classifier head remains the same logistic regression on top of embeddings.

Commit plan (small, safe steps)

Commit 1 - Meta compatibility scaffold (no behavior change)
- Update TrainingArtifacts in tools/clip_training.py to store:
  - encoder_type ("clip" or "dinov3")
  - encoder_model (string name)
  - embedding_dim
- Write these fields into .meta.pkl during training publish (localinferenceapi.py::_publish_clip_training_artifacts).
- Update meta readers to default encoder_type="clip" when missing:
  - localinferenceapi.py::_load_clip_head_from_classifier
  - localinferenceapi.py::_list_clip_classifiers
- Add a small helper to resolve encoder meta consistently (e.g., _resolve_clip_head_encoder_meta).

Verification
- Load an existing CLIP head; confirm encoder_type defaults to "clip" and behavior is unchanged.

Commit 2 - Training API: encoder_type + model selection
- Extend /clip/train payload (pydantic model) to accept encoder_type + encoder_model.
  - Default encoder_type="clip"; encoder_model uses existing clip_model field.
- Pass encoder_type/model into train_clip_from_yolo.
- Ensure training logs include encoder_type/model.

Verification
- Start a CLIP training job with default fields; behavior unchanged.

Commit 3 - DINOv3 frozen encoder in tools/clip_training.py
- Add a DINOv3 encoder loader:
  - transformers.AutoImageProcessor + AutoModel
  - Use outputs.pooler_output; fallback to outputs.last_hidden_state[:,0,:]
  - L2-normalize embeddings to match CLIP-style normalization
- Add a new embedding backend in train_clip_from_yolo:
  - switch on encoder_type
  - same batching, caching, and background-class handling
- Update embedding cache signature to include encoder_type + encoder_model.
- Persist embedding_dim in TrainingArtifacts based on encoder output.

Verification
- Train a tiny DINOv3 head; meta shows encoder_type=dinov3 and correct embedding_dim.

Commit 4 - Inference: generic encoder loader + dispatch
- Add DINOv3 encoder state (model + processor + locks) alongside CLIP state in localinferenceapi.py.
- Implement a unified image-encode function, e.g. _encode_pil_batch(encoder_type, ...), that:
  - uses CLIP path for encoder_type="clip"
  - uses DINOv3 path for encoder_type="dinov3"
- Update auto-class and recipe gating to use encoder_type from head meta.
- Validate classifier coef dimension against encoder embedding_dim.

Verification
- Activate CLIP head and predict; no changes.
- Activate DINOv3 head and predict; embeddings produce valid logits and class output.

Commit 5 - Recipe mining: prompt prefilter gating + compute estimate
- Skip CLIP prompt prefilter if encoder_type != "clip":
  - Backend: log "prompt prefilter disabled for DINOv3" and bypass text embeddings.
  - Compute estimate: remove prefilter speed factor when disabled.
- UI: disable the prompt prefilter toggle (or show read-only OFF) when DINOv3 head is selected.

Verification
- Run recipe mining with DINOv3 head; no text-precompute errors, logs show prefilter off.

Commit 6 - UI: training + model management surfacing
- CLIP training panel:
  - Add encoder type select (CLIP, DINOv3).
  - When CLIP is selected, show CLIP backbone dropdown (current behavior).
  - When DINOv3 is selected, show DINOv3 backbone dropdown.
- CLIP Class Predictor Settings:
  - Display encoder_type + encoder_model for each saved head.
  - Include encoder info in the download ZIP metadata note.
- Recipe mining panel:
  - Show encoder_type/model for selected head.
  - If DINOv3, note that prompt prefilter is unavailable.

Verification
- UI payloads include encoder_type/model and backend accepts them.

Commit 7 - Docs + error handling
- Add concise docs to readme.md:
  - DINOv3 as optional frozen encoder for CLIP heads.
  - CLIP remains default and fully supported.
  - Prompt prefilter is CLIP-only.
- If HuggingFace model is gated/missing, surface a clean error with action.

Verification
- Attempt to load gated DINOv3 without access -> clear error in logs/UI.

Commit 8 - Smoke tests / regression checklist
- CLIP path: train -> activate -> auto-class -> recipe mining (unchanged).
- DINOv3 path: train -> activate -> auto-class -> recipe mining (prefilter off).
- Cache: confirm DINOv3 embeddings are separate from CLIP embeddings.

Notes on compatibility
- Existing heads without encoder_type are treated as CLIP heads.
- DINOv3 heads never call CLIP text encoder.
- Do not remove or rename existing CLIP fields (clip_model, proba_mode, etc.).

Candidate DINOv3 backbones (initial list)
- facebook/dinov3-vits16-pretrain-lvd1689m
- facebook/dinov3-vitb16-pretrain-lvd1689m
- facebook/dinov3-vitl16-pretrain-lvd1689m

Potential follow-up (not in scope now)
- Two-encoder mode: use CLIP text prefilter while DINOv3 is the head encoder.
