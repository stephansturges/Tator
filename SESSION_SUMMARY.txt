Branch: qwen-training (local).

Future objective
- Create a self-contained “SAM3-lite” training module in-repo (no external sam3 clone): keep only box-only essentials (model_builder, tokenizer, train loop, transforms, losses, eval postprocessors, BPE asset), drop unused cluster/submitit/Triton bits, simplify configs/entrypoint, slim requirements, add a tiny smoke test, and point the backend/UI to the new path.

SAM3 box-only fine-tune + activation (pending)
- Data pipeline: reuse staged Qwen datasets; convert YOLO images/labels/labelmap to COCO train/val on the server; cache by dataset signature.
- Upload handling: reuse Qwen chunked upload flow; short-circuit reupload if signature matches; expose convert/list endpoints.
- Training config/launch: generated YAML under uploads; enable_segmentation=False; BPE at `sam3/assets/bpe_simple_vocab_16e6.txt.gz`; logs/checkpoints under `uploads/sam3_runs/<job>`; overridables (batch sizes, workers, epochs, resolution, lr scale, grad_accum, val_freq, target_epoch_size, inst interactivity).
- Training job API/UI: POST `/sam3/train/jobs`, stream logs, show loss chart, list/status/cancel, surface latest checkpoint; UI “Activate checkpoint” button.
- Activation: load custom checkpoint (box-only) and switch active SAM3 model; clear caches; status should expose active model info.
- Inference: SAM endpoints must honor active checkpoint and work box-only (no masks).

Other follow-ups
- SAM3 text prompt batching support.
- Smoke test/CLI to load SAM3, run point+bbox prompt, log VRAM/mask output.
- Update docs (README/AGENTS) with dataset reuse/no-local-path constraint, training tunables, activation flow, GPU expectations.
- Future enhancement: optional SAM3 hard-example mining toggle (e.g., replay top-loss batches after a burn-in epoch), off by default to keep the lite trainer simple.
