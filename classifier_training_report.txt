Classifier Training Report
===========================

Scope
-----
This report summarizes the latest classifier sweep run on qwen_dataset using cached embeddings,
a fixed group split (seed 42, 20% test), background class augmentation (bg=5), and the MLP
training pipeline with the new knobs (embedding center/standardize, effective-number weights,
hard-mining epochs, GELU+LayerNorm, temperature calibration, and full combinations).
The sweep covers 406 labelled runs (profiles x label-smoothing x encoder families x MLP sizes).

Key Findings (Macro F1, foreground-only)
---------------------------------------
- DINOv3 (LVD) dominates CLIP: avg macro_f1_fg=0.745 vs CLIP=0.579.
- DINOv3 SAT models underperform LVD: avg macro_f1_fg=0.662 vs 0.762 (LVD).
- Label smoothing 0.1 consistently helps: avg macro_f1_fg ls0=0.689, ls0p1=0.732.

Best single models (macro_f1_fg)
--------------------------------
- Best overall: qwen_dataset_dinov3_vitl16_mlp1024_mix0p1_balnorm_ls0p1_full_bg5 → 0.845
- Best DINOv3 SAT: qwen_dataset_dinov3_vit7b16_sat493m_mlp1536_768_mix0p1_balnorm_ls0p1_effective_bg5 → 0.767
- Best CLIP: qwen_dataset_clip_vitl14_mlp768_384_mix0p1_balnorm_ls0_effective_bg5 → 0.712

Profile Averages (foreground macro metrics)
------------------------------------------
profile       count   macro_f1_fg  weighted_f1_fg  macro_recall_fg  macro_precision_fg
base            58   0.683       0.778          0.800            0.646
center_std      58   0.680       0.775          0.799            0.642
effective       58   0.784       0.863          0.808            0.769
hardmine        58   0.681       0.777          0.800            0.644
gelu_ln         58   0.683       0.776          0.800            0.647
calib           58   0.681       0.776          0.799            0.645
full            58   0.781       0.862          0.807            0.765

Interpretation of knobs
-----------------------
- effective-number weights: the largest boost; macro_f1_fg jumps ~+0.10 vs base while also
  lifting precision substantially. This suggests rare-class emphasis was the biggest leverage point.
- full profile (effective + center/std + hardmine + gelu_ln + calibration): matches or slightly
  trails effective-only on average, but delivers the strongest top-end scores and best overall model.
- center_std: small but consistent improvement over base; most visible on larger DINOv3 backbones.
- hardmine: modest gains, largely recall-heavy; helps mid-sized DINOv3 heads but not enough alone.
- gelu_ln: slight improvement vs base; more stable on deep MLPs (768+ hidden sizes).
- calibration: macro_f1 barely moves (expected, argmax unchanged) but probabilities are better
  behaved downstream; keep enabled when you rely on thresholds or margins.

Top-10 Overall (macro_f1_fg)
----------------------------
- qwen_dataset_dinov3_vitl16_mlp1024_mix0p1_balnorm_ls0p1_full_bg5 → 0.845 (weighted_f1_fg=0.903)
- qwen_dataset_dinov3_vitl16_mlp768_384_mix0p1_balnorm_ls0p1_full_bg5 → 0.843 (weighted_f1_fg=0.897)
- qwen_dataset_dinov3_vitl16_mlp1024_512_mix0p1_balnorm_ls0p1_full_bg5 → 0.842 (weighted_f1_fg=0.900)
- qwen_dataset_dinov3_vitl16_mlp768_384_mix0p1_balnorm_ls0p1_effective_bg5 → 0.842 (weighted_f1_fg=0.901)
- qwen_dataset_dinov3_vits16_mlp384_192_mix0p1_balnorm_ls0p1_full_bg5 → 0.841 (weighted_f1_fg=0.901)
- qwen_dataset_dinov3_vitl16_mlp768_mix0p1_balnorm_ls0p1_full_bg5 → 0.840 (weighted_f1_fg=0.899)
- qwen_dataset_dinov3_vitl16_mlp512_mix0p1_balnorm_ls0p1_effective_bg5 → 0.840 (weighted_f1_fg=0.900)
- qwen_dataset_dinov3_vitl16_mlp1024_mix0p1_balnorm_ls0p1_effective_bg5 → 0.839 (weighted_f1_fg=0.899)
- qwen_dataset_dinov3_vits16_mlp384_mix0p1_balnorm_ls0p1_full_bg5 → 0.838 (weighted_f1_fg=0.899)
- qwen_dataset_dinov3_vitl16_mlp768_mix0p1_balnorm_ls0p1_effective_bg5 → 0.837 (weighted_f1_fg=0.899)

Recommended defaults based on this sweep
----------------------------------------
- Encoder: DINOv3 ViT-L/16 (LVD) preferred for highest accuracy, ViT-B/16 for faster runs.
- Label smoothing: 0.1 (ls0p1) by default.
- Class weights: effective-number (beta=0.9999).
- MLP: 2-layer (768,384) or (1024,512) for ViT-L/16; GELU + LayerNorm on.
- Calibration: temperature scaling on for cleaner probability thresholds.

Implementation Brief: Practical DINOv3 Improvements
--------------------------------------------------
Goal: extend the current frozen-backbone classifier without replacing it,
and benchmark gains against the baseline reported above (this file).

Commit plan (incremental, safe to roll back):
1) Balanced Softmax / Logit Adjustment
   - Add optional logit adjustment (add -log priors) to both logreg and MLP training.
   - Store adjustment values in meta and allow inference-time toggles for calibration tests.
   - UI: “Logit adjustment (long-tail)” with a tooltip and default OFF.

2) ArcFace-style Margin Head
   - Implement new classifier_type="arcface" for MLP heads (normalized features + margin).
   - Parameters: margin m, scale s. Keep standard CE for comparison.
   - UI: compact ArcFace panel (m, s) with preset defaults.

3) Supervised Contrastive Auxiliary Loss
   - Add projection head (small MLP) + SupCon loss in parallel with CE loss.
   - Support class-balanced sampling and optional class-weighted contrastive term.
   - UI: toggle + temperature + loss weight.

4) Feature-space Augmentations + Manifold Mixup
   - Add Gaussian noise and channel-wise scaling on embeddings (probabilistic).
   - Implement feature-space mixup (manifold mixup) with soft labels.
   - UI: add “feature aug” and “mixup in embedding space” controls with safe defaults.

5) Prototype / Center Loss (optional)
   - Maintain trainable class centers; add center loss weight.
   - Useful for tighter clustering on rare classes.

6) Two-stage re-training (cRT)
   - After initial training, freeze head and retrain classifier weights with a balanced sampler for N epochs.
   - UI: “Rebalance phase” epochs + sampler choice.

Benchmark plan (to compare against this report)
------------------------------------------------
A. Baseline: run the current “full” profile sweep (as in this report).
B. For each new feature, run A/B pairs on the top 3 backbones:
   - DINOv3 ViT-L/16 (LVD), DINOv3 ViT-B/16 (LVD), CLIP ViT-L/14.
   - MLP sizes: [768,384] and [1024,512].
   - Label smoothing fixed at 0.1; class weights = effective-number.
C. Metrics: macro_f1_fg, weighted_f1_fg, macro precision/recall, plus probability calibration (ECE).
D. Record phase timings to compare compute overhead.
E. Append results to clip_dinov3_metrics_20241224.* and a new diff summary table.

End of report.

DINOv3 Improvements Sweep (2025-01-05)

Total runs: 36
Best overall (macro_f1_fg): qwen_dataset_dinov3_vitl16_mlp1024_512_mix0p1_balnorm_ls0p1_full_bg5 -> 0.8508

Profile averages (macro_f1_fg / weighted_f1_fg) and best per profile:
- arcface: avg_macro=0.8292, avg_weighted=0.8960, best_macro=0.8371 (qwen_dataset_dinov3_vitb16_mlp768_mix0p1_balnorm_ls0p1_arcface_bg5)
- full: avg_macro=0.8364, avg_weighted=0.8976, best_macro=0.8508 (qwen_dataset_dinov3_vitl16_mlp1024_512_mix0p1_balnorm_ls0p1_full_bg5)
- logit_both: avg_macro=0.8344, avg_weighted=0.8973, best_macro=0.8507 (qwen_dataset_dinov3_vitl16_mlp1024_512_mix0p1_balnorm_ls0p1_logit_both_bg5)
- logit_infer: avg_macro=0.7635, avg_weighted=0.8708, best_macro=0.8087 (qwen_dataset_dinov3_vitl16_mlp1024_512_mix0p1_balnorm_ls0p1_logit_infer_bg5)
- logit_train: avg_macro=0.7673, avg_weighted=0.8676, best_macro=0.8102 (qwen_dataset_dinov3_vitl16_mlp768_384_mix0p1_balnorm_ls0p1_logit_train_bg5)
- supcon: avg_macro=0.8298, avg_weighted=0.8955, best_macro=0.8453 (qwen_dataset_dinov3_vitl16_mlp1024_512_mix0p1_balnorm_ls0p1_supcon_bg5)

Delta vs full average (macro_f1_fg):
- arcface: -0.0072 vs full avg
- logit_both: -0.0019 vs full avg
- logit_infer: -0.0729 vs full avg
- logit_train: -0.0690 vs full avg
- supcon: -0.0065 vs full avg

Best per backbone/profile (macro_f1_fg):
- vitb16 / arcface: 0.8371 (qwen_dataset_dinov3_vitb16_mlp768_mix0p1_balnorm_ls0p1_arcface_bg5)
- vitb16 / full: 0.8360 (qwen_dataset_dinov3_vitb16_mlp768_mix0p1_balnorm_ls0p1_full_bg5)
- vitb16 / logit_both: 0.8359 (qwen_dataset_dinov3_vitb16_mlp768_mix0p1_balnorm_ls0p1_logit_both_bg5)
- vitb16 / logit_infer: 0.7611 (qwen_dataset_dinov3_vitb16_mlp512_mix0p1_balnorm_ls0p1_logit_infer_bg5)
- vitb16 / logit_train: 0.7548 (qwen_dataset_dinov3_vitb16_mlp768_mix0p1_balnorm_ls0p1_logit_train_bg5)
- vitb16 / supcon: 0.8348 (qwen_dataset_dinov3_vitb16_mlp768_mix0p1_balnorm_ls0p1_supcon_bg5)
- vitl16 / arcface: 0.8344 (qwen_dataset_dinov3_vitl16_mlp768_384_mix0p1_balnorm_ls0p1_arcface_bg5)
- vitl16 / full: 0.8508 (qwen_dataset_dinov3_vitl16_mlp1024_512_mix0p1_balnorm_ls0p1_full_bg5)
- vitl16 / logit_both: 0.8507 (qwen_dataset_dinov3_vitl16_mlp1024_512_mix0p1_balnorm_ls0p1_logit_both_bg5)
- vitl16 / logit_infer: 0.8087 (qwen_dataset_dinov3_vitl16_mlp1024_512_mix0p1_balnorm_ls0p1_logit_infer_bg5)
- vitl16 / logit_train: 0.8102 (qwen_dataset_dinov3_vitl16_mlp768_384_mix0p1_balnorm_ls0p1_logit_train_bg5)
- vitl16 / supcon: 0.8453 (qwen_dataset_dinov3_vitl16_mlp1024_512_mix0p1_balnorm_ls0p1_supcon_bg5)
- vits16 / arcface: 0.8233 (qwen_dataset_dinov3_vits16_mlp384_mix0p1_balnorm_ls0p1_arcface_bg5)
- vits16 / full: 0.8250 (qwen_dataset_dinov3_vits16_mlp256_mix0p1_balnorm_ls0p1_full_bg5)
- vits16 / logit_both: 0.8271 (qwen_dataset_dinov3_vits16_mlp256_mix0p1_balnorm_ls0p1_logit_both_bg5)
- vits16 / logit_infer: 0.7310 (qwen_dataset_dinov3_vits16_mlp384_mix0p1_balnorm_ls0p1_logit_infer_bg5)
- vits16 / logit_train: 0.7503 (qwen_dataset_dinov3_vits16_mlp256_mix0p1_balnorm_ls0p1_logit_train_bg5)
- vits16 / supcon: 0.8170 (qwen_dataset_dinov3_vits16_mlp384_mix0p1_balnorm_ls0p1_supcon_bg5)

DINOv3 Minimal Baseline Sweep (2025-01-05, GPU merged)
------------------------------------------------------
This sweep disables the “full” stack (effective weights, standardize, hard mining,
calibration, GELU+LN) to isolate second‑gen knobs. It shows which new features help
when the baseline is intentionally stripped down.

Total runs: 36
Best overall (macro_f1_fg): qwen_dataset_dinov3_vitl16_mlp768_384_mix0p1_balnorm_ls0p1_logit_train_bg5 -> 0.8428

Profile averages (macro_f1_fg / weighted_f1_fg) and best per profile:
- baseline_minimal: avg_macro=0.7578, avg_weighted=0.8440, best_macro=0.7933 (qwen_dataset_dinov3_vitb16_mlp768_mix0p1_balnorm_ls0p1_baseline_minimal_bg5)
- logit_train: avg_macro=0.8160, avg_weighted=0.8836, best_macro=0.8428 (qwen_dataset_dinov3_vitl16_mlp768_384_mix0p1_balnorm_ls0p1_logit_train_bg5)
- logit_both: avg_macro=0.7586, avg_weighted=0.8436, best_macro=0.8148 (qwen_dataset_dinov3_vitl16_mlp1024_512_mix0p1_balnorm_ls0p1_logit_both_bg5)
- logit_infer: avg_macro=0.5984, avg_weighted=0.6984, best_macro=0.6995 (qwen_dataset_dinov3_vitl16_mlp768_384_mix0p1_balnorm_ls0p1_logit_infer_bg5)
- arcface: avg_macro=0.7391, avg_weighted=0.8230, best_macro=0.7956 (qwen_dataset_dinov3_vitl16_mlp768_384_mix0p1_balnorm_ls0p1_arcface_bg5)
- supcon: avg_macro=0.7681, avg_weighted=0.8492, best_macro=0.8142 (qwen_dataset_dinov3_vitl16_mlp768_384_mix0p1_balnorm_ls0p1_supcon_bg5)

Key takeaways:
- Logit adjustment in training is the only second‑gen knob that materially lifts
  performance when the first‑gen stack is removed (+0.058 macro_f1_fg vs baseline_minimal).
- Logit adjustment at inference alone is harmful in this setup.
- ArcFace and SupCon offer small, inconsistent gains in the stripped‑down setting.

Default classifier recipe (recommended)
---------------------------------------
Based on the full sweep, the highest‑performing and most stable setup remains the
“full” profile with DINOv3 ViT‑L/16 (LVD) + MLP(1024,512), label smoothing 0.1,
effective weights, embedding standardize, hard‑example mining, GELU+LayerNorm, and
temperature calibration. This still beats the best minimal configuration.

Second‑gen knobs are best offered as optional toggles:
- logit_adjustment_mode=both is effectively tied with full baseline on ViT‑L/16,
  so it can be a “long‑tail mode” switch when desired.
- ArcFace and SupCon should remain advanced options, not defaults.

Default recipe confirmation run
-------------------------------
Configuration tested (single run, reuse embeddings):
- Encoder: DINOv3 ViT‑L/16 (LVD)
- MLP: [1024, 512], label smoothing 0.1, mixup 0.1
- First‑gen stack: effective weights, embedding standardize, hard mining,
  GELU+LayerNorm, temperature calibration
- Second‑gen: logit_adjustment_mode=both

Result (fg‑only):
- macro_f1_fg=0.8399, weighted_f1_fg=0.9031, macro_precision_fg=0.8538, macro_recall_fg=0.8286

Notes:
- This single run landed slightly below the best sweep score (0.8508),
  suggesting variance across splits or hyper‑stability differences.
- We should keep the “full” baseline as the default and offer logit_both
  as an advanced toggle rather than enabling it by default.
