# Core runtime (pinned with broad bounds)
fastapi>=0.110,<1.0
uvicorn[standard]>=0.24,<1.0
numpy>=1.24,<3.0
pillow>=10.0,<11.0
albumentations>=1.4,<2.0
joblib>=1.3,<2.0
scikit-learn>=1.3,<2.0
scipy>=1.10,<2.0  # for reorder_labelmap.py (Hungarian assignment)
pydantic>=1.10,<2.0
python-multipart>=0.0.6,<1.0
psutil>=5.9,<6.0
ultralytics>=8.2,<9.0

# Torch (select the build matching your CUDA/CPU environment)
torch
torchvision

# CLIP and helpers
ftfy
regex
tqdm
git+https://github.com/openai/CLIP.git

# Segment Anything (SAM)
git+https://github.com/facebookresearch/segment-anything.git

# Segment Anything 3 (SAM3) â€” optional (see sam3integration.txt for usage)
# git+https://github.com/facebookresearch/sam3.git

# SAM3 training dependencies
hydra-core>=1.3,<1.4
submitit>=1.5,<2.0
fvcore>=0.1.5,<0.1.6
iopath>=0.1.10,<0.1.11
tensorboard>=2.14,<3.0

# Qwen 2.5 visual-language (optional but enabled by default)
transformers>=4.56,<5.0
accelerate>=0.30,<1.0
qwen-vl-utils>=0.0.8
safetensors>=0.4,<0.5
sentencepiece>=0.1.99,<0.2
peft>=0.11,<1.0
bitsandbytes>=0.43,<1.0
lightning>=2.4,<3.0
nltk>=3.8,<4.0
decord>=0.6,<1.0

# Optional extras for advanced training
# timm                # for DINOv2 encoders
# opencv-python       # common image ops (not strictly required)
# matplotlib          # plotting in notebooks/scripts

# Observability (optional)
prometheus-client>=0.16,<1.0
