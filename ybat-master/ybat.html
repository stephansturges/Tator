<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>ü•î Tator Annotation Tool</title>
    <link href="ybat.css?v=20251220" rel="stylesheet">

    <!-- 1) Keep scripts in an order that ensures ybat.js is available  
         before we call listenImageCrop().
    -->
    <script src="canvas.min.js?v=20251220"></script>
    <script src="jszip.min.js?v=20251220"></script>
    <script src="filesaver.min.js?v=20251220"></script>
    <script src="ybat.js?v=20251220"></script>
</head>
<body>
    <div id="ingestProgress" class="ingest-progress" aria-live="polite" aria-atomic="true">
        <div id="ingestProgressLabel" class="ingest-progress__label">Loading‚Ä¶</div>
        <div id="ingestProgressDetail" class="ingest-progress__detail">0/0</div>
    </div>
    <div id="backgroundLoadModal" class="modal" aria-hidden="true">
        <div class="modal__backdrop" data-modal-dismiss="background"></div>
        <div class="modal__dialog" role="dialog" aria-modal="true" aria-labelledby="backgroundLoadTitle">
            <div class="modal__title" id="backgroundLoadTitle">Still loading‚Ä¶</div>
            <p class="modal__body" id="backgroundLoadMessage">Images and annotations are still being prepared in the background.</p>
            <div class="modal__actions">
                <button type="button" class="modal__btn modal__btn--primary" id="backgroundLoadDismiss">OK</button>
            </div>
        </div>
    </div>
    <div id="batchTweakModal" class="modal" aria-hidden="true">
        <div class="modal__backdrop" data-modal-dismiss="batchTweak"></div>
        <div class="modal__dialog" role="dialog" aria-modal="true" aria-labelledby="batchTweakTitle">
            <div class="modal__title" id="batchTweakTitle">Apply SAM tweak?</div>
            <p class="modal__body" id="batchTweakMessage">
                Apply SAM bbox tweak to all boxes in <span id="batchTweakClass">this class</span>?
            </p>
            <div class="modal__actions">
                <button type="button" class="modal__btn modal__btn--primary" id="batchTweakConfirm">Yes</button>
                <button type="button" class="modal__btn" id="batchTweakCancel">No</button>
            </div>
        </div>
	    </div>
	    <div id="trainingPackagingModal" class="modal" aria-hidden="true">
	        <div class="modal__backdrop" data-modal-dismiss="trainingPackaging"></div>
	        <div class="modal__dialog" role="dialog" aria-modal="true" aria-labelledby="trainingPackagingTitle">
            <div class="modal__title" id="trainingPackagingTitle">Packaging dataset‚Ä¶</div>
            <p class="training-packaging__stats" id="trainingPackagingStats">Estimating selection‚Ä¶</p>
            <div class="training-packaging-progress">
                <div class="training-packaging-progress__fill" id="trainingPackagingProgressFill"></div>
            </div>
            <p class="training-packaging__progress" id="trainingPackagingProgressText">Preparing‚Ä¶</p>
            <p class="training-packaging__eta" id="trainingPackagingEta">Estimating upload time‚Ä¶</p>
            <p class="training-packaging__elapsed" id="trainingPackagingElapsed">Elapsed: 0s</p>
            <p class="training-packaging__hint" id="trainingPackagingHint">Keep this tab open while we stage files and upload them to the server. Larger datasets can take a few minutes.</p>
            <div class="modal__actions">
                <button type="button" class="modal__btn" id="trainingPackagingDismiss">Hide</button>
            </div>
	        </div>
	    </div>
	    <div id="taskQueue" class="task-queue" aria-live="polite" aria-atomic="true"></div>
	    <div id="classScrollIndicator" class="class-scroll-indicator" aria-live="polite" aria-atomic="true"></div>
	    <div class="tab-shell">
	        <div class="tab-bar" role="tablist">
	            <button class="tab-button active" data-tab="labeling" id="tabLabelingButton" type="button">Label Images</button>
            <button class="tab-button" data-tab="training" id="tabTrainingButton" type="button">Train CLIP</button>
            <button class="tab-button" data-tab="qwen-train" id="tabQwenTrainButton" type="button">Train Qwen</button>
            <button class="tab-button" data-tab="sam3-train" id="tabSam3TrainButton" type="button">Train SAM3</button>
            <button class="tab-button" data-tab="agent-mining" id="tabAgentMiningButton" type="button">SAM3 Recipe Mining</button>
            <button class="tab-button" data-tab="prompt-helper" id="tabPromptHelperButton" type="button">SAM3 Vocabulary Explorer</button>
            <button class="tab-button" data-tab="datasets" id="tabDatasetsButton" type="button">Dataset Management</button>
            <button class="tab-button" data-tab="sam3-prompt-models" id="tabSam3PromptModelsButton" type="button">SAM3 Prompt Model Selection</button>
            <button class="tab-button" data-tab="active" id="tabActiveButton" type="button">CLIP Class Predictor Settings</button>
            <button class="tab-button" data-tab="qwen" id="tabQwenButton" type="button">Qwen Models</button>
            <button class="tab-button" data-tab="predictors" id="tabPredictorsButton" type="button">SAM Predictors</button>
            <button class="tab-button" data-tab="settings" id="tabSettingsButton" type="button">Backend Config</button>
        </div>
        <div class="tab-panels">
            <div class="tab-panel active" id="tabLabeling" data-tab-panel="labeling">
                <div class="container" id="container">
                    <div class="left">
                        <form action="">
                            <label for="images">Images:</label>
                            <input type="file" id="images" name="images[]" accept="image/jpeg, image/png" multiple class="file-input-hidden" />
                            <label for="images" id="imagesSelect" class="training-button" role="button" tabindex="0">Choose Images‚Ä¶</label>
                            <button type="button" name="cropImages" id="cropImages" class="training-button">Crop &amp; Save</button>
                            <br />
                            <label for="imageSearch">Search:</label>
                            <input type="text" id="imageSearch" name="imageSearch" />
                            <label for="imageList"></label>
                            <select name="imageList" id="imageList" size="10" multiple></select>
                            <div id="imageInformation"></div>
                            <div id="datasetTypeBadge" class="training-help"></div>
                            <div class="training-actions">
                                <button type="button" id="polygonDrawToggle" class="training-button secondary" aria-pressed="true">
                                    Polygon draw: On (P)
                                </button>
                            </div>
                            <div class="training-help">Seg mode only: P toggles polygon drawing. When off, clicks select/move polygons without adding new ones.</div>
                            <label for="classes">Classes:</label>
                            <input type="file" id="classes" name="classes" accept="text/plain" class="file-input-hidden" />
                            <label for="classes" id="classesSelect" class="training-button" role="button" tabindex="0">Load Classes‚Ä¶</label>
                            <label for="classList"></label>
                            <select name="classList" id="classList" size="10" multiple></select>
                            <div id="bboxInformation"></div>
                            <hr />
                            <div class="training-field training-field--inline">
                                <label for="autoMode">Auto Class</label>
                                <input type="checkbox" id="autoMode" name="autoMode" />
                            </div>
                            <div class="training-field training-field--inline">
                                <label for="samMode">SAM Mode</label>
                                <input type="checkbox" id="samMode" name="samMode" />
                            </div>
                            <div class="training-field">
                                <label for="samVariant">SAM Model</label>
                                <select id="samVariant" name="samVariant">
                                    <option value="sam1" selected>SAM 1</option>
                                    <option value="sam3">SAM 3</option>
                                </select>
                            </div>
                            <div class="training-field training-field--inline">
                                <label for="samPreload">Preload SAM</label>
                                <input type="checkbox" id="samPreload" name="samPreload" />
                            </div>
                            <div class="training-field training-field--inline">
                                <label for="pointMode">Point Mode</label>
                                <input type="checkbox" id="pointMode" name="pointMode" />
                            </div>
                            <div class="training-field training-field--inline">
                                <label for="multiPointMode">Multi-Point Mode</label>
                                <input type="checkbox" id="multiPointMode" name="multiPointMode" />
                            </div>
                            <div class="training-field" id="polygonSimplifyField">
                                <label for="polygonSimplifyEpsilon">Polygon detail</label>
                                <input type="range" id="polygonSimplifyEpsilon" name="polygonSimplifyEpsilon" min="0" max="40" step="0.5" value="20" />
                                <div class="training-help">Left = simpler polygons, right = more detail (more points).</div>
                            </div>
                            <div id="samStatus" class="sam-status" role="status" aria-live="polite"></div>
                            <div id="samStatusProgress" class="sam-status-progress" aria-hidden="true">
                                <div id="samStatusProgressFill"></div>
                            </div>
                            <br />
                            <div id="sam3SimilarityRow" class="training-actions" style="margin-bottom: 12px;">
                                <button type="button" id="sam3SimilarityButton" class="training-button secondary">
                                    SAM3 similarity prompt (use selected box)
                                </button>
                                <label for="sam3SimilarityThreshold" class="training-help" style="margin-top: 6px;">Similarity score (higher = stricter)</label>
                                <input type="range" id="sam3SimilarityThreshold" min="0" max="1" step="0.05" value="0.5" />
                            </div>
                            <section class="qwen-panel" aria-labelledby="qwenPanelTitle">
                                <div class="qwen-panel__header">
                                    <span id="qwenPanelTitle">Qwen 2.5 Assist</span>
                                    <span id="qwenActiveModelLabel" class="qwen-active-model-label"></span>
                                    <span id="qwenStatusLabel" class="qwen-status-label">Checking‚Ä¶</span>
                                </div>
                                <label for="qwenItems">Objects / keywords</label>
                                <textarea id="qwenItems" rows="3" placeholder="List what to detect (comma or line separated)"></textarea>
                                <button type="button" id="qwenAdvancedToggle" class="qwen-advanced-toggle" aria-expanded="false">Show advanced overrides</button>
                                <div id="qwenAdvancedPanel" class="qwen-advanced" aria-hidden="true">
                                    <label for="qwenCustomPrompt">Full prompt override</label>
                                    <textarea id="qwenCustomPrompt" rows="3" placeholder="Optional: supply the exact prompt to send to Qwen"></textarea>
                                    <label for="qwenImageType">Image type description</label>
                                    <input type="text" id="qwenImageType" placeholder="Pulled from active Qwen model" />
                                    <label for="qwenExtraContext">Extra context</label>
                                    <textarea id="qwenExtraContext" rows="2" placeholder="Optional notes (lighting, camera, etc.)"></textarea>
                                </div>
                                <label for="qwenClassSelect">Assign detections to class</label>
                                <select id="qwenClassSelect"></select>
                                <label for="qwenPromptType">Prompt output</label>
                                <select id="qwenPromptType">
                                    <option value="bbox" selected>Bounding boxes</option>
                                    <option value="bbox_sam">Bounding boxes ‚Üí SAM cleanup</option>
                                    <option value="point">Points ‚Üí SAM boxes</option>
                                </select>
                                <label for="qwenMaxResults">Max detections</label>
                                <input type="number" id="qwenMaxResults" min="1" max="50" value="8" />
                                <button type="button" id="qwenRunButton" class="training-button" disabled>Use Qwen</button>
                                <p class="qwen-hint">Uses the prompts saved with the active Qwen model; add extra context via the advanced overrides when needed.</p>
                                <section class="sam3-text-panel" id="sam3TextPanel" aria-labelledby="sam3TextTitle">
                                    <div class="sam3-text-panel__header">
                                        <span id="sam3TextTitle"><strong>SAM3 Text Prompt</strong></span>
                                        <span id="sam3TextStatus" class="sam3-text-status" aria-live="polite"></span>
                                    </div>
                                    <p class="sam3-text-hint">Runs text-driven segmentation with SAM3. Can be used alongside the bbox tools.</p>
                                    <label for="sam3TextPrompt">Prompt (describe what to segment)</label>
                                    <input type="text" id="sam3TextPrompt" placeholder="e.g., red helmet" />
                                    <div class="sam3-text-grid">
                                        <label for="sam3Threshold">Score threshold</label>
                                        <input type="number" id="sam3Threshold" step="0.05" min="0" max="1" value="0.5" />
                                        <label for="sam3MaskThreshold">Mask threshold</label>
                                        <input type="number" id="sam3MaskThreshold" step="0.05" min="0" max="1" value="0.5" />
                                        <label for="sam3MaxResults">Max results</label>
                                        <input type="number" id="sam3MaxResults" min="1" max="100" value="20" />
                                        <label for="sam3MinSize">Min size (px¬≤)</label>
                                        <input type="number" id="sam3MinSize" min="0" value="0" title="Ignore detections smaller than this pixel area. 0 keeps all." />
                                        <label for="sam3MaxPoints">Max points/polygon</label>
                                        <input type="number" id="sam3MaxPoints" min="10" max="5000" value="500" title="Simplification cap per polygon. Lower for smoother shapes; higher preserves detail." />
                                        <label for="sam3SimplifyEpsilon">Simplify epsilon (px)</label>
                                        <input type="number" id="sam3SimplifyEpsilon" min="0" step="0.1" value="1.0" title="Douglas‚ÄìPeucker epsilon in pixels on the downsampled mask: higher = smoother polygons, lower = more detail." />
                                    </div>
	                                    <label for="sam3ClassSelect">Assign detections to class</label>
	                                    <select id="sam3ClassSelect"></select>
		                                    <div class="sam3-text-buttons">
		                                        <button type="button" id="sam3RunButton" class="training-button">Run SAM3</button>
		                                        <button type="button" id="sam3RunAutoButton" class="training-button">Run SAM3 + Auto Class</button>
		                                        <label for="sam3RecipeFile" class="training-button secondary" title="Import a portable recipe zip (from Agent Mining or another machine). New recipe zips contain recipe.json + embedded pretrained CLIP head artifacts (clip_head/). Legacy zips may also include example crops.">Import recipe (zip)</label>
		                                        <input type="file" id="sam3RecipeFile" accept=".zip" class="file-input-hidden" />
		                                        <button type="button" id="sam3RecipePresetRefresh" class="training-button secondary">Refresh recipes</button>

		                                        <div class="training-help" style="margin-top: 8px;">
		                                            <strong>Recipe cascade:</strong> run multiple saved recipes in order. Each step can re-label its outputs and choose whether it participates in cross-class de-dupe.
		                                        </div>

		                                        <div id="sam3CascadeSteps"></div>
		                                        <button type="button" id="sam3CascadeAddStep" class="training-button secondary" title="Add another recipe to the cascade (you can reorder steps).">+ Add recipe step</button>

		                                        <div class="training-grid" style="margin-top: 10px;">
		                                            <div>
		                                                <label for="sam3CascadePerClassIou" title="Removes near-duplicate overlaps within the same output class. IoU means how much two boxes overlap. Lower = more aggressive.">Per-class de-dupe IoU</label>
		                                                <input type="number" id="sam3CascadePerClassIou" step="0.05" min="0" max="1" value="0.5" title="Overlap threshold (IoU = intersection over union). Lower = more suppression; set to 1 to effectively disable." />
			                                            </div>
			                                            <div>
			                                                <label title="Also remove duplicates across different classes (e.g. car vs truck). Use different dedupe group names (e.g. person vs bike) to avoid removing valid overlaps."><input type="checkbox" id="sam3CascadeCrossDedupeToggle" /> Cross-class de-dupe</label>
			                                                <select id="sam3CascadeCrossScope" title="Choose whether cross-class de-dupe only happens within the same dedupe group, or across all participating steps.">
			                                                    <option value="groups" selected>Within dedupe groups</option>
			                                                    <option value="global">Across all participating steps</option>
			                                                </select>
			                                            </div>
			                                            <div>
			                                                <label for="sam3CascadeCrossIou" title="How much two detections must overlap (IoU) to be considered the same object across different classes. Lower = more aggressive.">Cross-class IoU</label>
			                                                <input type="number" id="sam3CascadeCrossIou" step="0.05" min="0" max="1" value="0.5" title="Overlap threshold (IoU = intersection over union) for cross-class de-dupe. Lower = more suppression; set to 1 to effectively disable." />
			                                            </div>
			                                            <div>
			                                                <label for="sam3CascadeConfidence" title="How we pick the 'winner' when two detections overlap.">Confidence (de-dupe)</label>
			                                                <select id="sam3CascadeConfidence">
			                                                    <option value="sam_score" selected>SAM score</option>
			                                                    <option value="clip_head_prob">CLIP head prob</option>
			                                                    <option value="clip_head_margin">CLIP head margin</option>
			                                                </select>
			                                            </div>
			                                            <div>
			                                                <label for="sam3CascadeClipHeadSource" title="Only used when Confidence is set to a CLIP head option. Select which saved recipe provides the embedded CLIP head.">CLIP head source recipe</label>
			                                                <select id="sam3CascadeClipHeadSource"></select>
			                                            </div>
			                                        </div>

		                                        <div class="training-grid" style="grid-template-columns: 1fr 1fr; margin-top: 10px;">
		                                            <div>
		                                                <label for="sam3CascadePresetSelect" title="Saved cascades on the backend. Use Refresh if you just saved/imported one.">Cascade presets</label>
		                                                <select id="sam3CascadePresetSelect" title="Saved cascades on the backend. Use Refresh if you just saved/imported one."></select>
		                                            </div>
		                                            <div>
		                                                <label for="sam3CascadePresetName" title="Optional friendly name when saving the current cascade into your preset list.">Save cascade as</label>
		                                                <input type="text" id="sam3CascadePresetName" placeholder="Optional label" />
		                                            </div>
		                                        </div>
		                                        <div class="training-actions">
			                                            <button type="button" id="sam3CascadePresetRefresh" class="training-button secondary" title="Reload the list of saved cascades from the backend.">Refresh cascades</button>
			                                            <button type="button" id="sam3CascadePresetSave" class="training-button secondary" title="Save the current cascade settings as a reusable preset.">Save cascade preset</button>
			                                            <button type="button" id="sam3CascadePresetLoad" class="training-button secondary" title="Load the selected preset into the editor below.">Load preset</button>
			                                            <button type="button" id="sam3CascadePresetDelete" class="training-button danger" title="Delete the selected preset from the backend.">Delete preset</button>
			                                            <button type="button" id="sam3CascadePresetExport" class="training-button secondary" title="Download a portable zip that includes the cascade and all referenced recipes.">Download preset (zip)</button>
			                                            <label for="sam3CascadeFile" class="training-button secondary" title="Import a portable cascade zip. It bundles cascade.json + all referenced recipe zips.">Import cascade (zip)</label>
			                                            <input type="file" id="sam3CascadeFile" accept=".zip" class="file-input-hidden" />
			                                            <button type="button" id="sam3CascadeApplyButton" class="training-button secondary" title="Run every enabled step on the current image and add the final detections.">Apply cascade to image</button>
			                                        </div>
		                                        <div id="sam3RecipeStatus" class="training-help" aria-live="polite"></div>
		                                    </div>
		                                </section>
	                            </section>
                            <br />
                            <hr />
                            <label for="bboxes">Bboxes:</label>
                            <input type="file" id="bboxes" name="bboxes[]" accept="text/plain,application/zip" disabled multiple class="file-input-hidden" />
                            <label for="bboxes" id="bboxesSelect" class="training-button button-disabled" aria-disabled="true" role="button" tabindex="-1">Import Bboxes‚Ä¶</label>
                            <input type="file" id="bboxesFolder" name="bboxesFolder[]" accept="text/plain" disabled multiple webkitdirectory directory class="file-input-hidden" />
                            <label for="bboxesFolder" id="bboxesSelectFolder" class="training-button button-disabled" aria-disabled="true" role="button" tabindex="-1">Import Bboxes Folder‚Ä¶</label>
                            <button type="button" name="saveBboxes" id="saveBboxes" class="training-button">Save YOLO</button>
                            <hr />
                            <div id="description">
                                SHORTCUTS:
                                <ul>
                                    <li><strong>X</strong> ‚Äî press with a bbox selected to run the SAM tweak</li>
                                    <li><strong>X</strong> twice ‚Äî batch tweak all bboxes in the class currently selected in the list</li>
                                    <li>Mouse wheel ‚Äî zoom</li>
                                    <li>Shift + wheel ‚Äî pan</li>
                                    <li>Right-click drag ‚Äî pan</li>
                                    <li>‚Üê / ‚Üí ‚Äî cycle images</li>
                                    <li>‚Üë / ‚Üì ‚Äî cycle classes</li>
                                    <li>Delete / Backspace / W ‚Äî remove selected bbox</li>
                                    <li>Q ‚Äî delete the most recently created bbox</li>
                                    <li>Enter ‚Äî submit multi-point mask</li>
                                    <li>Hold Z ‚Äî temporarily disable Auto Class and all SAM modes so you can select or delete the active bbox</li>
                                    <li>A ‚Äî toggle auto class</li>
                                    <li>S ‚Äî toggle SAM</li>
                                    <li>D ‚Äî toggle SAM point mode</li>
                                    <li>M ‚Äî toggle SAM multi-point</li>
                                    <li>F ‚Äî add positive point</li>
                                    <li>G ‚Äî add negative point</li>
                                </ul>
                            </div>
                        </form>
                    </div>
                    <div class="right" id="right">
                        <div id="clipProgressBar"><div id="clipProgressFill"></div></div>
                        <canvas id="canvas"></canvas>
                        <canvas id="hiddenCanvas"></canvas>
                    </div>
                </div>
            </div>
            <div class="tab-panel" id="tabTraining" data-tab-panel="training">
                <div class="training-wrapper two-col">
                    <section class="training-form-section">
                        <h2>CLIP Class Predictor Settings</h2>
                        <div class="training-field">
                            <label for="clipBackboneSelect">CLIP Backbone</label>
                            <select id="clipBackboneSelect"></select>
                        </div>
                        <div class="training-field">
                            <label for="qwenTrainDevices">Device IDs</label>
                            <input type="text" id="qwenTrainDevices" placeholder="e.g., 0,1" />
                            <div class="training-help">Leave blank to use a single GPU. Enter comma-separated CUDA device indices to split across multiple GPUs.</div>
                        </div>
                        <div class="training-field">
                            <label for="trainSolver">Solver</label>
                            <select id="trainSolver">
                                <option value="saga" selected>SAGA (elastic net-friendly)</option>
                                <option value="lbfgs">LBFGS</option>
                                <option value="liblinear">LibLinear</option>
                                <option value="sag">SAG</option>
                                <option value="newton-cg">Newton-CG</option>
                            </select>
                        </div>
                        <div class="training-field">
                            <label for="trainDatasetSelect">Cached dataset (YOLO)</label>
                            <div class="training-picker">
                                <select id="trainDatasetSelect">
                                    <option value="">Use local upload‚Ä¶</option>
                                </select>
                                <button type="button" class="training-button secondary" id="trainDatasetRefresh">Refresh</button>
                            </div>
                            <div class="training-help" id="trainDatasetSummary">Choose a cached YOLO dataset or upload folders below.</div>
                        </div>
                        <div class="training-field">
                            <label for="trainImagesBtn">Images Folder</label>
                            <div class="training-picker">
                                <button type="button" class="training-button" id="trainImagesBtn">Choose folder‚Ä¶</button>
                                <input type="file" id="trainImages" accept="image/jpeg,image/png,image/webp,image/bmp,image/tiff" webkitdirectory directory multiple class="file-input-hidden" />
                            </div>
                            <div class="training-help" id="trainImagesSummary">No folder selected</div>
                        </div>
                        <div class="training-field">
                            <label for="trainLabelsBtn">Labels Folder (YOLO txt)</label>
                            <div class="training-picker">
                                <button type="button" class="training-button" id="trainLabelsBtn">Choose folder‚Ä¶</button>
                                <input type="file" id="trainLabels" accept="text/plain" webkitdirectory directory multiple class="file-input-hidden" />
                            </div>
                            <div class="training-help" id="trainLabelsSummary">No folder selected</div>
                        </div>
                        <div class="training-field">
                            <label for="trainLabelmap">Labelmap (.txt or .pkl)</label>
                            <input type="file" id="trainLabelmap" accept=".txt,.pkl" />
                            <div class="training-help" id="trainLabelmapSummary">Optional</div>
                        </div>
                        <div class="training-field">
                            <label for="trainOutputDirBtn">Output Directory</label>
                            <button type="button" class="training-button" id="trainOutputDirBtn">Choose folder‚Ä¶</button>
                            <div class="training-help" id="trainOutputDirSummary">Server default (.)</div>
                        </div>
                        <div class="training-field-double">
                            <div>
                                <label for="trainModelFilename">Model Filename</label>
                                <input type="text" id="trainModelFilename" value="my_logreg_model.pkl" />
                            </div>
                            <div>
                                <label for="trainLabelmapFilename">Labelmap Filename</label>
                                <input type="text" id="trainLabelmapFilename" value="my_label_list.pkl" />
                            </div>
                        </div>
                        <div class="training-advanced-grid">
                            <div>
                                <label for="trainTestSize">Test Size</label>
                                <input type="number" id="trainTestSize" min="0" max="0.9" step="0.05" value="0.2" />
                                <div class="training-help">Fraction of images reserved for evaluation.</div>
                            </div>
                            <div>
                                <label for="trainRandomSeed">Seed</label>
                                <input type="number" id="trainRandomSeed" value="42" />
                                <div class="training-help">Controls deterministic train/test split.</div>
                            </div>
                            <div>
                                <label for="trainBatchSize">Batch Size</label>
                                <input type="number" id="trainBatchSize" value="64" min="1" />
                                <div class="training-help">Images encoded per CLIP forward pass.</div>
                            </div>
                            <div>
                                <label for="trainMaxIter">Max Iter</label>
                                <input type="number" id="trainMaxIter" value="1000" min="100" step="50" />
                                <div class="training-help">Upper bound on solver iterations.</div>
                            </div>
                            <div>
                                <label for="trainMinPerClass">Min / Class</label>
                                <input type="number" id="trainMinPerClass" value="2" min="1" />
                                <div class="training-help">Drops classes with fewer samples before training.</div>
                            </div>
                            <div>
                                <label for="trainRegC">C</label>
                                <input type="number" id="trainRegC" value="1.0" step="0.1" min="0.01" />
                                <div class="training-help">Inverse regularisation strength (higher = less regularisation).</div>
                            </div>
                            <div>
                                <label for="trainClassWeight">Class Weight</label>
                                <select id="trainClassWeight">
                                    <option value="none" selected>None</option>
                                    <option value="balanced">Balanced</option>
                                </select>
                                <div class="training-help">`balanced` reweights rare classes; `none` leaves raw counts.</div>
                            </div>
                            <div>
                                <label for="trainDeviceOverride">Device Override</label>
                                <input type="text" id="trainDeviceOverride" placeholder="cpu or cuda" />
                            </div>
                            <div>
                                <label for="trainHardMisWeight">Hard W (misclass)</label>
                                <input type="number" id="trainHardMisWeight" value="3.0" min="1" step="0.1" />
                                <div class="training-help">Multiplier applied to misclassified samples in hard mining.</div>
                            </div>
                            <div>
                                <label for="trainHardLowConfWeight">Hard W (low conf)</label>
                                <input type="number" id="trainHardLowConfWeight" value="2.0" min="1" step="0.1" />
                                <div class="training-help">Multiplier for low-confidence samples (below the thresholds).</div>
                            </div>
                            <div>
                                <label for="trainHardLowConfThreshold">Low-conf threshold</label>
                                <input type="number" id="trainHardLowConfThreshold" value="0.65" min="0" max="0.9999" step="0.01" />
                                <div class="training-help">Samples with max probability below this value get boosted.</div>
                            </div>
                            <div>
                                <label for="trainHardMarginThreshold">Margin threshold</label>
                                <input type="number" id="trainHardMarginThreshold" value="0.15" min="0" max="1" step="0.01" />
                                <div class="training-help">Boost samples whose top-1 vs top-2 gap falls under this margin.</div>
                            </div>
                            <div>
                                <label for="trainConvergenceTol">Convergence tol</label>
                                <input type="number" id="trainConvergenceTol" value="0.0001" min="0" step="0.00001" />
                                <div class="training-help">Lower tolerance forces more iterations before convergence.</div>
                            </div>
                        </div>
                        <button type="button" id="startTrainingBtn">Start Training</button>
                        <div class="training-toggle-row">
                            <label><input type="checkbox" id="trainReuseEmbeddings" checked /> Cache and reuse embeddings</label>
                            <label><input type="checkbox" id="trainHardMining" /> Hard example mining</label>
                        </div>
                        <div class="training-message" id="trainingMessage" role="status"></div>
                    </section>
                    <section class="training-status-section">
                        <h2>Training Status</h2>
                        <div class="training-progress" id="trainingProgressBar">
                            <div class="training-progress-fill" id="trainingProgressFill"></div>
                        </div>
                        <div class="training-status-text" id="trainingStatusText">Idle</div>
                        <div class="training-actions">
                            <button type="button" class="training-button training-button-danger" id="cancelTrainingBtn" disabled>Cancel Job</button>
                        </div>
                        <div class="training-summary" id="trainingSummary"></div>
                        <div class="training-logs">
                            <h3>Logs</h3>
                            <pre id="trainingLog"></pre>
                        </div>
                        <div class="training-history">
                            <h3>Recent Jobs</h3>
                            <div id="trainingHistory"></div>
                        </div>
                    </section>
                </div>
            </div>
            <div class="tab-panel" id="tabQwenTrain" data-tab-panel="qwen-train">
                <div class="training-wrapper two-col">
	                    <section class="training-form-section">
	                        <h2>Fine-tune Qwen 2.5 VL</h2>
	                        <p class="qwen-train-intro">
	                            Fine-tune Qwen on a dataset managed in the Dataset Management tab. Upload a YOLO / YOLO-seg dataset first, then build Qwen
	                            annotations (JSONL) so Qwen can learn to return JSON detections (bbox + point) for every class in your label map.
	                        </p>
                        <div class="qwen-train-callouts">
                            <div>
                                <strong>LoRA</strong>
                                <p>Trains lightweight adapters on top of the base FP16 model. Fast and easy when you have ‚â•24&nbsp;GB VRAM.</p>
                            </div>
                            <div>
                                <strong>QLoRA</strong>
                                <p>Quantizes the base model to 4-bit (NF4) and only trains adapters. Ideal for 18‚Äì24&nbsp;GB GPUs.</p>
                            </div>
                        </div>
                        <p class="qwen-train-note">
                            Each training sample reuses your description plus the system prompt here. For every epoch we randomly ask Qwen to return
                            either bounding boxes or click points, and we vary the scope of the request: sometimes all classes, sometimes a single class
                            that exists in the image, and sometimes a small subset. The targets are filtered to match that instruction so the adapters learn
                            both broad sweeps and class-specific prompts without duplicating the dataset on disk.
                        </p>
                        <div class="training-field">
                            <label for="qwenTrainRunName">Run name / subfolder</label>
                            <input type="text" id="qwenTrainRunName" placeholder="Optional; defaults to job id" />
                            <div class="training-help">Checkpoints land in <code>uploads/qwen_runs/&lt;run_name&gt;</code>.</div>
                        </div>
	                        <div class="training-field">
	                            <label>Dataset</label>
	                            <div class="qwen-dataset-cache-controls">
	                                <select id="qwenDatasetSelect"></select>
	                                <button type="button" class="training-button" id="qwenDatasetRefresh">Refresh</button>
	                            </div>
	                            <div class="training-help" id="qwenDatasetSummary">If the selected dataset says "needs Qwen build", go to Dataset Management and click "Build Qwen".</div>
	                        </div>
                        <div class="training-grid">
                            <div class="checkbox-row">
                                <label><input type="checkbox" id="qwenTrainRandomSplit" checked /> Random split (ignore existing train/val)</label>
                            </div>
                            <div>
                                <label for="qwenTrainValPercent">Val %</label>
                                <input type="number" id="qwenTrainValPercent" min="1" max="90" value="30" />
                            </div>
                            <div>
                                <label for="qwenTrainSplitSeed">Split seed</label>
                                <input type="number" id="qwenTrainSplitSeed" value="42" />
                            </div>
                        </div>
                        <div class="training-help">We rebuild the train/val split per job using this seed and percentage.</div>
                        <div class="training-grid">
                            <div class="training-help" id="qwenCacheInfo">Split cache: ‚Ä¶</div>
                            <div>
                                <button id="qwenCachePurge" class="secondary">Purge split cache</button>
                            </div>
                        </div>
                        <div class="training-field">
                            <label for="qwenTrainModelId">Base model repo</label>
                            <input type="text" id="qwenTrainModelId" value="Qwen/Qwen2.5-VL-3B-Instruct" />
                        </div>
                        <div class="training-field">
                            <label for="qwenTrainSystemPrompt">System prompt</label>
                            <textarea id="qwenTrainSystemPrompt" rows="3">You are an annotation assistant that only returns JSON shaped like {"detections":[{"label":"class","bbox":[x1,y1,x2,y2]} or {"label":"class","point":[x,y]}]}. Respond with JSON only.</textarea>
                            <div class="training-help">Shared system message for every conversation. The user prompt still toggles between bbox vs. point instructions per sample.</div>
                        </div>
                        <div class="training-field">
                            <label for="qwenTrainPromptNoise">System prompt noise (0‚Äì0.30)</label>
                            <input type="number" id="qwenTrainPromptNoise" min="0" max="0.3" step="0.01" value="0.05" />
                            <div class="training-help">We randomly drop this fraction of characters from the system prompt for each sample to make the model more robust.</div>
                        </div>
                        <div class="training-field">
                            <label for="qwenTrainAccelerator">Accelerator</label>
                            <select id="qwenTrainAccelerator">
                                <option value="gpu" selected>GPU (recommended)</option>
                                <option value="auto">Auto</option>
                                <option value="cpu">CPU (debug only)</option>
                            </select>
                        </div>
                        <fieldset class="qwen-radio-group">
                            <legend>Adapter strategy</legend>
                            <label class="qwen-radio">
                                <input type="radio" name="qwenLoraMode" value="qlora" checked />
                                <span>
                                    <strong>QLoRA (default)</strong>
                                    <small>Quantize the backbone to 4-bit NF4 and train adapters. Lower VRAM, same accuracy.</small>
                                </span>
                            </label>
                            <label class="qwen-radio">
                                <input type="radio" name="qwenLoraMode" value="lora" />
                                <span>
                                    <strong>LoRA</strong>
                                    <small>Keep the base model in FP16/BF16 and only train adapters. Requires more VRAM but slightly simpler.</small>
                                </span>
                            </label>
                        </fieldset>
                        <div class="training-advanced-grid">
                            <div>
                                <label for="qwenTrainBatchSize">Batch size</label>
                                <input type="number" id="qwenTrainBatchSize" value="1" min="1" />
                            </div>
                            <div>
                                <label for="qwenTrainEpochs">Max epochs</label>
                                <input type="number" id="qwenTrainEpochs" value="10" min="1" />
                            </div>
                            <div>
                                <label for="qwenTrainLR">Learning rate</label>
                                <input type="number" step="0.00001" id="qwenTrainLR" value="0.0002" />
                            </div>
                            <div>
                                <label for="qwenTrainAccumulate">Gradient accumulation</label>
                                <input type="number" id="qwenTrainAccumulate" value="8" min="1" />
                            </div>
                            <div>
                                <label for="qwenTrainDevices">Device IDs</label>
                                <input type="text" id="qwenTrainDevices" placeholder="e.g., 0,1" />
                                <div class="training-help">Leave blank for a single GPU. Enter comma-separated CUDA IDs to train on multiple GPUs.</div>
                            </div>
                            <div>
                                <label for="qwenTrainLoraRank">LoRA rank</label>
                                <input type="number" id="qwenTrainLoraRank" value="8" min="1" />
                            </div>
                            <div>
                                <label for="qwenTrainLoraAlpha">LoRA alpha</label>
                                <input type="number" id="qwenTrainLoraAlpha" value="16" min="1" />
                            </div>
                            <div>
                                <label for="qwenTrainLoraDropout">LoRA dropout</label>
                                <input type="number" id="qwenTrainLoraDropout" value="0.05" step="0.01" min="0" max="1" />
                            </div>
                            <div>
                                <label for="qwenTrainPatience">Early stop patience</label>
                                <input type="number" id="qwenTrainPatience" value="3" min="1" />
                            </div>
                            <div>
                                <label for="qwenTrainMaxImageDim">Max image dimension (px)</label>
                                <input type="number" id="qwenTrainMaxImageDim" value="1024" min="256" max="4096" step="64" />
                                <div class="training-help">Images larger than this shrink on the longest side before reaching Qwen.</div>
                            </div>
                            <div>
                                <label for="qwenTrainMaxDetections">Max detections per sample</label>
                                <input type="number" id="qwenTrainMaxDetections" value="200" min="1" max="200" />
                                <div class="training-help">Uses the same per-class-aware cap from the trainer; drop below 200 to ease VRAM pressure.</div>
                            </div>
                        </div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="qwenTrainStartBtn">Start Training</button>
                            <button type="button" class="training-button training-button-danger" id="qwenTrainCancelBtn" disabled>Cancel Job</button>
                        </div>
                        <div class="training-message" id="qwenTrainMessage" role="status"></div>
                    </section>
                    <section class="training-status-section">
                        <h2>Qwen Training Status</h2>
                        <div class="training-progress" id="qwenTrainProgressBar">
                            <div class="training-progress-fill" id="qwenTrainProgressFill"></div>
                        </div>
                        <div class="training-status-text" id="qwenTrainStatusText">Idle</div>
                        <div class="training-epoch-detail" id="qwenTrainEpochDetail">Waiting for telemetry‚Ä¶</div>
                        <div class="training-chart">
                            <div class="training-chart-header">
                                <h3>Training Loss</h3>
                                <label class="chart-smoothing">
                                    Smoothing
                                    <select id="qwenTrainChartSmoothing">
                                        <option value="1">None</option>
                                        <option value="5">5-point</option>
                                        <option value="15" selected>15-point</option>
                                    </select>
                                </label>
                            </div>
                            <canvas id="qwenTrainLossCanvas"></canvas>
                            <div class="training-help" id="qwenTrainChartStatus">Loss telemetry will appear while a job is running.</div>
                        </div>
                        <div class="qwen-sample-panel">
                            <div class="qwen-sample-header">
                                <h3>Random Training Sample</h3>
                                <button type="button" class="training-button" id="qwenSampleBtn">Generate random Qwen data sample</button>
                            </div>
                            <div class="qwen-sample-canvas">
                                <canvas id="qwenSampleCanvas" width="320" height="240"></canvas>
                                <div class="qwen-sample-status" id="qwenSampleMessage">Load images + label map, then click the button to preview how a training conversation is constructed.</div>
                            </div>
                            <div class="qwen-sample-meta" id="qwenSampleMeta"></div>
                            <div class="qwen-sample-details">
                                <label>Prompt sent to Qwen</label>
                                <pre id="qwenSamplePrompt"></pre>
                                <label>Expected JSON response</label>
                                <pre id="qwenSampleExpected"></pre>
                            </div>
                        </div>
                        <div class="training-summary" id="qwenTrainSummary"></div>
                        <div class="training-logs">
                            <h3>Logs</h3>
                            <pre id="qwenTrainLog"></pre>
                        </div>
                        <div class="training-history">
                            <h3>Recent Qwen Jobs</h3>
                            <div id="qwenTrainHistory"></div>
                        </div>
                    </section>
                </div>
            </div>
            <div class="tab-panel" id="tabSam3Train" data-tab-panel="sam3-train">
                <div class="training-wrapper two-col">
                    <section class="training-form-section">
                        <h2>Train SAM3 (text-promptable by default)</h2>
                        <p class="training-help">Pick a cached dataset (Qwen uploads or YOLO folders). By default we keep the SAM3 segmentation head so the resulting checkpoint works with text prompting; masks are only required if you opt in below.</p>
                        <div class="training-field">
                            <label>Dataset</label>
                            <div class="sam3-dataset-row">
                                <select id="sam3DatasetSelect"></select>
                                <button type="button" class="training-button" id="sam3DatasetRefresh">Refresh</button>
                                <button type="button" class="training-button" id="sam3DatasetConvert">Convert</button>
                            </div>
                            <div class="training-help" id="sam3DatasetSummary">We‚Äôll auto-convert to COCO when needed.</div>
                        </div>
                        <div class="training-grid">
                            <div class="checkbox-row">
                                <label><input type="checkbox" id="sam3RandomSplit" checked /> Random split (ignore existing train/val)</label>
                            </div>
                            <div>
                                <label for="sam3ValPercent">Val %</label>
                                <input type="number" id="sam3ValPercent" min="1" max="90" value="20" />
                            </div>
                            <div>
                                <label for="sam3SplitSeed">Split seed</label>
                                <input type="number" id="sam3SplitSeed" value="42" />
                            </div>
                        </div>
                        <div class="training-help">Train/val splits are rebuilt deterministically per job using this seed.</div>
                        <div class="training-grid">
                            <div class="training-help" id="sam3CacheInfo">Split cache: ‚Ä¶</div>
                            <div>
                                <button id="sam3CachePurge" class="secondary">Purge split cache</button>
                            </div>
                        </div>
                        <div class="training-field">
                            <label for="sam3RunName">Run name / log dir</label>
                            <input type="text" id="sam3RunName" placeholder="Optional; defaults to job id" />
                            <div class="training-help">Logs + checkpoints land in <code>uploads/sam3_runs/&lt;run_name&gt;</code>.</div>
                        </div>
                        <div class="training-grid">
                            <div>
                                <label for="sam3TrainBatch">Train batch size</label>
                                <input type="number" id="sam3TrainBatch" min="1" value="1" />
                            </div>
                            <div>
                                <label for="sam3ValBatch">Val batch size</label>
                                <input type="number" id="sam3ValBatch" min="1" value="1" />
                            </div>
                            <div>
                                <label for="sam3TrainWorkers">Train workers</label>
                                <input type="number" id="sam3TrainWorkers" min="0" value="4" />
                            </div>
                            <div>
                                <label for="sam3ValWorkers">Val workers</label>
                                <input type="number" id="sam3ValWorkers" min="0" value="2" />
                            </div>
                            <div>
                                <label for="sam3Epochs">Max epochs</label>
                                <input type="number" id="sam3Epochs" min="1" value="20" />
                            </div>
                            <div>
                                <label for="sam3Resolution">Resolution</label>
                                <input type="number" id="sam3Resolution" min="256" value="1008" step="16" />
                            </div>
                            <div>
                                <label for="sam3LrScale">LR scale</label>
                                <input type="number" id="sam3LrScale" step="0.01" value="0.1" />
                            </div>
                            <div>
                                <label for="sam3GradAccum">Grad accumulation</label>
                                <input type="number" id="sam3GradAccum" min="1" value="1" />
                            </div>
                            <div>
                                <label for="sam3ValFreq">Val every N epochs</label>
                                <input type="number" id="sam3ValFreq" min="1" value="10" />
                            </div>
                            <div>
                                <label for="sam3ValScoreThresh">Val score threshold</label>
                                <input type="number" id="sam3ValScoreThresh" step="0.01" min="0" max="1" value="0.2" title="Drop detections below this score before COCO eval." />
                            </div>
                            <div>
                                <label for="sam3ValMaxDets">Val max detections per image</label>
                                <input type="number" id="sam3ValMaxDets" min="1" value="1000" title="Keep only the top N detections per image before COCO eval." />
                            </div>
                            <div class="checkbox-row">
                                <label><input type="checkbox" id="sam3CapEpoch" checked /> Cap epoch size</label>
                                <label for="sam3TargetEpochSize">Batches per epoch</label>
                                <input type="number" id="sam3TargetEpochSize" min="1" value="1500" title="Number of batches to treat as one epoch. When capped, epochs stop after this many batches even if the dataset is larger." />
                            </div>
                            <div class="checkbox-row">
                                <label><input type="checkbox" id="sam3CapVal" /> Cap validation size</label>
                                <label for="sam3ValCapSize">Val images</label>
                                <input type="number" id="sam3ValCapSize" min="1" value="200" disabled title="Optional cap on number of validation images for quick smoke tests." />
                            </div>
                            <div>
                                <label for="sam3BalanceStrategy" title="Choose how to rebalance rare classes.">
                                    Balance strategy
                                </label>
                                <select id="sam3BalanceStrategy">
                                    <option value="none" selected>None (uniform sampling)</option>
                                    <option value="inv_sqrt">Mild: 1/sqrt(freq)</option>
                                    <option value="clipped_inv">Clipped inverse-freq</option>
                                    <option value="effective_num">Effective number of samples</option>
                                    <option value="focal">Focal sampling</option>
                                </select>
                                <div id="sam3BalanceDescription" class="training-help"></div>
                            </div>
                            <div class="balance-param-row sam3-balance-param" data-param="power">
                                <label for="sam3BalancePower" title="Exponent for inverse-frequency weighting (1/freq^power). Lower = milder.">
                                    Inverse power
                                </label>
                                <input type="number" id="sam3BalancePower" step="0.1" min="0" value="0.5" />
                            </div>
                            <div class="balance-param-row sam3-balance-param" data-param="clip">
                                <label for="sam3BalanceClip" title="Clamp the ratio between highest and lowest weights. e.g., 10 means rare classes get at most 10x the weight of common classes.">
                                    Clip ratio (max/min)
                                </label>
                                <input type="number" id="sam3BalanceClip" step="1" min="1" value="10" />
                            </div>
                            <div class="balance-param-row sam3-balance-param" data-param="beta">
                                <label for="sam3BalanceBeta" title="Beta for effective number of samples weighting. Higher (0.99-0.999) = stronger emphasis on rare classes.">
                                    Beta (effective num)
                                </label>
                                <input type="number" id="sam3BalanceBeta" step="0.001" min="0" max="0.9999" value="0.99" />
                            </div>
                            <div class="balance-param-row sam3-balance-param" data-param="gamma">
                                <label for="sam3BalanceGamma" title="Gamma for focal-style sampling. Higher = more boost to rare/low-freq classes.">
                                    Gamma (focal)
                                </label>
                                <input type="number" id="sam3BalanceGamma" step="0.1" min="0" value="0.5" />
                            </div>
                            <div>
                                <label for="sam3Warmup">Warmup steps</label>
                                <input type="number" id="sam3Warmup" min="0" value="20" title="Warmup steps: start the LR near zero and ramp up over this many steps. Higher = gentler/safer start, lower = hotter start. 0 disables warmup." />
                            </div>
                            <div>
                                <label for="sam3Timescale">Scheduler timescale</label>
                                <input type="number" id="sam3Timescale" min="1" value="20" title="Scheduler timescale: stretches the LR schedule. Higher = slower decay, steadier training. Lower = faster decay, spikier early changes." />
                            </div>
                        </div>
                        <div class="training-toggle-row">
                            <label title="For debugging only: prints a log line every minibatch (instead of every Nth). Creates very large logs."><input type="checkbox" id="sam3LogAll" /> Log every minibatch (very verbose)</label>
                        </div>
                        <div class="training-toggle-row">
                            <label title="Stops updating the language/text encoder to preserve base vocabulary. Overrides the language LR override below."><input type="checkbox" id="sam3FreezeLanguage" /> Freeze language backbone (preserve base text prompts)</label>
                        </div>
                        <div class="training-field">
                            <label for="sam3LanguageLr">Language backbone LR override</label>
                            <input type="number" id="sam3LanguageLr" step="0.000001" min="0" placeholder="‚âà5e-5 √ó LR scale (default)" />
                            <div class="training-help">Optional: set a specific LR for the text encoder. Set to 0 (or check ‚ÄúFreeze language‚Äù) to keep base vocabulary; leave blank to use the config default.</div>
                        </div>
                        <div class="training-field">
                            <label for="sam3PromptVariants">Prompt variants per class (text grounding)</label>
                            <textarea id="sam3PromptVariants" rows="3" placeholder="class_one: variant a, variant b&#10;hard_hat: helmet, safety helmet"></textarea>
                            <div class="training-help">One class per line: <code>class_name: alt1, alt2</code>. We swap the class name for one of these phrases during training; leave blank to use the class names as-is.</div>
                            <div class="training-toggle-row">
                                <label title="When on, pick a random variant per datapoint during training; when off, always use the first variant. Validation always uses the first variant."><input type="checkbox" id="sam3PromptRandomize" checked /> Randomize variants per datapoint</label>
                            </div>
                        </div>
                        <div class="training-toggle-row">
                            <label title="Keep the SAM3 segmentation head loaded so text prompting works. Uses the base head weights if you don‚Äôt train masks."><input type="checkbox" id="sam3SegHead" checked /> Keep segmentation head for text prompts</label>
                        </div>
                        <div class="training-toggle-row">
                            <label title="Only turn this on if your dataset includes segmentation masks. It will feed masks into training and update the segmentation head; leave off for bbox-only datasets (head stays frozen for prompting)."><input type="checkbox" id="sam3SegTrain" /> Train segmentation head with masks (requires masks)</label>
                            <div class="training-help">Default OFF for bbox-only runs. Turn ON only when you have mask annotations and want the segmentation head to learn them; otherwise the head stays frozen but still serves text prompts.</div>
                        </div>
                        <div class="training-toggle-row">
                            <label title="Disables the segmentation head entirely. Smaller checkpoints, faster training, BUT no text prompting and only bbox refinement works."><input type="checkbox" id="sam3BBoxOnly" /> Bbox-refiner only mode (no text prompts, no masks)</label>
                            <div class="training-help">Use only if you explicitly want a bbox-only refiner. Default OFF so text prompting remains available.</div>
                        </div>
                        <button type="button" id="sam3StartBtn" class="training-button">Start SAM3 Training</button>
                            <div class="training-note">
                                <strong>Tuning tips:</strong><br />
                                ‚Ä¢ Lower LR scale (e.g., 0.05 or 0.02) to reduce early loss spikes.<br />
                                ‚Ä¢ Meta‚Äôs default LR scale is 0.1; that makes transformer LR 8e-5, vision 2.5e-5, language 5e-6. Halving/dropping LR scale makes starts gentler.<br />
                                ‚Ä¢ Warmup steps: start near-zero LR and ramp up over these steps; higher is gentler/safer, lower is hotter/risks spikes (try 20‚Äì100).<br />
                                ‚Ä¢ Timescale: stretches the LR schedule; higher keeps LR steady longer, lower decays faster (try 20‚Äì40 for short runs).<br />
                                ‚Ä¢ To simulate larger batches without extra VRAM, keep batch size at 1 and raise grad accumulation (e.g., 2‚Äì4).<br />
                                ‚Ä¢ Typical recipe: resolution 1008, LR scale 0.05, grad accumulation 2, batch sizes 1/1, epochs 20.
                            </div>
                        <div class="training-note">
                            <strong>Text prompt alignment (read this):</strong><br />
                            ‚Ä¢ SAM3 class scores come from text embeddings. If you leave variants blank, we use your raw class names as prompts.<br />
                            ‚Ä¢ Weird names (e.g., <code>light_vehicle</code>) often don‚Äôt match the base model. Add synonyms per class or unfreeze the language backbone (set a small LR, or uncheck ‚ÄúFreeze language‚Äù).<br />
                            ‚Ä¢ Freezing the language encoder is only sensible if you stick to the base vocabulary and just fine-tune boxes/masks.<br />
                            ‚Ä¢ COCO eval is class-ID based; bad prompt alignment ‚áí wrong class IDs ‚áí low AP even if boxes look reasonable.
                        </div>
                        <div class="training-message" id="sam3Message" role="status"></div>
                    </section>
                    <section class="training-status-section">
                        <h2>Training Status</h2>
                        <div class="training-progress" id="sam3ProgressBar">
                            <div class="training-progress-fill" id="sam3ProgressFill"></div>
                        </div>
                        <div class="training-status-text" id="sam3StatusText">Idle</div>
                        <div class="training-help" id="sam3EtaText">ETA: estimating‚Ä¶</div>
                        <div class="training-actions">
                            <button type="button" class="training-button training-button-danger" id="sam3CancelBtn" disabled>Cancel Job</button>
                        </div>
                        <div class="training-summary" id="sam3Summary"></div>
                        <div class="training-summary" id="sam3BalanceSummary"></div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="sam3ActivateBtn" disabled>Activate checkpoint</button>
                        </div>
                        <div class="training-chart">
                            <h3>Loss</h3>
                            <canvas id="sam3LossChart" height="120"></canvas>
                            <div class="training-chart-legend">
                                <span class="legend-item legend-blue">Blue: rolling average loss (last ~10 batches; computed every batch even if not all are logged).</span>
                                <span class="legend-item legend-orange">Orange: logged batch loss (printed every Nth minibatch; some batches are skipped in logs).</span>
                                <span class="legend-item">Log lines show <code>batch=</code> and <code>avg10=</code> (last ~10 minibatches).</span>
                                <div class="training-help">
                                    Smoothed trend (EMA): <input type="range" id="sam3TrendSmooth" min="0.01" max="0.5" step="0.01" value="0.05" />
                                    <span id="sam3TrendSmoothValue">0.05</span> (lower = smoother)
                                </div>
                            </div>
                        </div>
                        <div class="training-chart">
                            <h3>Validation (COCO bbox)</h3>
                            <div id="sam3ValMetrics" class="training-help">No validation metrics yet.</div>
                        </div>
                        <div class="training-logs">
                            <h3>Logs</h3>
                            <pre id="sam3Log"></pre>
                        </div>
                        <div class="training-history">
                            <h3>Recent SAM3 Jobs</h3>
                            <div id="sam3TrainingHistory"></div>
                        </div>
                        <div class="training-history">
                            <div class="storage-header">
                                <h3>Run storage (logs & checkpoints)</h3>
                                <button type="button" class="training-button" id="sam3StorageRefresh">Refresh</button>
                            </div>
                            <div class="training-help">Lists folders under <code>uploads/sam3_runs/</code>; delete logs/dumps/checkpoints you no longer need.</div>
                            <div id="sam3StorageList" class="storage-list"></div>
                        </div>
                    </section>
                </div>
            </div>
	            <div class="tab-panel" id="tabAgentMining" data-tab-panel="agent-mining">
	                <div class="training-wrapper two-col">
		                    <section class="training-form-section">
		                        <h2>SAM3 Recipe Mining</h2>
			                        <p class="training-help">Automatically discover ‚Äúrecipes‚Äù that help SAM3 find each class in your dataset. A recipe is a portable zip (JSON + embedded pretrained CLIP head) you can reuse in the labeling panel.</p>
			                        <div class="training-help" style="color: #b91c1c; font-weight: 600;">Recipe mining UI build: 2025-12-20 (flat layout)</div>
			                        <div id="agentMiningHowItWorks" style="margin-bottom: 12px; display: block !important; padding: 12px 14px; border: 1px solid #e2e8f0; border-radius: 12px; background: #f8fafc;">
			                            <div style="font-weight: 700; margin-bottom: 6px;">How recipe mining works</div>
			                            <div class="training-help">
			                                <strong>Prerequisite:</strong> Train a CLIP head first (CLIP tab). Recipe mining uses that head to score detections and to auto-tune ‚Äúcleanliness‚Äù (precision) thresholds. This is required.
			                            </div>
			                            <div class="training-help" style="margin-top: 10px;">
			                                <div style="font-weight: 600;">Full flow + how to tune it</div>
			                                <div style="font-weight: 600; margin: 10px 0 6px;">High-level flow</div>
			                                <pre class="training-help" style="white-space: pre; overflow-x: auto; padding: 10px; background: #f8fafc; border: 1px solid #e2e8f0; border-radius: 8px;">Dataset (YOLO/COCO) + trained CLIP head
        ‚îÇ
        ‚îú‚îÄ Split images (Train/Val)  [repeatable by seed]
        ‚îÇ
        ‚îú‚îÄ Build prompt list per class
        ‚îÇ    - class name + your extra prompts
        ‚îÇ    - optional GPT prompt expansion
        ‚îÇ    - base prompts (e.g., ‚Äúobject‚Äù, ‚Äúsmall object‚Äù)
        ‚îÇ
        ‚îú‚îÄ Seed-stage prompt eval (Val, text-only)
        ‚îÇ    - run SAM3 text inference per prompt
        ‚îÇ    - build a seed-threshold curve per prompt (coverage vs false positives)
        ‚îÇ
        ‚îú‚îÄ Build a multi-step recipe (per class)
        ‚îÇ    - pick a short chain of prompt steps (usually 3‚Äì8)
        ‚îÇ    - each step has its own seed threshold (from the prompt curve)
        ‚îÇ
        ‚îú‚îÄ Run the full steps pipeline (Val)
        ‚îÇ    - for each step:
        ‚îÇ        1) text seeds ‚Üí pick diverse seeds
        ‚îÇ        2) expand with SAM3 ‚Äúsimilarity prompt‚Äù
        ‚îÇ        3) de-dupe
        ‚îÇ
        ‚îú‚îÄ CLIP head gating (Val)
        ‚îÇ    - crop each detection and score with the pretrained CLIP head
        ‚îÇ    - auto-tune min prob + (optional) margin to hit your ‚Äúcleanliness‚Äù target
        ‚îÇ
        ‚îî‚îÄ Save portable recipe zip
             - recipe.json (schema v2, multi-step)
             - clip_head/head.npz + meta.json (embedded head + tuned thresholds)</pre>
			                                <div style="margin-top: 10px;">
			                                    <strong>Why recipes may look ‚Äútext-only‚Äù:</strong> in schema‚Äëv2, the ‚Äúsimilarity prompt‚Äù is SAM3‚Äôs <em>visual</em> stage (seed boxes ‚Üí expand similar masks/boxes). The recipe stores the <em>text</em> prompts + thresholds; it does <em>not</em> need a fixed crop-bank of exemplars because seeds are chosen per image at inference time.
			                                </div>
			                                <div style="font-weight: 600; margin: 12px 0 6px;">What we optimize for (‚Äúcleanliness‚Äù)</div>
			                                <div>
			                                    We optimize to <strong>maximize coverage</strong> (find as many GT objects as possible) while meeting your <strong>precision target</strong>:
			                                    <ul style="margin: 6px 0 0 18px;">
			                                        <li><strong>Match</strong> = a detection overlaps an unmatched GT box for that class with IoU ‚â• ‚ÄúEval IoU‚Äù.</li>
			                                        <li><strong>Precision</strong> = matches / (matches + false positives). Higher = fewer wrong boxes.</li>
			                                        <li><strong>Coverage</strong> = matches / GT count. Higher = finds more objects.</li>
			                                        <li><strong>Duplicates</strong> = extra detections on the same GT object (tracked separately).</li>
			                                    </ul>
			                                </div>
			                                <div style="font-weight: 600; margin: 12px 0 6px;">Multi-step tuning modes (what to enable)</div>
			                                <ul style="margin: 6px 0 0 18px;">
			                                    <li><strong>Default (no extra tuning):</strong> seed curves ‚Üí greedy prompt selection ‚Üí full pipeline ‚Üí CLIP thresholds tuned to match your cleanliness.</li>
			                                    <li><strong>Refine prompt subset:</strong> quick ‚Äútry a few swaps‚Äù pass (seed-stage only; no extra full SAM runs). Use when the chosen prompts look ‚Äúobviously wrong‚Äù.</li>
			                                    <li><strong>Tier‚Äë1:</strong> small search over <code>expand_threshold</code> + <code>seeds/step</code> (shared across steps). Good first knob search if Global is off.</li>
			                                    <li><strong>Tier‚Äë2:</strong> small search over de‚Äëdupe overlaps (IoU) (shared across steps). Helps duplicates / false positives.</li>
			                                    <li><strong>Global optimizer (best quality):</strong> tries many full recipe variants and keeps the best (slowest, most thorough).</li>
			                                </ul>
			                                <div style="font-weight: 600; margin: 12px 0 6px;">Global optimizer: what knobs it can change</div>
			                                <div>
			                                    A <strong>trial</strong> is a full per-class recipe (ordered steps + per-step settings). We create new trials by making small changes to the current best:
			                                    <ul style="margin: 6px 0 0 18px;">
			                                        <li><strong>Prompt subset:</strong> add / drop / swap a step‚Äôs prompt.</li>
			                                        <li><strong>Per-step seed threshold:</strong> how strict the text prompt must be before a box becomes a seed.</li>
			                                        <li><strong>Per-step expand threshold</strong> and <strong>seeds per image</strong> (how many seeds we expand).</li>
			                                        <li><strong>Per-step de-dupe overlap (IoU):</strong> seed de-dupe and output de-dupe.</li>
			                                        <li><strong>Optional:</strong> step order and per-step max dets/image.</li>
			                                    </ul>
			                                    Each trial is scored by running the <em>real</em> steps pipeline on the same (repeatable) subset of validation images, then sweeping CLIP thresholds to match your cleanliness target.
			                                </div>
			                                <div style="font-weight: 600; margin: 12px 0 6px;">Breadth vs depth (simple mental model)</div>
			                                <ul style="margin: 6px 0 0 18px;">
			                                    <li><strong>Breadth (‚Äúexploration‚Äù):</strong> try more recipe ideas. Increase when results feel ‚Äústuck‚Äù.</li>
			                                    <li><strong>Depth (‚Äúreliability‚Äù):</strong> test each idea on more validation (test) images. Increase when results feel noisy or unstable.</li>
			                                    <li><strong>Rule of thumb:</strong> if you raise breadth a lot, add some depth too.</li>
			                                </ul>
			                                <div style="font-weight: 600; margin: 12px 0 6px;">Global optimizer controls (what they mean)</div>
			                                <ul style="margin: 6px 0 0 18px;">
			                                    <li><strong>Stage‚Äë1/2/3 eval cap:</strong> how many validation images we use per stage (bigger = more reliable, slower). Stage‚Äë1 ranks quickly; Stage‚Äë2/3 re-test the best candidates.</li>
			                                    <li><strong>Max trials:</strong> how many candidate recipe configs we consider per round (higher = more thorough, slower).</li>
			                                    <li><strong>Keep ratio:</strong> how aggressively we prune candidates each stage (0.5 keeps the top half).</li>
			                                    <li><strong>Rounds</strong> + <strong>Variants per round:</strong> how long we keep exploring around the current best recipe.</li>
			                                    <li><strong>Tune step order</strong> / <strong>Tune max dets/image:</strong> optional extra dimensions the optimizer is allowed to change.</li>
			                                </ul>
			                                <div style="font-weight: 600; margin: 12px 0 6px;">How to influence results (practical)</div>
			                                <ul style="margin: 6px 0 0 18px;">
			                                    <li><strong>Cleaner / fewer FPs:</strong> raise ‚ÄúRecipe cleanliness‚Äù. Expect lower coverage.</li>
			                                    <li><strong>More coverage:</strong> lower ‚ÄúRecipe cleanliness‚Äù, increase Max recipe steps, and consider lowering Seed curve floor (lets per-step thresholds go below the base).</li>
			                                    <li><strong>More thorough search:</strong> set ‚ÄúSearch budget‚Äù to Balanced/Best and increase Max trials / Rounds / Variants per round (slower).</li>
			                                    <li><strong>Faster runs:</strong> use Test mode first; lower Stage‚Äë3 eval cap; lower Max trials.</li>
			                                </ul>
			                                <div style="margin-top: 12px; color: #64748b;">
			                                    Note: we no longer generate the old crop-bank recipe style; new recipes are schema‚Äëv2 multi-step and always embed a pretrained CLIP head. Imported legacy recipes can still be applied for backward compatibility.
			                                </div>
			                            </div>
			                        </div>
		                        <div class="training-field">
		                            <label for="agentDatasetSelect">Dataset<span class="help-icon" title="Choose the dataset to mine recipes from.&#10;Datasets are created/converted in the Dataset Management tab.">?</span></label>
		                            <div class="sam3-dataset-row">
		                                <select id="agentDatasetSelect"></select>
		                                <button type="button" class="training-button" id="agentDatasetRefresh">Refresh</button>
		                            </div>
	                            <div class="training-help" id="agentDatasetSummary">Pick a converted SAM3/Qwen dataset.</div>
			                        </div>
				                        <div class="training-grid">
			                            <div>
			                                <label for="agentValPercent">Val %<span class="help-icon" title="How many images are held out for evaluation.&#10;We split your dataset into Train and Val.&#10;Mining uses Train; scoring uses Val.&#10;Higher = more reliable scores (slower).&#10;Lower = faster but noisier scores.">?</span></label>
			                                <input type="number" id="agentValPercent" min="5" max="95" value="30" title="How many images are held out to test the recipe. Example: 30 means we test on 30% of images." />
			                            </div>
			                            <div>
			                                <label for="agentSplitSeed">Split seed<span class="help-icon" title="Random seed for the train/val split.&#10;Same seed = same split (repeatable).&#10;Change it if you want a different random split.&#10;Default is 42.">?</span></label>
			                                <input type="number" id="agentSplitSeed" value="42" title="Same seed = same train/val split. Change it if you want a different random split." />
			                            </div>
				                            <div class="training-field training-field--inline">
				                                <label for="agentReuseSplit">Reuse cached split<span class="help-icon" title="Reuse the cached train/val split for this dataset+seed. Turn off to rebuild the split (still deterministic).">?</span></label>
				                                <input type="checkbox" id="agentReuseSplit" checked />
				                            </div>
				                            <div class="training-field training-field--inline">
				                                <label for="agentReuseCache">Reuse cached SAM results<span class="help-icon" title="Speeds up repeated runs by reusing cached SAM3 prompt evaluations and expansions when possible.&#10;Turn off if you changed the SAM3 prompt model/checkpoint and want to be extra safe.&#10;Use ‚ÄúPurge cache‚Äù to free disk space.">?</span></label>
				                                <input type="checkbox" id="agentReuseCache" checked />
				                            </div>
					                            <div>
					                                <label for="agentIouThreshold">Match overlap (Eval IoU)<span class="help-icon" title="Used only for scoring (evaluation), not for running the recipe.&#10;IoU = how much the predicted box overlaps the ground-truth box (0‚Äì1).&#10;0.5 ‚âà ‚Äúat least about half overlap‚Äù.&#10;Higher = stricter scoring (coverage may look lower).">?</span></label>
					                                <input type="number" id="agentIouThreshold" step="0.05" min="0" max="1" value="0.5" />
					                            </div>
						                            <div>
						                                <label for="agentWorkersPerGpu">Predictors per GPU<span class="help-icon" title="How many SAM3 workers to run per GPU during mining.&#10;1 = one worker per GPU (safest).&#10;Higher can speed up mining if you have VRAM headroom, but uses more GPU memory and can trigger CUDA OOM.">?</span></label>
						                                <input type="number" id="agentWorkersPerGpu" min="1" max="8" value="1" />
						                            </div>
						                            <div>
						                                <label for="agentMaxWorkers">Max predictors total (optional)<span class="help-icon" title="Optional cap on total parallel SAM3 workers across all GPUs.&#10;Leave blank to use all available GPUs with your ‚ÄúPredictors per GPU‚Äù setting.&#10;Lower values reduce VRAM usage and can make the UI more responsive.">?</span></label>
						                                <input type="number" id="agentMaxWorkers" min="1" max="256" placeholder="auto" />
						                            </div>
						                            <div>
						                                <label for="agentSearchMode">Search mode<span class="help-icon" title="Recipe mining now always uses Multi-step: it builds a chain of prompt steps and tunes CLIP head thresholds for your cleanliness target.&#10;Legacy modes are deprecated and are no longer exposed in the UI.">?</span></label>
						                                <select id="agentSearchMode">
						                                    <option value="steps" selected>Multi-step (precision-first)</option>
					                                </select>
					                            </div>
					                        </div>
							                        <div id="agentStepsOptions" style="margin-bottom: 12px; display: block !important; padding: 12px 14px; border: 1px solid #e2e8f0; border-radius: 12px; background: #f8fafc;">
							                            <div style="font-weight: 700; margin-bottom: 6px;">Recipe search settings</div>
							                                <div class="training-help" style="margin-bottom: 10px; color: #475569;">Need a refresher? Open <strong>How recipe mining works</strong> above.</div>
						                                <div class="training-grid">
						                                    <div>
						                                        <label for="agentStepsMaxSteps">Max recipe steps<span class="help-icon" title="How many prompt steps we are allowed to select for a class.&#10;More steps can increase coverage but costs more compute and can add false positives.&#10;A good starting point is 4‚Äì8.">?</span></label>
						                                        <input type="number" id="agentStepsMaxSteps" min="1" max="50" value="6" />
					                                    </div>
					                                    <div>
					                                        <label for="agentStepsMaxSeedsPerStep">Visual seeds per step<span class="help-icon" title="Per step and per image, how many seed detections we expand with SAM3 visual prompting.&#10;Higher can improve coverage on diverse objects, but is slower.&#10;Typical: 3‚Äì8.">?</span></label>
						                                        <input type="number" id="agentStepsMaxSeedsPerStep" min="0" max="500" value="5" />
						                                    </div>
						                                </div>
						                                <div class="training-subsection" style="margin-top: 10px;">
						                                    <div class="training-subsection__title">Search budget</div>
						                                    <div class="training-grid" style="margin-top: 4px;">
						                                        <div>
							                                            <label for="agentStepsGlobalPreset">Global optimizer preset<span class="help-icon" title="Sets a good starting point for the Global optimizer.&#10;&#10;Off: no global optimizer (fastest).&#10;Fast/Balanced/Best: enable the optimizer and set both:&#10;- Breadth (how many recipe ideas we try)&#10;- Depth (how carefully we score each idea)&#10;Custom: keep your current numbers.&#10;&#10;How it works (successive halving): start with many candidates ‚Üí test quickly on Stage‚Äë1 images ‚Üí keep the best fraction ‚Üí retest on Stage‚Äë2/3 to pick a winner.">?</span></label>
						                                            <select id="agentStepsGlobalPreset">
						                                                <option value="off" selected>Off (fastest)</option>
						                                                <option value="fast">Fast</option>
						                                                <option value="balanced">Balanced (recommended)</option>
						                                                <option value="best">Best (slowest)</option>
						                                                <option value="custom">Custom</option>
						                                            </select>
							                                            <div class="training-help" style="margin-top: 6px; color: #64748b;">
							                                                Global optimizer runs a small ‚Äútournament‚Äù (successive halving). Use <strong>Breadth</strong> to explore more ideas and <strong>Depth</strong> to make comparisons more reliable.
							                                            </div>
						                                            <div class="training-help" style="margin-top: 6px; color: #64748b;">
						                                                Tip: changing any advanced optimizer fields below will switch ‚ÄúSearch budget‚Äù to Custom.
						                                            </div>
						                                        </div>
						                                    </div>
						                                    <div class="training-help" style="margin-top: 10px; padding: 10px 12px; border: 1px solid #e2e8f0; border-radius: 12px; background: #f8fafc;">
						                                        <div style="display: flex; align-items: center; justify-content: space-between; gap: 10px;">
						                                            <div style="font-weight: 700; color: #0f172a;">
							                                                Compute estimate<span class="help-icon" title="Rough, relative compute estimate (not seconds). Updates live as you change settings.&#10;&#10;What it tracks: Val images √ó (steps √ó (1 + seeds/step)), plus extra work from the Global optimizer.&#10;&#10;Global optimizer cost grows with:&#10;- Breadth (trials, rounds, variants/round, max step changes)&#10;- Depth (stage eval caps, keep ratio)&#10;&#10;Real runtime depends on GPU, model, image sizes, caching, and dataset content. Use this to see how cost scales as you turn knobs.">?</span>
						                                            </div>
						                                            <span id="agentStepsBudgetBadge" class="badge">‚Ä¶</span>
						                                        </div>
						                                        <div id="agentStepsBudgetText" class="training-help" style="margin-top: 6px; color: #475569; white-space: pre-wrap;">‚Ä¶</div>
						                                        <div class="training-progress" style="margin-top: 8px;">
						                                            <div id="agentStepsBudgetFill" class="training-progress-fill"></div>
						                                        </div>
						                                    </div>
							                                    <div class="training-help" style="margin-top: 8px; color: #64748b;">
							                                        Advanced (below): Global optimizer breadth/depth knobs, Tier‚Äë1/Tier‚Äë2, prompt subset refinement, and seed-curve floors.
							                                    </div>
						                                </div>
						                            </div>
											                                <div id="agentStepsAdvancedDetails" style="margin-top: 12px; padding: 12px 14px; border: 1px solid #e2e8f0; border-radius: 12px; background: #ffffff;">
											                                    <div style="font-weight: 700; margin-bottom: 6px;">Advanced optimization</div>
											                                        <div class="training-help" style="color: #475569; margin-bottom: 10px;">
											                                            Optional knobs for Global optimizer breadth/depth, Tier‚Äë1/Tier‚Äë2 tuning, prompt subset refinement, and seed-curve floors.
											                                        </div>
							                                        <div style="font-weight: 600; margin-top: 10px;">Global optimizer (successive halving)</div>
							                                        <div class="training-help" style="margin-top: 6px; color: #475569;">
							                                            Tune it with two levers:
							                                            <strong>Breadth</strong> = try more recipe ideas ‚Ä¢ <strong>Depth</strong> = score each idea more carefully.
						                                        </div>
						                                        <div class="two-col-grid" style="margin-top: 10px;">
						                                            <div class="training-subsection">
						                                                <div class="training-subsection__title">Breadth (exploration)</div>
						                                                <div class="training-help">Try more candidates. Helpful when results plateau or prompts look ‚Äúobviously wrong‚Äù.</div>
						                                                <div class="training-grid" style="margin-top: 8px;">
						                                                    <div>
						                                                        <label for="agentStepsGlobalMaxTrials">Max trials<span class="help-icon" title="Breadth.&#10;How many candidate recipes we test in Stage‚Äë1 (per class).&#10;Higher = explores more ideas (slower).&#10;Typical: 16‚Äì64.">?</span></label>
						                                                        <input type="number" id="agentStepsGlobalMaxTrials" min="1" max="4096" value="36" />
						                                                    </div>
						                                                    <div>
						                                                        <label for="agentStepsGlobalRounds">Rounds<span class="help-icon" title="Breadth.&#10;How many times we repeat the tournament around the current best recipe.&#10;More rounds = more chances to improve (slower).&#10;Typical: 1‚Äì3.">?</span></label>
						                                                        <input type="number" id="agentStepsGlobalRounds" min="1" max="20" value="2" />
						                                                    </div>
						                                                    <div>
						                                                        <label for="agentStepsGlobalMutationsPerRound">Variants per round<span class="help-icon" title="Breadth.&#10;How many new recipe variants we generate around the current best per round.&#10;Higher = explores more tweaks (slower).&#10;Typical: 12‚Äì40.">?</span></label>
						                                                        <input type="number" id="agentStepsGlobalMutationsPerRound" min="1" max="10000" value="24" />
						                                                    </div>
						                                                    <div>
						                                                        <label for="agentStepsGlobalMaxStepsMutated">Max steps changed per variant<span class="help-icon" title="Breadth.&#10;How big each candidate change is allowed to be.&#10;1 = tiny, stable tweaks; 2‚Äì3 = coordinated changes (more powerful, less predictable).">?</span></label>
						                                                        <input type="number" id="agentStepsGlobalMaxStepsMutated" min="1" max="10" value="2" />
												                                    </div>
												                                </div>
						                                                <div class="training-help" style="margin-top: 10px;">
						                                                    <div class="training-field training-field--inline">
						                                                        <label style="display: inline-flex; align-items: center; gap: 8px;" title="Breadth (optional). Allows the optimizer to swap step order. Usually small effect, but can matter when Max dets/image is small.">
						                                                            <input type="checkbox" id="agentStepsGlobalEnableOrdering" />
						                                                            Tune step order
						                                                        </label>
						                                                    </div>
						                                                    <div class="training-field training-field--inline">
						                                                        <label style="display: inline-flex; align-items: center; gap: 8px;" title="Breadth (optional). Allows the optimizer to tune per-step Max dets/image. Can reduce false positives, but can also miss objects and change runtime.">
						                                                            <input type="checkbox" id="agentStepsGlobalEnableMaxResults" />
						                                                            Tune max dets/image
						                                                        </label>
						                                                    </div>
						                                                </div>
						                                            </div>
						                                            <div class="training-subsection">
						                                                <div class="training-subsection__title">Depth (evaluation reliability)</div>
						                                                <div class="training-help">Score candidates on more validation images and prune less aggressively.</div>
						                                                <div class="training-grid" style="margin-top: 8px;">
						                                                    <div>
						                                                        <label for="agentStepsGlobalEvalCap1">Stage‚Äë1 eval cap<span class="help-icon" title="Depth.&#10;How many validation images we use for the first (fast) ranking stage.&#10;Low = fast but noisy rankings.&#10;Typical: 25‚Äì100.">?</span></label>
						                                                        <input type="number" id="agentStepsGlobalEvalCap1" min="1" max="50000" value="50" />
						                                                    </div>
						                                                    <div>
						                                                        <label for="agentStepsGlobalEvalCap2">Stage‚Äë2 eval cap<span class="help-icon" title="Depth.&#10;How many validation images we use for the second stage.&#10;Higher = more reliable (slower).&#10;Typical: 100‚Äì500.">?</span></label>
						                                                        <input type="number" id="agentStepsGlobalEvalCap2" min="1" max="50000" value="200" />
						                                                    </div>
						                                                    <div>
						                                                        <label for="agentStepsGlobalEvalCap3">Stage‚Äë3 eval cap<span class="help-icon" title="Depth.&#10;How many validation images we use for the final stage.&#10;Typical: 500‚Äì2000.&#10;If your val split is smaller, we automatically use the full val split.">?</span></label>
						                                                        <input type="number" id="agentStepsGlobalEvalCap3" min="1" max="50000" value="1000" />
						                                                    </div>
						                                                    <div>
						                                                        <label for="agentStepsGlobalKeepRatio">Keep ratio<span class="help-icon" title="Depth.&#10;How many candidates survive each stage. 0.5 keeps the top half.&#10;Higher = safer (less likely to prune a good candidate early), slower.&#10;Lower = faster, riskier.">?</span></label>
						                                                        <input type="number" id="agentStepsGlobalKeepRatio" step="0.05" min="0.1" max="0.9" value="0.5" />
						                                                    </div>
						                                                </div>
						                                            </div>
						                                        </div>
						                                        <div class="training-help" style="margin-top: 6px; color: #64748b;">
							                                            Note: When ‚ÄúSearch budget‚Äù is not Off, Tier‚Äë1 and Tier‚Äë2 toggles are ignored.
						                                        </div>
					                                        <hr style="margin: 12px 0; border: none; border-top: 1px solid #e2e8f0;" />
					                                        <label style="display: inline-flex; align-items: center; gap: 8px;" title="Optional (advanced).&#10;When enabled, we try a small number of candidate settings to improve coverage at your cleanliness target.&#10;This tunes the recipe‚Äôs expand threshold + seeds per step.&#10;Requires a pretrained CLIP head.&#10;Slower than the default.">
					                                            <input type="checkbox" id="agentStepsTier1Optimize" />
					                                            Auto-tune expand threshold &amp; seed count (Tier‚Äë1)
					                                        </label>
					                                        <div class="training-grid" style="margin-top: 10px;">
					                                            <div>
					                                                <label for="agentStepsTier1EvalCap">Tier‚Äë1 eval cap (val images)<span class="help-icon" title="How many validation images we use while comparing candidate settings.&#10;Higher = more reliable tuning (slower).&#10;Lower = faster but noisier.&#10;Typical: 100‚Äì500.">?</span></label>
					                                                <input type="number" id="agentStepsTier1EvalCap" min="10" max="50000" value="200" />
					                                            </div>
					                                            <div>
					                                                <label for="agentStepsTier1MaxTrials">Tier‚Äë1 max trials<span class="help-icon" title="How many candidate settings we try during Tier‚Äë1 tuning.&#10;More trials can find better settings but costs time.&#10;Typical: 6‚Äì12.">?</span></label>
					                                                <input type="number" id="agentStepsTier1MaxTrials" min="1" max="256" value="9" />
					                                            </div>
					                                        </div>
					                                        <div class="training-help" style="margin-top: 6px; color: #64748b;">
					                                            Note: Tier‚Äë1 tuning is only used in Multi-step mode and only when a pretrained CLIP head is selected.
					                                        </div>
					                                        <div class="training-help" style="margin-top: 10px;">
					                                            <label style="display: inline-flex; align-items: center; gap: 8px;" title="Optional (advanced).&#10;When enabled, we try a small number of candidate de-duplication settings to reduce duplicates and false positives at your cleanliness target.&#10;This tunes the seed de-dupe IoU + output de-dupe IoU for the class (applied across all steps).&#10;Requires a pretrained CLIP head.&#10;Slower than the default.">
					                                                <input type="checkbox" id="agentStepsTier2Optimize" />
					                                                Auto-tune dedupe overlaps (Tier‚Äë2)
					                                            </label>
					                                            <div class="training-grid" style="margin-top: 10px;">
					                                                <div>
					                                                    <label for="agentStepsTier2EvalCap">Tier‚Äë2 eval cap (val images)<span class="help-icon" title="How many validation images we use while comparing candidate de-dupe settings.&#10;Higher = more reliable tuning (slower).&#10;Lower = faster but noisier.&#10;Typical: 100‚Äì500.">?</span></label>
					                                                    <input type="number" id="agentStepsTier2EvalCap" min="10" max="50000" value="200" />
					                                                </div>
					                                                <div>
					                                                    <label for="agentStepsTier2MaxTrials">Tier‚Äë2 max trials<span class="help-icon" title="How many candidate de-dupe settings we try during Tier‚Äë2 tuning.&#10;More trials can find better settings but costs time.&#10;Typical: 6‚Äì16.">?</span></label>
					                                                    <input type="number" id="agentStepsTier2MaxTrials" min="1" max="256" value="12" />
					                                                </div>
					                                            </div>
					                                            <div class="training-help" style="margin-top: 6px; color: #64748b;">
					                                                Note: Tier‚Äë2 currently applies one set of IoU values per class (shared across all steps).
					                                            </div>
					                                        </div>
					                                        <div class="training-help" style="margin-top: 10px;">
					                                            <label style="display: inline-flex; align-items: center; gap: 8px;" title="Optional (advanced).&#10;After the initial prompt selection, do a small local search that can add/drop/swap prompt steps.&#10;This is seed-stage only (fast) and does not run extra SAM evaluations.&#10;Useful when the greedy selection gets stuck in a suboptimal prompt set.">
					                                                <input type="checkbox" id="agentStepsRefinePromptSubset" />
					                                                Refine prompt subset (local search)
					                                            </label>
					                                            <div class="training-grid" style="margin-top: 10px;">
					                                                <div>
					                                                    <label for="agentStepsRefineMaxIters">Refine iterations<span class="help-icon" title="How many local-search improvement steps to try.&#10;0 = disabled even if the checkbox is on.&#10;Typical: 3‚Äì10.">?</span></label>
					                                                    <input type="number" id="agentStepsRefineMaxIters" min="0" max="100" value="6" />
					                                                </div>
					                                                <div>
					                                                    <label for="agentStepsRefineTopK">Candidates per iter<span class="help-icon" title="How many add/swap candidates to consider per iteration (higher = more thorough, slower).&#10;Typical: 4‚Äì10.">?</span></label>
					                                                    <input type="number" id="agentStepsRefineTopK" min="1" max="50" value="6" />
					                                                </div>
					                                            </div>
					                                        </div>
					                                        <hr style="margin: 12px 0; border: none; border-top: 1px solid #e2e8f0;" />
					                                        <div class="training-help" style="margin-top: 6px;">
					                                            <div style="font-weight: 600; margin-bottom: 6px;">Seed-threshold curve collection</div>
					                                            <div class="training-grid">
					                                                <div>
					                                                    <label for="agentStepsSeedEvalFloor">Seed curve floor (optional)<span class="help-icon" title="Multi-step mode measures each prompt at many seed thresholds.&#10;By default we only see detections above your Seed text threshold, so thresholds below it can‚Äôt improve coverage.&#10;Set a lower floor (e.g., 0.0‚Äì0.01) so the curve can include lower thresholds.&#10;Warning: lower floor can be slower.">?</span></label>
					                                                    <input type="number" id="agentStepsSeedEvalFloor" step="0.01" min="0" max="1" placeholder="auto" />
					                                                </div>
					                                                <div>
					                                                    <label for="agentStepsSeedEvalMaxResults">Seed curve max dets (optional)<span class="help-icon" title="Safety valve for very low seed curve floors.&#10;Caps how many seed detections we consider per prompt per image while building curves.&#10;Lower = faster/cleaner curves, but can miss low-score matches.&#10;Leave blank to reuse Max dets/image.">?</span></label>
					                                                    <input type="number" id="agentStepsSeedEvalMaxResults" min="1" max="5000" placeholder="auto" />
					                                                </div>
					                                            </div>
					                                        </div>
											                                </div>
						                        <div class="training-grid">
						                            <div>
						                                <label for="agentSeedThreshold">Seed text threshold<span class="help-icon" title="First pass on each image: run the text prompt(s) and keep boxes with score ‚â• this.&#10;Lower = more candidate boxes (better recall, more noise, slower).&#10;Higher = fewer boxes (cleaner/faster, can miss objects).">?</span></label>
						                                <input type="number" id="agentSeedThreshold" step="0.01" min="0" max="1" value="0.05" />
			                            </div>
			                            <div>
			                                <label for="agentExpandThreshold">Expand threshold<span class="help-icon" title="Second pass (optional): after choosing seed boxes, we run a refine/expand pass from those boxes.&#10;This threshold controls how strict that refine pass is.&#10;Higher = fewer but cleaner results.">?</span></label>
			                                <input type="number" id="agentExpandThreshold" step="0.01" min="0" max="1" value="0.3" />
			                            </div>
				                            <div id="agentMaxVisualSeedsField">
				                                <label for="agentMaxVisualSeeds">Max visual seeds<span class="help-icon" title="How many seed boxes per image we select for the refine/expand pass.&#10;0 = skip refine/expand (only use seed text detections).&#10;Higher = more compute; can increase coverage on hard objects.">?</span></label>
				                                <input type="number" id="agentMaxVisualSeeds" min="0" value="25" />
				                            </div>
			                            <div>
			                                <label for="agentSeedDedupeIou">Seed de-dupe overlap (IoU)<span class="help-icon" title="After the seed text pass, we remove near-duplicate seed boxes.&#10;If two seed boxes overlap more than this IoU, we keep one.&#10;Higher (e.g., 0.9) only removes almost-identical boxes.&#10;Lower removes more overlaps (can collapse crowded scenes).">?</span></label>
			                                <input type="number" id="agentSeedDedupeIou" step="0.05" min="0" max="1" value="0.9" />
			                            </div>
			                            <div>
			                                <label for="agentDedupeIou">Output de-dupe overlap (IoU)<span class="help-icon" title="Final de-duplication across all detections the recipe outputs.&#10;If two output boxes overlap more than this IoU, we keep the best one.&#10;Lower = more aggressive de-dup (fewer duplicates; may drop close objects).&#10;Higher = allows more overlaps (risk duplicate labels).">?</span></label>
			                                <input type="number" id="agentDedupeIou" step="0.05" min="0" max="1" value="0.5" />
			                            </div>
				                            <div id="agentSimilarityScoreField">
				                                <label for="agentSimilarityScore">Image similarity filter (CLIP)<span class="help-icon" title="Crop-bank mode only.&#10;For each candidate box, we crop the image and compare it to saved example crops using CLIP.&#10;We keep boxes with similarity ‚â• this number.&#10;Higher = stricter (fewer false positives, can miss true positives).&#10;Ignored when a pretrained CLIP head is selected.">?</span></label>
				                                <input type="number" id="agentSimilarityScore" step="0.01" min="0" max="1" value="0.25" />
				                            </div>
			                        </div>
			                        <div class="training-grid">
			                            <div>
			                                <label for="agentMaskThreshold">Mask threshold<span class="help-icon" title="Polygons only.&#10;Controls how the mask is turned into a polygon (higher trims uncertain pixels).&#10;Higher = tighter/smaller shapes; lower = looser/bigger shapes.">?</span></label>
			                                <input type="number" id="agentMaskThreshold" step="0.05" min="0" max="1" value="0.5" />
			                            </div>
			                            <div>
			                                <label for="agentMaxResults">Max dets/image<span class="help-icon" title="Safety cap: maximum number of detections kept per image per prompt/pass.&#10;Prevents runaway outputs on noisy prompts.&#10;If too low, crowded images may be clipped.">?</span></label>
			                                <input type="number" id="agentMaxResults" min="1" value="1000" />
			                            </div>
			                            <div>
			                                <label for="agentMinSize">Min area (px¬≤)<span class="help-icon" title="Drop detections smaller than this pixel area.&#10;Useful to remove tiny specks.&#10;0 keeps all (including very small objects).">?</span></label>
			                                <input type="number" id="agentMinSize" min="0" value="0" />
			                            </div>
			                            <div>
			                                <label for="agentSimplifyEps">Simplify Œµ<span class="help-icon" title="Polygons only.&#10;Douglas‚ÄìPeucker simplification in pixels.&#10;Higher = fewer points (simpler shapes).&#10;0 = no simplification (max detail).">?</span></label>
			                                <input type="number" id="agentSimplifyEps" min="0" step="0.1" value="0.0" />
			                            </div>
			                        </div>
				                        <div class="training-grid">
					                            <div>
					                                <label for="agentClipHeadSelect">Pretrained CLIP head (required)<span class="help-icon" title="Required for recipe mining.&#10;Agent Mining uses a pretrained CLIP classifier head (trained earlier on your dataset) to filter detections and tune ‚Äúcleanliness‚Äù (precision).&#10;The head (and its tuned thresholds) is embedded into saved recipe zips (portable).&#10;Tip: you can also tighten CLIP thresholds later at inference time per cascade step (cumulative).">?</span></label>
						                                <select id="agentClipHeadSelect">
						                                    <option value="">Select a CLIP head‚Ä¶</option>
						                                </select>
						                                <div class="training-help">Required. Select a classifier under <code>uploads/classifiers/</code> (trained via the CLIP tab or uploaded). Recommended: keep auto-tune on and use a high target precision for clean recipes.</div>
						                            </div>
					                            <div class="training-field training-field--inline">
					                                <label for="agentClipHeadAutoTune">Auto-tune thresholds<span class="help-icon" title="CLIP head mode only (recommended).&#10;Automatically searches per-class CLIP head thresholds on the validation split so recipes are clean (few false positives).&#10;These tuned thresholds are baked into the saved recipe ZIP and used whenever the recipe runs.&#10;Turn this off only if you want to force fixed thresholds for all classes.">?</span></label>
					                                <input type="checkbox" id="agentClipHeadAutoTune" checked />
					                            </div>
					                            <div>
					                                <label for="agentClipHeadTargetPrecision">Recipe cleanliness (precision target)<span class="help-icon" title="CLIP head auto-tune only.&#10;&#10;What ‚Äúprecision‚Äù means here:&#10;- Precision = matches / (matches + false positives) on the validation split.&#10;- A detection is a ‚Äúmatch‚Äù if it overlaps an unmatched ground-truth box for that class with IoU ‚â• the job‚Äôs IoU threshold.&#10;- Detections that match no ground-truth box count as false positives.&#10;- Extra detections on the same object are counted as ‚ÄúDuplicates‚Äù (shown separately).&#10;&#10;How to tune:&#10;- Move toward ‚ÄúCleaner‚Äù to reduce false positives (but you may miss objects).&#10;- Typical clean recipes: 0.90‚Äì0.98.&#10;&#10;This sets the per-class CLIP thresholds that get baked into the saved recipe ZIP.">?</span></label>
					                                <div class="training-help" style="display: flex; align-items: center; gap: 10px; margin-top: 4px;">
					                                    <span style="white-space: nowrap;" title="Lower target precision = more detections, but noisier (more false positives).">More detections (noisier)</span>
					                                    <input type="range" id="agentClipHeadTargetPrecision" min="0.5" max="0.99" step="0.01" value="0.9" style="flex: 1;" />
					                                    <span id="agentClipHeadTargetPrecisionValue" style="min-width: 3.5em; text-align: right;">0.90</span>
					                                    <span style="white-space: nowrap;" title="Higher target precision = cleaner results (fewer false positives), but can miss objects.">Cleaner (fewer false positives)</span>
					                                </div>
					                                <details class="training-help" style="margin-top: 6px;">
					                                    <summary style="cursor: pointer;">Advanced</summary>
					                                    <div class="training-help" style="margin-top: 6px;">
					                                        <label style="display: inline-flex; align-items: center; gap: 8px;" title="Enable precision targets below 0.50 (0.10‚Äì0.99). This is mainly useful for recall-first debugging and usually produces very noisy recipes.">
					                                            <input type="checkbox" id="agentClipHeadAllowLowPrecision" />
					                                            Allow very low precision targets (0.10‚Äì0.99)
					                                        </label>
					                                        <div class="training-help" style="margin-top: 6px; color: #b91c1c;">
					                                            Warning: targets below 0.50 often create extremely noisy recipes (huge false positives) and can increase mining time + output size. Use only for recall-first experiments / debugging.
					                                        </div>
					                                    </div>
					                                </details>
					                            </div>
				                        </div>
				                        <div class="training-grid">
					                            <div>
					                                <label for="agentClipHeadMinProb">Manual head min prob<span class="help-icon" title="CLIP head manual mode only (auto-tune OFF).&#10;Minimum probability required for the target class.&#10;Higher = fewer detections, cleaner results; lower = more detections (more false positives).&#10;This value is baked into the saved recipe ZIP.">?</span></label>
					                                <input type="number" id="agentClipHeadMinProb" step="0.05" min="0" max="1" value="0.5" />
					                            </div>
				                            <div>
				                                <label for="agentClipHeadMargin">Manual head margin<span class="help-icon" title="CLIP head manual mode only (auto-tune OFF).&#10;Require the target class probability to beat the best other class by this margin.&#10;0 disables the margin check.&#10;Higher can reduce confusion between similar classes.&#10;This value is baked into the saved recipe ZIP.">?</span></label>
				                                <input type="number" id="agentClipHeadMargin" step="0.05" min="0" max="1" value="0.0" />
				                            </div>
				                        </div>
				                        <details class="training-card" style="margin-bottom: 12px;">
				                            <summary style="cursor: pointer; font-weight: 600;">Legacy crop-bank settings (deprecated)</summary>
				                            <div class="training-card__body">
				                                <div class="training-help" style="color: #64748b;">
				                                    New recipes always embed a pretrained CLIP head and do not use crop banks. These settings are kept only for legacy recipe zips and are ignored during mining.
				                                </div>
				                                <div class="training-grid" style="margin-top: 10px;">
				                                    <div>
				                                        <label for="agentExemplars">Example crops/class<span class="help-icon" title="Crop-bank mode only.&#10;How many positive example cut-outs we save into the recipe zip for each class.&#10;More examples can reduce false positives but increases mining time and zip size.&#10;Ignored when a pretrained CLIP head is selected.">?</span></label>
				                                        <input type="number" id="agentExemplars" min="0" value="20" />
				                                    </div>
				                                    <div>
				                                        <label for="agentExemplarPoolValue">Candidate pool size<span class="help-icon" title="Crop-bank mode only.&#10;We build a pool of labeled boxes from the training split, then choose diverse examples from that pool.&#10;This controls how many candidate boxes we consider before picking the final crops.&#10;Bigger = more diversity, more CLIP work.">?</span></label>
				                                        <input type="number" id="agentExemplarPoolValue" min="1" value="25" />
				                                    </div>
				                                    <div>
				                                        <label for="agentExemplarPoolMode">Pool mode<span class="help-icon" title="How to interpret ‚Äúcandidate pool size‚Äù.&#10;% mode: pool = that percent of all labeled boxes for the class.&#10;Count mode: pool = fixed number of boxes.&#10;Only used when selecting example crops.">?</span></label>
				                                        <select id="agentExemplarPoolMode">
				                                            <option value="percent" selected>% of labeled objects</option>
				                                            <option value="count">Fixed count</option>
				                                        </select>
				                                    </div>
				                                    <div class="training-field training-field--inline">
				                                        <label for="agentClusterExemplars">Diversify crops<span class="help-icon" title="Crop-bank mode only.&#10;Try to pick different-looking example crops (sizes/angles) instead of many near-identical ones.&#10;Recommended on.&#10;Ignored when a pretrained CLIP head is selected.">?</span></label>
				                                        <input type="checkbox" id="agentClusterExemplars" checked />
				                                    </div>
				                                    <div class="training-field training-field--inline">
				                                        <label for="agentClipGuard">Use CLIP filter<span class="help-icon" title="Crop-bank mode only.&#10;Use CLIP image similarity to keep detections that look like your saved example crops, and (optionally) suppress detections that look like negatives.&#10;Recommended on.&#10;Ignored when a pretrained CLIP head is selected.">?</span></label>
				                                        <input type="checkbox" id="agentClipGuard" checked />
				                                    </div>
				                                </div>
				                                <div class="training-grid" style="margin-top: 10px;">
				                                    <div class="training-field training-field--inline">
				                                        <label for="agentUseNegExemplars">Use negative crops<span class="help-icon" title="Crop-bank mode only.&#10;Helps reduce false positives (e.g., scooters mistaken as cars) by learning what ‚Äúnot this class‚Äù looks like from other classes.&#10;Ignored when a pretrained CLIP head is selected.">?</span></label>
				                                        <input type="checkbox" id="agentUseNegExemplars" checked />
				                                    </div>
				                                    <div>
				                                        <label for="agentMaxNegExemplars">Max negatives/class<span class="help-icon" title="Crop-bank mode only.&#10;How many negative example crops we collect from other classes.&#10;Negatives suppress detections that look like another class.&#10;More negatives can reduce false positives but costs time and increases recipe zip size.&#10;Ignored when a pretrained CLIP head is selected.">?</span></label>
				                                        <input type="number" id="agentMaxNegExemplars" min="0" max="256" value="25" />
				                                    </div>
				                                    <div>
				                                        <label for="agentNegStrength">Negative filter strength<span class="help-icon" title="Crop-bank mode only.&#10;How strongly negative crops suppress detections.&#10;0 disables negatives.&#10;Higher = more suppression (fewer false positives; can also remove true positives).&#10;Ignored when a pretrained CLIP head is selected.">?</span></label>
				                                        <input type="number" id="agentNegStrength" step="0.05" min="0" max="5" value="0.5" />
				                                    </div>
				                                </div>
				                            </div>
				                        </details>
		                        <div class="two-col-grid">
		                            <div>
		                                <label for="agentClasses">Class IDs (optional)<span class="help-icon" title="Optional: restrict mining to specific class IDs. Leave blank to mine all classes.&#10;Example: 1,2,5">?</span></label>
		                                <input type="text" id="agentClasses" placeholder="blank = all" />
		                            </div>
		                            <div>
		                                <label for="agentQwenMaxPrompts">Extra text prompts/class<span class="help-icon" title="Ask GPT-OSS for additional words to try for each class (in addition to the class name).&#10;0 = only use the class name; higher values try more alternative phrases.">?</span></label>
		                                <input type="number" id="agentQwenMaxPrompts" min="0" max="20" value="0" title="0 = only use the class name; higher values try more alternative phrases." />
		                            </div>
		                            <div>
		                                <label for="agentPromptReasoning">GPT-OSS reasoning<span class="help-icon" title="Controls how much the model ‚Äúthinks‚Äù before proposing extra words.&#10;Higher can be slower and less stable.&#10;We recommend ‚Äúnone‚Äù for now.">?</span></label>
		                                <select id="agentPromptReasoning" title="How much GPT-OSS ‚Äúthinks‚Äù before proposing extra words. Higher uses more time and can be less stable.">
		                                    <option value="none" selected>none (fastest)</option>
	                                    <option value="low">low</option>
	                                    <option value="medium">medium</option>
	                                    <option value="high">high</option>
	                                </select>
	                                <div class="training-help" style="color: #b91c1c;">Only ‚Äúnone‚Äù is stable right now.</div>
		                            </div>
		                            <div>
		                                <label for="agentPromptMaxTokens">GPT-OSS max tokens<span class="help-icon" title="Maximum length of the model output when proposing extra prompts.&#10;Higher can be slower and use more GPU memory.&#10;If you see truncated lists, increase this.">?</span></label>
		                                <input type="number" id="agentPromptMaxTokens" min="16" max="400" value="160" title="Maximum length of GPT-OSS output when proposing extra words." />
		                            </div>
		                            <div class="training-field training-field--inline training-field--inline-tight">
		                                <label for="agentTestMode">Test mode<span class="help-icon" title="Runs on a small number of images so you can verify everything works (fast).">?</span></label>
		                                <input type="checkbox" id="agentTestMode" checked />
			                            </div>
		                            <div>
		                                <label for="agentTrainLimit">Test train imgs<span class="help-icon" title="Test mode only.&#10;Number of training images sampled for mining.&#10;Smaller = faster smoke test.">?</span></label>
		                                <input type="number" id="agentTrainLimit" min="1" value="10" />
	                            </div>
	                            <div>
	                                <label for="agentValLimit">Test val imgs<span class="help-icon" title="Test mode only.&#10;Number of validation images sampled for scoring.&#10;Smaller = faster smoke test; larger = more reliable scores.">?</span></label>
	                                <input type="number" id="agentValLimit" min="1" value="10" />
	                            </div>
		                        </div>
				                        <div class="training-field">
				                            <label for="agentExtraPrompts">Extra prompts (optional)<span class="help-icon" title="Optional JSON dict of extra prompts to try.&#10;Keys: class names.&#10;Values: lists of strings.&#10;Special key ‚Äú__base__‚Äù applies to all classes.&#10;These are appended to GPT-OSS suggestions.">?</span></label>
				                            <textarea id="agentExtraPrompts" rows="8" style="min-width: 480px;" placeholder='{"__base__": ["object", "small object"], "light_vehicle": ["car", "sedan"], "person": ["human", "pedestrian"]}' title="Optional JSON dict of class_name -> list of extra prompt phrases. Special key &quot;__base__&quot; applies to all classes. Example: {&quot;__base__&quot;: [&quot;object&quot;, &quot;small object&quot;], &quot;light_vehicle&quot;: [&quot;car&quot;, &quot;sedan&quot;]}"></textarea>
				                            <div class="training-help">Optional extra words/phrases to try. They are appended to GPT-OSS suggestions. Use &quot;__base__&quot; to add prompts for all classes.</div>
				                            <div class="training-help" id="agentExtraPromptsParseStatus" aria-live="polite"></div>
				                        </div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="agentRunBtn">Run SAM3 Recipe Mining</button>
                            <button type="button" class="training-button secondary" id="agentRefreshBtn">Refresh latest</button>
	                            <button type="button" class="training-button danger" id="agentCancelBtn">Cancel job</button>
	                        </div>
	                        <div class="training-message" id="agentStatus" role="status">Idle</div>
	                        <div class="training-help" id="agentCacheSize">Cache: ‚Ä¶</div>
	                        <div class="training-progress" aria-hidden="true">
	                            <div id="agentProgressFill" class="training-progress-fill"></div>
	                        </div>
	                        <div class="training-help" id="agentProgressText" aria-live="polite"></div>
	                    </section>
	                    <section class="training-status-section">
			                        <details class="training-card">
			                            <summary style="font-weight: 600; cursor: pointer;">Recipe mining quick reference</summary>
			                            <div class="training-card__body">
			                                <p><strong>Workflow:</strong> Train a CLIP head ‚Üí run mining (test mode first) ‚Üí review per-class results ‚Üí save recipes ‚Üí apply (and optionally chain) recipes from the labeling tab.</p>
			                                <p class="training-help" style="margin-top: 10px;"><strong>How to read scores:</strong></p>
			                                <ul class="training-help" style="margin-top: 6px;">
			                                    <li><strong>Coverage</strong> = matches / GT objects (higher is better).</li>
			                                    <li><strong>Precision</strong> = matches / (matches + false positives) (higher is cleaner).</li>
			                                    <li><strong>FPs</strong> = detections that match no ground-truth box for that class (lower is better).</li>
			                                    <li><strong>Duplicates</strong> = extra detections on the same GT object (lower is better; tracked separately).</li>
			                                    <li><strong>Det rate</strong> = % of validation images that produced at least one detection (useful ‚Äúis anything happening?‚Äù sanity check).</li>
			                                </ul>
			                                <p class="training-help" style="margin-top: 10px;"><strong>If results look ‚Äúdead‚Äù:</strong> verify your CLIP head includes the dataset class names, and try lowering ‚ÄúSeed curve floor‚Äù or temporarily lowering ‚ÄúRecipe cleanliness‚Äù to confirm SAM3 is producing seeds before tightening filters.</p>
			                            </div>
			                        </details>
                        <h3>Progress & Logs</h3>
                        <div class="training-help">Live status and logs for the latest SAM3 Recipe Mining job.</div>
                        <div class="training-actions">
                            <button type="button" class="training-button danger" id="agentPurgeCacheBtn" title="Purge cached detections to free disk space.">Purge cache</button>
                        </div>
                        <div class="training-logs" style="margin-bottom: 12px;">
                            <div id="agentLogs" class="training-log" style="max-height: 220px;"></div>
                        </div>
	                        <div class="training-divider"></div>
	                        <h3>Results</h3>
	                        <div class="training-help">Coverage = % of ground-truth objects found. FPs = extra detections that shouldn‚Äôt be there. Higher coverage and lower FPs is better.</div>
	                        <div id="agentResults"></div>
		                        <div class="training-divider"></div>
		                        <h3>Saved Recipes</h3>
		                        <div class="training-grid agent-recipe-controls">
		                            <div class="training-field">
		                                <label for="agentRecipeSelect">Saved recipes<span class="help-icon" title="Saved recipe zips on the backend. Use Refresh if you just mined or imported a recipe.">?</span></label>
		                                <select id="agentRecipeSelect"></select>
		                            </div>
		                        </div>
		                        <div class="training-grid training-grid--buttons agent-recipe-actions">
		                            <button type="button" class="training-button" id="agentRecipeRefresh">Refresh list</button>
		                            <button type="button" class="training-button secondary" id="agentRecipeDownload">Download zip</button>
		                            <button type="button" class="training-button danger" id="agentRecipeDelete">Delete recipe</button>
		                        </div>
		                        <div class="training-field">
			                            <label for="agentRecipeFile">Import recipe zip<span class="help-icon" title="Import a recipe zip from another machine or previous run.&#10;New zips contain recipe.json + embedded CLIP head artifacts (clip_head/). Legacy zips may also include example crops.">?</span></label>
		                            <div class="training-grid training-grid--buttons">
		                                <input type="file" id="agentRecipeFile" accept=".zip" />
		                                <button type="button" class="training-button secondary" id="agentRecipeImport">Import</button>
		                            </div>
			                            <div class="training-help">Packages include recipe.json plus embedded CLIP head artifacts (and legacy crops when present) so they are portable between machines.</div>
		                        </div>
		                        <div class="training-help">Refresh, download, import, or delete saved recipes. Apply recipes to images from the labeling tab.</div>
		                    </section>
		                </div>
		            </div>
            <div class="tab-panel" id="tabPromptHelper" data-tab-panel="prompt-helper">
                <div class="training-wrapper">
                    <section class="training-form-section">
                        <h2>SAM3 Vocabulary Explorer</h2>
                        <p class="training-help">Generate alternative text prompts for your dataset classes and score them with SAM3 on a sampled subset to find the best wording.</p>
                        <div class="training-field">
                            <label>Dataset</label>
                            <div class="sam3-dataset-row">
                                <select id="promptHelperDatasetSelect"></select>
                                <button type="button" class="training-button" id="promptHelperDatasetRefresh">Refresh</button>
                            </div>
                            <div class="training-help" id="promptHelperDatasetSummary">Pick a converted SAM3/Qwen dataset.</div>
                        </div>
                        <div class="training-grid">
                            <div>
                                <label for="promptHelperSampleSize">Images per class</label>
                                <input type="number" id="promptHelperSampleSize" min="1" value="20" title="How many images per class to sample for scoring prompts." />
                            </div>
                            <div>
                                <label for="promptHelperMaxSynonyms">Max extra prompts per class</label>
                                <input type="number" id="promptHelperMaxSynonyms" min="0" max="10" value="3" title="Number of alternative phrases to try (original name is always included)." />
                            </div>
                            <div>
                                <label for="promptHelperScoreThresh">Score threshold</label>
                                <input type="number" id="promptHelperScoreThresh" step="0.01" min="0" max="1" value="0.2" title="Drop detections below this score when scoring prompts." />
                            </div>
                            <div>
                                <label for="promptHelperMaxDets">Max detections/image</label>
                                <input type="number" id="promptHelperMaxDets" min="1" value="100" title="Keep only the top N detections per image when scoring prompts." />
                            </div>
                            <div>
                                <label for="promptHelperIouThresh">IoU threshold</label>
                                <input type="number" id="promptHelperIouThresh" step="0.05" min="0" max="1" value="0.5" title="IoU threshold for a match when comparing SAM3 boxes to ground truth." />
                            </div>
                            <div>
                                <label for="promptHelperSeed">Random seed</label>
                                <input type="number" id="promptHelperSeed" value="42" title="Controls image sampling so reruns are reproducible." />
                            </div>
                        </div>
                        <div class="training-toggle-row">
                            <label title="Use Qwen to brainstorm human-friendly phrases. Turn off to only use your raw class names + simple cleaned variants."><input type="checkbox" id="promptHelperUseQwen" checked /> Use Qwen to propose phrases</label>
                        </div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="promptHelperGenerateBtn">Generate prompts</button>
                            <button type="button" class="training-button" id="promptHelperEvaluateBtn" disabled>Evaluate with SAM3</button>
                        </div>
                        <div class="training-field">
                            <label>Prompt presets</label>
                            <div class="training-grid">
                                <div>
                                    <input type="text" id="promptHelperPresetName" placeholder="Preset name (optional)" />
                                    <button type="button" class="training-button" id="promptHelperPresetSave">Save prompts</button>
                                </div>
                                <div>
                                    <select id="promptHelperPresetSelect"></select>
                                    <button type="button" class="training-button" id="promptHelperPresetLoad">Load preset</button>
                                </div>
                            </div>
                            <div class="training-help">Save/load prompt sets for reuse with this dataset.</div>
                        </div>
                        <div class="training-note">
                            <strong>Flow:</strong> generate/edit prompts, then run evaluation. Scoring samples images per class, runs SAM3 text prompts one at a time, and reports precision/recall-style stats against ground truth boxes.
                        </div>
                        <div class="training-message" id="promptHelperMessage" role="status"></div>
                        <div class="training-divider"></div>
                        <h3>Prompt Search (beta)</h3>
                        <p class="training-help">Use your edited prompts above, then run a targeted search that samples positives and negatives to find the safest wording (prioritizes recall but penalizes prompts below a precision floor).</p>
                        <div class="training-field">
                            <label for="promptSearchClassSelect">Class to search</label>
                            <select id="promptSearchClassSelect" style="width: 100%; min-width: 200px;"></select>
                            <div class="training-help">Default is all classes. Pick one to iterate faster on a single label (generate prompts first to populate).</div>
                        </div>
                        <div class="training-grid">
                            <div>
                                <label for="promptSearchSampleSize">Positive images/class</label>
                                <input type="number" id="promptSearchSampleSize" min="1" value="20" title="How many images that contain the class to sample." />
                            </div>
                            <div>
                                <label for="promptSearchNegatives">Negative images/class</label>
                                <input type="number" id="promptSearchNegatives" min="0" value="20" title="How many images without the class to include (catches false positives)." />
                            </div>
                            <div>
                                <label for="promptSearchPrecisionFloor">Precision floor</label>
                                <input type="number" id="promptSearchPrecisionFloor" step="0.05" min="0" max="1" value="0.9" title="Prompts below this precision get penalized heavily in the search score." />
                            </div>
                            <div>
                                <label for="promptSearchScoreThresh">Score threshold</label>
                                <input type="number" id="promptSearchScoreThresh" step="0.01" min="0" max="1" value="0.2" title="Drop detections below this score when scoring prompts." />
                            </div>
                            <div>
                                <label for="promptSearchMaxDets">Max detections/image</label>
                                <input type="number" id="promptSearchMaxDets" min="1" value="100" title="Keep only the top N detections per image when searching." />
                            </div>
                            <div>
                                <label for="promptSearchIouThresh">IoU threshold</label>
                                <input type="number" id="promptSearchIouThresh" step="0.05" min="0" max="1" value="0.5" title="IoU threshold for a match when comparing SAM3 boxes to ground truth." />
                            </div>
                            <div>
                                <label for="promptSearchSeed">Random seed</label>
                                <input type="number" id="promptSearchSeed" value="42" title="Controls sampling so reruns are reproducible." />
                            </div>
                        </div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="promptSearchRunBtn">Search best prompts</button>
                        </div>
                        <div class="training-message" id="promptSearchMessage" role="status"></div>
                        <div class="training-note">
                            Uses the current prompt list above (including any edits or presets). Positive samples only include images with the class; negatives are sampled from images without the class to reveal false positives.
                        </div>
                        <div class="training-divider"></div>
                        <h3>Prompt Recipe Mining</h3>
                        <p class="training-help">Find an ordered set of prompts + thresholds that covers all GTs for a class with zero (or minimal) false positives.</p>
                        <div class="training-field">
                            <label for="promptRecipeClassSelect">Class to target</label>
                            <select id="promptRecipeClassSelect"></select>
                            <div class="training-help">Runs on a deterministic sample for this class.</div>
                        </div>
                        <div class="training-grid">
                            <div>
                                <label for="promptRecipeSampleSize">Positive images</label>
                                <input type="number" id="promptRecipeSampleSize" min="1" value="30" title="How many images containing the class to sample." />
                            </div>
                            <div>
                                <label for="promptRecipeNegatives">Negative images</label>
                                <input type="number" id="promptRecipeNegatives" min="0" value="10" title="How many images without the class to include for FP checks." />
                            </div>
                            <div>
                                <label for="promptRecipeThresholds">Thresholds (comma sep)</label>
                                <input type="text" id="promptRecipeThresholds" value="0.2,0.3,0.4" title="Score thresholds to try for each prompt." />
                            </div>
                            <div>
                                <label for="promptRecipeMaxDets">Max detections/image</label>
                                <input type="number" id="promptRecipeMaxDets" min="1" value="100" />
                            </div>
                            <div>
                                <label for="promptRecipeIouThresh">IoU threshold</label>
                                <input type="number" id="promptRecipeIouThresh" step="0.05" min="0" max="1" value="0.5" />
                            </div>
                            <div>
                                <label for="promptRecipeSeed">Random seed</label>
                                <input type="number" id="promptRecipeSeed" value="42" />
                            </div>
                            <div>
                                <label for="promptRecipeExpandCount">Qwen expand (max)</label>
                                <input type="number" id="promptRecipeExpandCount" min="0" max="50" value="10" title="How many new prompts to request from Qwen." />
                            </div>
                        </div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="promptRecipeExpandBtn">Expand prompts with Qwen</button>
                            <button type="button" class="training-button" id="promptRecipeRunBtn">Mine recipe</button>
                            <button type="button" class="training-button secondary" id="promptRecipeApplyBtn" title="Copy the last mined recipe prompts/thresholds into the prompt editor so you can save as a preset.">
                                Apply last recipe to prompts
                            </button>
                        </div>
                        <div class="training-message" id="promptRecipeMessage" role="status"></div>
                        <div class="training-note">Uses prompts for the selected class (edit in the list above). Runs on positives + optional negatives; tries each threshold per prompt and proposes an ordered recipe.</div>
                    </section>
                    <section class="training-status-section">
                        <h2>Prompts & Results</h2>
                        <div class="training-help" id="promptHelperStatus">Idle</div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="promptHelperApplyBtn" disabled>Apply top prompts to SAM3 training</button>
                        </div>
                        <div class="training-subsection">
                            <h4>Prompts to evaluate</h4>
                            <div id="promptHelperPrompts"></div>
                        </div>
                        <div class="training-subsection">
                            <h4>Evaluation log</h4>
                            <div id="promptHelperLogs" class="training-log"></div>
                        </div>
                        <div class="training-summary" id="promptHelperSummary"></div>
                        <div id="promptHelperResults" class="training-history"></div>
                        <div class="training-subsection">
                            <h4>Prompt Search (beta)</h4>
                            <div class="training-help" id="promptSearchStatus">Idle</div>
                            <div class="training-subsection">
                                <h5>Search log</h5>
                                <div id="promptSearchLogs" class="training-log"></div>
                            </div>
                            <div id="promptSearchResults" class="training-history"></div>
                            <div class="training-subsection">
                                <h5>Prompt Recipe Mining</h5>
                                <div class="training-help" id="promptRecipeStatus">Idle</div>
                                <div class="training-subsection">
                                    <h5>Recipe log</h5>
                                    <div id="promptRecipeLogs" class="training-log"></div>
                                </div>
                                <div id="promptRecipeResults" class="training-history"></div>
                            </div>
                        </div>
                    </section>
                </div>
            </div>
            <div class="tab-panel" id="tabDatasets" data-tab-panel="datasets">
                <div class="training-wrapper dataset-wrapper">
                    <div class="two-col-grid">
                        <div class="training-card">
                            <div class="training-card__header">
                                <div class="training-card__title">Dataset Management</div>
                                <div class="training-help">Upload YOLO/YOLO-seg datasets once and reuse them across CLIP, Qwen, and SAM3 training.</div>
                            </div>
	                        <div class="training-card__body dataset-manager">
	                                <div class="training-subsection">
	                                    <div class="training-subsection__title">Upload current labeling session</div>
	                                    <div class="training-help">Packages the images + labels you have loaded in the labeling tab and uploads them as a YOLO / YOLO-seg dataset (train split) so you can reuse it for CLIP, Qwen, and SAM3 flows.</div>
	                                    <div class="training-grid">
	                                        <div>
	                                            <label for="datasetUploadCurrentName">Dataset name (optional)</label>
	                                            <input type="text" id="datasetUploadCurrentName" placeholder="Defaults to labeling_session" />
	                                        </div>
	                                        <div>
	                                            <label for="datasetUploadCurrentContext">Dataset context (optional)</label>
	                                            <input type="text" id="datasetUploadCurrentContext" placeholder="Optional: used for Qwen instruction text" />
	                                        </div>
	                                    </div>
	                                    <div class="training-actions">
	                                        <button type="button" id="datasetUploadCurrentBtn" class="training-button">Save as YOLO dataset</button>
	                                        <div id="datasetUploadCurrentSummary" class="training-help"></div>
	                                    </div>
	                                </div>
                                <div class="training-subsection">
                                    <div class="training-subsection__title">Upload YOLO/YOLO-seg zip</div>
                                    <label for="datasetUploadFile">Zip file (root: labelmap.txt + images + labels)</label>
                                    <div class="training-help">
                                        Expect exactly: labelmap.txt (class names, one per line) at the root, plus images/ and labels/ in YOLO format. If you already have train/val splits, include them; otherwise a single images/labels pair is fine.
                                    </div>
                                    <input type="file" id="datasetUploadFile" accept=".zip" />
                                    <div class="training-grid">
                                        <div>
                                            <label for="datasetUploadName">Dataset name (optional)</label>
                                            <input type="text" id="datasetUploadName" placeholder="Defaults to zip name" />
                                        </div>
                                        <div>
                                            <label for="datasetUploadType">Dataset type</label>
                                            <select id="datasetUploadType">
                                                <option value="">Auto-detect</option>
                                                <option value="bbox">BBox (YOLO)</option>
                                                <option value="seg">Polygon masks (YOLO-seg)</option>
                                            </select>
                                        </div>
                                    </div>
                                    <div class="training-actions">
                                        <button type="button" id="datasetUploadBtn" class="training-button">Upload dataset</button>
                                        <button type="button" id="datasetListRefresh" class="training-button training-button--ghost">Refresh list</button>
                                    </div>
                                    <div id="datasetUploadMessage" class="training-message"></div>
                                </div>
                            </div>
                        </div>
                        <div class="training-card">
                            <div class="training-card__header">
                                <div class="training-card__title">Datasets on disk</div>
                                <div class="training-help">All cached datasets discovered on disk. Use the refresh button after adding or removing datasets manually.</div>
                            </div>
                            <div class="training-card__body">
                                <div class="training-actions">
                                    <button type="button" id="datasetListRefreshTop" class="training-button">Refresh list</button>
                                </div>
                                <div id="datasetList" class="training-history"></div>
                            </div>
                        </div>
                    </div>
                    <div class="training-card">
                        <div class="training-card__header">
                            <div class="training-card__title">Build segmentation datasets from bbox datasets</div>
                            <div class="training-help">Creates YOLO polygon masks using SAM (bbox ‚Üí polygon). Originals stay untouched.</div>
                        </div>
                        <div class="training-card__body">
                            <label for="segBuilderDatasetSelect">Source bbox dataset</label>
                            <select id="segBuilderDatasetSelect"></select>
                            <div id="segBuilderDatasetSummary" class="training-help">Pick a dataset to convert.</div>
                            <label for="segBuilderOutputName">Output dataset name</label>
                            <input type="text" id="segBuilderOutputName" placeholder="Defaults to &lt;dataset&gt;_seg" />
                            <label for="segBuilderVariant">SAM variant</label>
                            <select id="segBuilderVariant">
                                <option value="sam3">SAM 3</option>
                                <option value="sam1">SAM 1</option>
                            </select>
                            <div class="training-help">Polygons only (YOLO-seg). Output is kept separate; originals stay untouched.</div>
                            <div class="training-actions">
                                <button type="button" id="segBuilderStartBtn" class="training-button">Start build</button>
                                <button type="button" id="segBuilderRefreshBtn" class="training-button training-button--ghost">Refresh datasets</button>
                            </div>
                            <div id="segBuilderMessage" class="training-message"></div>
                            <div id="segBuilderLog" class="training-log"></div>
                        </div>
                        <div class="training-card__footer">
                            <div class="training-subsection__title">Segmentation build jobs</div>
                            <div id="segBuilderJobs" class="training-history"></div>
                            <div class="training-actions">
                                <button type="button" id="segBuilderJobsRefresh" class="training-button training-button--ghost">Refresh jobs</button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="tab-panel" id="tabSam3PromptModels" data-tab-panel="sam3-prompt-models">
                <div class="training-wrapper single-wide">
                    <section class="training-form-section">
                        <h2>SAM3 Correction Model Selection</h2>
                        <p class="training-help">Pick a locally trained SAM3 checkpoint to use for bbox prompting in the labeling page.</p>
                        <div class="training-field">
                            <label for="sam3PromptModelSelect">Available models</label>
                            <div class="sam3-dataset-row">
                                <select id="sam3PromptModelSelect"></select>
                                <button type="button" class="training-button" id="sam3PromptRefresh">Refresh</button>
                            </div>
                            <div class="training-help" id="sam3PromptModelSummary"></div>
                        </div>
                        <div class="training-actions">
                            <button type="button" class="training-button" id="sam3PromptActivate">Activate model</button>
                        </div>
                        <div class="training-message" id="sam3PromptMessage" role="status"></div>
                    </section>
                </div>
            </div>
            <div class="tab-panel" id="tabActive" data-tab-panel="active">
                <div class="active-wrapper">
                    <section class="active-controls">
                        <h2>Switch Models</h2>
                        <div class="active-message" id="activeMessage" role="status"></div>
                        <p class="active-help">When you activate a trained classifier, the system loads the CLIP backbone saved in its metadata. Use these controls if you need to override the paths manually.</p>
                        <div class="active-field">
                            <label for="activeClipSelect">CLIP Backbone</label>
                            <select id="activeClipSelect"></select>
                            <div class="active-help">CLIP encoder that will run before the logistic regression head.</div>
                        </div>
                        <div class="active-field">
                            <label for="activeClassifierPath">Classifier Path</label>
                            <div class="active-picker">
                                <input type="text" id="activeClassifierPath" placeholder="./my_logreg_model.pkl" />
                                <button type="button" class="training-button" id="activeClassifierBrowse">Browse‚Ä¶</button>
                            </div>
                            <div class="active-help">Choose the logistic-regression `.pkl` returned by training.</div>
                        </div>
                        <div class="active-field">
                            <label for="activeLabelmapPath">Labelmap Path</label>
                            <div class="active-picker">
                                <input type="text" id="activeLabelmapPath" placeholder="Optional" />
                                <button type="button" class="training-button" id="activeLabelmapBrowse">Browse‚Ä¶</button>
                            </div>
                            <div class="active-help">Optional `.pkl`/`.txt` mapping to keep YOLO order.</div>
                        </div>
                        <div class="active-field active-fallback">
                            <h3>Fallback: Dilate Crop on Low Confidence</h3>
                            <p class="active-help">When Auto Class is on, CLIP may occasionally return a low-confidence score for a rough bbox. Enable this fallback to automatically enlarge (dilate) the crop and retry the prediction before surfacing an error.</p>
                            <label class="active-inline">
                                <input type="checkbox" id="useFallbackDilate" name="useFallbackDilate" />
                                Turn on fallback dilation when CLIP probability is below the threshold.
                            </label>
                            <div class="fallback-options">
                                <label for="minProba">Min CLIP confidence (0‚Äì1)</label>
                                <input type="number" id="minProba" name="minProba" min="0" max="1" step="0.05" value="0.55" />
                                <label for="dilateRatio">Dilate ratio</label>
                                <input type="number" id="dilateRatio" name="dilateRatio" min="0" max="1" step="0.05" value="0.10" />
                            </div>
                            <p class="active-help">Example: a 0.55 threshold + 0.10 ratio expands each low-confidence bbox by 10% in every direction before rerunning CLIP. Leave disabled if you prefer to review low-confidence crops manually.</p>
                        </div>
                        <div class="active-buttons">
                            <button type="button" class="training-button" id="activateLatestModelBtn" disabled>Activate Latest Training Run</button>
                            <button type="button" class="training-button" id="applyActiveModelBtn">Apply Changes</button>
                            <button type="button" class="training-button" id="refreshActiveModelBtn">Refresh</button>
                        </div>
                        <input type="file" id="activeClassifierUpload" accept=".pkl" class="file-input-hidden" />
                        <input type="file" id="activeLabelmapUpload" accept=".pkl,.txt" class="file-input-hidden" />
                    </section>
                </div>
            </div>
            <div class="tab-panel" id="tabQwen" data-tab-panel="qwen">
                <div class="qwen-models-wrapper">
                    <section class="qwen-models-list">
                        <h2>Qwen Models</h2>
                        <p class="qwen-models-help">Pick which fine-tuned adapters should power the Assist panel. Each run stores the exact prompts, context, and class list it learned from.</p>
                        <div class="qwen-model-status" id="qwenModelStatus">Loading‚Ä¶</div>
                        <div class="qwen-model-toolbar">
                            <button type="button" class="training-button" id="qwenModelRefreshBtn">Refresh</button>
                        </div>
                        <div class="qwen-model-grid" id="qwenModelList"></div>
                    </section>
                    <section class="qwen-model-details">
                        <h2>Active Model Details</h2>
                        <div id="qwenModelDetails" class="qwen-model-details__body"></div>
                    </section>
                </div>
            </div>
            <div class="tab-panel" id="tabPredictors" data-tab-panel="predictors">
                <div class="predictor-panel">
                    <div class="predictor-card">
                        <h2>Predictor Budget</h2>
                        <p>Choose how many SAM predictors stay loaded so you can flip between images without delays.</p>
                        <div class="predictor-control">
                            <label for="predictorCount">Concurrent predictors</label>
                            <div class="predictor-control-inline">
                                <input type="number" id="predictorCount" min="1" max="3" value="3" />
                                <button type="button" class="training-button" id="predictorApply">Apply</button>
                            </div>
                        </div>
                        <div class="predictor-message" id="predictorMessage"></div>
                    </div>
                    <div class="predictor-grid">
                        <div class="predictor-stat">
                            <span>Active slots</span>
                            <strong id="predictorActiveCount">--</strong>
                        </div>
                        <div class="predictor-stat">
                            <span>Loaded predictors</span>
                            <strong id="predictorLoadedCount">--</strong>
                        </div>
                        <div class="predictor-stat">
                            <span>Predictor RAM</span>
                            <strong id="predictorImageRam">--</strong>
                        </div>
                        <div class="predictor-stat">
                            <span>Process RAM</span>
                            <strong id="predictorProcessRam">--</strong>
                        </div>
                        <div class="predictor-stat">
                            <span>GPU free / total (fallback: system)</span>
                            <strong id="predictorSystemFreeRam">--</strong>
                        </div>
                    </div>
                </div>
            </div>
            <div class="tab-panel" id="tabSettings" data-tab-panel="settings">
                <div class="settings-panel">
                    <h3>Backend Connection</h3>
                    <p>The UI talks to the FastAPI server via an HTTP base URL. Enter the URL (including port) below to point the interface at a local or remote backend.</p>
                    <label for="settingsApiRoot">API base URL</label>
                    <input type="text" id="settingsApiRoot" placeholder="http://localhost:8000" />
                    <div class="settings-actions">
                        <button type="button" id="settingsApply" class="training-button">Save</button>
                        <button type="button" id="settingsTest" class="training-button">Test Connection</button>
                    </div>
                    <div id="settingsStatus" class="settings-status" role="status" aria-live="polite"></div>
                    <p class="settings-help">Tip: when tunnelling to a remote GPU box (e.g. <code>ssh -L 8000:127.0.0.1:8000 user@gpu-host</code>), set the URL to <code>http://localhost:8000</code> so the browser routes through the tunnel.</p>
                    <div class="training-divider"></div>
                    <h3>Backend Fuzzer</h3>
                    <p class="settings-help">Runs a quick smoke test with random data against key API endpoints (SAM/Qwen). Useful to sanity-check that the backend is alive and models are loaded. Tests may fail if weights are missing.</p>
                    <div class="training-grid">
                            <label><input type="checkbox" id="fuzzerIncludeQwen" checked /> Include Qwen tests</label>
                            <label><input type="checkbox" id="fuzzerIncludeSam3" /> Include SAM3 tests</label>
                            <label><input type="checkbox" id="fuzzerIncludeClip" /> Include CLIP tests</label>
                            <label><input type="checkbox" id="fuzzerIncludeAgent" /> Include Agent Mining tests</label>
                    </div>
                    <div class="settings-actions">
                        <button type="button" id="runBackendFuzzer" class="training-button">Run fuzzer</button>
                    </div>
                    <div id="backendFuzzerStatus" class="settings-status" role="status" aria-live="polite"></div>
                    <pre id="backendFuzzerLog" class="training-log" style="max-height: 220px; overflow-y: auto;"></pre>
                    <div class="training-divider"></div>
                    <h3>Default upload and payload limits</h3>
                    <p class="settings-help">
                        Defaults are tuned for LAN use. You can override any of these on the backend by setting
                        environment variables (e.g. <code>CLIP_TRAIN_UPLOAD_QUOTA_BYTES</code>,
                        <code>DATASET_ZIP_MAX_BYTES</code>, <code>AGENT_RECIPE_MAX_BYTES</code>,
                        <code>AGENT_MINING_CACHE_MAX_BYTES</code>, <code>MAX_RESPONSE_DETECTIONS</code>,
                        <code>SAM_PRELOAD_MAX_BYTES</code>).
                        If the backend is configured with lower limits, uploads over those caps will be rejected.
                    </p>
                    <ul class="settings-help">
                        <li>Dataset zips (SAM3/Qwen): up to ~100GB zip, ~50GB per entry.</li>
                        <li>CLIP/Qwen chunked uploads: ~10GB per file, ~100GB per job quota.</li>
                        <li>CLIP training uploads: ~10GB per file, ~100GB per job quota.</li>
                        <li>Classifier/labelmap/general assets: ~10GB per file, ~100GB per uploads quota.</li>
                        <li>Agent recipe zips: ~2GB total; crops capped (~1000 pngs / ~512MB total).</li>
                        <li>Responses: max ~5000 detections / 2000 masks; SAM preload cache ~2GB.</li>
                        <li>Agent mining cache: ~80GB default; TTL off by default (set <code>AGENT_MINING_CACHE_TTL_HOURS</code> to enable purge).</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>

    <script>
        document.addEventListener("DOMContentLoaded", function() {
          if (typeof listenImageCrop === "function") {
            listenImageCrop();
          }
        });
        </script>
</body>
</html>
